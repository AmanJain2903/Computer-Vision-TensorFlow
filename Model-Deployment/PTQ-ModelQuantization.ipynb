{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MODEL QUANTIZATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "import onnx\n",
    "import onnxruntime as rt\n",
    "from onnxruntime.quantization import quantize_dynamic, QuantType\n",
    "import os\n",
    "import numpy as np\n",
    "import cv2\n",
    "import time\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 6799 files belonging to 3 classes.\n",
      "Using 5440 files for training.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-02 20:01:10.010587: I metal_plugin/src/device/metal_device.cc:1154] Metal device set to: Apple M3 Pro\n",
      "2025-02-02 20:01:10.010628: I metal_plugin/src/device/metal_device.cc:296] systemMemory: 18.00 GB\n",
      "2025-02-02 20:01:10.010636: I metal_plugin/src/device/metal_device.cc:313] maxCacheSize: 6.00 GB\n",
      "2025-02-02 20:01:10.010826: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:305] Could not identify NUMA node of platform GPU ID 0, defaulting to 0. Your kernel may not have been built with NUMA support.\n",
      "2025-02-02 20:01:10.010842: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:271] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 0 MB memory) -> physical PluggableDevice (device: 0, name: METAL, pci bus id: <undefined>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 6799 files belonging to 3 classes.\n",
      "Using 1359 files for validation.\n",
      "Found 2278 files belonging to 3 classes.\n"
     ]
    }
   ],
   "source": [
    "CONFIGURATION = {\n",
    "    \"CLASS_NAMES\" : ['angry', 'happy', 'sad'],\n",
    "    \"BATCH_SIZE\" : 32,\n",
    "    \"IMAGE_SIZE\" : 224,\n",
    "    \"LEARNING_RATE\" : 0.01,\n",
    "    \"N_EPOCHS\" : 20,\n",
    "    \"DROPOUT_RATE\": 0.0,\n",
    "    \"REGULARIZATION_RATE\" : 0.0,\n",
    "    \"N_FILTERS\" : 6,\n",
    "    \"KERNEL_SIZE\" : 3,\n",
    "    \"N_STRIDES\" : 1,\n",
    "    \"POOL_SIZE\" : 2,\n",
    "    \"N_DENSE_1\" : 128,\n",
    "    \"N_DENSE_2\" : 128,\n",
    "    \"NUM_CLASSES\" : 3,\n",
    "    \"PATCH_SIZE\" : 16,\n",
    "}\n",
    "\n",
    "trainDirectory = \"/Users/aman/Documents/Work/Machine Learning/Computer-Vision-TensorFlow/Human-Emotions-Detection/Dataset/Emotions Dataset/Emotions Dataset/train\"\n",
    "testDirectory = \"/Users/aman/Documents/Work/Machine Learning/Computer-Vision-TensorFlow/Human-Emotions-Detection/Dataset/Emotions Dataset/Emotions Dataset/test\"\n",
    "\n",
    "trainDataset = tf.keras.utils.image_dataset_from_directory(\n",
    "    trainDirectory,\n",
    "    labels='inferred',\n",
    "    label_mode='categorical',\n",
    "    class_names=CONFIGURATION[\"CLASS_NAMES\"],\n",
    "    color_mode='rgb',\n",
    "    batch_size=CONFIGURATION[\"BATCH_SIZE\"],\n",
    "    image_size=(CONFIGURATION[\"IMAGE_SIZE\"], CONFIGURATION[\"IMAGE_SIZE\"]),\n",
    "    shuffle=True,\n",
    "    seed=99,\n",
    "    validation_split=0.2,\n",
    "    subset='training',\n",
    ")\n",
    "\n",
    "valDataset = tf.keras.utils.image_dataset_from_directory(\n",
    "    trainDirectory,\n",
    "    labels='inferred',\n",
    "    label_mode='categorical',\n",
    "    class_names=CONFIGURATION[\"CLASS_NAMES\"],\n",
    "    color_mode='rgb',\n",
    "    batch_size=CONFIGURATION[\"BATCH_SIZE\"],\n",
    "    image_size=(CONFIGURATION[\"IMAGE_SIZE\"], CONFIGURATION[\"IMAGE_SIZE\"]),\n",
    "    shuffle=True,\n",
    "    seed=99,\n",
    "    validation_split=0.2,\n",
    "    subset='validation',\n",
    ")\n",
    "\n",
    "testDataset = tf.keras.utils.image_dataset_from_directory(\n",
    "    testDirectory,\n",
    "    labels='inferred',\n",
    "    label_mode='categorical',\n",
    "    class_names=CONFIGURATION[\"CLASS_NAMES\"],\n",
    "    color_mode='rgb',\n",
    "    batch_size=CONFIGURATION[\"BATCH_SIZE\"],\n",
    "    image_size=(CONFIGURATION[\"IMAGE_SIZE\"], CONFIGURATION[\"IMAGE_SIZE\"]),\n",
    "    shuffle=True,\n",
    "    seed=99,\n",
    "    validation_split=None,\n",
    "    subset=None,\n",
    ")\n",
    "\n",
    "trainDataset = trainDataset.prefetch(tf.data.AUTOTUNE)\n",
    "testDataset = testDataset.prefetch(tf.data.AUTOTUNE)\n",
    "valDataset = valDataset.prefetch(tf.data.AUTOTUNE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Please consider to run pre-processing before quantization. Refer to example: https://github.com/microsoft/onnxruntime-inference-examples/blob/main/quantization/image_classification/cpu/ReadMe.md \n"
     ]
    }
   ],
   "source": [
    "ViTModelKeras = 'Models/EmotionDetectionViT.keras'\n",
    "ViTModelONNX = 'Models/EmotionDetectionViT.onnx'\n",
    "ViTQuantizedModel = 'Models/EmotionDetectionViT_Quantized.onnx'\n",
    "\n",
    "quantizedViTModel = quantize_dynamic(ViTModelONNX, ViTQuantizedModel, weight_type=QuantType.QUInt8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comparing Model's Sizes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Keras Model Size:  330.03 MB\n",
      "ONNX Model Size:  327.59 MB\n",
      "Quantized ONNX Model Size:  83.12 MB\n"
     ]
    }
   ],
   "source": [
    "def convert_size(size_bytes):\n",
    "    for unit in ['B', 'KB', 'MB', 'GB', 'TB']:\n",
    "        if size_bytes < 1024:\n",
    "            return f\"{size_bytes:.2f} {unit}\"\n",
    "        size_bytes /= 1024\n",
    "ViTModelKerasSize = os.path.getsize(ViTModelKeras)\n",
    "ViTModelONNXSize = os.path.getsize(ViTModelONNX)\n",
    "ViTQuantizedModelSize = os.path.getsize(ViTQuantizedModel)\n",
    "print(\"Keras Model Size: \", convert_size(ViTModelKerasSize))\n",
    "print(\"ONNX Model Size: \", convert_size(ViTModelONNXSize))\n",
    "print(\"Quantized ONNX Model Size: \", convert_size(ViTQuantizedModelSize))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "testImage = cv2.imread(\"/Users/aman/Documents/Work/Machine Learning/Computer-Vision-TensorFlow/Human-Emotions-Detection/Dataset/Emotions Dataset/Emotions Dataset/test/happy/2705.jpg_rotation_1.jpg\")\n",
    "testImage = cv2.resize(testImage, (CONFIGURATION[\"IMAGE_SIZE\"], CONFIGURATION[\"IMAGE_SIZE\"]))\n",
    "testImage = testImage.astype(np.float32)\n",
    "testImage = np.expand_dims(testImage, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[array([[7.9681049e-05, 9.9959975e-01, 3.2057121e-04]], dtype=float32)]\n",
      "happy\n"
     ]
    }
   ],
   "source": [
    "providers = ['CPUExecutionProvider']\n",
    "m = rt.InferenceSession(\"Models/EmotionDetectionViT_Quantized.onnx\", providers=providers)\n",
    "\n",
    "onnxPred = m.run(['dense'], {'image': testImage})\n",
    "print(onnxPred)\n",
    "print(CONFIGURATION['CLASS_NAMES'][np.argmax(onnxPred[0])])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comapring Runtimes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ONNX Model Time on CPU:  0.1537320613861084\n",
      "\n",
      "\n",
      "Quantized ONNX Model Time on CPU:  0.12828421592712402\n"
     ]
    }
   ],
   "source": [
    "m = rt.InferenceSession(\"Models/EmotionDetectionViT.onnx\", providers=providers)\n",
    "mQuant = rt.InferenceSession(\"Models/EmotionDetectionViT_Quantized.onnx\", providers=providers)\n",
    "\n",
    "startTime = time.time()\n",
    "m.run(['dense'], {'image': testImage})\n",
    "endTime = time.time()\n",
    "print(\"ONNX Model Time on CPU: \", endTime - startTime)\n",
    "\n",
    "print(\"\\n\")\n",
    "\n",
    "startTime = time.time()\n",
    "mQuant.run(['dense'], {'image': testImage})\n",
    "endTime = time.time()\n",
    "print(\"Quantized ONNX Model Time on CPU: \", endTime - startTime)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comparing Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model):\n",
    "    total, acc = 0, 0\n",
    "    for images, labels in valDataset.take(100):\n",
    "        onnxPred = model.run(['dense'], {'image': images.numpy()})\n",
    "        for i in range(len(onnxPred)):\n",
    "            pred = np.argmax(onnxPred[0][i])\n",
    "            if pred == np.argmax(labels[i]):\n",
    "                acc += 1\n",
    "        total += len(onnxPred)\n",
    "    return acc / total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-02 20:31:44.814010: W tensorflow/core/framework/local_rendezvous.cc:404] Local rendezvous is aborting with status: OUT_OF_RANGE: End of sequence\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Onnx Model Accuracy:  0.8604651162790697\n",
      "Quantized Onnx Model Accuracy:  0.8837209302325582\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-02 20:34:00.973434: W tensorflow/core/framework/local_rendezvous.cc:404] Local rendezvous is aborting with status: OUT_OF_RANGE: End of sequence\n"
     ]
    }
   ],
   "source": [
    "print(\"Onnx Model Accuracy: \", evaluate(m))\n",
    "print(\"Quantized Onnx Model Accuracy: \", evaluate(mQuant))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generally the quantized model has a lower accuracy than the normal model. But, due to some reasons, it may increase the accuracy.\n",
    "### 1. Acts as a regularizer and reduces overfitting.\n",
    "### 2. Reduces noise and acts like a dropout.\n",
    "### 3. PTQ sometimes smoothens the decision boundaries."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ComputerVision",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
