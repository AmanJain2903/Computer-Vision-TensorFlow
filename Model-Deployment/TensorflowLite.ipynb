{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CONVERTING MODEL TO TENSORFLOW LITE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from transformers import TFViTModel\n",
    "import os\n",
    "import time\n",
    "import cv2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 6799 files belonging to 3 classes.\n",
      "Using 5440 files for training.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-03 19:13:37.993556: I metal_plugin/src/device/metal_device.cc:1154] Metal device set to: Apple M3 Pro\n",
      "2025-02-03 19:13:37.993603: I metal_plugin/src/device/metal_device.cc:296] systemMemory: 18.00 GB\n",
      "2025-02-03 19:13:37.993607: I metal_plugin/src/device/metal_device.cc:313] maxCacheSize: 6.00 GB\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "I0000 00:00:1738638817.994063  262425 pluggable_device_factory.cc:305] Could not identify NUMA node of platform GPU ID 0, defaulting to 0. Your kernel may not have been built with NUMA support.\n",
      "I0000 00:00:1738638817.994352  262425 pluggable_device_factory.cc:271] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 0 MB memory) -> physical PluggableDevice (device: 0, name: METAL, pci bus id: <undefined>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 6799 files belonging to 3 classes.\n",
      "Using 1359 files for validation.\n",
      "Found 2278 files belonging to 3 classes.\n"
     ]
    }
   ],
   "source": [
    "CONFIGURATION = {\n",
    "    \"CLASS_NAMES\" : ['angry', 'happy', 'sad'],\n",
    "    \"BATCH_SIZE\" : 32,\n",
    "    \"IMAGE_SIZE\" : 224,\n",
    "    \"LEARNING_RATE\" : 0.01,\n",
    "    \"N_EPOCHS\" : 20,\n",
    "    \"DROPOUT_RATE\": 0.0,\n",
    "    \"REGULARIZATION_RATE\" : 0.0,\n",
    "    \"N_FILTERS\" : 6,\n",
    "    \"KERNEL_SIZE\" : 3,\n",
    "    \"N_STRIDES\" : 1,\n",
    "    \"POOL_SIZE\" : 2,\n",
    "    \"N_DENSE_1\" : 128,\n",
    "    \"N_DENSE_2\" : 128,\n",
    "    \"NUM_CLASSES\" : 3,\n",
    "    \"PATCH_SIZE\" : 16,\n",
    "}\n",
    "\n",
    "trainDirectory = \"/Users/aman/Documents/Work/Machine Learning/Computer-Vision-TensorFlow/Human-Emotions-Detection/Dataset/Emotions Dataset/Emotions Dataset/train\"\n",
    "testDirectory = \"/Users/aman/Documents/Work/Machine Learning/Computer-Vision-TensorFlow/Human-Emotions-Detection/Dataset/Emotions Dataset/Emotions Dataset/test\"\n",
    "\n",
    "trainDataset = tf.keras.utils.image_dataset_from_directory(\n",
    "    trainDirectory,\n",
    "    labels='inferred',\n",
    "    label_mode='categorical',\n",
    "    class_names=CONFIGURATION[\"CLASS_NAMES\"],\n",
    "    color_mode='rgb',\n",
    "    batch_size=CONFIGURATION[\"BATCH_SIZE\"],\n",
    "    image_size=(CONFIGURATION[\"IMAGE_SIZE\"], CONFIGURATION[\"IMAGE_SIZE\"]),\n",
    "    shuffle=True,\n",
    "    seed=99,\n",
    "    validation_split=0.2,\n",
    "    subset='training',\n",
    ")\n",
    "\n",
    "valDataset = tf.keras.utils.image_dataset_from_directory(\n",
    "    trainDirectory,\n",
    "    labels='inferred',\n",
    "    label_mode='categorical',\n",
    "    class_names=CONFIGURATION[\"CLASS_NAMES\"],\n",
    "    color_mode='rgb',\n",
    "    batch_size=CONFIGURATION[\"BATCH_SIZE\"],\n",
    "    image_size=(CONFIGURATION[\"IMAGE_SIZE\"], CONFIGURATION[\"IMAGE_SIZE\"]),\n",
    "    shuffle=True,\n",
    "    seed=99,\n",
    "    validation_split=0.2,\n",
    "    subset='validation',\n",
    ")\n",
    "\n",
    "testDataset = tf.keras.utils.image_dataset_from_directory(\n",
    "    testDirectory,\n",
    "    labels='inferred',\n",
    "    label_mode='categorical',\n",
    "    class_names=CONFIGURATION[\"CLASS_NAMES\"],\n",
    "    color_mode='rgb',\n",
    "    batch_size=CONFIGURATION[\"BATCH_SIZE\"],\n",
    "    image_size=(CONFIGURATION[\"IMAGE_SIZE\"], CONFIGURATION[\"IMAGE_SIZE\"]),\n",
    "    shuffle=True,\n",
    "    seed=99,\n",
    "    validation_split=None,\n",
    "    subset=None,\n",
    ")\n",
    "\n",
    "trainDataset = trainDataset.prefetch(tf.data.AUTOTUNE)\n",
    "testDataset = testDataset.prefetch(tf.data.AUTOTUNE)\n",
    "valDataset = valDataset.prefetch(tf.data.AUTOTUNE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-03 19:14:16.910943: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:117] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "72/72 [==============================] - 42s 527ms/step - loss: 0.5523 - accuracy: 0.8749 - top_k_accuracy: 0.9680\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.5522751212120056, 0.8748902678489685, 0.9679543375968933]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ViTModelKeras = 'Models/EmotionDetectionViT.keras'\n",
    "kerasModel = tf.keras.models.load_model(ViTModelKeras)\n",
    "kerasModel.evaluate(testDataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Converting to Tensorflow Lite"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "def representativeDatagen():\n",
    "    for input_value, _ in trainDataset:\n",
    "        yield [input_value]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "converter = tf.lite.TFLiteConverter.from_keras_model(kerasModel)\n",
    "converter.optimizations = [tf.lite.Optimize.DEFAULT] # Used to quantize the model\n",
    "converter.inference_input_type = tf.uint8 # Used to convert the model to uint8\n",
    "converter.inference_output_type = tf.uint8 # Used to convert the model to uint8\n",
    "converter.representative_dataset = representativeDatagen # Used to calculate scale and zero point for static quantization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /var/folders/8v/j561wc497p1672n3xvshbbd00000gn/T/tmp5mvgkyfl/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /var/folders/8v/j561wc497p1672n3xvshbbd00000gn/T/tmp5mvgkyfl/assets\n",
      "/opt/homebrew/Caskroom/miniforge/base/envs/HuggingFaceTransformers/lib/python3.11/site-packages/tensorflow/lite/python/convert.py:997: UserWarning: Statistics for quantized inputs were expected, but not specified; continuing anyway.\n",
      "  warnings.warn(\n",
      "W0000 00:00:1738642481.828844  262425 tf_tfl_flatbuffer_helpers.cc:365] Ignored output_format.\n",
      "W0000 00:00:1738642481.829175  262425 tf_tfl_flatbuffer_helpers.cc:368] Ignored drop_control_dependency.\n",
      "2025-02-03 20:14:41.829734: I tensorflow/cc/saved_model/reader.cc:83] Reading SavedModel from: /var/folders/8v/j561wc497p1672n3xvshbbd00000gn/T/tmp5mvgkyfl\n",
      "2025-02-03 20:14:41.842618: I tensorflow/cc/saved_model/reader.cc:52] Reading meta graph with tags { serve }\n",
      "2025-02-03 20:14:41.842628: I tensorflow/cc/saved_model/reader.cc:147] Reading SavedModel debug info (if present) from: /var/folders/8v/j561wc497p1672n3xvshbbd00000gn/T/tmp5mvgkyfl\n",
      "2025-02-03 20:14:41.936057: I tensorflow/cc/saved_model/loader.cc:236] Restoring SavedModel bundle.\n",
      "2025-02-03 20:14:42.264133: I tensorflow/cc/saved_model/loader.cc:220] Running initialization op on SavedModel bundle at path: /var/folders/8v/j561wc497p1672n3xvshbbd00000gn/T/tmp5mvgkyfl\n",
      "2025-02-03 20:14:42.377867: I tensorflow/cc/saved_model/loader.cc:466] SavedModel load for tags { serve }; Status: success: OK. Took 548133 microseconds.\n",
      "fully_quantize: 0, inference_type: 6, input_inference_type: UINT8, output_inference_type: UINT8\n"
     ]
    }
   ],
   "source": [
    "tfLiteModelPath = \"Models/EmotionDetectionViT.tflite\"\n",
    "tfLiteModel = converter.convert()\n",
    "with open(tfLiteModelPath, 'wb') as f:\n",
    "    f.write(tfLiteModel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TensorFlow Model Size:  330.03 MB\n",
      "TensorFlow Lite Model Size:  83.04 MB\n"
     ]
    }
   ],
   "source": [
    "def convert_size(size_bytes):\n",
    "    for unit in ['B', 'KB', 'MB', 'GB', 'TB']:\n",
    "        if size_bytes < 1024:\n",
    "            return f\"{size_bytes:.2f} {unit}\"\n",
    "        size_bytes /= 1024\n",
    "print(\"TensorFlow Model Size: \", convert_size(os.path.getsize(ViTModelKeras)))\n",
    "print(\"TensorFlow Lite Model Size: \", convert_size(os.path.getsize(tfLiteModelPath)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tensorflow Lite Runtime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: ai-edge-litert in /opt/homebrew/Caskroom/miniforge/base/envs/HuggingFaceTransformers/lib/python3.11/site-packages (1.0.1)\n",
      "Requirement already satisfied: numpy>=1.23.2 in /opt/homebrew/Caskroom/miniforge/base/envs/HuggingFaceTransformers/lib/python3.11/site-packages (from ai-edge-litert) (1.26.4)\n"
     ]
    }
   ],
   "source": [
    "! python3 -m pip install ai-edge-litert"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ai_edge_litert.interpreter import Interpreter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "testImage = cv2.imread(\"/Users/aman/Documents/Work/Machine Learning/Computer-Vision-TensorFlow/Human-Emotions-Detection/Dataset/Emotions Dataset/Emotions Dataset/test/happy/13736.jpg\")\n",
    "testImage = cv2.resize(testImage, (CONFIGURATION[\"IMAGE_SIZE\"], CONFIGURATION[\"IMAGE_SIZE\"]))\n",
    "testImage = np.expand_dims(testImage, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "angry\n"
     ]
    }
   ],
   "source": [
    "interpreter = Interpreter(model_path=tfLiteModelPath)\n",
    "interpreter.allocate_tensors()\n",
    "\n",
    "input_details = interpreter.get_input_details()[0]\n",
    "output_details = interpreter.get_output_details()[0]\n",
    "\n",
    "interpreter.set_tensor(input_details['index'], testImage)\n",
    "interpreter.invoke()\n",
    "\n",
    "output_data = interpreter.get_tensor(output_details['index'])[0]\n",
    "print(CONFIGURATION[\"CLASS_NAMES\"][np.argmax(output_data)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(modelPath, dataset):\n",
    "    total, acc = 0, 0\n",
    "\n",
    "    interpreter = Interpreter(model_path=modelPath)\n",
    "    interpreter.allocate_tensors()\n",
    "    input_details = interpreter.get_input_details()[0]\n",
    "    output_details = interpreter.get_output_details()[0]\n",
    "\n",
    "    for images, labels in dataset.take(1):\n",
    "        for i in range(len(images)):\n",
    "            image = np.expand_dims(images[i], axis=0).astype(input_details['dtype'])\n",
    "            interpreter.set_tensor(input_details['index'], image)\n",
    "            interpreter.invoke()\n",
    "            output_data = interpreter.get_tensor(output_details['index'])\n",
    "            if np.argmax(output_data) == np.argmax(labels[i]):\n",
    "                acc += 1\n",
    "            total += 1\n",
    "    return acc / total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy TF Lite Model:  0.28550404709345106\n"
     ]
    }
   ],
   "source": [
    "accuracy = evaluate(tfLiteModelPath, valDataset)\n",
    "print(\"Accuracy TF Lite Model: \", accuracy)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "HuggingFaceTransformers",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
