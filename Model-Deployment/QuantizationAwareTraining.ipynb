{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# QUANTIZATION AWARE TRAINING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ! pip install tensorflow-model-optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow_model_optimization as tfmot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 6799 files belonging to 3 classes.\n",
      "Using 5440 files for training.\n",
      "Found 6799 files belonging to 3 classes.\n",
      "Using 1359 files for validation.\n",
      "Found 2278 files belonging to 3 classes.\n"
     ]
    }
   ],
   "source": [
    "CONFIGURATION = {\n",
    "    \"CLASS_NAMES\" : ['angry', 'happy', 'sad'],\n",
    "    \"BATCH_SIZE\" : 32,\n",
    "    \"IMAGE_SIZE\" : 256,\n",
    "    \"LEARNING_RATE\" : 0.01,\n",
    "    \"N_EPOCHS\" : 20,\n",
    "    \"DROPOUT_RATE\": 0.0,\n",
    "    \"REGULARIZATION_RATE\" : 0.0,\n",
    "    \"N_FILTERS\" : 6,\n",
    "    \"KERNEL_SIZE\" : 3,\n",
    "    \"N_STRIDES\" : 1,\n",
    "    \"POOL_SIZE\" : 2,\n",
    "    \"N_DENSE_1\" : 1024,\n",
    "    \"N_DENSE_2\" : 128,\n",
    "    \"NUM_CLASSES\" : 3,\n",
    "    \"PATCH_SIZE\" : 16,\n",
    "}\n",
    "\n",
    "trainDirectory = \"/Users/aman/Documents/Work/Machine Learning/Computer-Vision-TensorFlow/Human-Emotions-Detection/Dataset/Emotions Dataset/Emotions Dataset/train\"\n",
    "testDirectory = \"/Users/aman/Documents/Work/Machine Learning/Computer-Vision-TensorFlow/Human-Emotions-Detection/Dataset/Emotions Dataset/Emotions Dataset/test\"\n",
    "\n",
    "trainDataset = tf.keras.utils.image_dataset_from_directory(\n",
    "    trainDirectory,\n",
    "    labels='inferred',\n",
    "    label_mode='categorical',\n",
    "    class_names=CONFIGURATION[\"CLASS_NAMES\"],\n",
    "    color_mode='rgb',\n",
    "    batch_size=CONFIGURATION[\"BATCH_SIZE\"],\n",
    "    image_size=(CONFIGURATION[\"IMAGE_SIZE\"], CONFIGURATION[\"IMAGE_SIZE\"]),\n",
    "    shuffle=True,\n",
    "    seed=99,\n",
    "    validation_split=0.2,\n",
    "    subset='training',\n",
    ")\n",
    "\n",
    "valDataset = tf.keras.utils.image_dataset_from_directory(\n",
    "    trainDirectory,\n",
    "    labels='inferred',\n",
    "    label_mode='categorical',\n",
    "    class_names=CONFIGURATION[\"CLASS_NAMES\"],\n",
    "    color_mode='rgb',\n",
    "    batch_size=CONFIGURATION[\"BATCH_SIZE\"],\n",
    "    image_size=(CONFIGURATION[\"IMAGE_SIZE\"], CONFIGURATION[\"IMAGE_SIZE\"]),\n",
    "    shuffle=True,\n",
    "    seed=99,\n",
    "    validation_split=0.2,\n",
    "    subset='validation',\n",
    ")\n",
    "\n",
    "testDataset = tf.keras.utils.image_dataset_from_directory(\n",
    "    testDirectory,\n",
    "    labels='inferred',\n",
    "    label_mode='categorical',\n",
    "    class_names=CONFIGURATION[\"CLASS_NAMES\"],\n",
    "    color_mode='rgb',\n",
    "    batch_size=CONFIGURATION[\"BATCH_SIZE\"],\n",
    "    image_size=(CONFIGURATION[\"IMAGE_SIZE\"], CONFIGURATION[\"IMAGE_SIZE\"]),\n",
    "    shuffle=True,\n",
    "    seed=99,\n",
    "    validation_split=None,\n",
    "    subset=None,\n",
    ")\n",
    "\n",
    "trainDataset = trainDataset.prefetch(tf.data.AUTOTUNE)\n",
    "testDataset = testDataset.prefetch(tf.data.AUTOTUNE)\n",
    "valDataset = valDataset.prefetch(tf.data.AUTOTUNE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_4\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " conv2d_8 (Conv2D)           (None, 254, 254, 6)       168       \n",
      "                                                                 \n",
      " max_pooling2d_8 (MaxPoolin  (None, 253, 253, 6)       0         \n",
      " g2D)                                                            \n",
      "                                                                 \n",
      " conv2d_9 (Conv2D)           (None, 251, 251, 6)       330       \n",
      "                                                                 \n",
      " max_pooling2d_9 (MaxPoolin  (None, 250, 250, 6)       0         \n",
      " g2D)                                                            \n",
      "                                                                 \n",
      " flatten_4 (Flatten)         (None, 375000)            0         \n",
      "                                                                 \n",
      " dense_18 (Dense)            (None, 1024)              384001024 \n",
      "                                                                 \n",
      " dense_19 (Dense)            (None, 128)               131200    \n",
      "                                                                 \n",
      " dense_20 (Dense)            (None, 3)                 387       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 384133109 (1.43 GB)\n",
      "Trainable params: 384133109 (1.43 GB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Input(shape=(CONFIGURATION['IMAGE_SIZE'], CONFIGURATION['IMAGE_SIZE'], 3)),\n",
    "\n",
    "    tf.keras.layers.Conv2D(filters=CONFIGURATION[\"N_FILTERS\"], kernel_size=CONFIGURATION[\"KERNEL_SIZE\"], strides=CONFIGURATION[\"N_STRIDES\"], padding='valid', activation='relu'),\n",
    "    tf.keras.layers.MaxPooling2D(pool_size=CONFIGURATION[\"POOL_SIZE\"], strides=CONFIGURATION[\"N_STRIDES\"]),\n",
    "\n",
    "    tf.keras.layers.Conv2D(filters=CONFIGURATION[\"N_FILTERS\"], kernel_size=CONFIGURATION[\"KERNEL_SIZE\"], strides=CONFIGURATION[\"N_STRIDES\"], padding='valid', activation='relu'),\n",
    "    tf.keras.layers.MaxPooling2D(pool_size=CONFIGURATION[\"POOL_SIZE\"], strides=CONFIGURATION[\"N_STRIDES\"]),\n",
    "\n",
    "    tf.keras.layers.Flatten(),\n",
    "\n",
    "    tf.keras.layers.Dense(CONFIGURATION[\"N_DENSE_1\"], activation='relu'),\n",
    "    tf.keras.layers.Dense(CONFIGURATION[\"N_DENSE_2\"], activation='relu'),\n",
    "    tf.keras.layers.Dense(CONFIGURATION[\"NUM_CLASSES\"], activation='softmax'),\n",
    "])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Making The Full Model Quant Aware"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "quantAwareModel = tfmot.quantization.keras.quantize_model(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Making The Model Quant Aware Layer-by-Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading Pre-Trained Efficient Net B4 Model trained on ImageNet Dataset\n",
    "backbone = tf.keras.applications.EfficientNetB4(\n",
    "    include_top = False, # Include Classifier or Not\n",
    "    weights = 'imagenet',\n",
    "    input_shape = (CONFIGURATION[\"IMAGE_SIZE\"], CONFIGURATION[\"IMAGE_SIZE\"], 3)\n",
    ")\n",
    "\n",
    "\n",
    "x = tf.keras.layers.GlobalAveragePooling2D()(backbone.output)\n",
    "x = tf.keras.layers.Dense(CONFIGURATION['N_DENSE_1'], activation='relu')(x)\n",
    "x = tf.keras.layers.BatchNormalization()(x)\n",
    "x = tf.keras.layers.Dense(CONFIGURATION['N_DENSE_2'], activation='relu')(x)\n",
    "output = tf.keras.layers.Dense(CONFIGURATION['NUM_CLASSES'], activation='softmax')(x)\n",
    " \n",
    "EfficientNetModel = tf.keras.models.Model(backbone.input, output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Annotating The Layers To Quantize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_3\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                Output Shape                 Param #   Connected to                  \n",
      "==================================================================================================\n",
      " input_9 (InputLayer)        [(None, 256, 256, 3)]        0         []                            \n",
      "                                                                                                  \n",
      " rescaling_3 (Rescaling)     (None, 256, 256, 3)          0         ['input_9[0][0]']             \n",
      "                                                                                                  \n",
      " normalization_3 (Normaliza  (None, 256, 256, 3)          7         ['rescaling_3[2][0]']         \n",
      " tion)                                                                                            \n",
      "                                                                                                  \n",
      " tf.math.multiply_3 (TFOpLa  (None, 256, 256, 3)          0         ['normalization_3[2][0]']     \n",
      " mbda)                                                                                            \n",
      "                                                                                                  \n",
      " quantize_annotate_705 (Qua  (None, 257, 257, 3)          0         ['tf.math.multiply_3[2][0]']  \n",
      " ntizeAnnotate)                                                                                   \n",
      "                                                                                                  \n",
      " quantize_annotate_706 (Qua  (None, 128, 128, 48)         1296      ['quantize_annotate_705[0][0]'\n",
      " ntizeAnnotate)                                                     ]                             \n",
      "                                                                                                  \n",
      " stem_bn (BatchNormalizatio  (None, 128, 128, 48)         192       ['quantize_annotate_706[0][0]'\n",
      " n)                                                                 ]                             \n",
      "                                                                                                  \n",
      " stem_activation (Activatio  (None, 128, 128, 48)         0         ['stem_bn[2][0]']             \n",
      " n)                                                                                               \n",
      "                                                                                                  \n",
      " quantize_annotate_707 (Qua  (None, 128, 128, 48)         432       ['stem_activation[2][0]']     \n",
      " ntizeAnnotate)                                                                                   \n",
      "                                                                                                  \n",
      " block1a_bn (BatchNormaliza  (None, 128, 128, 48)         192       ['quantize_annotate_707[0][0]'\n",
      " tion)                                                              ]                             \n",
      "                                                                                                  \n",
      " block1a_activation (Activa  (None, 128, 128, 48)         0         ['block1a_bn[2][0]']          \n",
      " tion)                                                                                            \n",
      "                                                                                                  \n",
      " block1a_se_squeeze (Global  (None, 48)                   0         ['block1a_activation[2][0]']  \n",
      " AveragePooling2D)                                                                                \n",
      "                                                                                                  \n",
      " block1a_se_reshape (Reshap  (None, 1, 1, 48)             0         ['block1a_se_squeeze[2][0]']  \n",
      " e)                                                                                               \n",
      "                                                                                                  \n",
      " block1a_se_reduce (Conv2D)  (None, 1, 1, 12)             588       ['block1a_se_reshape[2][0]']  \n",
      "                                                                                                  \n",
      " block1a_se_expand (Conv2D)  (None, 1, 1, 48)             624       ['block1a_se_reduce[2][0]']   \n",
      "                                                                                                  \n",
      " block1a_se_excite (Multipl  (None, 128, 128, 48)         0         ['block1a_activation[2][0]',  \n",
      " y)                                                                  'block1a_se_expand[2][0]']   \n",
      "                                                                                                  \n",
      " quantize_annotate_708 (Qua  (None, 128, 128, 24)         1152      ['block1a_se_excite[2][0]']   \n",
      " ntizeAnnotate)                                                                                   \n",
      "                                                                                                  \n",
      " block1a_project_bn (BatchN  (None, 128, 128, 24)         96        ['quantize_annotate_708[0][0]'\n",
      " ormalization)                                                      ]                             \n",
      "                                                                                                  \n",
      " quantize_annotate_709 (Qua  (None, 128, 128, 24)         216       ['block1a_project_bn[2][0]']  \n",
      " ntizeAnnotate)                                                                                   \n",
      "                                                                                                  \n",
      " block1b_bn (BatchNormaliza  (None, 128, 128, 24)         96        ['quantize_annotate_709[0][0]'\n",
      " tion)                                                              ]                             \n",
      "                                                                                                  \n",
      " block1b_activation (Activa  (None, 128, 128, 24)         0         ['block1b_bn[2][0]']          \n",
      " tion)                                                                                            \n",
      "                                                                                                  \n",
      " block1b_se_squeeze (Global  (None, 24)                   0         ['block1b_activation[2][0]']  \n",
      " AveragePooling2D)                                                                                \n",
      "                                                                                                  \n",
      " block1b_se_reshape (Reshap  (None, 1, 1, 24)             0         ['block1b_se_squeeze[2][0]']  \n",
      " e)                                                                                               \n",
      "                                                                                                  \n",
      " block1b_se_reduce (Conv2D)  (None, 1, 1, 6)              150       ['block1b_se_reshape[2][0]']  \n",
      "                                                                                                  \n",
      " block1b_se_expand (Conv2D)  (None, 1, 1, 24)             168       ['block1b_se_reduce[2][0]']   \n",
      "                                                                                                  \n",
      " block1b_se_excite (Multipl  (None, 128, 128, 24)         0         ['block1b_activation[2][0]',  \n",
      " y)                                                                  'block1b_se_expand[2][0]']   \n",
      "                                                                                                  \n",
      " quantize_annotate_710 (Qua  (None, 128, 128, 24)         576       ['block1b_se_excite[2][0]']   \n",
      " ntizeAnnotate)                                                                                   \n",
      "                                                                                                  \n",
      " block1b_project_bn (BatchN  (None, 128, 128, 24)         96        ['quantize_annotate_710[0][0]'\n",
      " ormalization)                                                      ]                             \n",
      "                                                                                                  \n",
      " block1b_drop (Dropout)      (None, 128, 128, 24)         0         ['block1b_project_bn[2][0]']  \n",
      "                                                                                                  \n",
      " block1b_add (Add)           (None, 128, 128, 24)         0         ['block1b_drop[2][0]',        \n",
      "                                                                     'block1a_project_bn[2][0]']  \n",
      "                                                                                                  \n",
      " quantize_annotate_711 (Qua  (None, 128, 128, 144)        3456      ['block1b_add[2][0]']         \n",
      " ntizeAnnotate)                                                                                   \n",
      "                                                                                                  \n",
      " block2a_expand_bn (BatchNo  (None, 128, 128, 144)        576       ['quantize_annotate_711[0][0]'\n",
      " rmalization)                                                       ]                             \n",
      "                                                                                                  \n",
      " block2a_expand_activation   (None, 128, 128, 144)        0         ['block2a_expand_bn[2][0]']   \n",
      " (Activation)                                                                                     \n",
      "                                                                                                  \n",
      " quantize_annotate_712 (Qua  (None, 129, 129, 144)        0         ['block2a_expand_activation[2]\n",
      " ntizeAnnotate)                                                     [0]']                         \n",
      "                                                                                                  \n",
      " quantize_annotate_713 (Qua  (None, 64, 64, 144)          1296      ['quantize_annotate_712[0][0]'\n",
      " ntizeAnnotate)                                                     ]                             \n",
      "                                                                                                  \n",
      " block2a_bn (BatchNormaliza  (None, 64, 64, 144)          576       ['quantize_annotate_713[0][0]'\n",
      " tion)                                                              ]                             \n",
      "                                                                                                  \n",
      " block2a_activation (Activa  (None, 64, 64, 144)          0         ['block2a_bn[2][0]']          \n",
      " tion)                                                                                            \n",
      "                                                                                                  \n",
      " block2a_se_squeeze (Global  (None, 144)                  0         ['block2a_activation[2][0]']  \n",
      " AveragePooling2D)                                                                                \n",
      "                                                                                                  \n",
      " block2a_se_reshape (Reshap  (None, 1, 1, 144)            0         ['block2a_se_squeeze[2][0]']  \n",
      " e)                                                                                               \n",
      "                                                                                                  \n",
      " block2a_se_reduce (Conv2D)  (None, 1, 1, 6)              870       ['block2a_se_reshape[2][0]']  \n",
      "                                                                                                  \n",
      " block2a_se_expand (Conv2D)  (None, 1, 1, 144)            1008      ['block2a_se_reduce[2][0]']   \n",
      "                                                                                                  \n",
      " block2a_se_excite (Multipl  (None, 64, 64, 144)          0         ['block2a_activation[2][0]',  \n",
      " y)                                                                  'block2a_se_expand[2][0]']   \n",
      "                                                                                                  \n",
      " quantize_annotate_714 (Qua  (None, 64, 64, 32)           4608      ['block2a_se_excite[2][0]']   \n",
      " ntizeAnnotate)                                                                                   \n",
      "                                                                                                  \n",
      " block2a_project_bn (BatchN  (None, 64, 64, 32)           128       ['quantize_annotate_714[0][0]'\n",
      " ormalization)                                                      ]                             \n",
      "                                                                                                  \n",
      " quantize_annotate_715 (Qua  (None, 64, 64, 192)          6144      ['block2a_project_bn[2][0]']  \n",
      " ntizeAnnotate)                                                                                   \n",
      "                                                                                                  \n",
      " block2b_expand_bn (BatchNo  (None, 64, 64, 192)          768       ['quantize_annotate_715[0][0]'\n",
      " rmalization)                                                       ]                             \n",
      "                                                                                                  \n",
      " block2b_expand_activation   (None, 64, 64, 192)          0         ['block2b_expand_bn[2][0]']   \n",
      " (Activation)                                                                                     \n",
      "                                                                                                  \n",
      " quantize_annotate_716 (Qua  (None, 64, 64, 192)          1728      ['block2b_expand_activation[2]\n",
      " ntizeAnnotate)                                                     [0]']                         \n",
      "                                                                                                  \n",
      " block2b_bn (BatchNormaliza  (None, 64, 64, 192)          768       ['quantize_annotate_716[0][0]'\n",
      " tion)                                                              ]                             \n",
      "                                                                                                  \n",
      " block2b_activation (Activa  (None, 64, 64, 192)          0         ['block2b_bn[2][0]']          \n",
      " tion)                                                                                            \n",
      "                                                                                                  \n",
      " block2b_se_squeeze (Global  (None, 192)                  0         ['block2b_activation[2][0]']  \n",
      " AveragePooling2D)                                                                                \n",
      "                                                                                                  \n",
      " block2b_se_reshape (Reshap  (None, 1, 1, 192)            0         ['block2b_se_squeeze[2][0]']  \n",
      " e)                                                                                               \n",
      "                                                                                                  \n",
      " block2b_se_reduce (Conv2D)  (None, 1, 1, 8)              1544      ['block2b_se_reshape[2][0]']  \n",
      "                                                                                                  \n",
      " block2b_se_expand (Conv2D)  (None, 1, 1, 192)            1728      ['block2b_se_reduce[2][0]']   \n",
      "                                                                                                  \n",
      " block2b_se_excite (Multipl  (None, 64, 64, 192)          0         ['block2b_activation[2][0]',  \n",
      " y)                                                                  'block2b_se_expand[2][0]']   \n",
      "                                                                                                  \n",
      " quantize_annotate_717 (Qua  (None, 64, 64, 32)           6144      ['block2b_se_excite[2][0]']   \n",
      " ntizeAnnotate)                                                                                   \n",
      "                                                                                                  \n",
      " block2b_project_bn (BatchN  (None, 64, 64, 32)           128       ['quantize_annotate_717[0][0]'\n",
      " ormalization)                                                      ]                             \n",
      "                                                                                                  \n",
      " block2b_drop (Dropout)      (None, 64, 64, 32)           0         ['block2b_project_bn[2][0]']  \n",
      "                                                                                                  \n",
      " block2b_add (Add)           (None, 64, 64, 32)           0         ['block2b_drop[2][0]',        \n",
      "                                                                     'block2a_project_bn[2][0]']  \n",
      "                                                                                                  \n",
      " quantize_annotate_718 (Qua  (None, 64, 64, 192)          6144      ['block2b_add[2][0]']         \n",
      " ntizeAnnotate)                                                                                   \n",
      "                                                                                                  \n",
      " block2c_expand_bn (BatchNo  (None, 64, 64, 192)          768       ['quantize_annotate_718[0][0]'\n",
      " rmalization)                                                       ]                             \n",
      "                                                                                                  \n",
      " block2c_expand_activation   (None, 64, 64, 192)          0         ['block2c_expand_bn[2][0]']   \n",
      " (Activation)                                                                                     \n",
      "                                                                                                  \n",
      " quantize_annotate_719 (Qua  (None, 64, 64, 192)          1728      ['block2c_expand_activation[2]\n",
      " ntizeAnnotate)                                                     [0]']                         \n",
      "                                                                                                  \n",
      " block2c_bn (BatchNormaliza  (None, 64, 64, 192)          768       ['quantize_annotate_719[0][0]'\n",
      " tion)                                                              ]                             \n",
      "                                                                                                  \n",
      " block2c_activation (Activa  (None, 64, 64, 192)          0         ['block2c_bn[2][0]']          \n",
      " tion)                                                                                            \n",
      "                                                                                                  \n",
      " block2c_se_squeeze (Global  (None, 192)                  0         ['block2c_activation[2][0]']  \n",
      " AveragePooling2D)                                                                                \n",
      "                                                                                                  \n",
      " block2c_se_reshape (Reshap  (None, 1, 1, 192)            0         ['block2c_se_squeeze[2][0]']  \n",
      " e)                                                                                               \n",
      "                                                                                                  \n",
      " block2c_se_reduce (Conv2D)  (None, 1, 1, 8)              1544      ['block2c_se_reshape[2][0]']  \n",
      "                                                                                                  \n",
      " block2c_se_expand (Conv2D)  (None, 1, 1, 192)            1728      ['block2c_se_reduce[2][0]']   \n",
      "                                                                                                  \n",
      " block2c_se_excite (Multipl  (None, 64, 64, 192)          0         ['block2c_activation[2][0]',  \n",
      " y)                                                                  'block2c_se_expand[2][0]']   \n",
      "                                                                                                  \n",
      " quantize_annotate_720 (Qua  (None, 64, 64, 32)           6144      ['block2c_se_excite[2][0]']   \n",
      " ntizeAnnotate)                                                                                   \n",
      "                                                                                                  \n",
      " block2c_project_bn (BatchN  (None, 64, 64, 32)           128       ['quantize_annotate_720[0][0]'\n",
      " ormalization)                                                      ]                             \n",
      "                                                                                                  \n",
      " block2c_drop (Dropout)      (None, 64, 64, 32)           0         ['block2c_project_bn[2][0]']  \n",
      "                                                                                                  \n",
      " block2c_add (Add)           (None, 64, 64, 32)           0         ['block2c_drop[2][0]',        \n",
      "                                                                     'block2b_add[2][0]']         \n",
      "                                                                                                  \n",
      " quantize_annotate_721 (Qua  (None, 64, 64, 192)          6144      ['block2c_add[2][0]']         \n",
      " ntizeAnnotate)                                                                                   \n",
      "                                                                                                  \n",
      " block2d_expand_bn (BatchNo  (None, 64, 64, 192)          768       ['quantize_annotate_721[0][0]'\n",
      " rmalization)                                                       ]                             \n",
      "                                                                                                  \n",
      " block2d_expand_activation   (None, 64, 64, 192)          0         ['block2d_expand_bn[2][0]']   \n",
      " (Activation)                                                                                     \n",
      "                                                                                                  \n",
      " quantize_annotate_722 (Qua  (None, 64, 64, 192)          1728      ['block2d_expand_activation[2]\n",
      " ntizeAnnotate)                                                     [0]']                         \n",
      "                                                                                                  \n",
      " block2d_bn (BatchNormaliza  (None, 64, 64, 192)          768       ['quantize_annotate_722[0][0]'\n",
      " tion)                                                              ]                             \n",
      "                                                                                                  \n",
      " block2d_activation (Activa  (None, 64, 64, 192)          0         ['block2d_bn[2][0]']          \n",
      " tion)                                                                                            \n",
      "                                                                                                  \n",
      " block2d_se_squeeze (Global  (None, 192)                  0         ['block2d_activation[2][0]']  \n",
      " AveragePooling2D)                                                                                \n",
      "                                                                                                  \n",
      " block2d_se_reshape (Reshap  (None, 1, 1, 192)            0         ['block2d_se_squeeze[2][0]']  \n",
      " e)                                                                                               \n",
      "                                                                                                  \n",
      " block2d_se_reduce (Conv2D)  (None, 1, 1, 8)              1544      ['block2d_se_reshape[2][0]']  \n",
      "                                                                                                  \n",
      " block2d_se_expand (Conv2D)  (None, 1, 1, 192)            1728      ['block2d_se_reduce[2][0]']   \n",
      "                                                                                                  \n",
      " block2d_se_excite (Multipl  (None, 64, 64, 192)          0         ['block2d_activation[2][0]',  \n",
      " y)                                                                  'block2d_se_expand[2][0]']   \n",
      "                                                                                                  \n",
      " quantize_annotate_723 (Qua  (None, 64, 64, 32)           6144      ['block2d_se_excite[2][0]']   \n",
      " ntizeAnnotate)                                                                                   \n",
      "                                                                                                  \n",
      " block2d_project_bn (BatchN  (None, 64, 64, 32)           128       ['quantize_annotate_723[0][0]'\n",
      " ormalization)                                                      ]                             \n",
      "                                                                                                  \n",
      " block2d_drop (Dropout)      (None, 64, 64, 32)           0         ['block2d_project_bn[2][0]']  \n",
      "                                                                                                  \n",
      " block2d_add (Add)           (None, 64, 64, 32)           0         ['block2d_drop[2][0]',        \n",
      "                                                                     'block2c_add[2][0]']         \n",
      "                                                                                                  \n",
      " quantize_annotate_724 (Qua  (None, 64, 64, 192)          6144      ['block2d_add[2][0]']         \n",
      " ntizeAnnotate)                                                                                   \n",
      "                                                                                                  \n",
      " block3a_expand_bn (BatchNo  (None, 64, 64, 192)          768       ['quantize_annotate_724[0][0]'\n",
      " rmalization)                                                       ]                             \n",
      "                                                                                                  \n",
      " block3a_expand_activation   (None, 64, 64, 192)          0         ['block3a_expand_bn[2][0]']   \n",
      " (Activation)                                                                                     \n",
      "                                                                                                  \n",
      " quantize_annotate_725 (Qua  (None, 67, 67, 192)          0         ['block3a_expand_activation[2]\n",
      " ntizeAnnotate)                                                     [0]']                         \n",
      "                                                                                                  \n",
      " quantize_annotate_726 (Qua  (None, 32, 32, 192)          4800      ['quantize_annotate_725[0][0]'\n",
      " ntizeAnnotate)                                                     ]                             \n",
      "                                                                                                  \n",
      " block3a_bn (BatchNormaliza  (None, 32, 32, 192)          768       ['quantize_annotate_726[0][0]'\n",
      " tion)                                                              ]                             \n",
      "                                                                                                  \n",
      " block3a_activation (Activa  (None, 32, 32, 192)          0         ['block3a_bn[2][0]']          \n",
      " tion)                                                                                            \n",
      "                                                                                                  \n",
      " block3a_se_squeeze (Global  (None, 192)                  0         ['block3a_activation[2][0]']  \n",
      " AveragePooling2D)                                                                                \n",
      "                                                                                                  \n",
      " block3a_se_reshape (Reshap  (None, 1, 1, 192)            0         ['block3a_se_squeeze[2][0]']  \n",
      " e)                                                                                               \n",
      "                                                                                                  \n",
      " block3a_se_reduce (Conv2D)  (None, 1, 1, 8)              1544      ['block3a_se_reshape[2][0]']  \n",
      "                                                                                                  \n",
      " block3a_se_expand (Conv2D)  (None, 1, 1, 192)            1728      ['block3a_se_reduce[2][0]']   \n",
      "                                                                                                  \n",
      " block3a_se_excite (Multipl  (None, 32, 32, 192)          0         ['block3a_activation[2][0]',  \n",
      " y)                                                                  'block3a_se_expand[2][0]']   \n",
      "                                                                                                  \n",
      " quantize_annotate_727 (Qua  (None, 32, 32, 56)           10752     ['block3a_se_excite[2][0]']   \n",
      " ntizeAnnotate)                                                                                   \n",
      "                                                                                                  \n",
      " block3a_project_bn (BatchN  (None, 32, 32, 56)           224       ['quantize_annotate_727[0][0]'\n",
      " ormalization)                                                      ]                             \n",
      "                                                                                                  \n",
      " quantize_annotate_728 (Qua  (None, 32, 32, 336)          18816     ['block3a_project_bn[2][0]']  \n",
      " ntizeAnnotate)                                                                                   \n",
      "                                                                                                  \n",
      " block3b_expand_bn (BatchNo  (None, 32, 32, 336)          1344      ['quantize_annotate_728[0][0]'\n",
      " rmalization)                                                       ]                             \n",
      "                                                                                                  \n",
      " block3b_expand_activation   (None, 32, 32, 336)          0         ['block3b_expand_bn[2][0]']   \n",
      " (Activation)                                                                                     \n",
      "                                                                                                  \n",
      " quantize_annotate_729 (Qua  (None, 32, 32, 336)          8400      ['block3b_expand_activation[2]\n",
      " ntizeAnnotate)                                                     [0]']                         \n",
      "                                                                                                  \n",
      " block3b_bn (BatchNormaliza  (None, 32, 32, 336)          1344      ['quantize_annotate_729[0][0]'\n",
      " tion)                                                              ]                             \n",
      "                                                                                                  \n",
      " block3b_activation (Activa  (None, 32, 32, 336)          0         ['block3b_bn[2][0]']          \n",
      " tion)                                                                                            \n",
      "                                                                                                  \n",
      " block3b_se_squeeze (Global  (None, 336)                  0         ['block3b_activation[2][0]']  \n",
      " AveragePooling2D)                                                                                \n",
      "                                                                                                  \n",
      " block3b_se_reshape (Reshap  (None, 1, 1, 336)            0         ['block3b_se_squeeze[2][0]']  \n",
      " e)                                                                                               \n",
      "                                                                                                  \n",
      " block3b_se_reduce (Conv2D)  (None, 1, 1, 14)             4718      ['block3b_se_reshape[2][0]']  \n",
      "                                                                                                  \n",
      " block3b_se_expand (Conv2D)  (None, 1, 1, 336)            5040      ['block3b_se_reduce[2][0]']   \n",
      "                                                                                                  \n",
      " block3b_se_excite (Multipl  (None, 32, 32, 336)          0         ['block3b_activation[2][0]',  \n",
      " y)                                                                  'block3b_se_expand[2][0]']   \n",
      "                                                                                                  \n",
      " quantize_annotate_730 (Qua  (None, 32, 32, 56)           18816     ['block3b_se_excite[2][0]']   \n",
      " ntizeAnnotate)                                                                                   \n",
      "                                                                                                  \n",
      " block3b_project_bn (BatchN  (None, 32, 32, 56)           224       ['quantize_annotate_730[0][0]'\n",
      " ormalization)                                                      ]                             \n",
      "                                                                                                  \n",
      " block3b_drop (Dropout)      (None, 32, 32, 56)           0         ['block3b_project_bn[2][0]']  \n",
      "                                                                                                  \n",
      " block3b_add (Add)           (None, 32, 32, 56)           0         ['block3b_drop[2][0]',        \n",
      "                                                                     'block3a_project_bn[2][0]']  \n",
      "                                                                                                  \n",
      " quantize_annotate_731 (Qua  (None, 32, 32, 336)          18816     ['block3b_add[2][0]']         \n",
      " ntizeAnnotate)                                                                                   \n",
      "                                                                                                  \n",
      " block3c_expand_bn (BatchNo  (None, 32, 32, 336)          1344      ['quantize_annotate_731[0][0]'\n",
      " rmalization)                                                       ]                             \n",
      "                                                                                                  \n",
      " block3c_expand_activation   (None, 32, 32, 336)          0         ['block3c_expand_bn[2][0]']   \n",
      " (Activation)                                                                                     \n",
      "                                                                                                  \n",
      " quantize_annotate_732 (Qua  (None, 32, 32, 336)          8400      ['block3c_expand_activation[2]\n",
      " ntizeAnnotate)                                                     [0]']                         \n",
      "                                                                                                  \n",
      " block3c_bn (BatchNormaliza  (None, 32, 32, 336)          1344      ['quantize_annotate_732[0][0]'\n",
      " tion)                                                              ]                             \n",
      "                                                                                                  \n",
      " block3c_activation (Activa  (None, 32, 32, 336)          0         ['block3c_bn[2][0]']          \n",
      " tion)                                                                                            \n",
      "                                                                                                  \n",
      " block3c_se_squeeze (Global  (None, 336)                  0         ['block3c_activation[2][0]']  \n",
      " AveragePooling2D)                                                                                \n",
      "                                                                                                  \n",
      " block3c_se_reshape (Reshap  (None, 1, 1, 336)            0         ['block3c_se_squeeze[2][0]']  \n",
      " e)                                                                                               \n",
      "                                                                                                  \n",
      " block3c_se_reduce (Conv2D)  (None, 1, 1, 14)             4718      ['block3c_se_reshape[2][0]']  \n",
      "                                                                                                  \n",
      " block3c_se_expand (Conv2D)  (None, 1, 1, 336)            5040      ['block3c_se_reduce[2][0]']   \n",
      "                                                                                                  \n",
      " block3c_se_excite (Multipl  (None, 32, 32, 336)          0         ['block3c_activation[2][0]',  \n",
      " y)                                                                  'block3c_se_expand[2][0]']   \n",
      "                                                                                                  \n",
      " quantize_annotate_733 (Qua  (None, 32, 32, 56)           18816     ['block3c_se_excite[2][0]']   \n",
      " ntizeAnnotate)                                                                                   \n",
      "                                                                                                  \n",
      " block3c_project_bn (BatchN  (None, 32, 32, 56)           224       ['quantize_annotate_733[0][0]'\n",
      " ormalization)                                                      ]                             \n",
      "                                                                                                  \n",
      " block3c_drop (Dropout)      (None, 32, 32, 56)           0         ['block3c_project_bn[2][0]']  \n",
      "                                                                                                  \n",
      " block3c_add (Add)           (None, 32, 32, 56)           0         ['block3c_drop[2][0]',        \n",
      "                                                                     'block3b_add[2][0]']         \n",
      "                                                                                                  \n",
      " quantize_annotate_734 (Qua  (None, 32, 32, 336)          18816     ['block3c_add[2][0]']         \n",
      " ntizeAnnotate)                                                                                   \n",
      "                                                                                                  \n",
      " block3d_expand_bn (BatchNo  (None, 32, 32, 336)          1344      ['quantize_annotate_734[0][0]'\n",
      " rmalization)                                                       ]                             \n",
      "                                                                                                  \n",
      " block3d_expand_activation   (None, 32, 32, 336)          0         ['block3d_expand_bn[2][0]']   \n",
      " (Activation)                                                                                     \n",
      "                                                                                                  \n",
      " quantize_annotate_735 (Qua  (None, 32, 32, 336)          8400      ['block3d_expand_activation[2]\n",
      " ntizeAnnotate)                                                     [0]']                         \n",
      "                                                                                                  \n",
      " block3d_bn (BatchNormaliza  (None, 32, 32, 336)          1344      ['quantize_annotate_735[0][0]'\n",
      " tion)                                                              ]                             \n",
      "                                                                                                  \n",
      " block3d_activation (Activa  (None, 32, 32, 336)          0         ['block3d_bn[2][0]']          \n",
      " tion)                                                                                            \n",
      "                                                                                                  \n",
      " block3d_se_squeeze (Global  (None, 336)                  0         ['block3d_activation[2][0]']  \n",
      " AveragePooling2D)                                                                                \n",
      "                                                                                                  \n",
      " block3d_se_reshape (Reshap  (None, 1, 1, 336)            0         ['block3d_se_squeeze[2][0]']  \n",
      " e)                                                                                               \n",
      "                                                                                                  \n",
      " block3d_se_reduce (Conv2D)  (None, 1, 1, 14)             4718      ['block3d_se_reshape[2][0]']  \n",
      "                                                                                                  \n",
      " block3d_se_expand (Conv2D)  (None, 1, 1, 336)            5040      ['block3d_se_reduce[2][0]']   \n",
      "                                                                                                  \n",
      " block3d_se_excite (Multipl  (None, 32, 32, 336)          0         ['block3d_activation[2][0]',  \n",
      " y)                                                                  'block3d_se_expand[2][0]']   \n",
      "                                                                                                  \n",
      " quantize_annotate_736 (Qua  (None, 32, 32, 56)           18816     ['block3d_se_excite[2][0]']   \n",
      " ntizeAnnotate)                                                                                   \n",
      "                                                                                                  \n",
      " block3d_project_bn (BatchN  (None, 32, 32, 56)           224       ['quantize_annotate_736[0][0]'\n",
      " ormalization)                                                      ]                             \n",
      "                                                                                                  \n",
      " block3d_drop (Dropout)      (None, 32, 32, 56)           0         ['block3d_project_bn[2][0]']  \n",
      "                                                                                                  \n",
      " block3d_add (Add)           (None, 32, 32, 56)           0         ['block3d_drop[2][0]',        \n",
      "                                                                     'block3c_add[2][0]']         \n",
      "                                                                                                  \n",
      " quantize_annotate_737 (Qua  (None, 32, 32, 336)          18816     ['block3d_add[2][0]']         \n",
      " ntizeAnnotate)                                                                                   \n",
      "                                                                                                  \n",
      " block4a_expand_bn (BatchNo  (None, 32, 32, 336)          1344      ['quantize_annotate_737[0][0]'\n",
      " rmalization)                                                       ]                             \n",
      "                                                                                                  \n",
      " block4a_expand_activation   (None, 32, 32, 336)          0         ['block4a_expand_bn[2][0]']   \n",
      " (Activation)                                                                                     \n",
      "                                                                                                  \n",
      " quantize_annotate_738 (Qua  (None, 33, 33, 336)          0         ['block4a_expand_activation[2]\n",
      " ntizeAnnotate)                                                     [0]']                         \n",
      "                                                                                                  \n",
      " quantize_annotate_739 (Qua  (None, 16, 16, 336)          3024      ['quantize_annotate_738[0][0]'\n",
      " ntizeAnnotate)                                                     ]                             \n",
      "                                                                                                  \n",
      " block4a_bn (BatchNormaliza  (None, 16, 16, 336)          1344      ['quantize_annotate_739[0][0]'\n",
      " tion)                                                              ]                             \n",
      "                                                                                                  \n",
      " block4a_activation (Activa  (None, 16, 16, 336)          0         ['block4a_bn[2][0]']          \n",
      " tion)                                                                                            \n",
      "                                                                                                  \n",
      " block4a_se_squeeze (Global  (None, 336)                  0         ['block4a_activation[2][0]']  \n",
      " AveragePooling2D)                                                                                \n",
      "                                                                                                  \n",
      " block4a_se_reshape (Reshap  (None, 1, 1, 336)            0         ['block4a_se_squeeze[2][0]']  \n",
      " e)                                                                                               \n",
      "                                                                                                  \n",
      " block4a_se_reduce (Conv2D)  (None, 1, 1, 14)             4718      ['block4a_se_reshape[2][0]']  \n",
      "                                                                                                  \n",
      " block4a_se_expand (Conv2D)  (None, 1, 1, 336)            5040      ['block4a_se_reduce[2][0]']   \n",
      "                                                                                                  \n",
      " block4a_se_excite (Multipl  (None, 16, 16, 336)          0         ['block4a_activation[2][0]',  \n",
      " y)                                                                  'block4a_se_expand[2][0]']   \n",
      "                                                                                                  \n",
      " quantize_annotate_740 (Qua  (None, 16, 16, 112)          37632     ['block4a_se_excite[2][0]']   \n",
      " ntizeAnnotate)                                                                                   \n",
      "                                                                                                  \n",
      " block4a_project_bn (BatchN  (None, 16, 16, 112)          448       ['quantize_annotate_740[0][0]'\n",
      " ormalization)                                                      ]                             \n",
      "                                                                                                  \n",
      " quantize_annotate_741 (Qua  (None, 16, 16, 672)          75264     ['block4a_project_bn[2][0]']  \n",
      " ntizeAnnotate)                                                                                   \n",
      "                                                                                                  \n",
      " block4b_expand_bn (BatchNo  (None, 16, 16, 672)          2688      ['quantize_annotate_741[0][0]'\n",
      " rmalization)                                                       ]                             \n",
      "                                                                                                  \n",
      " block4b_expand_activation   (None, 16, 16, 672)          0         ['block4b_expand_bn[2][0]']   \n",
      " (Activation)                                                                                     \n",
      "                                                                                                  \n",
      " quantize_annotate_742 (Qua  (None, 16, 16, 672)          6048      ['block4b_expand_activation[2]\n",
      " ntizeAnnotate)                                                     [0]']                         \n",
      "                                                                                                  \n",
      " block4b_bn (BatchNormaliza  (None, 16, 16, 672)          2688      ['quantize_annotate_742[0][0]'\n",
      " tion)                                                              ]                             \n",
      "                                                                                                  \n",
      " block4b_activation (Activa  (None, 16, 16, 672)          0         ['block4b_bn[2][0]']          \n",
      " tion)                                                                                            \n",
      "                                                                                                  \n",
      " block4b_se_squeeze (Global  (None, 672)                  0         ['block4b_activation[2][0]']  \n",
      " AveragePooling2D)                                                                                \n",
      "                                                                                                  \n",
      " block4b_se_reshape (Reshap  (None, 1, 1, 672)            0         ['block4b_se_squeeze[2][0]']  \n",
      " e)                                                                                               \n",
      "                                                                                                  \n",
      " block4b_se_reduce (Conv2D)  (None, 1, 1, 28)             18844     ['block4b_se_reshape[2][0]']  \n",
      "                                                                                                  \n",
      " block4b_se_expand (Conv2D)  (None, 1, 1, 672)            19488     ['block4b_se_reduce[2][0]']   \n",
      "                                                                                                  \n",
      " block4b_se_excite (Multipl  (None, 16, 16, 672)          0         ['block4b_activation[2][0]',  \n",
      " y)                                                                  'block4b_se_expand[2][0]']   \n",
      "                                                                                                  \n",
      " quantize_annotate_743 (Qua  (None, 16, 16, 112)          75264     ['block4b_se_excite[2][0]']   \n",
      " ntizeAnnotate)                                                                                   \n",
      "                                                                                                  \n",
      " block4b_project_bn (BatchN  (None, 16, 16, 112)          448       ['quantize_annotate_743[0][0]'\n",
      " ormalization)                                                      ]                             \n",
      "                                                                                                  \n",
      " block4b_drop (Dropout)      (None, 16, 16, 112)          0         ['block4b_project_bn[2][0]']  \n",
      "                                                                                                  \n",
      " block4b_add (Add)           (None, 16, 16, 112)          0         ['block4b_drop[2][0]',        \n",
      "                                                                     'block4a_project_bn[2][0]']  \n",
      "                                                                                                  \n",
      " quantize_annotate_744 (Qua  (None, 16, 16, 672)          75264     ['block4b_add[2][0]']         \n",
      " ntizeAnnotate)                                                                                   \n",
      "                                                                                                  \n",
      " block4c_expand_bn (BatchNo  (None, 16, 16, 672)          2688      ['quantize_annotate_744[0][0]'\n",
      " rmalization)                                                       ]                             \n",
      "                                                                                                  \n",
      " block4c_expand_activation   (None, 16, 16, 672)          0         ['block4c_expand_bn[2][0]']   \n",
      " (Activation)                                                                                     \n",
      "                                                                                                  \n",
      " quantize_annotate_745 (Qua  (None, 16, 16, 672)          6048      ['block4c_expand_activation[2]\n",
      " ntizeAnnotate)                                                     [0]']                         \n",
      "                                                                                                  \n",
      " block4c_bn (BatchNormaliza  (None, 16, 16, 672)          2688      ['quantize_annotate_745[0][0]'\n",
      " tion)                                                              ]                             \n",
      "                                                                                                  \n",
      " block4c_activation (Activa  (None, 16, 16, 672)          0         ['block4c_bn[2][0]']          \n",
      " tion)                                                                                            \n",
      "                                                                                                  \n",
      " block4c_se_squeeze (Global  (None, 672)                  0         ['block4c_activation[2][0]']  \n",
      " AveragePooling2D)                                                                                \n",
      "                                                                                                  \n",
      " block4c_se_reshape (Reshap  (None, 1, 1, 672)            0         ['block4c_se_squeeze[2][0]']  \n",
      " e)                                                                                               \n",
      "                                                                                                  \n",
      " block4c_se_reduce (Conv2D)  (None, 1, 1, 28)             18844     ['block4c_se_reshape[2][0]']  \n",
      "                                                                                                  \n",
      " block4c_se_expand (Conv2D)  (None, 1, 1, 672)            19488     ['block4c_se_reduce[2][0]']   \n",
      "                                                                                                  \n",
      " block4c_se_excite (Multipl  (None, 16, 16, 672)          0         ['block4c_activation[2][0]',  \n",
      " y)                                                                  'block4c_se_expand[2][0]']   \n",
      "                                                                                                  \n",
      " quantize_annotate_746 (Qua  (None, 16, 16, 112)          75264     ['block4c_se_excite[2][0]']   \n",
      " ntizeAnnotate)                                                                                   \n",
      "                                                                                                  \n",
      " block4c_project_bn (BatchN  (None, 16, 16, 112)          448       ['quantize_annotate_746[0][0]'\n",
      " ormalization)                                                      ]                             \n",
      "                                                                                                  \n",
      " block4c_drop (Dropout)      (None, 16, 16, 112)          0         ['block4c_project_bn[2][0]']  \n",
      "                                                                                                  \n",
      " block4c_add (Add)           (None, 16, 16, 112)          0         ['block4c_drop[2][0]',        \n",
      "                                                                     'block4b_add[2][0]']         \n",
      "                                                                                                  \n",
      " quantize_annotate_747 (Qua  (None, 16, 16, 672)          75264     ['block4c_add[2][0]']         \n",
      " ntizeAnnotate)                                                                                   \n",
      "                                                                                                  \n",
      " block4d_expand_bn (BatchNo  (None, 16, 16, 672)          2688      ['quantize_annotate_747[0][0]'\n",
      " rmalization)                                                       ]                             \n",
      "                                                                                                  \n",
      " block4d_expand_activation   (None, 16, 16, 672)          0         ['block4d_expand_bn[2][0]']   \n",
      " (Activation)                                                                                     \n",
      "                                                                                                  \n",
      " quantize_annotate_748 (Qua  (None, 16, 16, 672)          6048      ['block4d_expand_activation[2]\n",
      " ntizeAnnotate)                                                     [0]']                         \n",
      "                                                                                                  \n",
      " block4d_bn (BatchNormaliza  (None, 16, 16, 672)          2688      ['quantize_annotate_748[0][0]'\n",
      " tion)                                                              ]                             \n",
      "                                                                                                  \n",
      " block4d_activation (Activa  (None, 16, 16, 672)          0         ['block4d_bn[2][0]']          \n",
      " tion)                                                                                            \n",
      "                                                                                                  \n",
      " block4d_se_squeeze (Global  (None, 672)                  0         ['block4d_activation[2][0]']  \n",
      " AveragePooling2D)                                                                                \n",
      "                                                                                                  \n",
      " block4d_se_reshape (Reshap  (None, 1, 1, 672)            0         ['block4d_se_squeeze[2][0]']  \n",
      " e)                                                                                               \n",
      "                                                                                                  \n",
      " block4d_se_reduce (Conv2D)  (None, 1, 1, 28)             18844     ['block4d_se_reshape[2][0]']  \n",
      "                                                                                                  \n",
      " block4d_se_expand (Conv2D)  (None, 1, 1, 672)            19488     ['block4d_se_reduce[2][0]']   \n",
      "                                                                                                  \n",
      " block4d_se_excite (Multipl  (None, 16, 16, 672)          0         ['block4d_activation[2][0]',  \n",
      " y)                                                                  'block4d_se_expand[2][0]']   \n",
      "                                                                                                  \n",
      " quantize_annotate_749 (Qua  (None, 16, 16, 112)          75264     ['block4d_se_excite[2][0]']   \n",
      " ntizeAnnotate)                                                                                   \n",
      "                                                                                                  \n",
      " block4d_project_bn (BatchN  (None, 16, 16, 112)          448       ['quantize_annotate_749[0][0]'\n",
      " ormalization)                                                      ]                             \n",
      "                                                                                                  \n",
      " block4d_drop (Dropout)      (None, 16, 16, 112)          0         ['block4d_project_bn[2][0]']  \n",
      "                                                                                                  \n",
      " block4d_add (Add)           (None, 16, 16, 112)          0         ['block4d_drop[2][0]',        \n",
      "                                                                     'block4c_add[2][0]']         \n",
      "                                                                                                  \n",
      " quantize_annotate_750 (Qua  (None, 16, 16, 672)          75264     ['block4d_add[2][0]']         \n",
      " ntizeAnnotate)                                                                                   \n",
      "                                                                                                  \n",
      " block4e_expand_bn (BatchNo  (None, 16, 16, 672)          2688      ['quantize_annotate_750[0][0]'\n",
      " rmalization)                                                       ]                             \n",
      "                                                                                                  \n",
      " block4e_expand_activation   (None, 16, 16, 672)          0         ['block4e_expand_bn[2][0]']   \n",
      " (Activation)                                                                                     \n",
      "                                                                                                  \n",
      " quantize_annotate_751 (Qua  (None, 16, 16, 672)          6048      ['block4e_expand_activation[2]\n",
      " ntizeAnnotate)                                                     [0]']                         \n",
      "                                                                                                  \n",
      " block4e_bn (BatchNormaliza  (None, 16, 16, 672)          2688      ['quantize_annotate_751[0][0]'\n",
      " tion)                                                              ]                             \n",
      "                                                                                                  \n",
      " block4e_activation (Activa  (None, 16, 16, 672)          0         ['block4e_bn[2][0]']          \n",
      " tion)                                                                                            \n",
      "                                                                                                  \n",
      " block4e_se_squeeze (Global  (None, 672)                  0         ['block4e_activation[2][0]']  \n",
      " AveragePooling2D)                                                                                \n",
      "                                                                                                  \n",
      " block4e_se_reshape (Reshap  (None, 1, 1, 672)            0         ['block4e_se_squeeze[2][0]']  \n",
      " e)                                                                                               \n",
      "                                                                                                  \n",
      " block4e_se_reduce (Conv2D)  (None, 1, 1, 28)             18844     ['block4e_se_reshape[2][0]']  \n",
      "                                                                                                  \n",
      " block4e_se_expand (Conv2D)  (None, 1, 1, 672)            19488     ['block4e_se_reduce[2][0]']   \n",
      "                                                                                                  \n",
      " block4e_se_excite (Multipl  (None, 16, 16, 672)          0         ['block4e_activation[2][0]',  \n",
      " y)                                                                  'block4e_se_expand[2][0]']   \n",
      "                                                                                                  \n",
      " quantize_annotate_752 (Qua  (None, 16, 16, 112)          75264     ['block4e_se_excite[2][0]']   \n",
      " ntizeAnnotate)                                                                                   \n",
      "                                                                                                  \n",
      " block4e_project_bn (BatchN  (None, 16, 16, 112)          448       ['quantize_annotate_752[0][0]'\n",
      " ormalization)                                                      ]                             \n",
      "                                                                                                  \n",
      " block4e_drop (Dropout)      (None, 16, 16, 112)          0         ['block4e_project_bn[2][0]']  \n",
      "                                                                                                  \n",
      " block4e_add (Add)           (None, 16, 16, 112)          0         ['block4e_drop[2][0]',        \n",
      "                                                                     'block4d_add[2][0]']         \n",
      "                                                                                                  \n",
      " quantize_annotate_753 (Qua  (None, 16, 16, 672)          75264     ['block4e_add[2][0]']         \n",
      " ntizeAnnotate)                                                                                   \n",
      "                                                                                                  \n",
      " block4f_expand_bn (BatchNo  (None, 16, 16, 672)          2688      ['quantize_annotate_753[0][0]'\n",
      " rmalization)                                                       ]                             \n",
      "                                                                                                  \n",
      " block4f_expand_activation   (None, 16, 16, 672)          0         ['block4f_expand_bn[2][0]']   \n",
      " (Activation)                                                                                     \n",
      "                                                                                                  \n",
      " quantize_annotate_754 (Qua  (None, 16, 16, 672)          6048      ['block4f_expand_activation[2]\n",
      " ntizeAnnotate)                                                     [0]']                         \n",
      "                                                                                                  \n",
      " block4f_bn (BatchNormaliza  (None, 16, 16, 672)          2688      ['quantize_annotate_754[0][0]'\n",
      " tion)                                                              ]                             \n",
      "                                                                                                  \n",
      " block4f_activation (Activa  (None, 16, 16, 672)          0         ['block4f_bn[2][0]']          \n",
      " tion)                                                                                            \n",
      "                                                                                                  \n",
      " block4f_se_squeeze (Global  (None, 672)                  0         ['block4f_activation[2][0]']  \n",
      " AveragePooling2D)                                                                                \n",
      "                                                                                                  \n",
      " block4f_se_reshape (Reshap  (None, 1, 1, 672)            0         ['block4f_se_squeeze[2][0]']  \n",
      " e)                                                                                               \n",
      "                                                                                                  \n",
      " block4f_se_reduce (Conv2D)  (None, 1, 1, 28)             18844     ['block4f_se_reshape[2][0]']  \n",
      "                                                                                                  \n",
      " block4f_se_expand (Conv2D)  (None, 1, 1, 672)            19488     ['block4f_se_reduce[2][0]']   \n",
      "                                                                                                  \n",
      " block4f_se_excite (Multipl  (None, 16, 16, 672)          0         ['block4f_activation[2][0]',  \n",
      " y)                                                                  'block4f_se_expand[2][0]']   \n",
      "                                                                                                  \n",
      " quantize_annotate_755 (Qua  (None, 16, 16, 112)          75264     ['block4f_se_excite[2][0]']   \n",
      " ntizeAnnotate)                                                                                   \n",
      "                                                                                                  \n",
      " block4f_project_bn (BatchN  (None, 16, 16, 112)          448       ['quantize_annotate_755[0][0]'\n",
      " ormalization)                                                      ]                             \n",
      "                                                                                                  \n",
      " block4f_drop (Dropout)      (None, 16, 16, 112)          0         ['block4f_project_bn[2][0]']  \n",
      "                                                                                                  \n",
      " block4f_add (Add)           (None, 16, 16, 112)          0         ['block4f_drop[2][0]',        \n",
      "                                                                     'block4e_add[2][0]']         \n",
      "                                                                                                  \n",
      " quantize_annotate_756 (Qua  (None, 16, 16, 672)          75264     ['block4f_add[2][0]']         \n",
      " ntizeAnnotate)                                                                                   \n",
      "                                                                                                  \n",
      " block5a_expand_bn (BatchNo  (None, 16, 16, 672)          2688      ['quantize_annotate_756[0][0]'\n",
      " rmalization)                                                       ]                             \n",
      "                                                                                                  \n",
      " block5a_expand_activation   (None, 16, 16, 672)          0         ['block5a_expand_bn[2][0]']   \n",
      " (Activation)                                                                                     \n",
      "                                                                                                  \n",
      " quantize_annotate_757 (Qua  (None, 16, 16, 672)          16800     ['block5a_expand_activation[2]\n",
      " ntizeAnnotate)                                                     [0]']                         \n",
      "                                                                                                  \n",
      " block5a_bn (BatchNormaliza  (None, 16, 16, 672)          2688      ['quantize_annotate_757[0][0]'\n",
      " tion)                                                              ]                             \n",
      "                                                                                                  \n",
      " block5a_activation (Activa  (None, 16, 16, 672)          0         ['block5a_bn[2][0]']          \n",
      " tion)                                                                                            \n",
      "                                                                                                  \n",
      " block5a_se_squeeze (Global  (None, 672)                  0         ['block5a_activation[2][0]']  \n",
      " AveragePooling2D)                                                                                \n",
      "                                                                                                  \n",
      " block5a_se_reshape (Reshap  (None, 1, 1, 672)            0         ['block5a_se_squeeze[2][0]']  \n",
      " e)                                                                                               \n",
      "                                                                                                  \n",
      " block5a_se_reduce (Conv2D)  (None, 1, 1, 28)             18844     ['block5a_se_reshape[2][0]']  \n",
      "                                                                                                  \n",
      " block5a_se_expand (Conv2D)  (None, 1, 1, 672)            19488     ['block5a_se_reduce[2][0]']   \n",
      "                                                                                                  \n",
      " block5a_se_excite (Multipl  (None, 16, 16, 672)          0         ['block5a_activation[2][0]',  \n",
      " y)                                                                  'block5a_se_expand[2][0]']   \n",
      "                                                                                                  \n",
      " quantize_annotate_758 (Qua  (None, 16, 16, 160)          107520    ['block5a_se_excite[2][0]']   \n",
      " ntizeAnnotate)                                                                                   \n",
      "                                                                                                  \n",
      " block5a_project_bn (BatchN  (None, 16, 16, 160)          640       ['quantize_annotate_758[0][0]'\n",
      " ormalization)                                                      ]                             \n",
      "                                                                                                  \n",
      " quantize_annotate_759 (Qua  (None, 16, 16, 960)          153600    ['block5a_project_bn[2][0]']  \n",
      " ntizeAnnotate)                                                                                   \n",
      "                                                                                                  \n",
      " block5b_expand_bn (BatchNo  (None, 16, 16, 960)          3840      ['quantize_annotate_759[0][0]'\n",
      " rmalization)                                                       ]                             \n",
      "                                                                                                  \n",
      " block5b_expand_activation   (None, 16, 16, 960)          0         ['block5b_expand_bn[2][0]']   \n",
      " (Activation)                                                                                     \n",
      "                                                                                                  \n",
      " quantize_annotate_760 (Qua  (None, 16, 16, 960)          24000     ['block5b_expand_activation[2]\n",
      " ntizeAnnotate)                                                     [0]']                         \n",
      "                                                                                                  \n",
      " block5b_bn (BatchNormaliza  (None, 16, 16, 960)          3840      ['quantize_annotate_760[0][0]'\n",
      " tion)                                                              ]                             \n",
      "                                                                                                  \n",
      " block5b_activation (Activa  (None, 16, 16, 960)          0         ['block5b_bn[2][0]']          \n",
      " tion)                                                                                            \n",
      "                                                                                                  \n",
      " block5b_se_squeeze (Global  (None, 960)                  0         ['block5b_activation[2][0]']  \n",
      " AveragePooling2D)                                                                                \n",
      "                                                                                                  \n",
      " block5b_se_reshape (Reshap  (None, 1, 1, 960)            0         ['block5b_se_squeeze[2][0]']  \n",
      " e)                                                                                               \n",
      "                                                                                                  \n",
      " block5b_se_reduce (Conv2D)  (None, 1, 1, 40)             38440     ['block5b_se_reshape[2][0]']  \n",
      "                                                                                                  \n",
      " block5b_se_expand (Conv2D)  (None, 1, 1, 960)            39360     ['block5b_se_reduce[2][0]']   \n",
      "                                                                                                  \n",
      " block5b_se_excite (Multipl  (None, 16, 16, 960)          0         ['block5b_activation[2][0]',  \n",
      " y)                                                                  'block5b_se_expand[2][0]']   \n",
      "                                                                                                  \n",
      " quantize_annotate_761 (Qua  (None, 16, 16, 160)          153600    ['block5b_se_excite[2][0]']   \n",
      " ntizeAnnotate)                                                                                   \n",
      "                                                                                                  \n",
      " block5b_project_bn (BatchN  (None, 16, 16, 160)          640       ['quantize_annotate_761[0][0]'\n",
      " ormalization)                                                      ]                             \n",
      "                                                                                                  \n",
      " block5b_drop (Dropout)      (None, 16, 16, 160)          0         ['block5b_project_bn[2][0]']  \n",
      "                                                                                                  \n",
      " block5b_add (Add)           (None, 16, 16, 160)          0         ['block5b_drop[2][0]',        \n",
      "                                                                     'block5a_project_bn[2][0]']  \n",
      "                                                                                                  \n",
      " quantize_annotate_762 (Qua  (None, 16, 16, 960)          153600    ['block5b_add[2][0]']         \n",
      " ntizeAnnotate)                                                                                   \n",
      "                                                                                                  \n",
      " block5c_expand_bn (BatchNo  (None, 16, 16, 960)          3840      ['quantize_annotate_762[0][0]'\n",
      " rmalization)                                                       ]                             \n",
      "                                                                                                  \n",
      " block5c_expand_activation   (None, 16, 16, 960)          0         ['block5c_expand_bn[2][0]']   \n",
      " (Activation)                                                                                     \n",
      "                                                                                                  \n",
      " quantize_annotate_763 (Qua  (None, 16, 16, 960)          24000     ['block5c_expand_activation[2]\n",
      " ntizeAnnotate)                                                     [0]']                         \n",
      "                                                                                                  \n",
      " block5c_bn (BatchNormaliza  (None, 16, 16, 960)          3840      ['quantize_annotate_763[0][0]'\n",
      " tion)                                                              ]                             \n",
      "                                                                                                  \n",
      " block5c_activation (Activa  (None, 16, 16, 960)          0         ['block5c_bn[2][0]']          \n",
      " tion)                                                                                            \n",
      "                                                                                                  \n",
      " block5c_se_squeeze (Global  (None, 960)                  0         ['block5c_activation[2][0]']  \n",
      " AveragePooling2D)                                                                                \n",
      "                                                                                                  \n",
      " block5c_se_reshape (Reshap  (None, 1, 1, 960)            0         ['block5c_se_squeeze[2][0]']  \n",
      " e)                                                                                               \n",
      "                                                                                                  \n",
      " block5c_se_reduce (Conv2D)  (None, 1, 1, 40)             38440     ['block5c_se_reshape[2][0]']  \n",
      "                                                                                                  \n",
      " block5c_se_expand (Conv2D)  (None, 1, 1, 960)            39360     ['block5c_se_reduce[2][0]']   \n",
      "                                                                                                  \n",
      " block5c_se_excite (Multipl  (None, 16, 16, 960)          0         ['block5c_activation[2][0]',  \n",
      " y)                                                                  'block5c_se_expand[2][0]']   \n",
      "                                                                                                  \n",
      " quantize_annotate_764 (Qua  (None, 16, 16, 160)          153600    ['block5c_se_excite[2][0]']   \n",
      " ntizeAnnotate)                                                                                   \n",
      "                                                                                                  \n",
      " block5c_project_bn (BatchN  (None, 16, 16, 160)          640       ['quantize_annotate_764[0][0]'\n",
      " ormalization)                                                      ]                             \n",
      "                                                                                                  \n",
      " block5c_drop (Dropout)      (None, 16, 16, 160)          0         ['block5c_project_bn[2][0]']  \n",
      "                                                                                                  \n",
      " block5c_add (Add)           (None, 16, 16, 160)          0         ['block5c_drop[2][0]',        \n",
      "                                                                     'block5b_add[2][0]']         \n",
      "                                                                                                  \n",
      " quantize_annotate_765 (Qua  (None, 16, 16, 960)          153600    ['block5c_add[2][0]']         \n",
      " ntizeAnnotate)                                                                                   \n",
      "                                                                                                  \n",
      " block5d_expand_bn (BatchNo  (None, 16, 16, 960)          3840      ['quantize_annotate_765[0][0]'\n",
      " rmalization)                                                       ]                             \n",
      "                                                                                                  \n",
      " block5d_expand_activation   (None, 16, 16, 960)          0         ['block5d_expand_bn[2][0]']   \n",
      " (Activation)                                                                                     \n",
      "                                                                                                  \n",
      " quantize_annotate_766 (Qua  (None, 16, 16, 960)          24000     ['block5d_expand_activation[2]\n",
      " ntizeAnnotate)                                                     [0]']                         \n",
      "                                                                                                  \n",
      " block5d_bn (BatchNormaliza  (None, 16, 16, 960)          3840      ['quantize_annotate_766[0][0]'\n",
      " tion)                                                              ]                             \n",
      "                                                                                                  \n",
      " block5d_activation (Activa  (None, 16, 16, 960)          0         ['block5d_bn[2][0]']          \n",
      " tion)                                                                                            \n",
      "                                                                                                  \n",
      " block5d_se_squeeze (Global  (None, 960)                  0         ['block5d_activation[2][0]']  \n",
      " AveragePooling2D)                                                                                \n",
      "                                                                                                  \n",
      " block5d_se_reshape (Reshap  (None, 1, 1, 960)            0         ['block5d_se_squeeze[2][0]']  \n",
      " e)                                                                                               \n",
      "                                                                                                  \n",
      " block5d_se_reduce (Conv2D)  (None, 1, 1, 40)             38440     ['block5d_se_reshape[2][0]']  \n",
      "                                                                                                  \n",
      " block5d_se_expand (Conv2D)  (None, 1, 1, 960)            39360     ['block5d_se_reduce[2][0]']   \n",
      "                                                                                                  \n",
      " block5d_se_excite (Multipl  (None, 16, 16, 960)          0         ['block5d_activation[2][0]',  \n",
      " y)                                                                  'block5d_se_expand[2][0]']   \n",
      "                                                                                                  \n",
      " quantize_annotate_767 (Qua  (None, 16, 16, 160)          153600    ['block5d_se_excite[2][0]']   \n",
      " ntizeAnnotate)                                                                                   \n",
      "                                                                                                  \n",
      " block5d_project_bn (BatchN  (None, 16, 16, 160)          640       ['quantize_annotate_767[0][0]'\n",
      " ormalization)                                                      ]                             \n",
      "                                                                                                  \n",
      " block5d_drop (Dropout)      (None, 16, 16, 160)          0         ['block5d_project_bn[2][0]']  \n",
      "                                                                                                  \n",
      " block5d_add (Add)           (None, 16, 16, 160)          0         ['block5d_drop[2][0]',        \n",
      "                                                                     'block5c_add[2][0]']         \n",
      "                                                                                                  \n",
      " quantize_annotate_768 (Qua  (None, 16, 16, 960)          153600    ['block5d_add[2][0]']         \n",
      " ntizeAnnotate)                                                                                   \n",
      "                                                                                                  \n",
      " block5e_expand_bn (BatchNo  (None, 16, 16, 960)          3840      ['quantize_annotate_768[0][0]'\n",
      " rmalization)                                                       ]                             \n",
      "                                                                                                  \n",
      " block5e_expand_activation   (None, 16, 16, 960)          0         ['block5e_expand_bn[2][0]']   \n",
      " (Activation)                                                                                     \n",
      "                                                                                                  \n",
      " quantize_annotate_769 (Qua  (None, 16, 16, 960)          24000     ['block5e_expand_activation[2]\n",
      " ntizeAnnotate)                                                     [0]']                         \n",
      "                                                                                                  \n",
      " block5e_bn (BatchNormaliza  (None, 16, 16, 960)          3840      ['quantize_annotate_769[0][0]'\n",
      " tion)                                                              ]                             \n",
      "                                                                                                  \n",
      " block5e_activation (Activa  (None, 16, 16, 960)          0         ['block5e_bn[2][0]']          \n",
      " tion)                                                                                            \n",
      "                                                                                                  \n",
      " block5e_se_squeeze (Global  (None, 960)                  0         ['block5e_activation[2][0]']  \n",
      " AveragePooling2D)                                                                                \n",
      "                                                                                                  \n",
      " block5e_se_reshape (Reshap  (None, 1, 1, 960)            0         ['block5e_se_squeeze[2][0]']  \n",
      " e)                                                                                               \n",
      "                                                                                                  \n",
      " block5e_se_reduce (Conv2D)  (None, 1, 1, 40)             38440     ['block5e_se_reshape[2][0]']  \n",
      "                                                                                                  \n",
      " block5e_se_expand (Conv2D)  (None, 1, 1, 960)            39360     ['block5e_se_reduce[2][0]']   \n",
      "                                                                                                  \n",
      " block5e_se_excite (Multipl  (None, 16, 16, 960)          0         ['block5e_activation[2][0]',  \n",
      " y)                                                                  'block5e_se_expand[2][0]']   \n",
      "                                                                                                  \n",
      " quantize_annotate_770 (Qua  (None, 16, 16, 160)          153600    ['block5e_se_excite[2][0]']   \n",
      " ntizeAnnotate)                                                                                   \n",
      "                                                                                                  \n",
      " block5e_project_bn (BatchN  (None, 16, 16, 160)          640       ['quantize_annotate_770[0][0]'\n",
      " ormalization)                                                      ]                             \n",
      "                                                                                                  \n",
      " block5e_drop (Dropout)      (None, 16, 16, 160)          0         ['block5e_project_bn[2][0]']  \n",
      "                                                                                                  \n",
      " block5e_add (Add)           (None, 16, 16, 160)          0         ['block5e_drop[2][0]',        \n",
      "                                                                     'block5d_add[2][0]']         \n",
      "                                                                                                  \n",
      " quantize_annotate_771 (Qua  (None, 16, 16, 960)          153600    ['block5e_add[2][0]']         \n",
      " ntizeAnnotate)                                                                                   \n",
      "                                                                                                  \n",
      " block5f_expand_bn (BatchNo  (None, 16, 16, 960)          3840      ['quantize_annotate_771[0][0]'\n",
      " rmalization)                                                       ]                             \n",
      "                                                                                                  \n",
      " block5f_expand_activation   (None, 16, 16, 960)          0         ['block5f_expand_bn[2][0]']   \n",
      " (Activation)                                                                                     \n",
      "                                                                                                  \n",
      " quantize_annotate_772 (Qua  (None, 16, 16, 960)          24000     ['block5f_expand_activation[2]\n",
      " ntizeAnnotate)                                                     [0]']                         \n",
      "                                                                                                  \n",
      " block5f_bn (BatchNormaliza  (None, 16, 16, 960)          3840      ['quantize_annotate_772[0][0]'\n",
      " tion)                                                              ]                             \n",
      "                                                                                                  \n",
      " block5f_activation (Activa  (None, 16, 16, 960)          0         ['block5f_bn[2][0]']          \n",
      " tion)                                                                                            \n",
      "                                                                                                  \n",
      " block5f_se_squeeze (Global  (None, 960)                  0         ['block5f_activation[2][0]']  \n",
      " AveragePooling2D)                                                                                \n",
      "                                                                                                  \n",
      " block5f_se_reshape (Reshap  (None, 1, 1, 960)            0         ['block5f_se_squeeze[2][0]']  \n",
      " e)                                                                                               \n",
      "                                                                                                  \n",
      " block5f_se_reduce (Conv2D)  (None, 1, 1, 40)             38440     ['block5f_se_reshape[2][0]']  \n",
      "                                                                                                  \n",
      " block5f_se_expand (Conv2D)  (None, 1, 1, 960)            39360     ['block5f_se_reduce[2][0]']   \n",
      "                                                                                                  \n",
      " block5f_se_excite (Multipl  (None, 16, 16, 960)          0         ['block5f_activation[2][0]',  \n",
      " y)                                                                  'block5f_se_expand[2][0]']   \n",
      "                                                                                                  \n",
      " quantize_annotate_773 (Qua  (None, 16, 16, 160)          153600    ['block5f_se_excite[2][0]']   \n",
      " ntizeAnnotate)                                                                                   \n",
      "                                                                                                  \n",
      " block5f_project_bn (BatchN  (None, 16, 16, 160)          640       ['quantize_annotate_773[0][0]'\n",
      " ormalization)                                                      ]                             \n",
      "                                                                                                  \n",
      " block5f_drop (Dropout)      (None, 16, 16, 160)          0         ['block5f_project_bn[2][0]']  \n",
      "                                                                                                  \n",
      " block5f_add (Add)           (None, 16, 16, 160)          0         ['block5f_drop[2][0]',        \n",
      "                                                                     'block5e_add[2][0]']         \n",
      "                                                                                                  \n",
      " quantize_annotate_774 (Qua  (None, 16, 16, 960)          153600    ['block5f_add[2][0]']         \n",
      " ntizeAnnotate)                                                                                   \n",
      "                                                                                                  \n",
      " block6a_expand_bn (BatchNo  (None, 16, 16, 960)          3840      ['quantize_annotate_774[0][0]'\n",
      " rmalization)                                                       ]                             \n",
      "                                                                                                  \n",
      " block6a_expand_activation   (None, 16, 16, 960)          0         ['block6a_expand_bn[2][0]']   \n",
      " (Activation)                                                                                     \n",
      "                                                                                                  \n",
      " quantize_annotate_775 (Qua  (None, 19, 19, 960)          0         ['block6a_expand_activation[2]\n",
      " ntizeAnnotate)                                                     [0]']                         \n",
      "                                                                                                  \n",
      " quantize_annotate_776 (Qua  (None, 8, 8, 960)            24000     ['quantize_annotate_775[0][0]'\n",
      " ntizeAnnotate)                                                     ]                             \n",
      "                                                                                                  \n",
      " block6a_bn (BatchNormaliza  (None, 8, 8, 960)            3840      ['quantize_annotate_776[0][0]'\n",
      " tion)                                                              ]                             \n",
      "                                                                                                  \n",
      " block6a_activation (Activa  (None, 8, 8, 960)            0         ['block6a_bn[2][0]']          \n",
      " tion)                                                                                            \n",
      "                                                                                                  \n",
      " block6a_se_squeeze (Global  (None, 960)                  0         ['block6a_activation[2][0]']  \n",
      " AveragePooling2D)                                                                                \n",
      "                                                                                                  \n",
      " block6a_se_reshape (Reshap  (None, 1, 1, 960)            0         ['block6a_se_squeeze[2][0]']  \n",
      " e)                                                                                               \n",
      "                                                                                                  \n",
      " block6a_se_reduce (Conv2D)  (None, 1, 1, 40)             38440     ['block6a_se_reshape[2][0]']  \n",
      "                                                                                                  \n",
      " block6a_se_expand (Conv2D)  (None, 1, 1, 960)            39360     ['block6a_se_reduce[2][0]']   \n",
      "                                                                                                  \n",
      " block6a_se_excite (Multipl  (None, 8, 8, 960)            0         ['block6a_activation[2][0]',  \n",
      " y)                                                                  'block6a_se_expand[2][0]']   \n",
      "                                                                                                  \n",
      " quantize_annotate_777 (Qua  (None, 8, 8, 272)            261120    ['block6a_se_excite[2][0]']   \n",
      " ntizeAnnotate)                                                                                   \n",
      "                                                                                                  \n",
      " block6a_project_bn (BatchN  (None, 8, 8, 272)            1088      ['quantize_annotate_777[0][0]'\n",
      " ormalization)                                                      ]                             \n",
      "                                                                                                  \n",
      " quantize_annotate_778 (Qua  (None, 8, 8, 1632)           443904    ['block6a_project_bn[2][0]']  \n",
      " ntizeAnnotate)                                                                                   \n",
      "                                                                                                  \n",
      " block6b_expand_bn (BatchNo  (None, 8, 8, 1632)           6528      ['quantize_annotate_778[0][0]'\n",
      " rmalization)                                                       ]                             \n",
      "                                                                                                  \n",
      " block6b_expand_activation   (None, 8, 8, 1632)           0         ['block6b_expand_bn[2][0]']   \n",
      " (Activation)                                                                                     \n",
      "                                                                                                  \n",
      " quantize_annotate_779 (Qua  (None, 8, 8, 1632)           40800     ['block6b_expand_activation[2]\n",
      " ntizeAnnotate)                                                     [0]']                         \n",
      "                                                                                                  \n",
      " block6b_bn (BatchNormaliza  (None, 8, 8, 1632)           6528      ['quantize_annotate_779[0][0]'\n",
      " tion)                                                              ]                             \n",
      "                                                                                                  \n",
      " block6b_activation (Activa  (None, 8, 8, 1632)           0         ['block6b_bn[2][0]']          \n",
      " tion)                                                                                            \n",
      "                                                                                                  \n",
      " block6b_se_squeeze (Global  (None, 1632)                 0         ['block6b_activation[2][0]']  \n",
      " AveragePooling2D)                                                                                \n",
      "                                                                                                  \n",
      " block6b_se_reshape (Reshap  (None, 1, 1, 1632)           0         ['block6b_se_squeeze[2][0]']  \n",
      " e)                                                                                               \n",
      "                                                                                                  \n",
      " block6b_se_reduce (Conv2D)  (None, 1, 1, 68)             111044    ['block6b_se_reshape[2][0]']  \n",
      "                                                                                                  \n",
      " block6b_se_expand (Conv2D)  (None, 1, 1, 1632)           112608    ['block6b_se_reduce[2][0]']   \n",
      "                                                                                                  \n",
      " block6b_se_excite (Multipl  (None, 8, 8, 1632)           0         ['block6b_activation[2][0]',  \n",
      " y)                                                                  'block6b_se_expand[2][0]']   \n",
      "                                                                                                  \n",
      " quantize_annotate_780 (Qua  (None, 8, 8, 272)            443904    ['block6b_se_excite[2][0]']   \n",
      " ntizeAnnotate)                                                                                   \n",
      "                                                                                                  \n",
      " block6b_project_bn (BatchN  (None, 8, 8, 272)            1088      ['quantize_annotate_780[0][0]'\n",
      " ormalization)                                                      ]                             \n",
      "                                                                                                  \n",
      " block6b_drop (Dropout)      (None, 8, 8, 272)            0         ['block6b_project_bn[2][0]']  \n",
      "                                                                                                  \n",
      " block6b_add (Add)           (None, 8, 8, 272)            0         ['block6b_drop[2][0]',        \n",
      "                                                                     'block6a_project_bn[2][0]']  \n",
      "                                                                                                  \n",
      " quantize_annotate_781 (Qua  (None, 8, 8, 1632)           443904    ['block6b_add[2][0]']         \n",
      " ntizeAnnotate)                                                                                   \n",
      "                                                                                                  \n",
      " block6c_expand_bn (BatchNo  (None, 8, 8, 1632)           6528      ['quantize_annotate_781[0][0]'\n",
      " rmalization)                                                       ]                             \n",
      "                                                                                                  \n",
      " block6c_expand_activation   (None, 8, 8, 1632)           0         ['block6c_expand_bn[2][0]']   \n",
      " (Activation)                                                                                     \n",
      "                                                                                                  \n",
      " quantize_annotate_782 (Qua  (None, 8, 8, 1632)           40800     ['block6c_expand_activation[2]\n",
      " ntizeAnnotate)                                                     [0]']                         \n",
      "                                                                                                  \n",
      " block6c_bn (BatchNormaliza  (None, 8, 8, 1632)           6528      ['quantize_annotate_782[0][0]'\n",
      " tion)                                                              ]                             \n",
      "                                                                                                  \n",
      " block6c_activation (Activa  (None, 8, 8, 1632)           0         ['block6c_bn[2][0]']          \n",
      " tion)                                                                                            \n",
      "                                                                                                  \n",
      " block6c_se_squeeze (Global  (None, 1632)                 0         ['block6c_activation[2][0]']  \n",
      " AveragePooling2D)                                                                                \n",
      "                                                                                                  \n",
      " block6c_se_reshape (Reshap  (None, 1, 1, 1632)           0         ['block6c_se_squeeze[2][0]']  \n",
      " e)                                                                                               \n",
      "                                                                                                  \n",
      " block6c_se_reduce (Conv2D)  (None, 1, 1, 68)             111044    ['block6c_se_reshape[2][0]']  \n",
      "                                                                                                  \n",
      " block6c_se_expand (Conv2D)  (None, 1, 1, 1632)           112608    ['block6c_se_reduce[2][0]']   \n",
      "                                                                                                  \n",
      " block6c_se_excite (Multipl  (None, 8, 8, 1632)           0         ['block6c_activation[2][0]',  \n",
      " y)                                                                  'block6c_se_expand[2][0]']   \n",
      "                                                                                                  \n",
      " quantize_annotate_783 (Qua  (None, 8, 8, 272)            443904    ['block6c_se_excite[2][0]']   \n",
      " ntizeAnnotate)                                                                                   \n",
      "                                                                                                  \n",
      " block6c_project_bn (BatchN  (None, 8, 8, 272)            1088      ['quantize_annotate_783[0][0]'\n",
      " ormalization)                                                      ]                             \n",
      "                                                                                                  \n",
      " block6c_drop (Dropout)      (None, 8, 8, 272)            0         ['block6c_project_bn[2][0]']  \n",
      "                                                                                                  \n",
      " block6c_add (Add)           (None, 8, 8, 272)            0         ['block6c_drop[2][0]',        \n",
      "                                                                     'block6b_add[2][0]']         \n",
      "                                                                                                  \n",
      " quantize_annotate_784 (Qua  (None, 8, 8, 1632)           443904    ['block6c_add[2][0]']         \n",
      " ntizeAnnotate)                                                                                   \n",
      "                                                                                                  \n",
      " block6d_expand_bn (BatchNo  (None, 8, 8, 1632)           6528      ['quantize_annotate_784[0][0]'\n",
      " rmalization)                                                       ]                             \n",
      "                                                                                                  \n",
      " block6d_expand_activation   (None, 8, 8, 1632)           0         ['block6d_expand_bn[2][0]']   \n",
      " (Activation)                                                                                     \n",
      "                                                                                                  \n",
      " quantize_annotate_785 (Qua  (None, 8, 8, 1632)           40800     ['block6d_expand_activation[2]\n",
      " ntizeAnnotate)                                                     [0]']                         \n",
      "                                                                                                  \n",
      " block6d_bn (BatchNormaliza  (None, 8, 8, 1632)           6528      ['quantize_annotate_785[0][0]'\n",
      " tion)                                                              ]                             \n",
      "                                                                                                  \n",
      " block6d_activation (Activa  (None, 8, 8, 1632)           0         ['block6d_bn[2][0]']          \n",
      " tion)                                                                                            \n",
      "                                                                                                  \n",
      " block6d_se_squeeze (Global  (None, 1632)                 0         ['block6d_activation[2][0]']  \n",
      " AveragePooling2D)                                                                                \n",
      "                                                                                                  \n",
      " block6d_se_reshape (Reshap  (None, 1, 1, 1632)           0         ['block6d_se_squeeze[2][0]']  \n",
      " e)                                                                                               \n",
      "                                                                                                  \n",
      " block6d_se_reduce (Conv2D)  (None, 1, 1, 68)             111044    ['block6d_se_reshape[2][0]']  \n",
      "                                                                                                  \n",
      " block6d_se_expand (Conv2D)  (None, 1, 1, 1632)           112608    ['block6d_se_reduce[2][0]']   \n",
      "                                                                                                  \n",
      " block6d_se_excite (Multipl  (None, 8, 8, 1632)           0         ['block6d_activation[2][0]',  \n",
      " y)                                                                  'block6d_se_expand[2][0]']   \n",
      "                                                                                                  \n",
      " quantize_annotate_786 (Qua  (None, 8, 8, 272)            443904    ['block6d_se_excite[2][0]']   \n",
      " ntizeAnnotate)                                                                                   \n",
      "                                                                                                  \n",
      " block6d_project_bn (BatchN  (None, 8, 8, 272)            1088      ['quantize_annotate_786[0][0]'\n",
      " ormalization)                                                      ]                             \n",
      "                                                                                                  \n",
      " block6d_drop (Dropout)      (None, 8, 8, 272)            0         ['block6d_project_bn[2][0]']  \n",
      "                                                                                                  \n",
      " block6d_add (Add)           (None, 8, 8, 272)            0         ['block6d_drop[2][0]',        \n",
      "                                                                     'block6c_add[2][0]']         \n",
      "                                                                                                  \n",
      " quantize_annotate_787 (Qua  (None, 8, 8, 1632)           443904    ['block6d_add[2][0]']         \n",
      " ntizeAnnotate)                                                                                   \n",
      "                                                                                                  \n",
      " block6e_expand_bn (BatchNo  (None, 8, 8, 1632)           6528      ['quantize_annotate_787[0][0]'\n",
      " rmalization)                                                       ]                             \n",
      "                                                                                                  \n",
      " block6e_expand_activation   (None, 8, 8, 1632)           0         ['block6e_expand_bn[2][0]']   \n",
      " (Activation)                                                                                     \n",
      "                                                                                                  \n",
      " quantize_annotate_788 (Qua  (None, 8, 8, 1632)           40800     ['block6e_expand_activation[2]\n",
      " ntizeAnnotate)                                                     [0]']                         \n",
      "                                                                                                  \n",
      " block6e_bn (BatchNormaliza  (None, 8, 8, 1632)           6528      ['quantize_annotate_788[0][0]'\n",
      " tion)                                                              ]                             \n",
      "                                                                                                  \n",
      " block6e_activation (Activa  (None, 8, 8, 1632)           0         ['block6e_bn[2][0]']          \n",
      " tion)                                                                                            \n",
      "                                                                                                  \n",
      " block6e_se_squeeze (Global  (None, 1632)                 0         ['block6e_activation[2][0]']  \n",
      " AveragePooling2D)                                                                                \n",
      "                                                                                                  \n",
      " block6e_se_reshape (Reshap  (None, 1, 1, 1632)           0         ['block6e_se_squeeze[2][0]']  \n",
      " e)                                                                                               \n",
      "                                                                                                  \n",
      " block6e_se_reduce (Conv2D)  (None, 1, 1, 68)             111044    ['block6e_se_reshape[2][0]']  \n",
      "                                                                                                  \n",
      " block6e_se_expand (Conv2D)  (None, 1, 1, 1632)           112608    ['block6e_se_reduce[2][0]']   \n",
      "                                                                                                  \n",
      " block6e_se_excite (Multipl  (None, 8, 8, 1632)           0         ['block6e_activation[2][0]',  \n",
      " y)                                                                  'block6e_se_expand[2][0]']   \n",
      "                                                                                                  \n",
      " quantize_annotate_789 (Qua  (None, 8, 8, 272)            443904    ['block6e_se_excite[2][0]']   \n",
      " ntizeAnnotate)                                                                                   \n",
      "                                                                                                  \n",
      " block6e_project_bn (BatchN  (None, 8, 8, 272)            1088      ['quantize_annotate_789[0][0]'\n",
      " ormalization)                                                      ]                             \n",
      "                                                                                                  \n",
      " block6e_drop (Dropout)      (None, 8, 8, 272)            0         ['block6e_project_bn[2][0]']  \n",
      "                                                                                                  \n",
      " block6e_add (Add)           (None, 8, 8, 272)            0         ['block6e_drop[2][0]',        \n",
      "                                                                     'block6d_add[2][0]']         \n",
      "                                                                                                  \n",
      " quantize_annotate_790 (Qua  (None, 8, 8, 1632)           443904    ['block6e_add[2][0]']         \n",
      " ntizeAnnotate)                                                                                   \n",
      "                                                                                                  \n",
      " block6f_expand_bn (BatchNo  (None, 8, 8, 1632)           6528      ['quantize_annotate_790[0][0]'\n",
      " rmalization)                                                       ]                             \n",
      "                                                                                                  \n",
      " block6f_expand_activation   (None, 8, 8, 1632)           0         ['block6f_expand_bn[2][0]']   \n",
      " (Activation)                                                                                     \n",
      "                                                                                                  \n",
      " quantize_annotate_791 (Qua  (None, 8, 8, 1632)           40800     ['block6f_expand_activation[2]\n",
      " ntizeAnnotate)                                                     [0]']                         \n",
      "                                                                                                  \n",
      " block6f_bn (BatchNormaliza  (None, 8, 8, 1632)           6528      ['quantize_annotate_791[0][0]'\n",
      " tion)                                                              ]                             \n",
      "                                                                                                  \n",
      " block6f_activation (Activa  (None, 8, 8, 1632)           0         ['block6f_bn[2][0]']          \n",
      " tion)                                                                                            \n",
      "                                                                                                  \n",
      " block6f_se_squeeze (Global  (None, 1632)                 0         ['block6f_activation[2][0]']  \n",
      " AveragePooling2D)                                                                                \n",
      "                                                                                                  \n",
      " block6f_se_reshape (Reshap  (None, 1, 1, 1632)           0         ['block6f_se_squeeze[2][0]']  \n",
      " e)                                                                                               \n",
      "                                                                                                  \n",
      " block6f_se_reduce (Conv2D)  (None, 1, 1, 68)             111044    ['block6f_se_reshape[2][0]']  \n",
      "                                                                                                  \n",
      " block6f_se_expand (Conv2D)  (None, 1, 1, 1632)           112608    ['block6f_se_reduce[2][0]']   \n",
      "                                                                                                  \n",
      " block6f_se_excite (Multipl  (None, 8, 8, 1632)           0         ['block6f_activation[2][0]',  \n",
      " y)                                                                  'block6f_se_expand[2][0]']   \n",
      "                                                                                                  \n",
      " quantize_annotate_792 (Qua  (None, 8, 8, 272)            443904    ['block6f_se_excite[2][0]']   \n",
      " ntizeAnnotate)                                                                                   \n",
      "                                                                                                  \n",
      " block6f_project_bn (BatchN  (None, 8, 8, 272)            1088      ['quantize_annotate_792[0][0]'\n",
      " ormalization)                                                      ]                             \n",
      "                                                                                                  \n",
      " block6f_drop (Dropout)      (None, 8, 8, 272)            0         ['block6f_project_bn[2][0]']  \n",
      "                                                                                                  \n",
      " block6f_add (Add)           (None, 8, 8, 272)            0         ['block6f_drop[2][0]',        \n",
      "                                                                     'block6e_add[2][0]']         \n",
      "                                                                                                  \n",
      " quantize_annotate_793 (Qua  (None, 8, 8, 1632)           443904    ['block6f_add[2][0]']         \n",
      " ntizeAnnotate)                                                                                   \n",
      "                                                                                                  \n",
      " block6g_expand_bn (BatchNo  (None, 8, 8, 1632)           6528      ['quantize_annotate_793[0][0]'\n",
      " rmalization)                                                       ]                             \n",
      "                                                                                                  \n",
      " block6g_expand_activation   (None, 8, 8, 1632)           0         ['block6g_expand_bn[2][0]']   \n",
      " (Activation)                                                                                     \n",
      "                                                                                                  \n",
      " quantize_annotate_794 (Qua  (None, 8, 8, 1632)           40800     ['block6g_expand_activation[2]\n",
      " ntizeAnnotate)                                                     [0]']                         \n",
      "                                                                                                  \n",
      " block6g_bn (BatchNormaliza  (None, 8, 8, 1632)           6528      ['quantize_annotate_794[0][0]'\n",
      " tion)                                                              ]                             \n",
      "                                                                                                  \n",
      " block6g_activation (Activa  (None, 8, 8, 1632)           0         ['block6g_bn[2][0]']          \n",
      " tion)                                                                                            \n",
      "                                                                                                  \n",
      " block6g_se_squeeze (Global  (None, 1632)                 0         ['block6g_activation[2][0]']  \n",
      " AveragePooling2D)                                                                                \n",
      "                                                                                                  \n",
      " block6g_se_reshape (Reshap  (None, 1, 1, 1632)           0         ['block6g_se_squeeze[2][0]']  \n",
      " e)                                                                                               \n",
      "                                                                                                  \n",
      " block6g_se_reduce (Conv2D)  (None, 1, 1, 68)             111044    ['block6g_se_reshape[2][0]']  \n",
      "                                                                                                  \n",
      " block6g_se_expand (Conv2D)  (None, 1, 1, 1632)           112608    ['block6g_se_reduce[2][0]']   \n",
      "                                                                                                  \n",
      " block6g_se_excite (Multipl  (None, 8, 8, 1632)           0         ['block6g_activation[2][0]',  \n",
      " y)                                                                  'block6g_se_expand[2][0]']   \n",
      "                                                                                                  \n",
      " quantize_annotate_795 (Qua  (None, 8, 8, 272)            443904    ['block6g_se_excite[2][0]']   \n",
      " ntizeAnnotate)                                                                                   \n",
      "                                                                                                  \n",
      " block6g_project_bn (BatchN  (None, 8, 8, 272)            1088      ['quantize_annotate_795[0][0]'\n",
      " ormalization)                                                      ]                             \n",
      "                                                                                                  \n",
      " block6g_drop (Dropout)      (None, 8, 8, 272)            0         ['block6g_project_bn[2][0]']  \n",
      "                                                                                                  \n",
      " block6g_add (Add)           (None, 8, 8, 272)            0         ['block6g_drop[2][0]',        \n",
      "                                                                     'block6f_add[2][0]']         \n",
      "                                                                                                  \n",
      " quantize_annotate_796 (Qua  (None, 8, 8, 1632)           443904    ['block6g_add[2][0]']         \n",
      " ntizeAnnotate)                                                                                   \n",
      "                                                                                                  \n",
      " block6h_expand_bn (BatchNo  (None, 8, 8, 1632)           6528      ['quantize_annotate_796[0][0]'\n",
      " rmalization)                                                       ]                             \n",
      "                                                                                                  \n",
      " block6h_expand_activation   (None, 8, 8, 1632)           0         ['block6h_expand_bn[2][0]']   \n",
      " (Activation)                                                                                     \n",
      "                                                                                                  \n",
      " quantize_annotate_797 (Qua  (None, 8, 8, 1632)           40800     ['block6h_expand_activation[2]\n",
      " ntizeAnnotate)                                                     [0]']                         \n",
      "                                                                                                  \n",
      " block6h_bn (BatchNormaliza  (None, 8, 8, 1632)           6528      ['quantize_annotate_797[0][0]'\n",
      " tion)                                                              ]                             \n",
      "                                                                                                  \n",
      " block6h_activation (Activa  (None, 8, 8, 1632)           0         ['block6h_bn[2][0]']          \n",
      " tion)                                                                                            \n",
      "                                                                                                  \n",
      " block6h_se_squeeze (Global  (None, 1632)                 0         ['block6h_activation[2][0]']  \n",
      " AveragePooling2D)                                                                                \n",
      "                                                                                                  \n",
      " block6h_se_reshape (Reshap  (None, 1, 1, 1632)           0         ['block6h_se_squeeze[2][0]']  \n",
      " e)                                                                                               \n",
      "                                                                                                  \n",
      " block6h_se_reduce (Conv2D)  (None, 1, 1, 68)             111044    ['block6h_se_reshape[2][0]']  \n",
      "                                                                                                  \n",
      " block6h_se_expand (Conv2D)  (None, 1, 1, 1632)           112608    ['block6h_se_reduce[2][0]']   \n",
      "                                                                                                  \n",
      " block6h_se_excite (Multipl  (None, 8, 8, 1632)           0         ['block6h_activation[2][0]',  \n",
      " y)                                                                  'block6h_se_expand[2][0]']   \n",
      "                                                                                                  \n",
      " quantize_annotate_798 (Qua  (None, 8, 8, 272)            443904    ['block6h_se_excite[2][0]']   \n",
      " ntizeAnnotate)                                                                                   \n",
      "                                                                                                  \n",
      " block6h_project_bn (BatchN  (None, 8, 8, 272)            1088      ['quantize_annotate_798[0][0]'\n",
      " ormalization)                                                      ]                             \n",
      "                                                                                                  \n",
      " block6h_drop (Dropout)      (None, 8, 8, 272)            0         ['block6h_project_bn[2][0]']  \n",
      "                                                                                                  \n",
      " block6h_add (Add)           (None, 8, 8, 272)            0         ['block6h_drop[2][0]',        \n",
      "                                                                     'block6g_add[2][0]']         \n",
      "                                                                                                  \n",
      " quantize_annotate_799 (Qua  (None, 8, 8, 1632)           443904    ['block6h_add[2][0]']         \n",
      " ntizeAnnotate)                                                                                   \n",
      "                                                                                                  \n",
      " block7a_expand_bn (BatchNo  (None, 8, 8, 1632)           6528      ['quantize_annotate_799[0][0]'\n",
      " rmalization)                                                       ]                             \n",
      "                                                                                                  \n",
      " block7a_expand_activation   (None, 8, 8, 1632)           0         ['block7a_expand_bn[2][0]']   \n",
      " (Activation)                                                                                     \n",
      "                                                                                                  \n",
      " quantize_annotate_800 (Qua  (None, 8, 8, 1632)           14688     ['block7a_expand_activation[2]\n",
      " ntizeAnnotate)                                                     [0]']                         \n",
      "                                                                                                  \n",
      " block7a_bn (BatchNormaliza  (None, 8, 8, 1632)           6528      ['quantize_annotate_800[0][0]'\n",
      " tion)                                                              ]                             \n",
      "                                                                                                  \n",
      " block7a_activation (Activa  (None, 8, 8, 1632)           0         ['block7a_bn[2][0]']          \n",
      " tion)                                                                                            \n",
      "                                                                                                  \n",
      " block7a_se_squeeze (Global  (None, 1632)                 0         ['block7a_activation[2][0]']  \n",
      " AveragePooling2D)                                                                                \n",
      "                                                                                                  \n",
      " block7a_se_reshape (Reshap  (None, 1, 1, 1632)           0         ['block7a_se_squeeze[2][0]']  \n",
      " e)                                                                                               \n",
      "                                                                                                  \n",
      " block7a_se_reduce (Conv2D)  (None, 1, 1, 68)             111044    ['block7a_se_reshape[2][0]']  \n",
      "                                                                                                  \n",
      " block7a_se_expand (Conv2D)  (None, 1, 1, 1632)           112608    ['block7a_se_reduce[2][0]']   \n",
      "                                                                                                  \n",
      " block7a_se_excite (Multipl  (None, 8, 8, 1632)           0         ['block7a_activation[2][0]',  \n",
      " y)                                                                  'block7a_se_expand[2][0]']   \n",
      "                                                                                                  \n",
      " quantize_annotate_801 (Qua  (None, 8, 8, 448)            731136    ['block7a_se_excite[2][0]']   \n",
      " ntizeAnnotate)                                                                                   \n",
      "                                                                                                  \n",
      " block7a_project_bn (BatchN  (None, 8, 8, 448)            1792      ['quantize_annotate_801[0][0]'\n",
      " ormalization)                                                      ]                             \n",
      "                                                                                                  \n",
      " quantize_annotate_802 (Qua  (None, 8, 8, 2688)           1204224   ['block7a_project_bn[2][0]']  \n",
      " ntizeAnnotate)                                                                                   \n",
      "                                                                                                  \n",
      " block7b_expand_bn (BatchNo  (None, 8, 8, 2688)           10752     ['quantize_annotate_802[0][0]'\n",
      " rmalization)                                                       ]                             \n",
      "                                                                                                  \n",
      " block7b_expand_activation   (None, 8, 8, 2688)           0         ['block7b_expand_bn[2][0]']   \n",
      " (Activation)                                                                                     \n",
      "                                                                                                  \n",
      " quantize_annotate_803 (Qua  (None, 8, 8, 2688)           24192     ['block7b_expand_activation[2]\n",
      " ntizeAnnotate)                                                     [0]']                         \n",
      "                                                                                                  \n",
      " block7b_bn (BatchNormaliza  (None, 8, 8, 2688)           10752     ['quantize_annotate_803[0][0]'\n",
      " tion)                                                              ]                             \n",
      "                                                                                                  \n",
      " block7b_activation (Activa  (None, 8, 8, 2688)           0         ['block7b_bn[2][0]']          \n",
      " tion)                                                                                            \n",
      "                                                                                                  \n",
      " block7b_se_squeeze (Global  (None, 2688)                 0         ['block7b_activation[2][0]']  \n",
      " AveragePooling2D)                                                                                \n",
      "                                                                                                  \n",
      " block7b_se_reshape (Reshap  (None, 1, 1, 2688)           0         ['block7b_se_squeeze[2][0]']  \n",
      " e)                                                                                               \n",
      "                                                                                                  \n",
      " block7b_se_reduce (Conv2D)  (None, 1, 1, 112)            301168    ['block7b_se_reshape[2][0]']  \n",
      "                                                                                                  \n",
      " block7b_se_expand (Conv2D)  (None, 1, 1, 2688)           303744    ['block7b_se_reduce[2][0]']   \n",
      "                                                                                                  \n",
      " block7b_se_excite (Multipl  (None, 8, 8, 2688)           0         ['block7b_activation[2][0]',  \n",
      " y)                                                                  'block7b_se_expand[2][0]']   \n",
      "                                                                                                  \n",
      " quantize_annotate_804 (Qua  (None, 8, 8, 448)            1204224   ['block7b_se_excite[2][0]']   \n",
      " ntizeAnnotate)                                                                                   \n",
      "                                                                                                  \n",
      " block7b_project_bn (BatchN  (None, 8, 8, 448)            1792      ['quantize_annotate_804[0][0]'\n",
      " ormalization)                                                      ]                             \n",
      "                                                                                                  \n",
      " block7b_drop (Dropout)      (None, 8, 8, 448)            0         ['block7b_project_bn[2][0]']  \n",
      "                                                                                                  \n",
      " block7b_add (Add)           (None, 8, 8, 448)            0         ['block7b_drop[2][0]',        \n",
      "                                                                     'block7a_project_bn[2][0]']  \n",
      "                                                                                                  \n",
      " quantize_annotate_805 (Qua  (None, 8, 8, 1792)           802816    ['block7b_add[2][0]']         \n",
      " ntizeAnnotate)                                                                                   \n",
      "                                                                                                  \n",
      " top_bn (BatchNormalization  (None, 8, 8, 1792)           7168      ['quantize_annotate_805[0][0]'\n",
      " )                                                                  ]                             \n",
      "                                                                                                  \n",
      " top_activation (Activation  (None, 8, 8, 1792)           0         ['top_bn[2][0]']              \n",
      " )                                                                                                \n",
      "                                                                                                  \n",
      " global_average_pooling2d_3  (None, 1792)                 0         ['top_activation[2][0]']      \n",
      "  (GlobalAveragePooling2D)                                                                        \n",
      "                                                                                                  \n",
      " dense_24 (Dense)            (None, 1024)                 1836032   ['global_average_pooling2d_3[2\n",
      "                                                                    ][0]']                        \n",
      "                                                                                                  \n",
      " batch_normalization_19 (Ba  (None, 1024)                 4096      ['dense_24[2][0]']            \n",
      " tchNormalization)                                                                                \n",
      "                                                                                                  \n",
      " dense_25 (Dense)            (None, 128)                  131200    ['batch_normalization_19[2][0]\n",
      "                                                                    ']                            \n",
      "                                                                                                  \n",
      " dense_26 (Dense)            (None, 3)                    387       ['dense_25[2][0]']            \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 19645538 (74.94 MB)\n",
      "Trainable params: 19518283 (74.46 MB)\n",
      "Non-trainable params: 127255 (497.09 KB)\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "def applyQuantizationToConvLayers(layer):\n",
    "    if 'conv' in layer.name:\n",
    "        return tfmot.quantization.keras.quantize_annotate_layer(layer)\n",
    "    return layer\n",
    "\n",
    "quantAwareEfficientNetModel = tf.keras.models.clone_model(\n",
    "    EfficientNetModel,\n",
    "    clone_function=applyQuantizationToConvLayers\n",
    ")\n",
    "quantAwareEfficientNetModel.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Quantizing The Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_3\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                Output Shape                 Param #   Connected to                  \n",
      "==================================================================================================\n",
      " input_9 (InputLayer)        [(None, 256, 256, 3)]        0         []                            \n",
      "                                                                                                  \n",
      " rescaling_3 (Rescaling)     (None, 256, 256, 3)          0         ['input_9[0][0]']             \n",
      "                                                                                                  \n",
      " normalization_3 (Normaliza  (None, 256, 256, 3)          7         ['rescaling_3[1][0]']         \n",
      " tion)                                                                                            \n",
      "                                                                                                  \n",
      " tf.math.multiply_3 (TFOpLa  (None, 256, 256, 3)          0         ['normalization_3[1][0]']     \n",
      " mbda)                                                                                            \n",
      "                                                                                                  \n",
      " quant_stem_conv_pad (Quant  (None, 257, 257, 3)          1         ['tf.math.multiply_3[1][0]']  \n",
      " izeWrapperV2)                                                                                    \n",
      "                                                                                                  \n",
      " quant_stem_conv (QuantizeW  (None, 128, 128, 48)         1395      ['quant_stem_conv_pad[0][0]'] \n",
      " rapperV2)                                                                                        \n",
      "                                                                                                  \n",
      " stem_bn (BatchNormalizatio  (None, 128, 128, 48)         192       ['quant_stem_conv[0][0]']     \n",
      " n)                                                                                               \n",
      "                                                                                                  \n",
      " quant_stem_activation (Qua  (None, 128, 128, 48)         3         ['stem_bn[1][0]']             \n",
      " ntizeWrapperV2)                                                                                  \n",
      "                                                                                                  \n",
      " quant_block1a_dwconv (Quan  (None, 128, 128, 48)         437       ['quant_stem_activation[0][0]'\n",
      " tizeWrapperV2)                                                     ]                             \n",
      "                                                                                                  \n",
      " block1a_bn (BatchNormaliza  (None, 128, 128, 48)         192       ['quant_block1a_dwconv[0][0]']\n",
      " tion)                                                                                            \n",
      "                                                                                                  \n",
      " block1a_activation (Activa  (None, 128, 128, 48)         0         ['block1a_bn[1][0]']          \n",
      " tion)                                                                                            \n",
      "                                                                                                  \n",
      " block1a_se_squeeze (Global  (None, 48)                   0         ['block1a_activation[1][0]']  \n",
      " AveragePooling2D)                                                                                \n",
      "                                                                                                  \n",
      " block1a_se_reshape (Reshap  (None, 1, 1, 48)             0         ['block1a_se_squeeze[1][0]']  \n",
      " e)                                                                                               \n",
      "                                                                                                  \n",
      " block1a_se_reduce (Conv2D)  (None, 1, 1, 12)             588       ['block1a_se_reshape[1][0]']  \n",
      "                                                                                                  \n",
      " block1a_se_expand (Conv2D)  (None, 1, 1, 48)             624       ['block1a_se_reduce[1][0]']   \n",
      "                                                                                                  \n",
      " block1a_se_excite (Multipl  (None, 128, 128, 48)         0         ['block1a_activation[1][0]',  \n",
      " y)                                                                  'block1a_se_expand[1][0]']   \n",
      "                                                                                                  \n",
      " quant_block1a_project_conv  (None, 128, 128, 24)         1203      ['block1a_se_excite[1][0]']   \n",
      "  (QuantizeWrapperV2)                                                                             \n",
      "                                                                                                  \n",
      " block1a_project_bn (BatchN  (None, 128, 128, 24)         96        ['quant_block1a_project_conv[0\n",
      " ormalization)                                                      ][0]']                        \n",
      "                                                                                                  \n",
      " quant_block1b_dwconv (Quan  (None, 128, 128, 24)         221       ['block1a_project_bn[1][0]']  \n",
      " tizeWrapperV2)                                                                                   \n",
      "                                                                                                  \n",
      " block1b_bn (BatchNormaliza  (None, 128, 128, 24)         96        ['quant_block1b_dwconv[0][0]']\n",
      " tion)                                                                                            \n",
      "                                                                                                  \n",
      " block1b_activation (Activa  (None, 128, 128, 24)         0         ['block1b_bn[1][0]']          \n",
      " tion)                                                                                            \n",
      "                                                                                                  \n",
      " block1b_se_squeeze (Global  (None, 24)                   0         ['block1b_activation[1][0]']  \n",
      " AveragePooling2D)                                                                                \n",
      "                                                                                                  \n",
      " block1b_se_reshape (Reshap  (None, 1, 1, 24)             0         ['block1b_se_squeeze[1][0]']  \n",
      " e)                                                                                               \n",
      "                                                                                                  \n",
      " block1b_se_reduce (Conv2D)  (None, 1, 1, 6)              150       ['block1b_se_reshape[1][0]']  \n",
      "                                                                                                  \n",
      " block1b_se_expand (Conv2D)  (None, 1, 1, 24)             168       ['block1b_se_reduce[1][0]']   \n",
      "                                                                                                  \n",
      " block1b_se_excite (Multipl  (None, 128, 128, 24)         0         ['block1b_activation[1][0]',  \n",
      " y)                                                                  'block1b_se_expand[1][0]']   \n",
      "                                                                                                  \n",
      " quant_block1b_project_conv  (None, 128, 128, 24)         627       ['block1b_se_excite[1][0]']   \n",
      "  (QuantizeWrapperV2)                                                                             \n",
      "                                                                                                  \n",
      " block1b_project_bn (BatchN  (None, 128, 128, 24)         96        ['quant_block1b_project_conv[0\n",
      " ormalization)                                                      ][0]']                        \n",
      "                                                                                                  \n",
      " block1b_drop (Dropout)      (None, 128, 128, 24)         0         ['block1b_project_bn[1][0]']  \n",
      "                                                                                                  \n",
      " quant_block1b_add (Quantiz  (None, 128, 128, 24)         3         ['block1b_drop[1][0]',        \n",
      " eWrapperV2)                                                         'block1a_project_bn[1][0]']  \n",
      "                                                                                                  \n",
      " quant_block2a_expand_conv   (None, 128, 128, 144)        3747      ['quant_block1b_add[0][0]']   \n",
      " (QuantizeWrapperV2)                                                                              \n",
      "                                                                                                  \n",
      " block2a_expand_bn (BatchNo  (None, 128, 128, 144)        576       ['quant_block2a_expand_conv[0]\n",
      " rmalization)                                                       [0]']                         \n",
      "                                                                                                  \n",
      " quant_block2a_expand_activ  (None, 128, 128, 144)        3         ['block2a_expand_bn[1][0]']   \n",
      " ation (QuantizeWrapperV2)                                                                        \n",
      "                                                                                                  \n",
      " quant_block2a_dwconv_pad (  (None, 129, 129, 144)        1         ['quant_block2a_expand_activat\n",
      " QuantizeWrapperV2)                                                 ion[0][0]']                   \n",
      "                                                                                                  \n",
      " quant_block2a_dwconv (Quan  (None, 64, 64, 144)          1301      ['quant_block2a_dwconv_pad[0][\n",
      " tizeWrapperV2)                                                     0]']                          \n",
      "                                                                                                  \n",
      " block2a_bn (BatchNormaliza  (None, 64, 64, 144)          576       ['quant_block2a_dwconv[0][0]']\n",
      " tion)                                                                                            \n",
      "                                                                                                  \n",
      " block2a_activation (Activa  (None, 64, 64, 144)          0         ['block2a_bn[1][0]']          \n",
      " tion)                                                                                            \n",
      "                                                                                                  \n",
      " block2a_se_squeeze (Global  (None, 144)                  0         ['block2a_activation[1][0]']  \n",
      " AveragePooling2D)                                                                                \n",
      "                                                                                                  \n",
      " block2a_se_reshape (Reshap  (None, 1, 1, 144)            0         ['block2a_se_squeeze[1][0]']  \n",
      " e)                                                                                               \n",
      "                                                                                                  \n",
      " block2a_se_reduce (Conv2D)  (None, 1, 1, 6)              870       ['block2a_se_reshape[1][0]']  \n",
      "                                                                                                  \n",
      " block2a_se_expand (Conv2D)  (None, 1, 1, 144)            1008      ['block2a_se_reduce[1][0]']   \n",
      "                                                                                                  \n",
      " block2a_se_excite (Multipl  (None, 64, 64, 144)          0         ['block2a_activation[1][0]',  \n",
      " y)                                                                  'block2a_se_expand[1][0]']   \n",
      "                                                                                                  \n",
      " quant_block2a_project_conv  (None, 64, 64, 32)           4675      ['block2a_se_excite[1][0]']   \n",
      "  (QuantizeWrapperV2)                                                                             \n",
      "                                                                                                  \n",
      " block2a_project_bn (BatchN  (None, 64, 64, 32)           128       ['quant_block2a_project_conv[0\n",
      " ormalization)                                                      ][0]']                        \n",
      "                                                                                                  \n",
      " quant_block2b_expand_conv   (None, 64, 64, 192)          6531      ['block2a_project_bn[1][0]']  \n",
      " (QuantizeWrapperV2)                                                                              \n",
      "                                                                                                  \n",
      " block2b_expand_bn (BatchNo  (None, 64, 64, 192)          768       ['quant_block2b_expand_conv[0]\n",
      " rmalization)                                                       [0]']                         \n",
      "                                                                                                  \n",
      " quant_block2b_expand_activ  (None, 64, 64, 192)          3         ['block2b_expand_bn[1][0]']   \n",
      " ation (QuantizeWrapperV2)                                                                        \n",
      "                                                                                                  \n",
      " quant_block2b_dwconv (Quan  (None, 64, 64, 192)          1733      ['quant_block2b_expand_activat\n",
      " tizeWrapperV2)                                                     ion[0][0]']                   \n",
      "                                                                                                  \n",
      " block2b_bn (BatchNormaliza  (None, 64, 64, 192)          768       ['quant_block2b_dwconv[0][0]']\n",
      " tion)                                                                                            \n",
      "                                                                                                  \n",
      " block2b_activation (Activa  (None, 64, 64, 192)          0         ['block2b_bn[1][0]']          \n",
      " tion)                                                                                            \n",
      "                                                                                                  \n",
      " block2b_se_squeeze (Global  (None, 192)                  0         ['block2b_activation[1][0]']  \n",
      " AveragePooling2D)                                                                                \n",
      "                                                                                                  \n",
      " block2b_se_reshape (Reshap  (None, 1, 1, 192)            0         ['block2b_se_squeeze[1][0]']  \n",
      " e)                                                                                               \n",
      "                                                                                                  \n",
      " block2b_se_reduce (Conv2D)  (None, 1, 1, 8)              1544      ['block2b_se_reshape[1][0]']  \n",
      "                                                                                                  \n",
      " block2b_se_expand (Conv2D)  (None, 1, 1, 192)            1728      ['block2b_se_reduce[1][0]']   \n",
      "                                                                                                  \n",
      " block2b_se_excite (Multipl  (None, 64, 64, 192)          0         ['block2b_activation[1][0]',  \n",
      " y)                                                                  'block2b_se_expand[1][0]']   \n",
      "                                                                                                  \n",
      " quant_block2b_project_conv  (None, 64, 64, 32)           6211      ['block2b_se_excite[1][0]']   \n",
      "  (QuantizeWrapperV2)                                                                             \n",
      "                                                                                                  \n",
      " block2b_project_bn (BatchN  (None, 64, 64, 32)           128       ['quant_block2b_project_conv[0\n",
      " ormalization)                                                      ][0]']                        \n",
      "                                                                                                  \n",
      " block2b_drop (Dropout)      (None, 64, 64, 32)           0         ['block2b_project_bn[1][0]']  \n",
      "                                                                                                  \n",
      " quant_block2b_add (Quantiz  (None, 64, 64, 32)           3         ['block2b_drop[1][0]',        \n",
      " eWrapperV2)                                                         'block2a_project_bn[1][0]']  \n",
      "                                                                                                  \n",
      " quant_block2c_expand_conv   (None, 64, 64, 192)          6531      ['quant_block2b_add[0][0]']   \n",
      " (QuantizeWrapperV2)                                                                              \n",
      "                                                                                                  \n",
      " block2c_expand_bn (BatchNo  (None, 64, 64, 192)          768       ['quant_block2c_expand_conv[0]\n",
      " rmalization)                                                       [0]']                         \n",
      "                                                                                                  \n",
      " quant_block2c_expand_activ  (None, 64, 64, 192)          3         ['block2c_expand_bn[1][0]']   \n",
      " ation (QuantizeWrapperV2)                                                                        \n",
      "                                                                                                  \n",
      " quant_block2c_dwconv (Quan  (None, 64, 64, 192)          1733      ['quant_block2c_expand_activat\n",
      " tizeWrapperV2)                                                     ion[0][0]']                   \n",
      "                                                                                                  \n",
      " block2c_bn (BatchNormaliza  (None, 64, 64, 192)          768       ['quant_block2c_dwconv[0][0]']\n",
      " tion)                                                                                            \n",
      "                                                                                                  \n",
      " block2c_activation (Activa  (None, 64, 64, 192)          0         ['block2c_bn[1][0]']          \n",
      " tion)                                                                                            \n",
      "                                                                                                  \n",
      " block2c_se_squeeze (Global  (None, 192)                  0         ['block2c_activation[1][0]']  \n",
      " AveragePooling2D)                                                                                \n",
      "                                                                                                  \n",
      " block2c_se_reshape (Reshap  (None, 1, 1, 192)            0         ['block2c_se_squeeze[1][0]']  \n",
      " e)                                                                                               \n",
      "                                                                                                  \n",
      " block2c_se_reduce (Conv2D)  (None, 1, 1, 8)              1544      ['block2c_se_reshape[1][0]']  \n",
      "                                                                                                  \n",
      " block2c_se_expand (Conv2D)  (None, 1, 1, 192)            1728      ['block2c_se_reduce[1][0]']   \n",
      "                                                                                                  \n",
      " block2c_se_excite (Multipl  (None, 64, 64, 192)          0         ['block2c_activation[1][0]',  \n",
      " y)                                                                  'block2c_se_expand[1][0]']   \n",
      "                                                                                                  \n",
      " quant_block2c_project_conv  (None, 64, 64, 32)           6211      ['block2c_se_excite[1][0]']   \n",
      "  (QuantizeWrapperV2)                                                                             \n",
      "                                                                                                  \n",
      " block2c_project_bn (BatchN  (None, 64, 64, 32)           128       ['quant_block2c_project_conv[0\n",
      " ormalization)                                                      ][0]']                        \n",
      "                                                                                                  \n",
      " block2c_drop (Dropout)      (None, 64, 64, 32)           0         ['block2c_project_bn[1][0]']  \n",
      "                                                                                                  \n",
      " quant_block2c_add (Quantiz  (None, 64, 64, 32)           3         ['block2c_drop[1][0]',        \n",
      " eWrapperV2)                                                         'quant_block2b_add[0][0]']   \n",
      "                                                                                                  \n",
      " quant_block2d_expand_conv   (None, 64, 64, 192)          6531      ['quant_block2c_add[0][0]']   \n",
      " (QuantizeWrapperV2)                                                                              \n",
      "                                                                                                  \n",
      " block2d_expand_bn (BatchNo  (None, 64, 64, 192)          768       ['quant_block2d_expand_conv[0]\n",
      " rmalization)                                                       [0]']                         \n",
      "                                                                                                  \n",
      " quant_block2d_expand_activ  (None, 64, 64, 192)          3         ['block2d_expand_bn[1][0]']   \n",
      " ation (QuantizeWrapperV2)                                                                        \n",
      "                                                                                                  \n",
      " quant_block2d_dwconv (Quan  (None, 64, 64, 192)          1733      ['quant_block2d_expand_activat\n",
      " tizeWrapperV2)                                                     ion[0][0]']                   \n",
      "                                                                                                  \n",
      " block2d_bn (BatchNormaliza  (None, 64, 64, 192)          768       ['quant_block2d_dwconv[0][0]']\n",
      " tion)                                                                                            \n",
      "                                                                                                  \n",
      " block2d_activation (Activa  (None, 64, 64, 192)          0         ['block2d_bn[1][0]']          \n",
      " tion)                                                                                            \n",
      "                                                                                                  \n",
      " block2d_se_squeeze (Global  (None, 192)                  0         ['block2d_activation[1][0]']  \n",
      " AveragePooling2D)                                                                                \n",
      "                                                                                                  \n",
      " block2d_se_reshape (Reshap  (None, 1, 1, 192)            0         ['block2d_se_squeeze[1][0]']  \n",
      " e)                                                                                               \n",
      "                                                                                                  \n",
      " block2d_se_reduce (Conv2D)  (None, 1, 1, 8)              1544      ['block2d_se_reshape[1][0]']  \n",
      "                                                                                                  \n",
      " block2d_se_expand (Conv2D)  (None, 1, 1, 192)            1728      ['block2d_se_reduce[1][0]']   \n",
      "                                                                                                  \n",
      " block2d_se_excite (Multipl  (None, 64, 64, 192)          0         ['block2d_activation[1][0]',  \n",
      " y)                                                                  'block2d_se_expand[1][0]']   \n",
      "                                                                                                  \n",
      " quant_block2d_project_conv  (None, 64, 64, 32)           6211      ['block2d_se_excite[1][0]']   \n",
      "  (QuantizeWrapperV2)                                                                             \n",
      "                                                                                                  \n",
      " block2d_project_bn (BatchN  (None, 64, 64, 32)           128       ['quant_block2d_project_conv[0\n",
      " ormalization)                                                      ][0]']                        \n",
      "                                                                                                  \n",
      " block2d_drop (Dropout)      (None, 64, 64, 32)           0         ['block2d_project_bn[1][0]']  \n",
      "                                                                                                  \n",
      " quant_block2d_add (Quantiz  (None, 64, 64, 32)           3         ['block2d_drop[1][0]',        \n",
      " eWrapperV2)                                                         'quant_block2c_add[0][0]']   \n",
      "                                                                                                  \n",
      " quant_block3a_expand_conv   (None, 64, 64, 192)          6531      ['quant_block2d_add[0][0]']   \n",
      " (QuantizeWrapperV2)                                                                              \n",
      "                                                                                                  \n",
      " block3a_expand_bn (BatchNo  (None, 64, 64, 192)          768       ['quant_block3a_expand_conv[0]\n",
      " rmalization)                                                       [0]']                         \n",
      "                                                                                                  \n",
      " quant_block3a_expand_activ  (None, 64, 64, 192)          3         ['block3a_expand_bn[1][0]']   \n",
      " ation (QuantizeWrapperV2)                                                                        \n",
      "                                                                                                  \n",
      " quant_block3a_dwconv_pad (  (None, 67, 67, 192)          1         ['quant_block3a_expand_activat\n",
      " QuantizeWrapperV2)                                                 ion[0][0]']                   \n",
      "                                                                                                  \n",
      " quant_block3a_dwconv (Quan  (None, 32, 32, 192)          4805      ['quant_block3a_dwconv_pad[0][\n",
      " tizeWrapperV2)                                                     0]']                          \n",
      "                                                                                                  \n",
      " block3a_bn (BatchNormaliza  (None, 32, 32, 192)          768       ['quant_block3a_dwconv[0][0]']\n",
      " tion)                                                                                            \n",
      "                                                                                                  \n",
      " block3a_activation (Activa  (None, 32, 32, 192)          0         ['block3a_bn[1][0]']          \n",
      " tion)                                                                                            \n",
      "                                                                                                  \n",
      " block3a_se_squeeze (Global  (None, 192)                  0         ['block3a_activation[1][0]']  \n",
      " AveragePooling2D)                                                                                \n",
      "                                                                                                  \n",
      " block3a_se_reshape (Reshap  (None, 1, 1, 192)            0         ['block3a_se_squeeze[1][0]']  \n",
      " e)                                                                                               \n",
      "                                                                                                  \n",
      " block3a_se_reduce (Conv2D)  (None, 1, 1, 8)              1544      ['block3a_se_reshape[1][0]']  \n",
      "                                                                                                  \n",
      " block3a_se_expand (Conv2D)  (None, 1, 1, 192)            1728      ['block3a_se_reduce[1][0]']   \n",
      "                                                                                                  \n",
      " block3a_se_excite (Multipl  (None, 32, 32, 192)          0         ['block3a_activation[1][0]',  \n",
      " y)                                                                  'block3a_se_expand[1][0]']   \n",
      "                                                                                                  \n",
      " quant_block3a_project_conv  (None, 32, 32, 56)           10867     ['block3a_se_excite[1][0]']   \n",
      "  (QuantizeWrapperV2)                                                                             \n",
      "                                                                                                  \n",
      " block3a_project_bn (BatchN  (None, 32, 32, 56)           224       ['quant_block3a_project_conv[0\n",
      " ormalization)                                                      ][0]']                        \n",
      "                                                                                                  \n",
      " quant_block3b_expand_conv   (None, 32, 32, 336)          19491     ['block3a_project_bn[1][0]']  \n",
      " (QuantizeWrapperV2)                                                                              \n",
      "                                                                                                  \n",
      " block3b_expand_bn (BatchNo  (None, 32, 32, 336)          1344      ['quant_block3b_expand_conv[0]\n",
      " rmalization)                                                       [0]']                         \n",
      "                                                                                                  \n",
      " quant_block3b_expand_activ  (None, 32, 32, 336)          3         ['block3b_expand_bn[1][0]']   \n",
      " ation (QuantizeWrapperV2)                                                                        \n",
      "                                                                                                  \n",
      " quant_block3b_dwconv (Quan  (None, 32, 32, 336)          8405      ['quant_block3b_expand_activat\n",
      " tizeWrapperV2)                                                     ion[0][0]']                   \n",
      "                                                                                                  \n",
      " block3b_bn (BatchNormaliza  (None, 32, 32, 336)          1344      ['quant_block3b_dwconv[0][0]']\n",
      " tion)                                                                                            \n",
      "                                                                                                  \n",
      " block3b_activation (Activa  (None, 32, 32, 336)          0         ['block3b_bn[1][0]']          \n",
      " tion)                                                                                            \n",
      "                                                                                                  \n",
      " block3b_se_squeeze (Global  (None, 336)                  0         ['block3b_activation[1][0]']  \n",
      " AveragePooling2D)                                                                                \n",
      "                                                                                                  \n",
      " block3b_se_reshape (Reshap  (None, 1, 1, 336)            0         ['block3b_se_squeeze[1][0]']  \n",
      " e)                                                                                               \n",
      "                                                                                                  \n",
      " block3b_se_reduce (Conv2D)  (None, 1, 1, 14)             4718      ['block3b_se_reshape[1][0]']  \n",
      "                                                                                                  \n",
      " block3b_se_expand (Conv2D)  (None, 1, 1, 336)            5040      ['block3b_se_reduce[1][0]']   \n",
      "                                                                                                  \n",
      " block3b_se_excite (Multipl  (None, 32, 32, 336)          0         ['block3b_activation[1][0]',  \n",
      " y)                                                                  'block3b_se_expand[1][0]']   \n",
      "                                                                                                  \n",
      " quant_block3b_project_conv  (None, 32, 32, 56)           18931     ['block3b_se_excite[1][0]']   \n",
      "  (QuantizeWrapperV2)                                                                             \n",
      "                                                                                                  \n",
      " block3b_project_bn (BatchN  (None, 32, 32, 56)           224       ['quant_block3b_project_conv[0\n",
      " ormalization)                                                      ][0]']                        \n",
      "                                                                                                  \n",
      " block3b_drop (Dropout)      (None, 32, 32, 56)           0         ['block3b_project_bn[1][0]']  \n",
      "                                                                                                  \n",
      " quant_block3b_add (Quantiz  (None, 32, 32, 56)           3         ['block3b_drop[1][0]',        \n",
      " eWrapperV2)                                                         'block3a_project_bn[1][0]']  \n",
      "                                                                                                  \n",
      " quant_block3c_expand_conv   (None, 32, 32, 336)          19491     ['quant_block3b_add[0][0]']   \n",
      " (QuantizeWrapperV2)                                                                              \n",
      "                                                                                                  \n",
      " block3c_expand_bn (BatchNo  (None, 32, 32, 336)          1344      ['quant_block3c_expand_conv[0]\n",
      " rmalization)                                                       [0]']                         \n",
      "                                                                                                  \n",
      " quant_block3c_expand_activ  (None, 32, 32, 336)          3         ['block3c_expand_bn[1][0]']   \n",
      " ation (QuantizeWrapperV2)                                                                        \n",
      "                                                                                                  \n",
      " quant_block3c_dwconv (Quan  (None, 32, 32, 336)          8405      ['quant_block3c_expand_activat\n",
      " tizeWrapperV2)                                                     ion[0][0]']                   \n",
      "                                                                                                  \n",
      " block3c_bn (BatchNormaliza  (None, 32, 32, 336)          1344      ['quant_block3c_dwconv[0][0]']\n",
      " tion)                                                                                            \n",
      "                                                                                                  \n",
      " block3c_activation (Activa  (None, 32, 32, 336)          0         ['block3c_bn[1][0]']          \n",
      " tion)                                                                                            \n",
      "                                                                                                  \n",
      " block3c_se_squeeze (Global  (None, 336)                  0         ['block3c_activation[1][0]']  \n",
      " AveragePooling2D)                                                                                \n",
      "                                                                                                  \n",
      " block3c_se_reshape (Reshap  (None, 1, 1, 336)            0         ['block3c_se_squeeze[1][0]']  \n",
      " e)                                                                                               \n",
      "                                                                                                  \n",
      " block3c_se_reduce (Conv2D)  (None, 1, 1, 14)             4718      ['block3c_se_reshape[1][0]']  \n",
      "                                                                                                  \n",
      " block3c_se_expand (Conv2D)  (None, 1, 1, 336)            5040      ['block3c_se_reduce[1][0]']   \n",
      "                                                                                                  \n",
      " block3c_se_excite (Multipl  (None, 32, 32, 336)          0         ['block3c_activation[1][0]',  \n",
      " y)                                                                  'block3c_se_expand[1][0]']   \n",
      "                                                                                                  \n",
      " quant_block3c_project_conv  (None, 32, 32, 56)           18931     ['block3c_se_excite[1][0]']   \n",
      "  (QuantizeWrapperV2)                                                                             \n",
      "                                                                                                  \n",
      " block3c_project_bn (BatchN  (None, 32, 32, 56)           224       ['quant_block3c_project_conv[0\n",
      " ormalization)                                                      ][0]']                        \n",
      "                                                                                                  \n",
      " block3c_drop (Dropout)      (None, 32, 32, 56)           0         ['block3c_project_bn[1][0]']  \n",
      "                                                                                                  \n",
      " quant_block3c_add (Quantiz  (None, 32, 32, 56)           3         ['block3c_drop[1][0]',        \n",
      " eWrapperV2)                                                         'quant_block3b_add[0][0]']   \n",
      "                                                                                                  \n",
      " quant_block3d_expand_conv   (None, 32, 32, 336)          19491     ['quant_block3c_add[0][0]']   \n",
      " (QuantizeWrapperV2)                                                                              \n",
      "                                                                                                  \n",
      " block3d_expand_bn (BatchNo  (None, 32, 32, 336)          1344      ['quant_block3d_expand_conv[0]\n",
      " rmalization)                                                       [0]']                         \n",
      "                                                                                                  \n",
      " quant_block3d_expand_activ  (None, 32, 32, 336)          3         ['block3d_expand_bn[1][0]']   \n",
      " ation (QuantizeWrapperV2)                                                                        \n",
      "                                                                                                  \n",
      " quant_block3d_dwconv (Quan  (None, 32, 32, 336)          8405      ['quant_block3d_expand_activat\n",
      " tizeWrapperV2)                                                     ion[0][0]']                   \n",
      "                                                                                                  \n",
      " block3d_bn (BatchNormaliza  (None, 32, 32, 336)          1344      ['quant_block3d_dwconv[0][0]']\n",
      " tion)                                                                                            \n",
      "                                                                                                  \n",
      " block3d_activation (Activa  (None, 32, 32, 336)          0         ['block3d_bn[1][0]']          \n",
      " tion)                                                                                            \n",
      "                                                                                                  \n",
      " block3d_se_squeeze (Global  (None, 336)                  0         ['block3d_activation[1][0]']  \n",
      " AveragePooling2D)                                                                                \n",
      "                                                                                                  \n",
      " block3d_se_reshape (Reshap  (None, 1, 1, 336)            0         ['block3d_se_squeeze[1][0]']  \n",
      " e)                                                                                               \n",
      "                                                                                                  \n",
      " block3d_se_reduce (Conv2D)  (None, 1, 1, 14)             4718      ['block3d_se_reshape[1][0]']  \n",
      "                                                                                                  \n",
      " block3d_se_expand (Conv2D)  (None, 1, 1, 336)            5040      ['block3d_se_reduce[1][0]']   \n",
      "                                                                                                  \n",
      " block3d_se_excite (Multipl  (None, 32, 32, 336)          0         ['block3d_activation[1][0]',  \n",
      " y)                                                                  'block3d_se_expand[1][0]']   \n",
      "                                                                                                  \n",
      " quant_block3d_project_conv  (None, 32, 32, 56)           18931     ['block3d_se_excite[1][0]']   \n",
      "  (QuantizeWrapperV2)                                                                             \n",
      "                                                                                                  \n",
      " block3d_project_bn (BatchN  (None, 32, 32, 56)           224       ['quant_block3d_project_conv[0\n",
      " ormalization)                                                      ][0]']                        \n",
      "                                                                                                  \n",
      " block3d_drop (Dropout)      (None, 32, 32, 56)           0         ['block3d_project_bn[1][0]']  \n",
      "                                                                                                  \n",
      " quant_block3d_add (Quantiz  (None, 32, 32, 56)           3         ['block3d_drop[1][0]',        \n",
      " eWrapperV2)                                                         'quant_block3c_add[0][0]']   \n",
      "                                                                                                  \n",
      " quant_block4a_expand_conv   (None, 32, 32, 336)          19491     ['quant_block3d_add[0][0]']   \n",
      " (QuantizeWrapperV2)                                                                              \n",
      "                                                                                                  \n",
      " block4a_expand_bn (BatchNo  (None, 32, 32, 336)          1344      ['quant_block4a_expand_conv[0]\n",
      " rmalization)                                                       [0]']                         \n",
      "                                                                                                  \n",
      " quant_block4a_expand_activ  (None, 32, 32, 336)          3         ['block4a_expand_bn[1][0]']   \n",
      " ation (QuantizeWrapperV2)                                                                        \n",
      "                                                                                                  \n",
      " quant_block4a_dwconv_pad (  (None, 33, 33, 336)          1         ['quant_block4a_expand_activat\n",
      " QuantizeWrapperV2)                                                 ion[0][0]']                   \n",
      "                                                                                                  \n",
      " quant_block4a_dwconv (Quan  (None, 16, 16, 336)          3029      ['quant_block4a_dwconv_pad[0][\n",
      " tizeWrapperV2)                                                     0]']                          \n",
      "                                                                                                  \n",
      " block4a_bn (BatchNormaliza  (None, 16, 16, 336)          1344      ['quant_block4a_dwconv[0][0]']\n",
      " tion)                                                                                            \n",
      "                                                                                                  \n",
      " block4a_activation (Activa  (None, 16, 16, 336)          0         ['block4a_bn[1][0]']          \n",
      " tion)                                                                                            \n",
      "                                                                                                  \n",
      " block4a_se_squeeze (Global  (None, 336)                  0         ['block4a_activation[1][0]']  \n",
      " AveragePooling2D)                                                                                \n",
      "                                                                                                  \n",
      " block4a_se_reshape (Reshap  (None, 1, 1, 336)            0         ['block4a_se_squeeze[1][0]']  \n",
      " e)                                                                                               \n",
      "                                                                                                  \n",
      " block4a_se_reduce (Conv2D)  (None, 1, 1, 14)             4718      ['block4a_se_reshape[1][0]']  \n",
      "                                                                                                  \n",
      " block4a_se_expand (Conv2D)  (None, 1, 1, 336)            5040      ['block4a_se_reduce[1][0]']   \n",
      "                                                                                                  \n",
      " block4a_se_excite (Multipl  (None, 16, 16, 336)          0         ['block4a_activation[1][0]',  \n",
      " y)                                                                  'block4a_se_expand[1][0]']   \n",
      "                                                                                                  \n",
      " quant_block4a_project_conv  (None, 16, 16, 112)          37859     ['block4a_se_excite[1][0]']   \n",
      "  (QuantizeWrapperV2)                                                                             \n",
      "                                                                                                  \n",
      " block4a_project_bn (BatchN  (None, 16, 16, 112)          448       ['quant_block4a_project_conv[0\n",
      " ormalization)                                                      ][0]']                        \n",
      "                                                                                                  \n",
      " quant_block4b_expand_conv   (None, 16, 16, 672)          76611     ['block4a_project_bn[1][0]']  \n",
      " (QuantizeWrapperV2)                                                                              \n",
      "                                                                                                  \n",
      " block4b_expand_bn (BatchNo  (None, 16, 16, 672)          2688      ['quant_block4b_expand_conv[0]\n",
      " rmalization)                                                       [0]']                         \n",
      "                                                                                                  \n",
      " quant_block4b_expand_activ  (None, 16, 16, 672)          3         ['block4b_expand_bn[1][0]']   \n",
      " ation (QuantizeWrapperV2)                                                                        \n",
      "                                                                                                  \n",
      " quant_block4b_dwconv (Quan  (None, 16, 16, 672)          6053      ['quant_block4b_expand_activat\n",
      " tizeWrapperV2)                                                     ion[0][0]']                   \n",
      "                                                                                                  \n",
      " block4b_bn (BatchNormaliza  (None, 16, 16, 672)          2688      ['quant_block4b_dwconv[0][0]']\n",
      " tion)                                                                                            \n",
      "                                                                                                  \n",
      " block4b_activation (Activa  (None, 16, 16, 672)          0         ['block4b_bn[1][0]']          \n",
      " tion)                                                                                            \n",
      "                                                                                                  \n",
      " block4b_se_squeeze (Global  (None, 672)                  0         ['block4b_activation[1][0]']  \n",
      " AveragePooling2D)                                                                                \n",
      "                                                                                                  \n",
      " block4b_se_reshape (Reshap  (None, 1, 1, 672)            0         ['block4b_se_squeeze[1][0]']  \n",
      " e)                                                                                               \n",
      "                                                                                                  \n",
      " block4b_se_reduce (Conv2D)  (None, 1, 1, 28)             18844     ['block4b_se_reshape[1][0]']  \n",
      "                                                                                                  \n",
      " block4b_se_expand (Conv2D)  (None, 1, 1, 672)            19488     ['block4b_se_reduce[1][0]']   \n",
      "                                                                                                  \n",
      " block4b_se_excite (Multipl  (None, 16, 16, 672)          0         ['block4b_activation[1][0]',  \n",
      " y)                                                                  'block4b_se_expand[1][0]']   \n",
      "                                                                                                  \n",
      " quant_block4b_project_conv  (None, 16, 16, 112)          75491     ['block4b_se_excite[1][0]']   \n",
      "  (QuantizeWrapperV2)                                                                             \n",
      "                                                                                                  \n",
      " block4b_project_bn (BatchN  (None, 16, 16, 112)          448       ['quant_block4b_project_conv[0\n",
      " ormalization)                                                      ][0]']                        \n",
      "                                                                                                  \n",
      " block4b_drop (Dropout)      (None, 16, 16, 112)          0         ['block4b_project_bn[1][0]']  \n",
      "                                                                                                  \n",
      " quant_block4b_add (Quantiz  (None, 16, 16, 112)          3         ['block4b_drop[1][0]',        \n",
      " eWrapperV2)                                                         'block4a_project_bn[1][0]']  \n",
      "                                                                                                  \n",
      " quant_block4c_expand_conv   (None, 16, 16, 672)          76611     ['quant_block4b_add[0][0]']   \n",
      " (QuantizeWrapperV2)                                                                              \n",
      "                                                                                                  \n",
      " block4c_expand_bn (BatchNo  (None, 16, 16, 672)          2688      ['quant_block4c_expand_conv[0]\n",
      " rmalization)                                                       [0]']                         \n",
      "                                                                                                  \n",
      " quant_block4c_expand_activ  (None, 16, 16, 672)          3         ['block4c_expand_bn[1][0]']   \n",
      " ation (QuantizeWrapperV2)                                                                        \n",
      "                                                                                                  \n",
      " quant_block4c_dwconv (Quan  (None, 16, 16, 672)          6053      ['quant_block4c_expand_activat\n",
      " tizeWrapperV2)                                                     ion[0][0]']                   \n",
      "                                                                                                  \n",
      " block4c_bn (BatchNormaliza  (None, 16, 16, 672)          2688      ['quant_block4c_dwconv[0][0]']\n",
      " tion)                                                                                            \n",
      "                                                                                                  \n",
      " block4c_activation (Activa  (None, 16, 16, 672)          0         ['block4c_bn[1][0]']          \n",
      " tion)                                                                                            \n",
      "                                                                                                  \n",
      " block4c_se_squeeze (Global  (None, 672)                  0         ['block4c_activation[1][0]']  \n",
      " AveragePooling2D)                                                                                \n",
      "                                                                                                  \n",
      " block4c_se_reshape (Reshap  (None, 1, 1, 672)            0         ['block4c_se_squeeze[1][0]']  \n",
      " e)                                                                                               \n",
      "                                                                                                  \n",
      " block4c_se_reduce (Conv2D)  (None, 1, 1, 28)             18844     ['block4c_se_reshape[1][0]']  \n",
      "                                                                                                  \n",
      " block4c_se_expand (Conv2D)  (None, 1, 1, 672)            19488     ['block4c_se_reduce[1][0]']   \n",
      "                                                                                                  \n",
      " block4c_se_excite (Multipl  (None, 16, 16, 672)          0         ['block4c_activation[1][0]',  \n",
      " y)                                                                  'block4c_se_expand[1][0]']   \n",
      "                                                                                                  \n",
      " quant_block4c_project_conv  (None, 16, 16, 112)          75491     ['block4c_se_excite[1][0]']   \n",
      "  (QuantizeWrapperV2)                                                                             \n",
      "                                                                                                  \n",
      " block4c_project_bn (BatchN  (None, 16, 16, 112)          448       ['quant_block4c_project_conv[0\n",
      " ormalization)                                                      ][0]']                        \n",
      "                                                                                                  \n",
      " block4c_drop (Dropout)      (None, 16, 16, 112)          0         ['block4c_project_bn[1][0]']  \n",
      "                                                                                                  \n",
      " quant_block4c_add (Quantiz  (None, 16, 16, 112)          3         ['block4c_drop[1][0]',        \n",
      " eWrapperV2)                                                         'quant_block4b_add[0][0]']   \n",
      "                                                                                                  \n",
      " quant_block4d_expand_conv   (None, 16, 16, 672)          76611     ['quant_block4c_add[0][0]']   \n",
      " (QuantizeWrapperV2)                                                                              \n",
      "                                                                                                  \n",
      " block4d_expand_bn (BatchNo  (None, 16, 16, 672)          2688      ['quant_block4d_expand_conv[0]\n",
      " rmalization)                                                       [0]']                         \n",
      "                                                                                                  \n",
      " quant_block4d_expand_activ  (None, 16, 16, 672)          3         ['block4d_expand_bn[1][0]']   \n",
      " ation (QuantizeWrapperV2)                                                                        \n",
      "                                                                                                  \n",
      " quant_block4d_dwconv (Quan  (None, 16, 16, 672)          6053      ['quant_block4d_expand_activat\n",
      " tizeWrapperV2)                                                     ion[0][0]']                   \n",
      "                                                                                                  \n",
      " block4d_bn (BatchNormaliza  (None, 16, 16, 672)          2688      ['quant_block4d_dwconv[0][0]']\n",
      " tion)                                                                                            \n",
      "                                                                                                  \n",
      " block4d_activation (Activa  (None, 16, 16, 672)          0         ['block4d_bn[1][0]']          \n",
      " tion)                                                                                            \n",
      "                                                                                                  \n",
      " block4d_se_squeeze (Global  (None, 672)                  0         ['block4d_activation[1][0]']  \n",
      " AveragePooling2D)                                                                                \n",
      "                                                                                                  \n",
      " block4d_se_reshape (Reshap  (None, 1, 1, 672)            0         ['block4d_se_squeeze[1][0]']  \n",
      " e)                                                                                               \n",
      "                                                                                                  \n",
      " block4d_se_reduce (Conv2D)  (None, 1, 1, 28)             18844     ['block4d_se_reshape[1][0]']  \n",
      "                                                                                                  \n",
      " block4d_se_expand (Conv2D)  (None, 1, 1, 672)            19488     ['block4d_se_reduce[1][0]']   \n",
      "                                                                                                  \n",
      " block4d_se_excite (Multipl  (None, 16, 16, 672)          0         ['block4d_activation[1][0]',  \n",
      " y)                                                                  'block4d_se_expand[1][0]']   \n",
      "                                                                                                  \n",
      " quant_block4d_project_conv  (None, 16, 16, 112)          75491     ['block4d_se_excite[1][0]']   \n",
      "  (QuantizeWrapperV2)                                                                             \n",
      "                                                                                                  \n",
      " block4d_project_bn (BatchN  (None, 16, 16, 112)          448       ['quant_block4d_project_conv[0\n",
      " ormalization)                                                      ][0]']                        \n",
      "                                                                                                  \n",
      " block4d_drop (Dropout)      (None, 16, 16, 112)          0         ['block4d_project_bn[1][0]']  \n",
      "                                                                                                  \n",
      " quant_block4d_add (Quantiz  (None, 16, 16, 112)          3         ['block4d_drop[1][0]',        \n",
      " eWrapperV2)                                                         'quant_block4c_add[0][0]']   \n",
      "                                                                                                  \n",
      " quant_block4e_expand_conv   (None, 16, 16, 672)          76611     ['quant_block4d_add[0][0]']   \n",
      " (QuantizeWrapperV2)                                                                              \n",
      "                                                                                                  \n",
      " block4e_expand_bn (BatchNo  (None, 16, 16, 672)          2688      ['quant_block4e_expand_conv[0]\n",
      " rmalization)                                                       [0]']                         \n",
      "                                                                                                  \n",
      " quant_block4e_expand_activ  (None, 16, 16, 672)          3         ['block4e_expand_bn[1][0]']   \n",
      " ation (QuantizeWrapperV2)                                                                        \n",
      "                                                                                                  \n",
      " quant_block4e_dwconv (Quan  (None, 16, 16, 672)          6053      ['quant_block4e_expand_activat\n",
      " tizeWrapperV2)                                                     ion[0][0]']                   \n",
      "                                                                                                  \n",
      " block4e_bn (BatchNormaliza  (None, 16, 16, 672)          2688      ['quant_block4e_dwconv[0][0]']\n",
      " tion)                                                                                            \n",
      "                                                                                                  \n",
      " block4e_activation (Activa  (None, 16, 16, 672)          0         ['block4e_bn[1][0]']          \n",
      " tion)                                                                                            \n",
      "                                                                                                  \n",
      " block4e_se_squeeze (Global  (None, 672)                  0         ['block4e_activation[1][0]']  \n",
      " AveragePooling2D)                                                                                \n",
      "                                                                                                  \n",
      " block4e_se_reshape (Reshap  (None, 1, 1, 672)            0         ['block4e_se_squeeze[1][0]']  \n",
      " e)                                                                                               \n",
      "                                                                                                  \n",
      " block4e_se_reduce (Conv2D)  (None, 1, 1, 28)             18844     ['block4e_se_reshape[1][0]']  \n",
      "                                                                                                  \n",
      " block4e_se_expand (Conv2D)  (None, 1, 1, 672)            19488     ['block4e_se_reduce[1][0]']   \n",
      "                                                                                                  \n",
      " block4e_se_excite (Multipl  (None, 16, 16, 672)          0         ['block4e_activation[1][0]',  \n",
      " y)                                                                  'block4e_se_expand[1][0]']   \n",
      "                                                                                                  \n",
      " quant_block4e_project_conv  (None, 16, 16, 112)          75491     ['block4e_se_excite[1][0]']   \n",
      "  (QuantizeWrapperV2)                                                                             \n",
      "                                                                                                  \n",
      " block4e_project_bn (BatchN  (None, 16, 16, 112)          448       ['quant_block4e_project_conv[0\n",
      " ormalization)                                                      ][0]']                        \n",
      "                                                                                                  \n",
      " block4e_drop (Dropout)      (None, 16, 16, 112)          0         ['block4e_project_bn[1][0]']  \n",
      "                                                                                                  \n",
      " quant_block4e_add (Quantiz  (None, 16, 16, 112)          3         ['block4e_drop[1][0]',        \n",
      " eWrapperV2)                                                         'quant_block4d_add[0][0]']   \n",
      "                                                                                                  \n",
      " quant_block4f_expand_conv   (None, 16, 16, 672)          76611     ['quant_block4e_add[0][0]']   \n",
      " (QuantizeWrapperV2)                                                                              \n",
      "                                                                                                  \n",
      " block4f_expand_bn (BatchNo  (None, 16, 16, 672)          2688      ['quant_block4f_expand_conv[0]\n",
      " rmalization)                                                       [0]']                         \n",
      "                                                                                                  \n",
      " quant_block4f_expand_activ  (None, 16, 16, 672)          3         ['block4f_expand_bn[1][0]']   \n",
      " ation (QuantizeWrapperV2)                                                                        \n",
      "                                                                                                  \n",
      " quant_block4f_dwconv (Quan  (None, 16, 16, 672)          6053      ['quant_block4f_expand_activat\n",
      " tizeWrapperV2)                                                     ion[0][0]']                   \n",
      "                                                                                                  \n",
      " block4f_bn (BatchNormaliza  (None, 16, 16, 672)          2688      ['quant_block4f_dwconv[0][0]']\n",
      " tion)                                                                                            \n",
      "                                                                                                  \n",
      " block4f_activation (Activa  (None, 16, 16, 672)          0         ['block4f_bn[1][0]']          \n",
      " tion)                                                                                            \n",
      "                                                                                                  \n",
      " block4f_se_squeeze (Global  (None, 672)                  0         ['block4f_activation[1][0]']  \n",
      " AveragePooling2D)                                                                                \n",
      "                                                                                                  \n",
      " block4f_se_reshape (Reshap  (None, 1, 1, 672)            0         ['block4f_se_squeeze[1][0]']  \n",
      " e)                                                                                               \n",
      "                                                                                                  \n",
      " block4f_se_reduce (Conv2D)  (None, 1, 1, 28)             18844     ['block4f_se_reshape[1][0]']  \n",
      "                                                                                                  \n",
      " block4f_se_expand (Conv2D)  (None, 1, 1, 672)            19488     ['block4f_se_reduce[1][0]']   \n",
      "                                                                                                  \n",
      " block4f_se_excite (Multipl  (None, 16, 16, 672)          0         ['block4f_activation[1][0]',  \n",
      " y)                                                                  'block4f_se_expand[1][0]']   \n",
      "                                                                                                  \n",
      " quant_block4f_project_conv  (None, 16, 16, 112)          75491     ['block4f_se_excite[1][0]']   \n",
      "  (QuantizeWrapperV2)                                                                             \n",
      "                                                                                                  \n",
      " block4f_project_bn (BatchN  (None, 16, 16, 112)          448       ['quant_block4f_project_conv[0\n",
      " ormalization)                                                      ][0]']                        \n",
      "                                                                                                  \n",
      " block4f_drop (Dropout)      (None, 16, 16, 112)          0         ['block4f_project_bn[1][0]']  \n",
      "                                                                                                  \n",
      " quant_block4f_add (Quantiz  (None, 16, 16, 112)          3         ['block4f_drop[1][0]',        \n",
      " eWrapperV2)                                                         'quant_block4e_add[0][0]']   \n",
      "                                                                                                  \n",
      " quant_block5a_expand_conv   (None, 16, 16, 672)          76611     ['quant_block4f_add[0][0]']   \n",
      " (QuantizeWrapperV2)                                                                              \n",
      "                                                                                                  \n",
      " block5a_expand_bn (BatchNo  (None, 16, 16, 672)          2688      ['quant_block5a_expand_conv[0]\n",
      " rmalization)                                                       [0]']                         \n",
      "                                                                                                  \n",
      " quant_block5a_expand_activ  (None, 16, 16, 672)          3         ['block5a_expand_bn[1][0]']   \n",
      " ation (QuantizeWrapperV2)                                                                        \n",
      "                                                                                                  \n",
      " quant_block5a_dwconv (Quan  (None, 16, 16, 672)          16805     ['quant_block5a_expand_activat\n",
      " tizeWrapperV2)                                                     ion[0][0]']                   \n",
      "                                                                                                  \n",
      " block5a_bn (BatchNormaliza  (None, 16, 16, 672)          2688      ['quant_block5a_dwconv[0][0]']\n",
      " tion)                                                                                            \n",
      "                                                                                                  \n",
      " block5a_activation (Activa  (None, 16, 16, 672)          0         ['block5a_bn[1][0]']          \n",
      " tion)                                                                                            \n",
      "                                                                                                  \n",
      " block5a_se_squeeze (Global  (None, 672)                  0         ['block5a_activation[1][0]']  \n",
      " AveragePooling2D)                                                                                \n",
      "                                                                                                  \n",
      " block5a_se_reshape (Reshap  (None, 1, 1, 672)            0         ['block5a_se_squeeze[1][0]']  \n",
      " e)                                                                                               \n",
      "                                                                                                  \n",
      " block5a_se_reduce (Conv2D)  (None, 1, 1, 28)             18844     ['block5a_se_reshape[1][0]']  \n",
      "                                                                                                  \n",
      " block5a_se_expand (Conv2D)  (None, 1, 1, 672)            19488     ['block5a_se_reduce[1][0]']   \n",
      "                                                                                                  \n",
      " block5a_se_excite (Multipl  (None, 16, 16, 672)          0         ['block5a_activation[1][0]',  \n",
      " y)                                                                  'block5a_se_expand[1][0]']   \n",
      "                                                                                                  \n",
      " quant_block5a_project_conv  (None, 16, 16, 160)          107843    ['block5a_se_excite[1][0]']   \n",
      "  (QuantizeWrapperV2)                                                                             \n",
      "                                                                                                  \n",
      " block5a_project_bn (BatchN  (None, 16, 16, 160)          640       ['quant_block5a_project_conv[0\n",
      " ormalization)                                                      ][0]']                        \n",
      "                                                                                                  \n",
      " quant_block5b_expand_conv   (None, 16, 16, 960)          155523    ['block5a_project_bn[1][0]']  \n",
      " (QuantizeWrapperV2)                                                                              \n",
      "                                                                                                  \n",
      " block5b_expand_bn (BatchNo  (None, 16, 16, 960)          3840      ['quant_block5b_expand_conv[0]\n",
      " rmalization)                                                       [0]']                         \n",
      "                                                                                                  \n",
      " quant_block5b_expand_activ  (None, 16, 16, 960)          3         ['block5b_expand_bn[1][0]']   \n",
      " ation (QuantizeWrapperV2)                                                                        \n",
      "                                                                                                  \n",
      " quant_block5b_dwconv (Quan  (None, 16, 16, 960)          24005     ['quant_block5b_expand_activat\n",
      " tizeWrapperV2)                                                     ion[0][0]']                   \n",
      "                                                                                                  \n",
      " block5b_bn (BatchNormaliza  (None, 16, 16, 960)          3840      ['quant_block5b_dwconv[0][0]']\n",
      " tion)                                                                                            \n",
      "                                                                                                  \n",
      " block5b_activation (Activa  (None, 16, 16, 960)          0         ['block5b_bn[1][0]']          \n",
      " tion)                                                                                            \n",
      "                                                                                                  \n",
      " block5b_se_squeeze (Global  (None, 960)                  0         ['block5b_activation[1][0]']  \n",
      " AveragePooling2D)                                                                                \n",
      "                                                                                                  \n",
      " block5b_se_reshape (Reshap  (None, 1, 1, 960)            0         ['block5b_se_squeeze[1][0]']  \n",
      " e)                                                                                               \n",
      "                                                                                                  \n",
      " block5b_se_reduce (Conv2D)  (None, 1, 1, 40)             38440     ['block5b_se_reshape[1][0]']  \n",
      "                                                                                                  \n",
      " block5b_se_expand (Conv2D)  (None, 1, 1, 960)            39360     ['block5b_se_reduce[1][0]']   \n",
      "                                                                                                  \n",
      " block5b_se_excite (Multipl  (None, 16, 16, 960)          0         ['block5b_activation[1][0]',  \n",
      " y)                                                                  'block5b_se_expand[1][0]']   \n",
      "                                                                                                  \n",
      " quant_block5b_project_conv  (None, 16, 16, 160)          153923    ['block5b_se_excite[1][0]']   \n",
      "  (QuantizeWrapperV2)                                                                             \n",
      "                                                                                                  \n",
      " block5b_project_bn (BatchN  (None, 16, 16, 160)          640       ['quant_block5b_project_conv[0\n",
      " ormalization)                                                      ][0]']                        \n",
      "                                                                                                  \n",
      " block5b_drop (Dropout)      (None, 16, 16, 160)          0         ['block5b_project_bn[1][0]']  \n",
      "                                                                                                  \n",
      " quant_block5b_add (Quantiz  (None, 16, 16, 160)          3         ['block5b_drop[1][0]',        \n",
      " eWrapperV2)                                                         'block5a_project_bn[1][0]']  \n",
      "                                                                                                  \n",
      " quant_block5c_expand_conv   (None, 16, 16, 960)          155523    ['quant_block5b_add[0][0]']   \n",
      " (QuantizeWrapperV2)                                                                              \n",
      "                                                                                                  \n",
      " block5c_expand_bn (BatchNo  (None, 16, 16, 960)          3840      ['quant_block5c_expand_conv[0]\n",
      " rmalization)                                                       [0]']                         \n",
      "                                                                                                  \n",
      " quant_block5c_expand_activ  (None, 16, 16, 960)          3         ['block5c_expand_bn[1][0]']   \n",
      " ation (QuantizeWrapperV2)                                                                        \n",
      "                                                                                                  \n",
      " quant_block5c_dwconv (Quan  (None, 16, 16, 960)          24005     ['quant_block5c_expand_activat\n",
      " tizeWrapperV2)                                                     ion[0][0]']                   \n",
      "                                                                                                  \n",
      " block5c_bn (BatchNormaliza  (None, 16, 16, 960)          3840      ['quant_block5c_dwconv[0][0]']\n",
      " tion)                                                                                            \n",
      "                                                                                                  \n",
      " block5c_activation (Activa  (None, 16, 16, 960)          0         ['block5c_bn[1][0]']          \n",
      " tion)                                                                                            \n",
      "                                                                                                  \n",
      " block5c_se_squeeze (Global  (None, 960)                  0         ['block5c_activation[1][0]']  \n",
      " AveragePooling2D)                                                                                \n",
      "                                                                                                  \n",
      " block5c_se_reshape (Reshap  (None, 1, 1, 960)            0         ['block5c_se_squeeze[1][0]']  \n",
      " e)                                                                                               \n",
      "                                                                                                  \n",
      " block5c_se_reduce (Conv2D)  (None, 1, 1, 40)             38440     ['block5c_se_reshape[1][0]']  \n",
      "                                                                                                  \n",
      " block5c_se_expand (Conv2D)  (None, 1, 1, 960)            39360     ['block5c_se_reduce[1][0]']   \n",
      "                                                                                                  \n",
      " block5c_se_excite (Multipl  (None, 16, 16, 960)          0         ['block5c_activation[1][0]',  \n",
      " y)                                                                  'block5c_se_expand[1][0]']   \n",
      "                                                                                                  \n",
      " quant_block5c_project_conv  (None, 16, 16, 160)          153923    ['block5c_se_excite[1][0]']   \n",
      "  (QuantizeWrapperV2)                                                                             \n",
      "                                                                                                  \n",
      " block5c_project_bn (BatchN  (None, 16, 16, 160)          640       ['quant_block5c_project_conv[0\n",
      " ormalization)                                                      ][0]']                        \n",
      "                                                                                                  \n",
      " block5c_drop (Dropout)      (None, 16, 16, 160)          0         ['block5c_project_bn[1][0]']  \n",
      "                                                                                                  \n",
      " quant_block5c_add (Quantiz  (None, 16, 16, 160)          3         ['block5c_drop[1][0]',        \n",
      " eWrapperV2)                                                         'quant_block5b_add[0][0]']   \n",
      "                                                                                                  \n",
      " quant_block5d_expand_conv   (None, 16, 16, 960)          155523    ['quant_block5c_add[0][0]']   \n",
      " (QuantizeWrapperV2)                                                                              \n",
      "                                                                                                  \n",
      " block5d_expand_bn (BatchNo  (None, 16, 16, 960)          3840      ['quant_block5d_expand_conv[0]\n",
      " rmalization)                                                       [0]']                         \n",
      "                                                                                                  \n",
      " quant_block5d_expand_activ  (None, 16, 16, 960)          3         ['block5d_expand_bn[1][0]']   \n",
      " ation (QuantizeWrapperV2)                                                                        \n",
      "                                                                                                  \n",
      " quant_block5d_dwconv (Quan  (None, 16, 16, 960)          24005     ['quant_block5d_expand_activat\n",
      " tizeWrapperV2)                                                     ion[0][0]']                   \n",
      "                                                                                                  \n",
      " block5d_bn (BatchNormaliza  (None, 16, 16, 960)          3840      ['quant_block5d_dwconv[0][0]']\n",
      " tion)                                                                                            \n",
      "                                                                                                  \n",
      " block5d_activation (Activa  (None, 16, 16, 960)          0         ['block5d_bn[1][0]']          \n",
      " tion)                                                                                            \n",
      "                                                                                                  \n",
      " block5d_se_squeeze (Global  (None, 960)                  0         ['block5d_activation[1][0]']  \n",
      " AveragePooling2D)                                                                                \n",
      "                                                                                                  \n",
      " block5d_se_reshape (Reshap  (None, 1, 1, 960)            0         ['block5d_se_squeeze[1][0]']  \n",
      " e)                                                                                               \n",
      "                                                                                                  \n",
      " block5d_se_reduce (Conv2D)  (None, 1, 1, 40)             38440     ['block5d_se_reshape[1][0]']  \n",
      "                                                                                                  \n",
      " block5d_se_expand (Conv2D)  (None, 1, 1, 960)            39360     ['block5d_se_reduce[1][0]']   \n",
      "                                                                                                  \n",
      " block5d_se_excite (Multipl  (None, 16, 16, 960)          0         ['block5d_activation[1][0]',  \n",
      " y)                                                                  'block5d_se_expand[1][0]']   \n",
      "                                                                                                  \n",
      " quant_block5d_project_conv  (None, 16, 16, 160)          153923    ['block5d_se_excite[1][0]']   \n",
      "  (QuantizeWrapperV2)                                                                             \n",
      "                                                                                                  \n",
      " block5d_project_bn (BatchN  (None, 16, 16, 160)          640       ['quant_block5d_project_conv[0\n",
      " ormalization)                                                      ][0]']                        \n",
      "                                                                                                  \n",
      " block5d_drop (Dropout)      (None, 16, 16, 160)          0         ['block5d_project_bn[1][0]']  \n",
      "                                                                                                  \n",
      " quant_block5d_add (Quantiz  (None, 16, 16, 160)          3         ['block5d_drop[1][0]',        \n",
      " eWrapperV2)                                                         'quant_block5c_add[0][0]']   \n",
      "                                                                                                  \n",
      " quant_block5e_expand_conv   (None, 16, 16, 960)          155523    ['quant_block5d_add[0][0]']   \n",
      " (QuantizeWrapperV2)                                                                              \n",
      "                                                                                                  \n",
      " block5e_expand_bn (BatchNo  (None, 16, 16, 960)          3840      ['quant_block5e_expand_conv[0]\n",
      " rmalization)                                                       [0]']                         \n",
      "                                                                                                  \n",
      " quant_block5e_expand_activ  (None, 16, 16, 960)          3         ['block5e_expand_bn[1][0]']   \n",
      " ation (QuantizeWrapperV2)                                                                        \n",
      "                                                                                                  \n",
      " quant_block5e_dwconv (Quan  (None, 16, 16, 960)          24005     ['quant_block5e_expand_activat\n",
      " tizeWrapperV2)                                                     ion[0][0]']                   \n",
      "                                                                                                  \n",
      " block5e_bn (BatchNormaliza  (None, 16, 16, 960)          3840      ['quant_block5e_dwconv[0][0]']\n",
      " tion)                                                                                            \n",
      "                                                                                                  \n",
      " block5e_activation (Activa  (None, 16, 16, 960)          0         ['block5e_bn[1][0]']          \n",
      " tion)                                                                                            \n",
      "                                                                                                  \n",
      " block5e_se_squeeze (Global  (None, 960)                  0         ['block5e_activation[1][0]']  \n",
      " AveragePooling2D)                                                                                \n",
      "                                                                                                  \n",
      " block5e_se_reshape (Reshap  (None, 1, 1, 960)            0         ['block5e_se_squeeze[1][0]']  \n",
      " e)                                                                                               \n",
      "                                                                                                  \n",
      " block5e_se_reduce (Conv2D)  (None, 1, 1, 40)             38440     ['block5e_se_reshape[1][0]']  \n",
      "                                                                                                  \n",
      " block5e_se_expand (Conv2D)  (None, 1, 1, 960)            39360     ['block5e_se_reduce[1][0]']   \n",
      "                                                                                                  \n",
      " block5e_se_excite (Multipl  (None, 16, 16, 960)          0         ['block5e_activation[1][0]',  \n",
      " y)                                                                  'block5e_se_expand[1][0]']   \n",
      "                                                                                                  \n",
      " quant_block5e_project_conv  (None, 16, 16, 160)          153923    ['block5e_se_excite[1][0]']   \n",
      "  (QuantizeWrapperV2)                                                                             \n",
      "                                                                                                  \n",
      " block5e_project_bn (BatchN  (None, 16, 16, 160)          640       ['quant_block5e_project_conv[0\n",
      " ormalization)                                                      ][0]']                        \n",
      "                                                                                                  \n",
      " block5e_drop (Dropout)      (None, 16, 16, 160)          0         ['block5e_project_bn[1][0]']  \n",
      "                                                                                                  \n",
      " quant_block5e_add (Quantiz  (None, 16, 16, 160)          3         ['block5e_drop[1][0]',        \n",
      " eWrapperV2)                                                         'quant_block5d_add[0][0]']   \n",
      "                                                                                                  \n",
      " quant_block5f_expand_conv   (None, 16, 16, 960)          155523    ['quant_block5e_add[0][0]']   \n",
      " (QuantizeWrapperV2)                                                                              \n",
      "                                                                                                  \n",
      " block5f_expand_bn (BatchNo  (None, 16, 16, 960)          3840      ['quant_block5f_expand_conv[0]\n",
      " rmalization)                                                       [0]']                         \n",
      "                                                                                                  \n",
      " quant_block5f_expand_activ  (None, 16, 16, 960)          3         ['block5f_expand_bn[1][0]']   \n",
      " ation (QuantizeWrapperV2)                                                                        \n",
      "                                                                                                  \n",
      " quant_block5f_dwconv (Quan  (None, 16, 16, 960)          24005     ['quant_block5f_expand_activat\n",
      " tizeWrapperV2)                                                     ion[0][0]']                   \n",
      "                                                                                                  \n",
      " block5f_bn (BatchNormaliza  (None, 16, 16, 960)          3840      ['quant_block5f_dwconv[0][0]']\n",
      " tion)                                                                                            \n",
      "                                                                                                  \n",
      " block5f_activation (Activa  (None, 16, 16, 960)          0         ['block5f_bn[1][0]']          \n",
      " tion)                                                                                            \n",
      "                                                                                                  \n",
      " block5f_se_squeeze (Global  (None, 960)                  0         ['block5f_activation[1][0]']  \n",
      " AveragePooling2D)                                                                                \n",
      "                                                                                                  \n",
      " block5f_se_reshape (Reshap  (None, 1, 1, 960)            0         ['block5f_se_squeeze[1][0]']  \n",
      " e)                                                                                               \n",
      "                                                                                                  \n",
      " block5f_se_reduce (Conv2D)  (None, 1, 1, 40)             38440     ['block5f_se_reshape[1][0]']  \n",
      "                                                                                                  \n",
      " block5f_se_expand (Conv2D)  (None, 1, 1, 960)            39360     ['block5f_se_reduce[1][0]']   \n",
      "                                                                                                  \n",
      " block5f_se_excite (Multipl  (None, 16, 16, 960)          0         ['block5f_activation[1][0]',  \n",
      " y)                                                                  'block5f_se_expand[1][0]']   \n",
      "                                                                                                  \n",
      " quant_block5f_project_conv  (None, 16, 16, 160)          153923    ['block5f_se_excite[1][0]']   \n",
      "  (QuantizeWrapperV2)                                                                             \n",
      "                                                                                                  \n",
      " block5f_project_bn (BatchN  (None, 16, 16, 160)          640       ['quant_block5f_project_conv[0\n",
      " ormalization)                                                      ][0]']                        \n",
      "                                                                                                  \n",
      " block5f_drop (Dropout)      (None, 16, 16, 160)          0         ['block5f_project_bn[1][0]']  \n",
      "                                                                                                  \n",
      " quant_block5f_add (Quantiz  (None, 16, 16, 160)          3         ['block5f_drop[1][0]',        \n",
      " eWrapperV2)                                                         'quant_block5e_add[0][0]']   \n",
      "                                                                                                  \n",
      " quant_block6a_expand_conv   (None, 16, 16, 960)          155523    ['quant_block5f_add[0][0]']   \n",
      " (QuantizeWrapperV2)                                                                              \n",
      "                                                                                                  \n",
      " block6a_expand_bn (BatchNo  (None, 16, 16, 960)          3840      ['quant_block6a_expand_conv[0]\n",
      " rmalization)                                                       [0]']                         \n",
      "                                                                                                  \n",
      " quant_block6a_expand_activ  (None, 16, 16, 960)          3         ['block6a_expand_bn[1][0]']   \n",
      " ation (QuantizeWrapperV2)                                                                        \n",
      "                                                                                                  \n",
      " quant_block6a_dwconv_pad (  (None, 19, 19, 960)          1         ['quant_block6a_expand_activat\n",
      " QuantizeWrapperV2)                                                 ion[0][0]']                   \n",
      "                                                                                                  \n",
      " quant_block6a_dwconv (Quan  (None, 8, 8, 960)            24005     ['quant_block6a_dwconv_pad[0][\n",
      " tizeWrapperV2)                                                     0]']                          \n",
      "                                                                                                  \n",
      " block6a_bn (BatchNormaliza  (None, 8, 8, 960)            3840      ['quant_block6a_dwconv[0][0]']\n",
      " tion)                                                                                            \n",
      "                                                                                                  \n",
      " block6a_activation (Activa  (None, 8, 8, 960)            0         ['block6a_bn[1][0]']          \n",
      " tion)                                                                                            \n",
      "                                                                                                  \n",
      " block6a_se_squeeze (Global  (None, 960)                  0         ['block6a_activation[1][0]']  \n",
      " AveragePooling2D)                                                                                \n",
      "                                                                                                  \n",
      " block6a_se_reshape (Reshap  (None, 1, 1, 960)            0         ['block6a_se_squeeze[1][0]']  \n",
      " e)                                                                                               \n",
      "                                                                                                  \n",
      " block6a_se_reduce (Conv2D)  (None, 1, 1, 40)             38440     ['block6a_se_reshape[1][0]']  \n",
      "                                                                                                  \n",
      " block6a_se_expand (Conv2D)  (None, 1, 1, 960)            39360     ['block6a_se_reduce[1][0]']   \n",
      "                                                                                                  \n",
      " block6a_se_excite (Multipl  (None, 8, 8, 960)            0         ['block6a_activation[1][0]',  \n",
      " y)                                                                  'block6a_se_expand[1][0]']   \n",
      "                                                                                                  \n",
      " quant_block6a_project_conv  (None, 8, 8, 272)            261667    ['block6a_se_excite[1][0]']   \n",
      "  (QuantizeWrapperV2)                                                                             \n",
      "                                                                                                  \n",
      " block6a_project_bn (BatchN  (None, 8, 8, 272)            1088      ['quant_block6a_project_conv[0\n",
      " ormalization)                                                      ][0]']                        \n",
      "                                                                                                  \n",
      " quant_block6b_expand_conv   (None, 8, 8, 1632)           447171    ['block6a_project_bn[1][0]']  \n",
      " (QuantizeWrapperV2)                                                                              \n",
      "                                                                                                  \n",
      " block6b_expand_bn (BatchNo  (None, 8, 8, 1632)           6528      ['quant_block6b_expand_conv[0]\n",
      " rmalization)                                                       [0]']                         \n",
      "                                                                                                  \n",
      " quant_block6b_expand_activ  (None, 8, 8, 1632)           3         ['block6b_expand_bn[1][0]']   \n",
      " ation (QuantizeWrapperV2)                                                                        \n",
      "                                                                                                  \n",
      " quant_block6b_dwconv (Quan  (None, 8, 8, 1632)           40805     ['quant_block6b_expand_activat\n",
      " tizeWrapperV2)                                                     ion[0][0]']                   \n",
      "                                                                                                  \n",
      " block6b_bn (BatchNormaliza  (None, 8, 8, 1632)           6528      ['quant_block6b_dwconv[0][0]']\n",
      " tion)                                                                                            \n",
      "                                                                                                  \n",
      " block6b_activation (Activa  (None, 8, 8, 1632)           0         ['block6b_bn[1][0]']          \n",
      " tion)                                                                                            \n",
      "                                                                                                  \n",
      " block6b_se_squeeze (Global  (None, 1632)                 0         ['block6b_activation[1][0]']  \n",
      " AveragePooling2D)                                                                                \n",
      "                                                                                                  \n",
      " block6b_se_reshape (Reshap  (None, 1, 1, 1632)           0         ['block6b_se_squeeze[1][0]']  \n",
      " e)                                                                                               \n",
      "                                                                                                  \n",
      " block6b_se_reduce (Conv2D)  (None, 1, 1, 68)             111044    ['block6b_se_reshape[1][0]']  \n",
      "                                                                                                  \n",
      " block6b_se_expand (Conv2D)  (None, 1, 1, 1632)           112608    ['block6b_se_reduce[1][0]']   \n",
      "                                                                                                  \n",
      " block6b_se_excite (Multipl  (None, 8, 8, 1632)           0         ['block6b_activation[1][0]',  \n",
      " y)                                                                  'block6b_se_expand[1][0]']   \n",
      "                                                                                                  \n",
      " quant_block6b_project_conv  (None, 8, 8, 272)            444451    ['block6b_se_excite[1][0]']   \n",
      "  (QuantizeWrapperV2)                                                                             \n",
      "                                                                                                  \n",
      " block6b_project_bn (BatchN  (None, 8, 8, 272)            1088      ['quant_block6b_project_conv[0\n",
      " ormalization)                                                      ][0]']                        \n",
      "                                                                                                  \n",
      " block6b_drop (Dropout)      (None, 8, 8, 272)            0         ['block6b_project_bn[1][0]']  \n",
      "                                                                                                  \n",
      " quant_block6b_add (Quantiz  (None, 8, 8, 272)            3         ['block6b_drop[1][0]',        \n",
      " eWrapperV2)                                                         'block6a_project_bn[1][0]']  \n",
      "                                                                                                  \n",
      " quant_block6c_expand_conv   (None, 8, 8, 1632)           447171    ['quant_block6b_add[0][0]']   \n",
      " (QuantizeWrapperV2)                                                                              \n",
      "                                                                                                  \n",
      " block6c_expand_bn (BatchNo  (None, 8, 8, 1632)           6528      ['quant_block6c_expand_conv[0]\n",
      " rmalization)                                                       [0]']                         \n",
      "                                                                                                  \n",
      " quant_block6c_expand_activ  (None, 8, 8, 1632)           3         ['block6c_expand_bn[1][0]']   \n",
      " ation (QuantizeWrapperV2)                                                                        \n",
      "                                                                                                  \n",
      " quant_block6c_dwconv (Quan  (None, 8, 8, 1632)           40805     ['quant_block6c_expand_activat\n",
      " tizeWrapperV2)                                                     ion[0][0]']                   \n",
      "                                                                                                  \n",
      " block6c_bn (BatchNormaliza  (None, 8, 8, 1632)           6528      ['quant_block6c_dwconv[0][0]']\n",
      " tion)                                                                                            \n",
      "                                                                                                  \n",
      " block6c_activation (Activa  (None, 8, 8, 1632)           0         ['block6c_bn[1][0]']          \n",
      " tion)                                                                                            \n",
      "                                                                                                  \n",
      " block6c_se_squeeze (Global  (None, 1632)                 0         ['block6c_activation[1][0]']  \n",
      " AveragePooling2D)                                                                                \n",
      "                                                                                                  \n",
      " block6c_se_reshape (Reshap  (None, 1, 1, 1632)           0         ['block6c_se_squeeze[1][0]']  \n",
      " e)                                                                                               \n",
      "                                                                                                  \n",
      " block6c_se_reduce (Conv2D)  (None, 1, 1, 68)             111044    ['block6c_se_reshape[1][0]']  \n",
      "                                                                                                  \n",
      " block6c_se_expand (Conv2D)  (None, 1, 1, 1632)           112608    ['block6c_se_reduce[1][0]']   \n",
      "                                                                                                  \n",
      " block6c_se_excite (Multipl  (None, 8, 8, 1632)           0         ['block6c_activation[1][0]',  \n",
      " y)                                                                  'block6c_se_expand[1][0]']   \n",
      "                                                                                                  \n",
      " quant_block6c_project_conv  (None, 8, 8, 272)            444451    ['block6c_se_excite[1][0]']   \n",
      "  (QuantizeWrapperV2)                                                                             \n",
      "                                                                                                  \n",
      " block6c_project_bn (BatchN  (None, 8, 8, 272)            1088      ['quant_block6c_project_conv[0\n",
      " ormalization)                                                      ][0]']                        \n",
      "                                                                                                  \n",
      " block6c_drop (Dropout)      (None, 8, 8, 272)            0         ['block6c_project_bn[1][0]']  \n",
      "                                                                                                  \n",
      " quant_block6c_add (Quantiz  (None, 8, 8, 272)            3         ['block6c_drop[1][0]',        \n",
      " eWrapperV2)                                                         'quant_block6b_add[0][0]']   \n",
      "                                                                                                  \n",
      " quant_block6d_expand_conv   (None, 8, 8, 1632)           447171    ['quant_block6c_add[0][0]']   \n",
      " (QuantizeWrapperV2)                                                                              \n",
      "                                                                                                  \n",
      " block6d_expand_bn (BatchNo  (None, 8, 8, 1632)           6528      ['quant_block6d_expand_conv[0]\n",
      " rmalization)                                                       [0]']                         \n",
      "                                                                                                  \n",
      " quant_block6d_expand_activ  (None, 8, 8, 1632)           3         ['block6d_expand_bn[1][0]']   \n",
      " ation (QuantizeWrapperV2)                                                                        \n",
      "                                                                                                  \n",
      " quant_block6d_dwconv (Quan  (None, 8, 8, 1632)           40805     ['quant_block6d_expand_activat\n",
      " tizeWrapperV2)                                                     ion[0][0]']                   \n",
      "                                                                                                  \n",
      " block6d_bn (BatchNormaliza  (None, 8, 8, 1632)           6528      ['quant_block6d_dwconv[0][0]']\n",
      " tion)                                                                                            \n",
      "                                                                                                  \n",
      " block6d_activation (Activa  (None, 8, 8, 1632)           0         ['block6d_bn[1][0]']          \n",
      " tion)                                                                                            \n",
      "                                                                                                  \n",
      " block6d_se_squeeze (Global  (None, 1632)                 0         ['block6d_activation[1][0]']  \n",
      " AveragePooling2D)                                                                                \n",
      "                                                                                                  \n",
      " block6d_se_reshape (Reshap  (None, 1, 1, 1632)           0         ['block6d_se_squeeze[1][0]']  \n",
      " e)                                                                                               \n",
      "                                                                                                  \n",
      " block6d_se_reduce (Conv2D)  (None, 1, 1, 68)             111044    ['block6d_se_reshape[1][0]']  \n",
      "                                                                                                  \n",
      " block6d_se_expand (Conv2D)  (None, 1, 1, 1632)           112608    ['block6d_se_reduce[1][0]']   \n",
      "                                                                                                  \n",
      " block6d_se_excite (Multipl  (None, 8, 8, 1632)           0         ['block6d_activation[1][0]',  \n",
      " y)                                                                  'block6d_se_expand[1][0]']   \n",
      "                                                                                                  \n",
      " quant_block6d_project_conv  (None, 8, 8, 272)            444451    ['block6d_se_excite[1][0]']   \n",
      "  (QuantizeWrapperV2)                                                                             \n",
      "                                                                                                  \n",
      " block6d_project_bn (BatchN  (None, 8, 8, 272)            1088      ['quant_block6d_project_conv[0\n",
      " ormalization)                                                      ][0]']                        \n",
      "                                                                                                  \n",
      " block6d_drop (Dropout)      (None, 8, 8, 272)            0         ['block6d_project_bn[1][0]']  \n",
      "                                                                                                  \n",
      " quant_block6d_add (Quantiz  (None, 8, 8, 272)            3         ['block6d_drop[1][0]',        \n",
      " eWrapperV2)                                                         'quant_block6c_add[0][0]']   \n",
      "                                                                                                  \n",
      " quant_block6e_expand_conv   (None, 8, 8, 1632)           447171    ['quant_block6d_add[0][0]']   \n",
      " (QuantizeWrapperV2)                                                                              \n",
      "                                                                                                  \n",
      " block6e_expand_bn (BatchNo  (None, 8, 8, 1632)           6528      ['quant_block6e_expand_conv[0]\n",
      " rmalization)                                                       [0]']                         \n",
      "                                                                                                  \n",
      " quant_block6e_expand_activ  (None, 8, 8, 1632)           3         ['block6e_expand_bn[1][0]']   \n",
      " ation (QuantizeWrapperV2)                                                                        \n",
      "                                                                                                  \n",
      " quant_block6e_dwconv (Quan  (None, 8, 8, 1632)           40805     ['quant_block6e_expand_activat\n",
      " tizeWrapperV2)                                                     ion[0][0]']                   \n",
      "                                                                                                  \n",
      " block6e_bn (BatchNormaliza  (None, 8, 8, 1632)           6528      ['quant_block6e_dwconv[0][0]']\n",
      " tion)                                                                                            \n",
      "                                                                                                  \n",
      " block6e_activation (Activa  (None, 8, 8, 1632)           0         ['block6e_bn[1][0]']          \n",
      " tion)                                                                                            \n",
      "                                                                                                  \n",
      " block6e_se_squeeze (Global  (None, 1632)                 0         ['block6e_activation[1][0]']  \n",
      " AveragePooling2D)                                                                                \n",
      "                                                                                                  \n",
      " block6e_se_reshape (Reshap  (None, 1, 1, 1632)           0         ['block6e_se_squeeze[1][0]']  \n",
      " e)                                                                                               \n",
      "                                                                                                  \n",
      " block6e_se_reduce (Conv2D)  (None, 1, 1, 68)             111044    ['block6e_se_reshape[1][0]']  \n",
      "                                                                                                  \n",
      " block6e_se_expand (Conv2D)  (None, 1, 1, 1632)           112608    ['block6e_se_reduce[1][0]']   \n",
      "                                                                                                  \n",
      " block6e_se_excite (Multipl  (None, 8, 8, 1632)           0         ['block6e_activation[1][0]',  \n",
      " y)                                                                  'block6e_se_expand[1][0]']   \n",
      "                                                                                                  \n",
      " quant_block6e_project_conv  (None, 8, 8, 272)            444451    ['block6e_se_excite[1][0]']   \n",
      "  (QuantizeWrapperV2)                                                                             \n",
      "                                                                                                  \n",
      " block6e_project_bn (BatchN  (None, 8, 8, 272)            1088      ['quant_block6e_project_conv[0\n",
      " ormalization)                                                      ][0]']                        \n",
      "                                                                                                  \n",
      " block6e_drop (Dropout)      (None, 8, 8, 272)            0         ['block6e_project_bn[1][0]']  \n",
      "                                                                                                  \n",
      " quant_block6e_add (Quantiz  (None, 8, 8, 272)            3         ['block6e_drop[1][0]',        \n",
      " eWrapperV2)                                                         'quant_block6d_add[0][0]']   \n",
      "                                                                                                  \n",
      " quant_block6f_expand_conv   (None, 8, 8, 1632)           447171    ['quant_block6e_add[0][0]']   \n",
      " (QuantizeWrapperV2)                                                                              \n",
      "                                                                                                  \n",
      " block6f_expand_bn (BatchNo  (None, 8, 8, 1632)           6528      ['quant_block6f_expand_conv[0]\n",
      " rmalization)                                                       [0]']                         \n",
      "                                                                                                  \n",
      " quant_block6f_expand_activ  (None, 8, 8, 1632)           3         ['block6f_expand_bn[1][0]']   \n",
      " ation (QuantizeWrapperV2)                                                                        \n",
      "                                                                                                  \n",
      " quant_block6f_dwconv (Quan  (None, 8, 8, 1632)           40805     ['quant_block6f_expand_activat\n",
      " tizeWrapperV2)                                                     ion[0][0]']                   \n",
      "                                                                                                  \n",
      " block6f_bn (BatchNormaliza  (None, 8, 8, 1632)           6528      ['quant_block6f_dwconv[0][0]']\n",
      " tion)                                                                                            \n",
      "                                                                                                  \n",
      " block6f_activation (Activa  (None, 8, 8, 1632)           0         ['block6f_bn[1][0]']          \n",
      " tion)                                                                                            \n",
      "                                                                                                  \n",
      " block6f_se_squeeze (Global  (None, 1632)                 0         ['block6f_activation[1][0]']  \n",
      " AveragePooling2D)                                                                                \n",
      "                                                                                                  \n",
      " block6f_se_reshape (Reshap  (None, 1, 1, 1632)           0         ['block6f_se_squeeze[1][0]']  \n",
      " e)                                                                                               \n",
      "                                                                                                  \n",
      " block6f_se_reduce (Conv2D)  (None, 1, 1, 68)             111044    ['block6f_se_reshape[1][0]']  \n",
      "                                                                                                  \n",
      " block6f_se_expand (Conv2D)  (None, 1, 1, 1632)           112608    ['block6f_se_reduce[1][0]']   \n",
      "                                                                                                  \n",
      " block6f_se_excite (Multipl  (None, 8, 8, 1632)           0         ['block6f_activation[1][0]',  \n",
      " y)                                                                  'block6f_se_expand[1][0]']   \n",
      "                                                                                                  \n",
      " quant_block6f_project_conv  (None, 8, 8, 272)            444451    ['block6f_se_excite[1][0]']   \n",
      "  (QuantizeWrapperV2)                                                                             \n",
      "                                                                                                  \n",
      " block6f_project_bn (BatchN  (None, 8, 8, 272)            1088      ['quant_block6f_project_conv[0\n",
      " ormalization)                                                      ][0]']                        \n",
      "                                                                                                  \n",
      " block6f_drop (Dropout)      (None, 8, 8, 272)            0         ['block6f_project_bn[1][0]']  \n",
      "                                                                                                  \n",
      " quant_block6f_add (Quantiz  (None, 8, 8, 272)            3         ['block6f_drop[1][0]',        \n",
      " eWrapperV2)                                                         'quant_block6e_add[0][0]']   \n",
      "                                                                                                  \n",
      " quant_block6g_expand_conv   (None, 8, 8, 1632)           447171    ['quant_block6f_add[0][0]']   \n",
      " (QuantizeWrapperV2)                                                                              \n",
      "                                                                                                  \n",
      " block6g_expand_bn (BatchNo  (None, 8, 8, 1632)           6528      ['quant_block6g_expand_conv[0]\n",
      " rmalization)                                                       [0]']                         \n",
      "                                                                                                  \n",
      " quant_block6g_expand_activ  (None, 8, 8, 1632)           3         ['block6g_expand_bn[1][0]']   \n",
      " ation (QuantizeWrapperV2)                                                                        \n",
      "                                                                                                  \n",
      " quant_block6g_dwconv (Quan  (None, 8, 8, 1632)           40805     ['quant_block6g_expand_activat\n",
      " tizeWrapperV2)                                                     ion[0][0]']                   \n",
      "                                                                                                  \n",
      " block6g_bn (BatchNormaliza  (None, 8, 8, 1632)           6528      ['quant_block6g_dwconv[0][0]']\n",
      " tion)                                                                                            \n",
      "                                                                                                  \n",
      " block6g_activation (Activa  (None, 8, 8, 1632)           0         ['block6g_bn[1][0]']          \n",
      " tion)                                                                                            \n",
      "                                                                                                  \n",
      " block6g_se_squeeze (Global  (None, 1632)                 0         ['block6g_activation[1][0]']  \n",
      " AveragePooling2D)                                                                                \n",
      "                                                                                                  \n",
      " block6g_se_reshape (Reshap  (None, 1, 1, 1632)           0         ['block6g_se_squeeze[1][0]']  \n",
      " e)                                                                                               \n",
      "                                                                                                  \n",
      " block6g_se_reduce (Conv2D)  (None, 1, 1, 68)             111044    ['block6g_se_reshape[1][0]']  \n",
      "                                                                                                  \n",
      " block6g_se_expand (Conv2D)  (None, 1, 1, 1632)           112608    ['block6g_se_reduce[1][0]']   \n",
      "                                                                                                  \n",
      " block6g_se_excite (Multipl  (None, 8, 8, 1632)           0         ['block6g_activation[1][0]',  \n",
      " y)                                                                  'block6g_se_expand[1][0]']   \n",
      "                                                                                                  \n",
      " quant_block6g_project_conv  (None, 8, 8, 272)            444451    ['block6g_se_excite[1][0]']   \n",
      "  (QuantizeWrapperV2)                                                                             \n",
      "                                                                                                  \n",
      " block6g_project_bn (BatchN  (None, 8, 8, 272)            1088      ['quant_block6g_project_conv[0\n",
      " ormalization)                                                      ][0]']                        \n",
      "                                                                                                  \n",
      " block6g_drop (Dropout)      (None, 8, 8, 272)            0         ['block6g_project_bn[1][0]']  \n",
      "                                                                                                  \n",
      " quant_block6g_add (Quantiz  (None, 8, 8, 272)            3         ['block6g_drop[1][0]',        \n",
      " eWrapperV2)                                                         'quant_block6f_add[0][0]']   \n",
      "                                                                                                  \n",
      " quant_block6h_expand_conv   (None, 8, 8, 1632)           447171    ['quant_block6g_add[0][0]']   \n",
      " (QuantizeWrapperV2)                                                                              \n",
      "                                                                                                  \n",
      " block6h_expand_bn (BatchNo  (None, 8, 8, 1632)           6528      ['quant_block6h_expand_conv[0]\n",
      " rmalization)                                                       [0]']                         \n",
      "                                                                                                  \n",
      " quant_block6h_expand_activ  (None, 8, 8, 1632)           3         ['block6h_expand_bn[1][0]']   \n",
      " ation (QuantizeWrapperV2)                                                                        \n",
      "                                                                                                  \n",
      " quant_block6h_dwconv (Quan  (None, 8, 8, 1632)           40805     ['quant_block6h_expand_activat\n",
      " tizeWrapperV2)                                                     ion[0][0]']                   \n",
      "                                                                                                  \n",
      " block6h_bn (BatchNormaliza  (None, 8, 8, 1632)           6528      ['quant_block6h_dwconv[0][0]']\n",
      " tion)                                                                                            \n",
      "                                                                                                  \n",
      " block6h_activation (Activa  (None, 8, 8, 1632)           0         ['block6h_bn[1][0]']          \n",
      " tion)                                                                                            \n",
      "                                                                                                  \n",
      " block6h_se_squeeze (Global  (None, 1632)                 0         ['block6h_activation[1][0]']  \n",
      " AveragePooling2D)                                                                                \n",
      "                                                                                                  \n",
      " block6h_se_reshape (Reshap  (None, 1, 1, 1632)           0         ['block6h_se_squeeze[1][0]']  \n",
      " e)                                                                                               \n",
      "                                                                                                  \n",
      " block6h_se_reduce (Conv2D)  (None, 1, 1, 68)             111044    ['block6h_se_reshape[1][0]']  \n",
      "                                                                                                  \n",
      " block6h_se_expand (Conv2D)  (None, 1, 1, 1632)           112608    ['block6h_se_reduce[1][0]']   \n",
      "                                                                                                  \n",
      " block6h_se_excite (Multipl  (None, 8, 8, 1632)           0         ['block6h_activation[1][0]',  \n",
      " y)                                                                  'block6h_se_expand[1][0]']   \n",
      "                                                                                                  \n",
      " quant_block6h_project_conv  (None, 8, 8, 272)            444451    ['block6h_se_excite[1][0]']   \n",
      "  (QuantizeWrapperV2)                                                                             \n",
      "                                                                                                  \n",
      " block6h_project_bn (BatchN  (None, 8, 8, 272)            1088      ['quant_block6h_project_conv[0\n",
      " ormalization)                                                      ][0]']                        \n",
      "                                                                                                  \n",
      " block6h_drop (Dropout)      (None, 8, 8, 272)            0         ['block6h_project_bn[1][0]']  \n",
      "                                                                                                  \n",
      " quant_block6h_add (Quantiz  (None, 8, 8, 272)            3         ['block6h_drop[1][0]',        \n",
      " eWrapperV2)                                                         'quant_block6g_add[0][0]']   \n",
      "                                                                                                  \n",
      " quant_block7a_expand_conv   (None, 8, 8, 1632)           447171    ['quant_block6h_add[0][0]']   \n",
      " (QuantizeWrapperV2)                                                                              \n",
      "                                                                                                  \n",
      " block7a_expand_bn (BatchNo  (None, 8, 8, 1632)           6528      ['quant_block7a_expand_conv[0]\n",
      " rmalization)                                                       [0]']                         \n",
      "                                                                                                  \n",
      " quant_block7a_expand_activ  (None, 8, 8, 1632)           3         ['block7a_expand_bn[1][0]']   \n",
      " ation (QuantizeWrapperV2)                                                                        \n",
      "                                                                                                  \n",
      " quant_block7a_dwconv (Quan  (None, 8, 8, 1632)           14693     ['quant_block7a_expand_activat\n",
      " tizeWrapperV2)                                                     ion[0][0]']                   \n",
      "                                                                                                  \n",
      " block7a_bn (BatchNormaliza  (None, 8, 8, 1632)           6528      ['quant_block7a_dwconv[0][0]']\n",
      " tion)                                                                                            \n",
      "                                                                                                  \n",
      " block7a_activation (Activa  (None, 8, 8, 1632)           0         ['block7a_bn[1][0]']          \n",
      " tion)                                                                                            \n",
      "                                                                                                  \n",
      " block7a_se_squeeze (Global  (None, 1632)                 0         ['block7a_activation[1][0]']  \n",
      " AveragePooling2D)                                                                                \n",
      "                                                                                                  \n",
      " block7a_se_reshape (Reshap  (None, 1, 1, 1632)           0         ['block7a_se_squeeze[1][0]']  \n",
      " e)                                                                                               \n",
      "                                                                                                  \n",
      " block7a_se_reduce (Conv2D)  (None, 1, 1, 68)             111044    ['block7a_se_reshape[1][0]']  \n",
      "                                                                                                  \n",
      " block7a_se_expand (Conv2D)  (None, 1, 1, 1632)           112608    ['block7a_se_reduce[1][0]']   \n",
      "                                                                                                  \n",
      " block7a_se_excite (Multipl  (None, 8, 8, 1632)           0         ['block7a_activation[1][0]',  \n",
      " y)                                                                  'block7a_se_expand[1][0]']   \n",
      "                                                                                                  \n",
      " quant_block7a_project_conv  (None, 8, 8, 448)            732035    ['block7a_se_excite[1][0]']   \n",
      "  (QuantizeWrapperV2)                                                                             \n",
      "                                                                                                  \n",
      " block7a_project_bn (BatchN  (None, 8, 8, 448)            1792      ['quant_block7a_project_conv[0\n",
      " ormalization)                                                      ][0]']                        \n",
      "                                                                                                  \n",
      " quant_block7b_expand_conv   (None, 8, 8, 2688)           1209603   ['block7a_project_bn[1][0]']  \n",
      " (QuantizeWrapperV2)                                                                              \n",
      "                                                                                                  \n",
      " block7b_expand_bn (BatchNo  (None, 8, 8, 2688)           10752     ['quant_block7b_expand_conv[0]\n",
      " rmalization)                                                       [0]']                         \n",
      "                                                                                                  \n",
      " quant_block7b_expand_activ  (None, 8, 8, 2688)           3         ['block7b_expand_bn[1][0]']   \n",
      " ation (QuantizeWrapperV2)                                                                        \n",
      "                                                                                                  \n",
      " quant_block7b_dwconv (Quan  (None, 8, 8, 2688)           24197     ['quant_block7b_expand_activat\n",
      " tizeWrapperV2)                                                     ion[0][0]']                   \n",
      "                                                                                                  \n",
      " block7b_bn (BatchNormaliza  (None, 8, 8, 2688)           10752     ['quant_block7b_dwconv[0][0]']\n",
      " tion)                                                                                            \n",
      "                                                                                                  \n",
      " block7b_activation (Activa  (None, 8, 8, 2688)           0         ['block7b_bn[1][0]']          \n",
      " tion)                                                                                            \n",
      "                                                                                                  \n",
      " block7b_se_squeeze (Global  (None, 2688)                 0         ['block7b_activation[1][0]']  \n",
      " AveragePooling2D)                                                                                \n",
      "                                                                                                  \n",
      " block7b_se_reshape (Reshap  (None, 1, 1, 2688)           0         ['block7b_se_squeeze[1][0]']  \n",
      " e)                                                                                               \n",
      "                                                                                                  \n",
      " block7b_se_reduce (Conv2D)  (None, 1, 1, 112)            301168    ['block7b_se_reshape[1][0]']  \n",
      "                                                                                                  \n",
      " block7b_se_expand (Conv2D)  (None, 1, 1, 2688)           303744    ['block7b_se_reduce[1][0]']   \n",
      "                                                                                                  \n",
      " block7b_se_excite (Multipl  (None, 8, 8, 2688)           0         ['block7b_activation[1][0]',  \n",
      " y)                                                                  'block7b_se_expand[1][0]']   \n",
      "                                                                                                  \n",
      " quant_block7b_project_conv  (None, 8, 8, 448)            1205123   ['block7b_se_excite[1][0]']   \n",
      "  (QuantizeWrapperV2)                                                                             \n",
      "                                                                                                  \n",
      " block7b_project_bn (BatchN  (None, 8, 8, 448)            1792      ['quant_block7b_project_conv[0\n",
      " ormalization)                                                      ][0]']                        \n",
      "                                                                                                  \n",
      " block7b_drop (Dropout)      (None, 8, 8, 448)            0         ['block7b_project_bn[1][0]']  \n",
      "                                                                                                  \n",
      " quant_block7b_add (Quantiz  (None, 8, 8, 448)            3         ['block7b_drop[1][0]',        \n",
      " eWrapperV2)                                                         'block7a_project_bn[1][0]']  \n",
      "                                                                                                  \n",
      " quant_top_conv (QuantizeWr  (None, 8, 8, 1792)           806403    ['quant_block7b_add[0][0]']   \n",
      " apperV2)                                                                                         \n",
      "                                                                                                  \n",
      " top_bn (BatchNormalization  (None, 8, 8, 1792)           7168      ['quant_top_conv[0][0]']      \n",
      " )                                                                                                \n",
      "                                                                                                  \n",
      " top_activation (Activation  (None, 8, 8, 1792)           0         ['top_bn[1][0]']              \n",
      " )                                                                                                \n",
      "                                                                                                  \n",
      " global_average_pooling2d_3  (None, 1792)                 0         ['top_activation[1][0]']      \n",
      "  (GlobalAveragePooling2D)                                                                        \n",
      "                                                                                                  \n",
      " dense_24 (Dense)            (None, 1024)                 1836032   ['global_average_pooling2d_3[1\n",
      "                                                                    ][0]']                        \n",
      "                                                                                                  \n",
      " batch_normalization_19 (Ba  (None, 1024)                 4096      ['dense_24[1][0]']            \n",
      " tchNormalization)                                                                                \n",
      "                                                                                                  \n",
      " dense_25 (Dense)            (None, 128)                  131200    ['batch_normalization_19[1][0]\n",
      "                                                                    ']                            \n",
      "                                                                                                  \n",
      " dense_26 (Dense)            (None, 3)                    387       ['dense_25[1][0]']            \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 19715535 (75.21 MB)\n",
      "Trainable params: 19518283 (74.46 MB)\n",
      "Non-trainable params: 197252 (770.52 KB)\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "quantAwareEfficientNetModel = tfmot.quantization.keras.quantize_apply(quantAwareEfficientNetModel)\n",
    "quantAwareEfficientNetModel.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "lossFunction = tf.keras.losses.CategoricalCrossentropy()\n",
    "METRICS = [tf.keras.metrics.CategoricalAccuracy(name=\"accuracy\"), \n",
    "           tf.keras.metrics.TopKCategoricalAccuracy(k=2, name=\"top_k_accuracy\")]\n",
    "\n",
    "quantAwareEfficientNetModel.compile(\n",
    "    optimizer = tf.keras.optimizers.legacy.Adam(learning_rate=CONFIGURATION[\"LEARNING_RATE\"]),\n",
    "    loss = lossFunction,\n",
    "    metrics=METRICS\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "10/10 [==============================] - 91s 9s/step - loss: 8.7212 - accuracy: 0.3594 - top_k_accuracy: 0.7094 - val_loss: 39.4370 - val_accuracy: 0.1875 - val_top_k_accuracy: 0.5000\n",
      "Epoch 2/20\n",
      "10/10 [==============================] - 96s 10s/step - loss: 4.9578 - accuracy: 0.3250 - top_k_accuracy: 0.6781 - val_loss: 30.1846 - val_accuracy: 0.3750 - val_top_k_accuracy: 0.6250\n",
      "Epoch 3/20\n",
      "10/10 [==============================] - 100s 10s/step - loss: 3.3222 - accuracy: 0.4000 - top_k_accuracy: 0.6938 - val_loss: 20.0077 - val_accuracy: 0.3125 - val_top_k_accuracy: 0.5625\n",
      "Epoch 4/20\n",
      "10/10 [==============================] - 100s 10s/step - loss: 2.4304 - accuracy: 0.3406 - top_k_accuracy: 0.6687 - val_loss: 34.3175 - val_accuracy: 0.3750 - val_top_k_accuracy: 0.6250\n",
      "Epoch 5/20\n",
      "10/10 [==============================] - 101s 10s/step - loss: 2.1971 - accuracy: 0.3969 - top_k_accuracy: 0.6844 - val_loss: 30.2055 - val_accuracy: 0.2188 - val_top_k_accuracy: 0.5312\n",
      "Epoch 6/20\n",
      "10/10 [==============================] - 96s 10s/step - loss: 2.0788 - accuracy: 0.3844 - top_k_accuracy: 0.6656 - val_loss: 24.8381 - val_accuracy: 0.0938 - val_top_k_accuracy: 0.4375\n",
      "Epoch 7/20\n",
      "10/10 [==============================] - 108s 11s/step - loss: 2.6515 - accuracy: 0.3438 - top_k_accuracy: 0.7000 - val_loss: 13.9066 - val_accuracy: 0.3438 - val_top_k_accuracy: 0.6250\n",
      "Epoch 8/20\n",
      "10/10 [==============================] - 90s 9s/step - loss: 3.3472 - accuracy: 0.3156 - top_k_accuracy: 0.7063 - val_loss: 25.3491 - val_accuracy: 0.2812 - val_top_k_accuracy: 0.6562\n",
      "Epoch 9/20\n",
      "10/10 [==============================] - 92s 9s/step - loss: 3.3602 - accuracy: 0.4375 - top_k_accuracy: 0.7219 - val_loss: 13.4740 - val_accuracy: 0.3750 - val_top_k_accuracy: 0.5625\n",
      "Epoch 10/20\n",
      "10/10 [==============================] - 101s 10s/step - loss: 2.4575 - accuracy: 0.3844 - top_k_accuracy: 0.7219 - val_loss: 6.7672 - val_accuracy: 0.2812 - val_top_k_accuracy: 0.6875\n",
      "Epoch 11/20\n",
      "10/10 [==============================] - 88s 9s/step - loss: 2.6578 - accuracy: 0.3938 - top_k_accuracy: 0.6656 - val_loss: 2.8214 - val_accuracy: 0.4688 - val_top_k_accuracy: 0.7188\n",
      "Epoch 12/20\n",
      "10/10 [==============================] - 85s 8s/step - loss: 2.2365 - accuracy: 0.3562 - top_k_accuracy: 0.7250 - val_loss: 4.6050 - val_accuracy: 0.1250 - val_top_k_accuracy: 0.7188\n",
      "Epoch 13/20\n",
      "10/10 [==============================] - 86s 9s/step - loss: 2.1098 - accuracy: 0.3812 - top_k_accuracy: 0.7312 - val_loss: 14.7658 - val_accuracy: 0.3438 - val_top_k_accuracy: 0.7500\n",
      "Epoch 14/20\n",
      "10/10 [==============================] - 90s 9s/step - loss: 1.9127 - accuracy: 0.3406 - top_k_accuracy: 0.6781 - val_loss: 45.6956 - val_accuracy: 0.2812 - val_top_k_accuracy: 0.3750\n",
      "Epoch 15/20\n",
      "10/10 [==============================] - 91s 9s/step - loss: 2.9479 - accuracy: 0.3438 - top_k_accuracy: 0.6844 - val_loss: 28.4239 - val_accuracy: 0.1875 - val_top_k_accuracy: 0.4688\n",
      "Epoch 16/20\n",
      "10/10 [==============================] - 86s 9s/step - loss: 2.0064 - accuracy: 0.3531 - top_k_accuracy: 0.7000 - val_loss: 46.9311 - val_accuracy: 0.2188 - val_top_k_accuracy: 0.5938\n",
      "Epoch 17/20\n",
      "10/10 [==============================] - 84s 8s/step - loss: 3.2334 - accuracy: 0.3656 - top_k_accuracy: 0.6625 - val_loss: 10.4997 - val_accuracy: 0.1875 - val_top_k_accuracy: 0.5312\n",
      "Epoch 18/20\n",
      "10/10 [==============================] - 88s 9s/step - loss: 2.6830 - accuracy: 0.3844 - top_k_accuracy: 0.6656 - val_loss: 4.6697 - val_accuracy: 0.2812 - val_top_k_accuracy: 0.6875\n",
      "Epoch 19/20\n",
      "10/10 [==============================] - 85s 9s/step - loss: 2.1599 - accuracy: 0.3625 - top_k_accuracy: 0.7250 - val_loss: 9.7242 - val_accuracy: 0.2812 - val_top_k_accuracy: 0.6875\n",
      "Epoch 20/20\n",
      "10/10 [==============================] - 90s 9s/step - loss: 1.7255 - accuracy: 0.3781 - top_k_accuracy: 0.7437 - val_loss: 10.8127 - val_accuracy: 0.3125 - val_top_k_accuracy: 0.5625\n"
     ]
    }
   ],
   "source": [
    "history = quantAwareEfficientNetModel.fit(\n",
    "    trainDataset.take(10),\n",
    "    validation_data = valDataset.take(1),\n",
    "    epochs = CONFIGURATION['N_EPOCHS'],\n",
    "    verbose=1,\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "HuggingFaceTransformers",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
