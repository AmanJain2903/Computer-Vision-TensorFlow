{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CONVERTING KERAS/TF MODEL TO ONNX FORMAT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/Caskroom/miniforge/base/envs/HuggingFaceTransformers/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import cv2\n",
    "import numpy as np \n",
    "import pandas as pd \n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import tensorflow as tf\n",
    "import sklearn\n",
    "from transformers import ViTFeatureExtractor, TFViTModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 6799 files belonging to 3 classes.\n",
      "Using 5440 files for training.\n",
      "Found 6799 files belonging to 3 classes.\n",
      "Using 1359 files for validation.\n",
      "Found 2278 files belonging to 3 classes.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-02 18:13:24.646975: I metal_plugin/src/device/metal_device.cc:1154] Metal device set to: Apple M3 Pro\n",
      "2025-02-02 18:13:24.647002: I metal_plugin/src/device/metal_device.cc:296] systemMemory: 18.00 GB\n",
      "2025-02-02 18:13:24.647009: I metal_plugin/src/device/metal_device.cc:313] maxCacheSize: 6.00 GB\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "I0000 00:00:1738548804.647048 18106103 pluggable_device_factory.cc:305] Could not identify NUMA node of platform GPU ID 0, defaulting to 0. Your kernel may not have been built with NUMA support.\n",
      "I0000 00:00:1738548804.647085 18106103 pluggable_device_factory.cc:271] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 0 MB memory) -> physical PluggableDevice (device: 0, name: METAL, pci bus id: <undefined>)\n"
     ]
    }
   ],
   "source": [
    "CONFIGURATION = {\n",
    "    \"CLASS_NAMES\" : ['angry', 'happy', 'sad'],\n",
    "    \"BATCH_SIZE\" : 32,\n",
    "    \"IMAGE_SIZE\" : 224,\n",
    "    \"LEARNING_RATE\" : 0.01,\n",
    "    \"N_EPOCHS\" : 20,\n",
    "    \"DROPOUT_RATE\": 0.0,\n",
    "    \"REGULARIZATION_RATE\" : 0.0,\n",
    "    \"N_FILTERS\" : 6,\n",
    "    \"KERNEL_SIZE\" : 3,\n",
    "    \"N_STRIDES\" : 1,\n",
    "    \"POOL_SIZE\" : 2,\n",
    "    \"N_DENSE_1\" : 128,\n",
    "    \"N_DENSE_2\" : 128,\n",
    "    \"NUM_CLASSES\" : 3,\n",
    "    \"PATCH_SIZE\" : 16,\n",
    "}\n",
    "\n",
    "trainDirectory = \"/Users/aman/Documents/Work/Machine Learning/Computer-Vision-TensorFlow/Human-Emotions-Detection/Dataset/Emotions Dataset/Emotions Dataset/train\"\n",
    "testDirectory = \"/Users/aman/Documents/Work/Machine Learning/Computer-Vision-TensorFlow/Human-Emotions-Detection/Dataset/Emotions Dataset/Emotions Dataset/test\"\n",
    "\n",
    "trainDataset = tf.keras.utils.image_dataset_from_directory(\n",
    "    trainDirectory,\n",
    "    labels='inferred',\n",
    "    label_mode='categorical',\n",
    "    class_names=CONFIGURATION[\"CLASS_NAMES\"],\n",
    "    color_mode='rgb',\n",
    "    batch_size=CONFIGURATION[\"BATCH_SIZE\"],\n",
    "    image_size=(CONFIGURATION[\"IMAGE_SIZE\"], CONFIGURATION[\"IMAGE_SIZE\"]),\n",
    "    shuffle=True,\n",
    "    seed=99,\n",
    "    validation_split=0.2,\n",
    "    subset='training',\n",
    ")\n",
    "\n",
    "valDataset = tf.keras.utils.image_dataset_from_directory(\n",
    "    trainDirectory,\n",
    "    labels='inferred',\n",
    "    label_mode='categorical',\n",
    "    class_names=CONFIGURATION[\"CLASS_NAMES\"],\n",
    "    color_mode='rgb',\n",
    "    batch_size=CONFIGURATION[\"BATCH_SIZE\"],\n",
    "    image_size=(CONFIGURATION[\"IMAGE_SIZE\"], CONFIGURATION[\"IMAGE_SIZE\"]),\n",
    "    shuffle=True,\n",
    "    seed=99,\n",
    "    validation_split=0.2,\n",
    "    subset='validation',\n",
    ")\n",
    "\n",
    "testDataset = tf.keras.utils.image_dataset_from_directory(\n",
    "    testDirectory,\n",
    "    labels='inferred',\n",
    "    label_mode='categorical',\n",
    "    class_names=CONFIGURATION[\"CLASS_NAMES\"],\n",
    "    color_mode='rgb',\n",
    "    batch_size=CONFIGURATION[\"BATCH_SIZE\"],\n",
    "    image_size=(CONFIGURATION[\"IMAGE_SIZE\"], CONFIGURATION[\"IMAGE_SIZE\"]),\n",
    "    shuffle=True,\n",
    "    seed=99,\n",
    "    validation_split=None,\n",
    "    subset=None,\n",
    ")\n",
    "\n",
    "trainDataset = trainDataset.prefetch(tf.data.AUTOTUNE)\n",
    "testDataset = testDataset.prefetch(tf.data.AUTOTUNE)\n",
    "valDataset = valDataset.prefetch(tf.data.AUTOTUNE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading Keras Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-02 18:13:40.598135: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:117] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "72/72 [==============================] - 39s 516ms/step - loss: 0.5523 - accuracy: 0.8749 - top_k_accuracy: 0.9680\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.5522751212120056, 0.8748902678489685, 0.9679543375968933]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ViTModel = tf.keras.models.load_model(\"Models/EmotionDetectionViT.keras\")\n",
    "ViTModel.evaluate(testDataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Installing ONNX Tools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ! pip install -U tf2onnx\n",
    "# ! pip install -U keras2onnx\n",
    "# ! pip install -U onnxruntime"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conversion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tf2onnx\n",
    "import onnxruntime as rt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0000 00:00:1738548908.728408 18106103 devices.cc:76] Number of eligible GPUs (core count >= 8, compute capability >= 0.0): 0 (Note: TensorFlow was not compiled with CUDA or ROCm support)\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "I0000 00:00:1738548908.728493 18106103 single_machine.cc:361] Starting new session\n",
      "I0000 00:00:1738548908.728638 18106103 pluggable_device_factory.cc:305] Could not identify NUMA node of platform GPU ID 0, defaulting to 0. Your kernel may not have been built with NUMA support.\n",
      "I0000 00:00:1738548908.728647 18106103 pluggable_device_factory.cc:271] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 0 MB memory) -> physical PluggableDevice (device: 0, name: METAL, pci bus id: <undefined>)\n",
      "I0000 00:00:1738548912.245098 18106103 pluggable_device_factory.cc:305] Could not identify NUMA node of platform GPU ID 0, defaulting to 0. Your kernel may not have been built with NUMA support.\n",
      "I0000 00:00:1738548912.245120 18106103 pluggable_device_factory.cc:271] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 0 MB memory) -> physical PluggableDevice (device: 0, name: METAL, pci bus id: <undefined>)\n",
      "I0000 00:00:1738548913.636188 18106103 devices.cc:76] Number of eligible GPUs (core count >= 8, compute capability >= 0.0): 0 (Note: TensorFlow was not compiled with CUDA or ROCm support)\n",
      "I0000 00:00:1738548913.636275 18106103 single_machine.cc:361] Starting new session\n",
      "I0000 00:00:1738548913.636505 18106103 pluggable_device_factory.cc:305] Could not identify NUMA node of platform GPU ID 0, defaulting to 0. Your kernel may not have been built with NUMA support.\n",
      "I0000 00:00:1738548913.636513 18106103 pluggable_device_factory.cc:271] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 0 MB memory) -> physical PluggableDevice (device: 0, name: METAL, pci bus id: <undefined>)\n"
     ]
    }
   ],
   "source": [
    "onnxModel, _ = tf2onnx.convert.from_keras(ViTModel, output_path=\"Models/EmotionDetectionViT.onnx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['dense']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outputNames = [output.name for output in onnxModel.graph.output]\n",
    "outputNames"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[name: \"image\"\n",
      "type {\n",
      "  tensor_type {\n",
      "    elem_type: 1\n",
      "    shape {\n",
      "      dim {\n",
      "        dim_param: \"unk__1508\"\n",
      "      }\n",
      "      dim {\n",
      "        dim_value: 224\n",
      "      }\n",
      "      dim {\n",
      "        dim_value: 224\n",
      "      }\n",
      "      dim {\n",
      "        dim_value: 3\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "}\n",
      "]\n",
      "\n",
      "\n",
      "[name: \"dense\"\n",
      "type {\n",
      "  tensor_type {\n",
      "    elem_type: 1\n",
      "    shape {\n",
      "      dim {\n",
      "        dim_param: \"unk__1509\"\n",
      "      }\n",
      "      dim {\n",
      "        dim_value: 3\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "}\n",
      "]\n"
     ]
    }
   ],
   "source": [
    "print(onnxModel.graph.input)\n",
    "print('\\n')\n",
    "print(onnxModel.graph.output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "testImage = cv2.imread(\"/Users/aman/Documents/Work/Machine Learning/Computer-Vision-TensorFlow/Human-Emotions-Detection/Dataset/Emotions Dataset/Emotions Dataset/test/happy/2705.jpg_rotation_1.jpg\")\n",
    "testImage = cv2.resize(testImage, (CONFIGURATION[\"IMAGE_SIZE\"], CONFIGURATION[\"IMAGE_SIZE\"]))\n",
    "testImage = testImage.astype(np.float32)\n",
    "testImage = np.expand_dims(testImage, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[array([[8.1243154e-05, 9.9953485e-01, 3.8389381e-04]], dtype=float32)]\n",
      "happy\n"
     ]
    }
   ],
   "source": [
    "providers = ['CPUExecutionProvider']\n",
    "m = rt.InferenceSession(\"Models/EmotionDetectionViT.onnx\", providers=providers)\n",
    "\n",
    "onnxPred = m.run(outputNames, {onnxModel.graph.input[0].name: testImage})\n",
    "print(onnxPred)\n",
    "print(CONFIGURATION['CLASS_NAMES'][np.argmax(onnxPred[0])])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comparing Model's Latency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Keras Model Time on GPU:  0.07996392250061035\n",
      "\n",
      "\n",
      "ONNX Model Time on CPU:  0.0737910270690918\n"
     ]
    }
   ],
   "source": [
    "startTime = time.time()\n",
    "ViTModel(testImage)\n",
    "endTime = time.time()\n",
    "print(\"Keras Model Time on GPU: \", endTime - startTime)\n",
    "\n",
    "print('\\n')\n",
    "\n",
    "startTime = time.time()\n",
    "m.run(outputNames, {onnxModel.graph.input[0].name: testImage})\n",
    "endTime = time.time()\n",
    "print(\"ONNX Model Time on CPU: \", endTime - startTime)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "HuggingFaceTransformers",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
