{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TENSORFLOW CALLBACKS\n",
    "## Functions we call during training which either gives us some key information of perform some tasks between iterations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Dependencies\n",
    "from tensorflow.keras.callbacks import Callback\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow_datasets as tfds\n",
    "from tensorflow.keras.layers import Input, Normalization, Conv2D, MaxPooling2D, Dense, Flatten, BatchNormalization\n",
    "from tensorflow.keras.metrics import BinaryAccuracy, FalsePositives, FalseNegatives, TrueNegatives, TruePositives, Precision, Recall, F1Score, AUC\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import roc_curve\n",
    "import import_ipynb\n",
    "import Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CALLBACK CLASS -> For Custom Callbacks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define Callback Class\n",
    "class LossCallback(Callback):\n",
    "    def on_epoch_end(self, epoch, logs):\n",
    "        print(\"\\nFor epoch number {} the model has a loss of {}\".format(epoch+1, logs[\"loss\"]))\n",
    "    def on_batch_end(self, batch, logs):\n",
    "        print(\"\\nFor batch number {} the model has a loss of {}\".format(batch+1, logs[\"loss\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-01-21 12:20:04.128021: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:117] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "For batch number 1 the model has a loss of 0.9490898847579956\n",
      "\u001b[1m  1/689\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m24:24\u001b[0m 2s/step - auc: 0.4286 - binary_accuracy: 0.5000 - false_negatives: 9.0000 - false_positives: 7.0000 - loss: 0.9491 - precision: 0.5625 - recall: 0.5000 - true_negatives: 7.0000 - true_positives: 9.0000\n",
      "For batch number 2 the model has a loss of 0.9843061566352844\n",
      "\u001b[1m  2/689\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m41s\u001b[0m 60ms/step - auc: 0.4675 - binary_accuracy: 0.5156 - false_negatives: 12.0000 - false_positives: 11.0000 - loss: 0.9667 - precision: 0.5540 - recall: 0.5227 - true_negatives: 11.5000 - true_positives: 13.5000\n",
      "For batch number 3 the model has a loss of 0.97020024061203\n",
      "\u001b[1m  3/689\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m39s\u001b[0m 58ms/step - auc: 0.4819 - binary_accuracy: 0.5243 - false_negatives: 15.3333 - false_positives: 14.6667 - loss: 0.9679 - precision: 0.5560 - recall: 0.5352 - true_negatives: 15.6667 - true_positives: 18.3333\n",
      "For batch number 4 the model has a loss of 0.939271092414856\n",
      "\u001b[1m  4/689\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m38s\u001b[0m 57ms/step - auc: 0.4927 - binary_accuracy: 0.5280 - false_negatives: 18.5000 - false_positives: 18.7500 - loss: 0.9607 - precision: 0.5530 - recall: 0.5437 - true_negatives: 19.7500 - true_positives: 23.0000\n",
      "For batch number 5 the model has a loss of 0.8905273675918579\n",
      "\u001b[1m  5/689\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m38s\u001b[0m 56ms/step - auc: 0.5027 - binary_accuracy: 0.5311 - false_negatives: 22.2000 - false_positives: 22.2000 - loss: 0.9467 - precision: 0.5577 - recall: 0.5489 - true_negatives: 23.4000 - true_positives: 28.2000\n",
      "For batch number 6 the model has a loss of 0.8734694123268127\n",
      "\u001b[1m  6/689\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m37s\u001b[0m 56ms/step - auc: 0.5088 - binary_accuracy: 0.5355 - false_negatives: 25.6667 - false_positives: 25.5000 - loss: 0.9345 - precision: 0.5641 - recall: 0.5558 - true_negatives: 27.0000 - true_positives: 33.8333\n",
      "For batch number 7 the model has a loss of 0.8499170541763306\n",
      "\u001b[1m  7/689\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m37s\u001b[0m 55ms/step - auc: 0.5141 - binary_accuracy: 0.5400 - false_negatives: 28.8571 - false_positives: 28.8571 - loss: 0.9224 - precision: 0.5690 - recall: 0.5626 - true_negatives: 30.8571 - true_positives: 39.4286\n",
      "For batch number 8 the model has a loss of 0.8276485204696655\n",
      "\u001b[1m  8/689\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m37s\u001b[0m 55ms/step - auc: 0.5191 - binary_accuracy: 0.5453 - false_negatives: 31.5000 - false_positives: 32.3750 - loss: 0.9106 - precision: 0.5724 - recall: 0.5706 - true_negatives: 35.1250 - true_positives: 45.0000\n",
      "For batch number 9 the model has a loss of 0.8051797151565552\n",
      "\u001b[1m  9/689\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m37s\u001b[0m 55ms/step - auc: 0.5244 - binary_accuracy: 0.5510 - false_negatives: 33.8889 - false_positives: 35.7778 - loss: 0.8988 - precision: 0.5761 - recall: 0.5791 - true_negatives: 39.5556 - true_positives: 50.7778\n",
      "For batch number 10 the model has a loss of 0.784364640712738\n",
      "\u001b[1m 10/689\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m37s\u001b[0m 55ms/step - auc: 0.5301 - binary_accuracy: 0.5565 - false_negatives: 36.1000 - false_positives: 39.2000 - loss: 0.8874 - precision: 0.5796 - recall: 0.5874 - true_negatives: 44.0000 - true_positives: 56.7000\n",
      "For batch number 11 the model has a loss of 0.7665026187896729\n",
      "\u001b[1m 11/689\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m37s\u001b[0m 55ms/step - auc: 0.5360 - binary_accuracy: 0.5623 - false_negatives: 38.2727 - false_positives: 42.3636 - loss: 0.8764 - precision: 0.5840 - recall: 0.5955 - true_negatives: 48.4545 - true_positives: 62.9091\n",
      "For batch number 12 the model has a loss of 0.7550821304321289\n",
      "\u001b[1m 12/689\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m37s\u001b[0m 55ms/step - auc: 0.5416 - binary_accuracy: 0.5673 - false_negatives: 40.5000 - false_positives: 45.5000 - loss: 0.8663 - precision: 0.5881 - recall: 0.6025 - true_negatives: 52.8333 - true_positives: 69.1667\n",
      "For batch number 13 the model has a loss of 0.7482592463493347\n",
      "\u001b[1m 13/689\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m36s\u001b[0m 55ms/step - auc: 0.5466 - binary_accuracy: 0.5715 - false_negatives: 42.7692 - false_positives: 48.6923 - loss: 0.8572 - precision: 0.5914 - recall: 0.6085 - true_negatives: 57.2308 - true_positives: 75.3077\n",
      "For batch number 14 the model has a loss of 0.740454375743866\n",
      "\u001b[1m 14/689\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m36s\u001b[0m 55ms/step - auc: 0.5511 - binary_accuracy: 0.5753 - false_negatives: 45.0000 - false_positives: 51.9286 - loss: 0.8489 - precision: 0.5948 - recall: 0.6144 - true_negatives: 61.2857 - true_positives: 81.7857\n",
      "For batch number 15 the model has a loss of 0.7316569089889526\n",
      "\u001b[1m 15/689\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m36s\u001b[0m 54ms/step - auc: 0.5556 - binary_accuracy: 0.5785 - false_negatives: 47.2000 - false_positives: 55.3333 - loss: 0.8411 - precision: 0.5974 - recall: 0.6198 - true_negatives: 65.2667 - true_positives: 88.2000\n",
      "For batch number 16 the model has a loss of 0.7231311202049255\n",
      "\u001b[1m 16/689\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m36s\u001b[0m 54ms/step - auc: 0.5600 - binary_accuracy: 0.5815 - false_negatives: 49.3125 - false_positives: 58.7500 - loss: 0.8337 - precision: 0.5996 - recall: 0.6249 - true_negatives: 69.3750 - true_positives: 94.5625\n",
      "For batch number 17 the model has a loss of 0.721341609954834\n",
      "\u001b[1m 17/689\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m36s\u001b[0m 54ms/step - auc: 0.5638 - binary_accuracy: 0.5840 - false_negatives: 51.2941 - false_positives: 62.4706 - loss: 0.8271 - precision: 0.6008 - recall: 0.6296 - true_negatives: 73.5294 - true_positives: 100.7059\n",
      "For batch number 18 the model has a loss of 0.7167384624481201\n",
      "\u001b[1m 18/689\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m36s\u001b[0m 54ms/step - auc: 0.5675 - binary_accuracy: 0.5864 - false_negatives: 53.1111 - false_positives: 66.2778 - loss: 0.8210 - precision: 0.6018 - recall: 0.6345 - true_negatives: 77.7222 - true_positives: 106.8889\n",
      "For batch number 19 the model has a loss of 0.7123576998710632\n",
      "\u001b[1m 19/689\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m36s\u001b[0m 54ms/step - auc: 0.5710 - binary_accuracy: 0.5884 - false_negatives: 54.9474 - false_positives: 70.1579 - loss: 0.8152 - precision: 0.6024 - recall: 0.6388 - true_negatives: 81.9474 - true_positives: 112.9474\n",
      "For batch number 20 the model has a loss of 0.7056235671043396\n",
      "\u001b[1m 20/689\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m36s\u001b[0m 54ms/step - auc: 0.5744 - binary_accuracy: 0.5904 - false_negatives: 56.6000 - false_positives: 74.1500 - loss: 0.8098 - precision: 0.6029 - recall: 0.6433 - true_negatives: 86.1000 - true_positives: 119.1500\n",
      "For batch number 21 the model has a loss of 0.6966975927352905\n",
      "\u001b[1m 21/689\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m36s\u001b[0m 54ms/step - auc: 0.5780 - binary_accuracy: 0.5925 - false_negatives: 58.2857 - false_positives: 77.9048 - loss: 0.8044 - precision: 0.6038 - recall: 0.6475 - true_negatives: 90.3810 - true_positives: 125.4286\n",
      "For batch number 22 the model has a loss of 0.6932046413421631\n",
      "\u001b[1m 22/689\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m36s\u001b[0m 54ms/step - auc: 0.5813 - binary_accuracy: 0.5945 - false_negatives: 60.0455 - false_positives: 81.5909 - loss: 0.7993 - precision: 0.6047 - recall: 0.6513 - true_negatives: 94.6364 - true_positives: 131.7273\n",
      "For batch number 23 the model has a loss of 0.6895977258682251\n",
      "\u001b[1m 23/689\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m36s\u001b[0m 54ms/step - auc: 0.5845 - binary_accuracy: 0.5962 - false_negatives: 61.7826 - false_positives: 85.4348 - loss: 0.7945 - precision: 0.6053 - recall: 0.6549 - true_negatives: 98.8261 - true_positives: 137.9565\n",
      "For batch number 24 the model has a loss of 0.6827955842018127\n",
      "\u001b[1m 24/689\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m36s\u001b[0m 54ms/step - auc: 0.5878 - binary_accuracy: 0.5981 - false_negatives: 63.4583 - false_positives: 89.1250 - loss: 0.7899 - precision: 0.6061 - recall: 0.6584 - true_negatives: 103.1250 - true_positives: 144.2917\n",
      "For batch number 25 the model has a loss of 0.6802176833152771\n",
      "\u001b[1m 25/689\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m36s\u001b[0m 54ms/step - auc: 0.5908 - binary_accuracy: 0.5996 - false_negatives: 65.1600 - false_positives: 92.9600 - loss: 0.7855 - precision: 0.6066 - recall: 0.6617 - true_negatives: 107.2800 - true_positives: 150.6000\n",
      "For batch number 26 the model has a loss of 0.6769526600837708\n",
      "\u001b[1m 26/689\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m35s\u001b[0m 54ms/step - auc: 0.5937 - binary_accuracy: 0.6010 - false_negatives: 66.8462 - false_positives: 96.8462 - loss: 0.7813 - precision: 0.6070 - recall: 0.6647 - true_negatives: 111.5000 - true_positives: 156.8077\n",
      "For batch number 27 the model has a loss of 0.6760984063148499\n",
      "\u001b[1m 27/689\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m35s\u001b[0m 54ms/step - auc: 0.5965 - binary_accuracy: 0.6023 - false_negatives: 68.5185 - false_positives: 100.7407 - loss: 0.7774 - precision: 0.6074 - recall: 0.6677 - true_negatives: 115.7037 - true_positives: 163.0370\n",
      "For batch number 28 the model has a loss of 0.6776537299156189\n",
      "\u001b[1m 28/689\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m35s\u001b[0m 54ms/step - auc: 0.5990 - binary_accuracy: 0.6034 - false_negatives: 70.2500 - false_positives: 104.6786 - loss: 0.7739 - precision: 0.6076 - recall: 0.6703 - true_negatives: 119.9286 - true_positives: 169.1429\n",
      "For batch number 29 the model has a loss of 0.6746406555175781\n",
      "\u001b[1m 29/689\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m35s\u001b[0m 54ms/step - auc: 0.6014 - binary_accuracy: 0.6046 - false_negatives: 72.0345 - false_positives: 108.4828 - loss: 0.7704 - precision: 0.6079 - recall: 0.6727 - true_negatives: 124.1724 - true_positives: 175.3103\n",
      "For batch number 30 the model has a loss of 0.6749674081802368\n",
      "\u001b[1m 30/689\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m35s\u001b[0m 54ms/step - auc: 0.6036 - binary_accuracy: 0.6057 - false_negatives: 73.8333 - false_positives: 112.2333 - loss: 0.7673 - precision: 0.6083 - recall: 0.6750 - true_negatives: 128.4667 - true_positives: 181.4667\n",
      "For batch number 31 the model has a loss of 0.6730425357818604\n",
      "\u001b[1m 31/689\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m35s\u001b[0m 54ms/step - auc: 0.6058 - binary_accuracy: 0.6068 - false_negatives: 75.7097 - false_positives: 115.9355 - loss: 0.7642 - precision: 0.6088 - recall: 0.6770 - true_negatives: 132.7097 - true_positives: 187.6452\n",
      "For batch number 32 the model has a loss of 0.6687221527099609\n",
      "\u001b[1m 32/689\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m35s\u001b[0m 55ms/step - auc: 0.6080 - binary_accuracy: 0.6078 - false_negatives: 77.5938 - false_positives: 119.5312 - loss: 0.7612 - precision: 0.6093 - recall: 0.6790 - true_negatives: 136.9375 - true_positives: 193.9375\n",
      "For batch number 33 the model has a loss of 0.6667502522468567\n",
      "\u001b[1m 33/689\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m35s\u001b[0m 55ms/step - auc: 0.6100 - binary_accuracy: 0.6088 - false_negatives: 79.4545 - false_positives: 123.2121 - loss: 0.7584 - precision: 0.6098 - recall: 0.6809 - true_negatives: 141.1818 - true_positives: 200.1515\n",
      "For batch number 34 the model has a loss of 0.664808452129364\n",
      "\u001b[1m 34/689\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m35s\u001b[0m 55ms/step - auc: 0.6121 - binary_accuracy: 0.6097 - false_negatives: 81.3235 - false_positives: 126.9118 - loss: 0.7556 - precision: 0.6102 - recall: 0.6827 - true_negatives: 145.3824 - true_positives: 206.3824\n",
      "For batch number 35 the model has a loss of 0.6623480916023254\n",
      "\u001b[1m 35/689\u001b[0m \u001b[32m━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m35s\u001b[0m 55ms/step - auc: 0.6140 - binary_accuracy: 0.6106 - false_negatives: 83.2857 - false_positives: 130.5429 - loss: 0.7530 - precision: 0.6106 - recall: 0.6843 - true_negatives: 149.5429 - true_positives: 212.6286\n",
      "For batch number 36 the model has a loss of 0.666641891002655\n",
      "\u001b[1m 36/689\u001b[0m \u001b[32m━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m35s\u001b[0m 55ms/step - auc: 0.6157 - binary_accuracy: 0.6113 - false_negatives: 85.3333 - false_positives: 134.1944 - loss: 0.7506 - precision: 0.6111 - recall: 0.6858 - true_negatives: 153.5556 - true_positives: 218.9167\n",
      "For batch number 37 the model has a loss of 0.6623741388320923\n",
      "\u001b[1m 37/689\u001b[0m \u001b[32m━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m35s\u001b[0m 55ms/step - auc: 0.6174 - binary_accuracy: 0.6121 - false_negatives: 87.4054 - false_positives: 137.6757 - loss: 0.7482 - precision: 0.6117 - recall: 0.6872 - true_negatives: 157.6487 - true_positives: 225.2703\n",
      "For batch number 38 the model has a loss of 0.6588700413703918\n",
      "\u001b[1m 38/689\u001b[0m \u001b[32m━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m35s\u001b[0m 55ms/step - auc: 0.6192 - binary_accuracy: 0.6130 - false_negatives: 89.5000 - false_positives: 141.0263 - loss: 0.7458 - precision: 0.6124 - recall: 0.6885 - true_negatives: 161.7895 - true_positives: 231.6842\n",
      "For batch number 39 the model has a loss of 0.6593056321144104\n",
      "\u001b[1m 39/689\u001b[0m \u001b[32m━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m35s\u001b[0m 55ms/step - auc: 0.6208 - binary_accuracy: 0.6138 - false_negatives: 91.5897 - false_positives: 144.3590 - loss: 0.7436 - precision: 0.6131 - recall: 0.6897 - true_negatives: 165.9744 - true_positives: 238.0769\n",
      "For batch number 40 the model has a loss of 0.6571401953697205\n",
      "\u001b[1m 40/689\u001b[0m \u001b[32m━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m35s\u001b[0m 54ms/step - auc: 0.6225 - binary_accuracy: 0.6146 - false_negatives: 93.7000 - false_positives: 147.6250 - loss: 0.7414 - precision: 0.6138 - recall: 0.6910 - true_negatives: 170.1500 - true_positives: 244.5250\n",
      "For batch number 41 the model has a loss of 0.6606590151786804\n",
      "\u001b[1m 41/689\u001b[0m \u001b[32m━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m35s\u001b[0m 54ms/step - auc: 0.6240 - binary_accuracy: 0.6154 - false_negatives: 95.8537 - false_positives: 150.8781 - loss: 0.7395 - precision: 0.6146 - recall: 0.6921 - true_negatives: 174.2439 - true_positives: 251.0244\n",
      "For batch number 42 the model has a loss of 0.6597995162010193\n",
      "\u001b[1m 42/689\u001b[0m \u001b[32m━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m35s\u001b[0m 54ms/step - auc: 0.6255 - binary_accuracy: 0.6162 - false_negatives: 97.9524 - false_positives: 154.1905 - loss: 0.7376 - precision: 0.6152 - recall: 0.6932 - true_negatives: 178.4286 - true_positives: 257.4286\n",
      "For batch number 43 the model has a loss of 0.6619142293930054\n",
      "\u001b[1m 43/689\u001b[0m \u001b[32m━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m35s\u001b[0m 54ms/step - auc: 0.6269 - binary_accuracy: 0.6169 - false_negatives: 100.0233 - false_positives: 157.5349 - loss: 0.7358 - precision: 0.6158 - recall: 0.6943 - true_negatives: 182.6512 - true_positives: 263.7907\n",
      "For batch number 44 the model has a loss of 0.6599371433258057\n",
      "\u001b[1m 44/689\u001b[0m \u001b[32m━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m35s\u001b[0m 54ms/step - auc: 0.6283 - binary_accuracy: 0.6177 - false_negatives: 102.0909 - false_positives: 160.8182 - loss: 0.7341 - precision: 0.6165 - recall: 0.6953 - true_negatives: 186.8409 - true_positives: 270.2500\n",
      "For batch number 45 the model has a loss of 0.6591859459877014\n",
      "\u001b[1m 45/689\u001b[0m \u001b[32m━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m35s\u001b[0m 54ms/step - auc: 0.6296 - binary_accuracy: 0.6184 - false_negatives: 104.2000 - false_positives: 164.1111 - loss: 0.7324 - precision: 0.6171 - recall: 0.6963 - true_negatives: 191.0667 - true_positives: 276.6222\n",
      "For batch number 46 the model has a loss of 0.6587050557136536\n",
      "\u001b[1m 46/689\u001b[0m \u001b[32m━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m34s\u001b[0m 54ms/step - auc: 0.6309 - binary_accuracy: 0.6190 - false_negatives: 106.3478 - false_positives: 167.4130 - loss: 0.7308 - precision: 0.6176 - recall: 0.6972 - true_negatives: 195.3044 - true_positives: 282.9348\n",
      "For batch number 47 the model has a loss of 0.6577099561691284\n",
      "\u001b[1m 47/689\u001b[0m \u001b[32m━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m35s\u001b[0m 55ms/step - auc: 0.6321 - binary_accuracy: 0.6196 - false_negatives: 108.4894 - false_positives: 170.7447 - loss: 0.7293 - precision: 0.6181 - recall: 0.6980 - true_negatives: 199.5532 - true_positives: 289.2128\n",
      "For batch number 48 the model has a loss of 0.6579235196113586\n",
      "\u001b[1m 48/689\u001b[0m \u001b[32m━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m35s\u001b[0m 55ms/step - auc: 0.6333 - binary_accuracy: 0.6201 - false_negatives: 110.6042 - false_positives: 174.1250 - loss: 0.7278 - precision: 0.6186 - recall: 0.6988 - true_negatives: 203.8125 - true_positives: 295.4583\n",
      "For batch number 49 the model has a loss of 0.6575320363044739\n",
      "\u001b[1m 49/689\u001b[0m \u001b[32m━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m35s\u001b[0m 55ms/step - auc: 0.6345 - binary_accuracy: 0.6207 - false_negatives: 112.7347 - false_positives: 177.5102 - loss: 0.7264 - precision: 0.6190 - recall: 0.6996 - true_negatives: 208.0204 - true_positives: 301.7347\n",
      "For batch number 50 the model has a loss of 0.6575198173522949\n",
      "\u001b[1m 50/689\u001b[0m \u001b[32m━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m34s\u001b[0m 55ms/step - auc: 0.6356 - binary_accuracy: 0.6212 - false_negatives: 114.9200 - false_positives: 180.8600 - loss: 0.7250 - precision: 0.6194 - recall: 0.7003 - true_negatives: 212.2000 - true_positives: 308.0200\n",
      "For batch number 51 the model has a loss of 0.6578561663627625\n",
      "\u001b[1m 51/689\u001b[0m \u001b[32m━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m34s\u001b[0m 55ms/step - auc: 0.6366 - binary_accuracy: 0.6216 - false_negatives: 117.1176 - false_positives: 184.2549 - loss: 0.7237 - precision: 0.6198 - recall: 0.7010 - true_negatives: 216.3725 - true_positives: 314.2549\n",
      "For batch number 52 the model has a loss of 0.6589660048484802\n",
      "\u001b[1m 52/689\u001b[0m \u001b[32m━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m34s\u001b[0m 55ms/step - auc: 0.6375 - binary_accuracy: 0.6220 - false_negatives: 119.3462 - false_positives: 187.7115 - loss: 0.7224 - precision: 0.6201 - recall: 0.7016 - true_negatives: 220.4808 - true_positives: 320.4615\n",
      "For batch number 53 the model has a loss of 0.6592593193054199\n",
      "\u001b[1m 53/689\u001b[0m \u001b[32m━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m34s\u001b[0m 55ms/step - auc: 0.6384 - binary_accuracy: 0.6223 - false_negatives: 121.6038 - false_positives: 191.1887 - loss: 0.7212 - precision: 0.6204 - recall: 0.7021 - true_negatives: 224.5660 - true_positives: 326.6415\n",
      "For batch number 54 the model has a loss of 0.658977210521698\n",
      "\u001b[1m 54/689\u001b[0m \u001b[32m━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m34s\u001b[0m 55ms/step - auc: 0.6392 - binary_accuracy: 0.6226 - false_negatives: 123.8889 - false_positives: 194.6667 - loss: 0.7201 - precision: 0.6207 - recall: 0.7026 - true_negatives: 228.6667 - true_positives: 332.7778\n",
      "For batch number 55 the model has a loss of 0.6586522459983826\n",
      "\u001b[1m 55/689\u001b[0m \u001b[32m━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m34s\u001b[0m 55ms/step - auc: 0.6400 - binary_accuracy: 0.6229 - false_negatives: 126.2182 - false_positives: 198.1273 - loss: 0.7190 - precision: 0.6209 - recall: 0.7031 - true_negatives: 232.7455 - true_positives: 338.9091\n",
      "For batch number 56 the model has a loss of 0.6602810621261597\n",
      "\u001b[1m 56/689\u001b[0m \u001b[32m━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m34s\u001b[0m 55ms/step - auc: 0.6408 - binary_accuracy: 0.6232 - false_negatives: 128.5179 - false_positives: 201.6786 - loss: 0.7179 - precision: 0.6211 - recall: 0.7035 - true_negatives: 236.8036 - true_positives: 345.0000\n",
      "For batch number 57 the model has a loss of 0.6606258153915405\n",
      "\u001b[1m 57/689\u001b[0m \u001b[32m━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m34s\u001b[0m 55ms/step - auc: 0.6415 - binary_accuracy: 0.6233 - false_negatives: 130.8421 - false_positives: 205.2807 - loss: 0.7169 - precision: 0.6212 - recall: 0.7039 - true_negatives: 240.8596 - true_positives: 351.0175\n",
      "For batch number 58 the model has a loss of 0.6605827808380127\n",
      "\u001b[1m 58/689\u001b[0m \u001b[32m━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m34s\u001b[0m 55ms/step - auc: 0.6421 - binary_accuracy: 0.6235 - false_negatives: 133.1552 - false_positives: 208.8793 - loss: 0.7159 - precision: 0.6213 - recall: 0.7042 - true_negatives: 244.9828 - true_positives: 356.9828\n",
      "For batch number 59 the model has a loss of 0.6599243879318237\n",
      "\u001b[1m 59/689\u001b[0m \u001b[32m━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m34s\u001b[0m 55ms/step - auc: 0.6428 - binary_accuracy: 0.6237 - false_negatives: 135.4407 - false_positives: 212.4746 - loss: 0.7150 - precision: 0.6214 - recall: 0.7046 - true_negatives: 249.1864 - true_positives: 362.8983\n",
      "For batch number 60 the model has a loss of 0.6595833897590637\n",
      "\u001b[1m 60/689\u001b[0m \u001b[32m━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m34s\u001b[0m 55ms/step - auc: 0.6434 - binary_accuracy: 0.6239 - false_negatives: 137.7167 - false_positives: 216.0500 - loss: 0.7141 - precision: 0.6215 - recall: 0.7049 - true_negatives: 253.4000 - true_positives: 368.8333\n",
      "For batch number 61 the model has a loss of 0.6594122052192688\n",
      "\u001b[1m 61/689\u001b[0m \u001b[32m━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m34s\u001b[0m 55ms/step - auc: 0.6440 - binary_accuracy: 0.6241 - false_negatives: 140.0656 - false_positives: 219.5902 - loss: 0.7132 - precision: 0.6216 - recall: 0.7052 - true_negatives: 257.5901 - true_positives: 374.7541\n",
      "For batch number 62 the model has a loss of 0.6601008772850037\n",
      "\u001b[1m 62/689\u001b[0m \u001b[32m━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m34s\u001b[0m 55ms/step - auc: 0.6446 - binary_accuracy: 0.6243 - false_negatives: 142.4355 - false_positives: 223.1129 - loss: 0.7123 - precision: 0.6217 - recall: 0.7054 - true_negatives: 261.8226 - true_positives: 380.6290\n",
      "For batch number 63 the model has a loss of 0.661154568195343\n",
      "\u001b[1m 63/689\u001b[0m \u001b[32m━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m34s\u001b[0m 55ms/step - auc: 0.6451 - binary_accuracy: 0.6244 - false_negatives: 144.8889 - false_positives: 226.6667 - loss: 0.7115 - precision: 0.6218 - recall: 0.7056 - true_negatives: 266.0159 - true_positives: 386.4286\n",
      "For batch number 64 the model has a loss of 0.6607268452644348\n",
      "\u001b[1m 64/689\u001b[0m \u001b[32m━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m34s\u001b[0m 54ms/step - auc: 0.6456 - binary_accuracy: 0.6245 - false_negatives: 147.3438 - false_positives: 230.1719 - loss: 0.7107 - precision: 0.6219 - recall: 0.7057 - true_negatives: 270.2500 - true_positives: 392.2344\n",
      "For batch number 65 the model has a loss of 0.6596026420593262\n",
      "\u001b[1m 65/689\u001b[0m \u001b[32m━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m33s\u001b[0m 54ms/step - auc: 0.6461 - binary_accuracy: 0.6247 - false_negatives: 149.8000 - false_positives: 233.6154 - loss: 0.7099 - precision: 0.6220 - recall: 0.7059 - true_negatives: 274.4923 - true_positives: 398.0923\n",
      "For batch number 66 the model has a loss of 0.658526599407196\n",
      "\u001b[1m 66/689\u001b[0m \u001b[32m━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m33s\u001b[0m 54ms/step - auc: 0.6466 - binary_accuracy: 0.6248 - false_negatives: 152.2576 - false_positives: 237.0000 - loss: 0.7091 - precision: 0.6222 - recall: 0.7061 - true_negatives: 278.7727 - true_positives: 403.9697\n",
      "For batch number 67 the model has a loss of 0.6586297154426575\n",
      "\u001b[1m 67/689\u001b[0m \u001b[32m━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m33s\u001b[0m 54ms/step - auc: 0.6471 - binary_accuracy: 0.6250 - false_negatives: 154.7164 - false_positives: 240.3582 - loss: 0.7084 - precision: 0.6223 - recall: 0.7062 - true_negatives: 283.0746 - true_positives: 409.8507\n",
      "For batch number 68 the model has a loss of 0.6581165790557861\n",
      "\u001b[1m 68/689\u001b[0m \u001b[32m━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m33s\u001b[0m 54ms/step - auc: 0.6476 - binary_accuracy: 0.6252 - false_negatives: 157.1765 - false_positives: 243.7353 - loss: 0.7076 - precision: 0.6225 - recall: 0.7063 - true_negatives: 287.3971 - true_positives: 415.6912\n",
      "For batch number 69 the model has a loss of 0.6574581861495972\n",
      "\u001b[1m 69/689\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m33s\u001b[0m 54ms/step - auc: 0.6481 - binary_accuracy: 0.6253 - false_negatives: 159.6232 - false_positives: 247.1304 - loss: 0.7069 - precision: 0.6226 - recall: 0.7064 - true_negatives: 291.7246 - true_positives: 421.5217\n",
      "For batch number 70 the model has a loss of 0.6575191617012024\n",
      "\u001b[1m 70/689\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m33s\u001b[0m 54ms/step - auc: 0.6486 - binary_accuracy: 0.6255 - false_negatives: 162.0714 - false_positives: 250.5000 - loss: 0.7062 - precision: 0.6227 - recall: 0.7066 - true_negatives: 296.0428 - true_positives: 427.3857\n",
      "For batch number 71 the model has a loss of 0.6577754020690918\n",
      "\u001b[1m 71/689\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m33s\u001b[0m 54ms/step - auc: 0.6490 - binary_accuracy: 0.6256 - false_negatives: 164.5352 - false_positives: 253.8592 - loss: 0.7055 - precision: 0.6229 - recall: 0.7067 - true_negatives: 300.3521 - true_positives: 433.2535\n",
      "For batch number 72 the model has a loss of 0.6587649583816528\n",
      "\u001b[1m 72/689\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m33s\u001b[0m 54ms/step - auc: 0.6494 - binary_accuracy: 0.6258 - false_negatives: 167.0694 - false_positives: 257.2083 - loss: 0.7049 - precision: 0.6230 - recall: 0.7067 - true_negatives: 304.6528 - true_positives: 439.0695\n",
      "For batch number 73 the model has a loss of 0.6587826609611511\n",
      "\u001b[1m 73/689\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m33s\u001b[0m 54ms/step - auc: 0.6498 - binary_accuracy: 0.6259 - false_negatives: 169.5890 - false_positives: 260.5342 - loss: 0.7042 - precision: 0.6232 - recall: 0.7068 - true_negatives: 309.0137 - true_positives: 444.8630\n",
      "For batch number 74 the model has a loss of 0.6588552594184875\n",
      "\u001b[1m 74/689\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m33s\u001b[0m 54ms/step - auc: 0.6502 - binary_accuracy: 0.6260 - false_negatives: 172.1351 - false_positives: 263.8378 - loss: 0.7036 - precision: 0.6233 - recall: 0.7068 - true_negatives: 313.3784 - true_positives: 450.6487\n",
      "For batch number 75 the model has a loss of 0.6583487391471863\n",
      "\u001b[1m 75/689\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m33s\u001b[0m 54ms/step - auc: 0.6505 - binary_accuracy: 0.6262 - false_negatives: 174.6933 - false_positives: 267.0933 - loss: 0.7030 - precision: 0.6235 - recall: 0.7069 - true_negatives: 317.7733 - true_positives: 456.4400\n",
      "For batch number 76 the model has a loss of 0.6585054397583008\n",
      "\u001b[1m 76/689\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m33s\u001b[0m 54ms/step - auc: 0.6509 - binary_accuracy: 0.6263 - false_negatives: 177.2763 - false_positives: 270.3421 - loss: 0.7024 - precision: 0.6236 - recall: 0.7069 - true_negatives: 322.1711 - true_positives: 462.2105\n",
      "For batch number 77 the model has a loss of 0.6582725644111633\n",
      "\u001b[1m 77/689\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m33s\u001b[0m 54ms/step - auc: 0.6512 - binary_accuracy: 0.6264 - false_negatives: 179.8961 - false_positives: 273.5844 - loss: 0.7019 - precision: 0.6237 - recall: 0.7068 - true_negatives: 326.6104 - true_positives: 467.9091\n",
      "For batch number 78 the model has a loss of 0.658185601234436\n",
      "\u001b[1m 78/689\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m33s\u001b[0m 54ms/step - auc: 0.6516 - binary_accuracy: 0.6265 - false_negatives: 182.5128 - false_positives: 276.8333 - loss: 0.7013 - precision: 0.6239 - recall: 0.7068 - true_negatives: 331.0641 - true_positives: 473.5898\n",
      "For batch number 79 the model has a loss of 0.6583953499794006\n",
      "\u001b[1m 79/689\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m33s\u001b[0m 54ms/step - auc: 0.6519 - binary_accuracy: 0.6266 - false_negatives: 185.1519 - false_positives: 280.0886 - loss: 0.7008 - precision: 0.6240 - recall: 0.7067 - true_negatives: 335.4810 - true_positives: 479.2785\n",
      "For batch number 80 the model has a loss of 0.6587585210800171\n",
      "\u001b[1m 80/689\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m33s\u001b[0m 54ms/step - auc: 0.6522 - binary_accuracy: 0.6267 - false_negatives: 187.8250 - false_positives: 283.3750 - loss: 0.7002 - precision: 0.6241 - recall: 0.7066 - true_negatives: 339.8500 - true_positives: 484.9500\n",
      "For batch number 81 the model has a loss of 0.6587644219398499\n",
      "\u001b[1m 81/689\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m32s\u001b[0m 54ms/step - auc: 0.6524 - binary_accuracy: 0.6267 - false_negatives: 190.4568 - false_positives: 286.7037 - loss: 0.6997 - precision: 0.6242 - recall: 0.7066 - true_negatives: 344.1975 - true_positives: 490.6420\n",
      "For batch number 82 the model has a loss of 0.6587525606155396\n",
      "\u001b[1m 82/689\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m32s\u001b[0m 54ms/step - auc: 0.6527 - binary_accuracy: 0.6268 - false_negatives: 193.1098 - false_positives: 290.0488 - loss: 0.6992 - precision: 0.6243 - recall: 0.7065 - true_negatives: 348.5122 - true_positives: 496.3293\n",
      "For batch number 83 the model has a loss of 0.6590450406074524\n",
      "\u001b[1m 83/689\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m32s\u001b[0m 54ms/step - auc: 0.6530 - binary_accuracy: 0.6268 - false_negatives: 195.7831 - false_positives: 293.3855 - loss: 0.6987 - precision: 0.6243 - recall: 0.7064 - true_negatives: 352.8072 - true_positives: 502.0241\n",
      "For batch number 84 the model has a loss of 0.6589067578315735\n",
      "\u001b[1m 84/689\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m32s\u001b[0m 54ms/step - auc: 0.6532 - binary_accuracy: 0.6269 - false_negatives: 198.4286 - false_positives: 296.7500 - loss: 0.6983 - precision: 0.6244 - recall: 0.7064 - true_negatives: 357.1071 - true_positives: 507.7143\n",
      "For batch number 85 the model has a loss of 0.659003734588623\n",
      "\u001b[1m 85/689\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m32s\u001b[0m 54ms/step - auc: 0.6535 - binary_accuracy: 0.6269 - false_negatives: 201.0588 - false_positives: 300.1294 - loss: 0.6978 - precision: 0.6245 - recall: 0.7063 - true_negatives: 361.4000 - true_positives: 513.4117\n",
      "For batch number 86 the model has a loss of 0.6584321856498718\n",
      "\u001b[1m 86/689\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m32s\u001b[0m 54ms/step - auc: 0.6537 - binary_accuracy: 0.6270 - false_negatives: 203.6628 - false_positives: 303.5000 - loss: 0.6974 - precision: 0.6245 - recall: 0.7063 - true_negatives: 365.7442 - true_positives: 519.0930\n",
      "For batch number 87 the model has a loss of 0.6583002805709839\n",
      "\u001b[1m 87/689\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m32s\u001b[0m 54ms/step - auc: 0.6539 - binary_accuracy: 0.6270 - false_negatives: 206.2414 - false_positives: 306.9310 - loss: 0.6969 - precision: 0.6245 - recall: 0.7062 - true_negatives: 370.0575 - true_positives: 524.7701\n",
      "For batch number 88 the model has a loss of 0.6574339270591736\n",
      "\u001b[1m 88/689\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m32s\u001b[0m 54ms/step - auc: 0.6542 - binary_accuracy: 0.6270 - false_negatives: 208.7727 - false_positives: 310.3750 - loss: 0.6965 - precision: 0.6246 - recall: 0.7062 - true_negatives: 374.3523 - true_positives: 530.5000\n",
      "For batch number 89 the model has a loss of 0.6561821699142456\n",
      "\u001b[1m 89/689\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m32s\u001b[0m 54ms/step - auc: 0.6544 - binary_accuracy: 0.6271 - false_negatives: 211.2584 - false_positives: 313.8090 - loss: 0.6960 - precision: 0.6246 - recall: 0.7062 - true_negatives: 378.6854 - true_positives: 536.2472\n",
      "For batch number 90 the model has a loss of 0.6557577252388\n",
      "\u001b[1m 90/689\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m32s\u001b[0m 54ms/step - auc: 0.6547 - binary_accuracy: 0.6272 - false_negatives: 213.7222 - false_positives: 317.2333 - loss: 0.6956 - precision: 0.6247 - recall: 0.7063 - true_negatives: 383.0333 - true_positives: 542.0111\n",
      "For batch number 91 the model has a loss of 0.6550406217575073\n",
      "\u001b[1m 91/689\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m32s\u001b[0m 54ms/step - auc: 0.6550 - binary_accuracy: 0.6273 - false_negatives: 216.1538 - false_positives: 320.6593 - loss: 0.6951 - precision: 0.6247 - recall: 0.7063 - true_negatives: 387.3956 - true_positives: 547.7912\n",
      "For batch number 92 the model has a loss of 0.6544725298881531\n",
      "\u001b[1m 92/689\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m32s\u001b[0m 54ms/step - auc: 0.6553 - binary_accuracy: 0.6274 - false_negatives: 218.5544 - false_positives: 324.0869 - loss: 0.6947 - precision: 0.6248 - recall: 0.7064 - true_negatives: 391.7935 - true_positives: 553.5652\n",
      "For batch number 93 the model has a loss of 0.656072735786438\n",
      "\u001b[1m 93/689\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m32s\u001b[0m 54ms/step - auc: 0.6555 - binary_accuracy: 0.6274 - false_negatives: 220.9570 - false_positives: 327.5269 - loss: 0.6943 - precision: 0.6248 - recall: 0.7064 - true_negatives: 396.2043 - true_positives: 559.3118\n",
      "For batch number 94 the model has a loss of 0.6573485732078552\n",
      "\u001b[1m 94/689\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m32s\u001b[0m 54ms/step - auc: 0.6558 - binary_accuracy: 0.6275 - false_negatives: 223.3723 - false_positives: 330.9787 - loss: 0.6939 - precision: 0.6248 - recall: 0.7065 - true_negatives: 400.6064 - true_positives: 565.0425\n",
      "For batch number 95 the model has a loss of 0.6580944061279297\n",
      "\u001b[1m 95/689\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m32s\u001b[0m 54ms/step - auc: 0.6560 - binary_accuracy: 0.6276 - false_negatives: 225.8211 - false_positives: 334.4000 - loss: 0.6935 - precision: 0.6249 - recall: 0.7065 - true_negatives: 405.0211 - true_positives: 570.7579\n",
      "For batch number 96 the model has a loss of 0.6575445532798767\n",
      "\u001b[1m 96/689\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m32s\u001b[0m 54ms/step - auc: 0.6562 - binary_accuracy: 0.6277 - false_negatives: 228.2604 - false_positives: 337.8333 - loss: 0.6931 - precision: 0.6249 - recall: 0.7065 - true_negatives: 409.4688 - true_positives: 576.4375\n",
      "For batch number 97 the model has a loss of 0.6568178534507751\n",
      "\u001b[1m 97/689\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m32s\u001b[0m 54ms/step - auc: 0.6565 - binary_accuracy: 0.6277 - false_negatives: 230.6804 - false_positives: 341.2680 - loss: 0.6927 - precision: 0.6249 - recall: 0.7065 - true_negatives: 413.9485 - true_positives: 582.1031\n",
      "For batch number 98 the model has a loss of 0.6559554934501648\n",
      "\u001b[1m 98/689\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m32s\u001b[0m 54ms/step - auc: 0.6567 - binary_accuracy: 0.6278 - false_negatives: 233.0816 - false_positives: 344.6735 - loss: 0.6924 - precision: 0.6249 - recall: 0.7066 - true_negatives: 418.4898 - true_positives: 587.7551\n",
      "For batch number 99 the model has a loss of 0.6561715006828308\n",
      "\u001b[1m 99/689\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m32s\u001b[0m 54ms/step - auc: 0.6569 - binary_accuracy: 0.6279 - false_negatives: 235.5253 - false_positives: 348.0606 - loss: 0.6920 - precision: 0.6250 - recall: 0.7066 - true_negatives: 423.0000 - true_positives: 593.4141\n",
      "For batch number 100 the model has a loss of 0.6572902798652649\n",
      "\u001b[1m100/689\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m31s\u001b[0m 54ms/step - auc: 0.6571 - binary_accuracy: 0.6280 - false_negatives: 237.9700 - false_positives: 351.4600 - loss: 0.6916 - precision: 0.6250 - recall: 0.7066 - true_negatives: 427.5200 - true_positives: 599.0500\n",
      "For batch number 101 the model has a loss of 0.6568174958229065\n",
      "\u001b[1m101/689\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m31s\u001b[0m 54ms/step - auc: 0.6573 - binary_accuracy: 0.6280 - false_negatives: 240.4257 - false_positives: 354.8317 - loss: 0.6913 - precision: 0.6250 - recall: 0.7066 - true_negatives: 432.0594 - true_positives: 604.6832\n",
      "For batch number 102 the model has a loss of 0.655954122543335\n",
      "\u001b[1m102/689\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m31s\u001b[0m 54ms/step - auc: 0.6575 - binary_accuracy: 0.6281 - false_negatives: 242.8627 - false_positives: 358.1667 - loss: 0.6910 - precision: 0.6251 - recall: 0.7066 - true_negatives: 436.6569 - true_positives: 610.3137\n",
      "For batch number 103 the model has a loss of 0.6551584601402283\n",
      "\u001b[1m103/689\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m31s\u001b[0m 54ms/step - auc: 0.6578 - binary_accuracy: 0.6282 - false_negatives: 245.2621 - false_positives: 361.5243 - loss: 0.6906 - precision: 0.6251 - recall: 0.7066 - true_negatives: 441.2913 - true_positives: 615.9223\n",
      "For batch number 104 the model has a loss of 0.6543505787849426\n",
      "\u001b[1m104/689\u001b[0m \u001b[32m━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m31s\u001b[0m 54ms/step - auc: 0.6580 - binary_accuracy: 0.6283 - false_negatives: 247.6635 - false_positives: 364.8365 - loss: 0.6903 - precision: 0.6251 - recall: 0.7066 - true_negatives: 445.9712 - true_positives: 621.5289\n",
      "For batch number 105 the model has a loss of 0.6531183123588562\n",
      "\u001b[1m105/689\u001b[0m \u001b[32m━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m31s\u001b[0m 54ms/step - auc: 0.6582 - binary_accuracy: 0.6284 - false_negatives: 250.0762 - false_positives: 368.0952 - loss: 0.6899 - precision: 0.6252 - recall: 0.7066 - true_negatives: 450.6857 - true_positives: 627.1429\n",
      "For batch number 106 the model has a loss of 0.6537291407585144\n",
      "\u001b[1m106/689\u001b[0m \u001b[32m━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m31s\u001b[0m 54ms/step - auc: 0.6585 - binary_accuracy: 0.6285 - false_negatives: 252.5000 - false_positives: 371.3491 - loss: 0.6896 - precision: 0.6252 - recall: 0.7066 - true_negatives: 455.4057 - true_positives: 632.7453\n",
      "For batch number 107 the model has a loss of 0.6524081230163574\n",
      "\u001b[1m107/689\u001b[0m \u001b[32m━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m31s\u001b[0m 54ms/step - auc: 0.6587 - binary_accuracy: 0.6287 - false_negatives: 254.8878 - false_positives: 374.5888 - loss: 0.6892 - precision: 0.6253 - recall: 0.7066 - true_negatives: 460.1682 - true_positives: 638.3552\n",
      "For batch number 108 the model has a loss of 0.6514506936073303\n",
      "\u001b[1m108/689\u001b[0m \u001b[32m━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m31s\u001b[0m 54ms/step - auc: 0.6590 - binary_accuracy: 0.6288 - false_negatives: 257.2500 - false_positives: 377.8055 - loss: 0.6889 - precision: 0.6254 - recall: 0.7066 - true_negatives: 464.9537 - true_positives: 643.9907\n",
      "For batch number 109 the model has a loss of 0.6511802077293396\n",
      "\u001b[1m109/689\u001b[0m \u001b[32m━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m31s\u001b[0m 54ms/step - auc: 0.6592 - binary_accuracy: 0.6289 - false_negatives: 259.6147 - false_positives: 381.0183 - loss: 0.6885 - precision: 0.6255 - recall: 0.7067 - true_negatives: 469.7339 - true_positives: 649.6331\n",
      "For batch number 110 the model has a loss of 0.6519274711608887\n",
      "\u001b[1m110/689\u001b[0m \u001b[32m━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m31s\u001b[0m 54ms/step - auc: 0.6595 - binary_accuracy: 0.6291 - false_negatives: 262.0091 - false_positives: 384.2091 - loss: 0.6882 - precision: 0.6255 - recall: 0.7067 - true_negatives: 474.5364 - true_positives: 655.2455\n",
      "For batch number 111 the model has a loss of 0.6511135697364807\n",
      "\u001b[1m111/689\u001b[0m \u001b[32m━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m31s\u001b[0m 54ms/step - auc: 0.6597 - binary_accuracy: 0.6292 - false_negatives: 264.3784 - false_positives: 387.4054 - loss: 0.6879 - precision: 0.6256 - recall: 0.7067 - true_negatives: 479.3513 - true_positives: 660.8649\n",
      "For batch number 112 the model has a loss of 0.6513999104499817\n",
      "\u001b[1m112/689\u001b[0m \u001b[32m━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m31s\u001b[0m 54ms/step - auc: 0.6599 - binary_accuracy: 0.6294 - false_negatives: 266.7500 - false_positives: 390.5804 - loss: 0.6875 - precision: 0.6257 - recall: 0.7067 - true_negatives: 484.1964 - true_positives: 666.4732\n",
      "For batch number 113 the model has a loss of 0.6499935388565063\n",
      "\u001b[1m113/689\u001b[0m \u001b[32m━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m31s\u001b[0m 54ms/step - auc: 0.6602 - binary_accuracy: 0.6295 - false_negatives: 269.1328 - false_positives: 393.7168 - loss: 0.6872 - precision: 0.6258 - recall: 0.7067 - true_negatives: 489.0708 - true_positives: 672.0797\n",
      "For batch number 114 the model has a loss of 0.6488427519798279\n",
      "\u001b[1m114/689\u001b[0m \u001b[32m━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m31s\u001b[0m 54ms/step - auc: 0.6605 - binary_accuracy: 0.6297 - false_negatives: 271.5088 - false_positives: 396.8158 - loss: 0.6869 - precision: 0.6259 - recall: 0.7068 - true_negatives: 493.9737 - true_positives: 677.7018\n",
      "For batch number 115 the model has a loss of 0.648489236831665\n",
      "\u001b[1m115/689\u001b[0m \u001b[32m━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m31s\u001b[0m 54ms/step - auc: 0.6607 - binary_accuracy: 0.6298 - false_negatives: 273.8957 - false_positives: 399.8957 - loss: 0.6865 - precision: 0.6260 - recall: 0.7068 - true_negatives: 498.9218 - true_positives: 683.2869\n",
      "For batch number 116 the model has a loss of 0.6481710076332092\n",
      "\u001b[1m116/689\u001b[0m \u001b[32m━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m31s\u001b[0m 54ms/step - auc: 0.6610 - binary_accuracy: 0.6300 - false_negatives: 276.3017 - false_positives: 402.9569 - loss: 0.6862 - precision: 0.6261 - recall: 0.7068 - true_negatives: 503.8707 - true_positives: 688.8707\n",
      "For batch number 117 the model has a loss of 0.6486057639122009\n",
      "\u001b[1m117/689\u001b[0m \u001b[32m━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m30s\u001b[0m 54ms/step - auc: 0.6612 - binary_accuracy: 0.6301 - false_negatives: 278.7265 - false_positives: 406.0171 - loss: 0.6859 - precision: 0.6262 - recall: 0.7067 - true_negatives: 508.8120 - true_positives: 694.4445\n",
      "For batch number 118 the model has a loss of 0.6479315161705017\n",
      "\u001b[1m118/689\u001b[0m \u001b[32m━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m30s\u001b[0m 54ms/step - auc: 0.6615 - binary_accuracy: 0.6303 - false_negatives: 281.1610 - false_positives: 409.0508 - loss: 0.6856 - precision: 0.6263 - recall: 0.7067 - true_negatives: 513.7712 - true_positives: 700.0170\n",
      "For batch number 119 the model has a loss of 0.6475870609283447\n",
      "\u001b[1m119/689\u001b[0m \u001b[32m━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m30s\u001b[0m 54ms/step - auc: 0.6617 - binary_accuracy: 0.6304 - false_negatives: 283.5714 - false_positives: 412.1008 - loss: 0.6852 - precision: 0.6264 - recall: 0.7067 - true_negatives: 518.7731 - true_positives: 705.5546\n",
      "For batch number 120 the model has a loss of 0.6474682092666626\n",
      "\u001b[1m120/689\u001b[0m \u001b[32m━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m30s\u001b[0m 54ms/step - auc: 0.6620 - binary_accuracy: 0.6306 - false_negatives: 285.9833 - false_positives: 415.1667 - loss: 0.6849 - precision: 0.6265 - recall: 0.7067 - true_negatives: 523.7750 - true_positives: 711.0750\n",
      "For batch number 121 the model has a loss of 0.6472957134246826\n",
      "\u001b[1m121/689\u001b[0m \u001b[32m━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m30s\u001b[0m 54ms/step - auc: 0.6622 - binary_accuracy: 0.6307 - false_negatives: 288.4132 - false_positives: 418.2149 - loss: 0.6846 - precision: 0.6266 - recall: 0.7067 - true_negatives: 528.7686 - true_positives: 716.6033\n",
      "For batch number 122 the model has a loss of 0.6468258500099182\n",
      "\u001b[1m122/689\u001b[0m \u001b[32m━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m30s\u001b[0m 54ms/step - auc: 0.6625 - binary_accuracy: 0.6308 - false_negatives: 290.8524 - false_positives: 421.2377 - loss: 0.6843 - precision: 0.6267 - recall: 0.7067 - true_negatives: 533.7787 - true_positives: 722.1312\n",
      "For batch number 123 the model has a loss of 0.6459856629371643\n",
      "\u001b[1m123/689\u001b[0m \u001b[32m━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m30s\u001b[0m 54ms/step - auc: 0.6627 - binary_accuracy: 0.6310 - false_negatives: 293.2683 - false_positives: 424.2683 - loss: 0.6840 - precision: 0.6268 - recall: 0.7066 - true_negatives: 538.8212 - true_positives: 727.6423\n",
      "For batch number 124 the model has a loss of 0.6457644104957581\n",
      "\u001b[1m124/689\u001b[0m \u001b[32m━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m30s\u001b[0m 54ms/step - auc: 0.6630 - binary_accuracy: 0.6311 - false_negatives: 295.6855 - false_positives: 427.3226 - loss: 0.6837 - precision: 0.6268 - recall: 0.7066 - true_negatives: 543.8629 - true_positives: 733.1290\n",
      "For batch number 125 the model has a loss of 0.6450231075286865\n",
      "\u001b[1m125/689\u001b[0m \u001b[32m━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m30s\u001b[0m 54ms/step - auc: 0.6632 - binary_accuracy: 0.6313 - false_negatives: 298.0800 - false_positives: 430.3920 - loss: 0.6834 - precision: 0.6269 - recall: 0.7066 - true_negatives: 548.9120 - true_positives: 738.6160\n",
      "For batch number 126 the model has a loss of 0.6442845463752747\n",
      "\u001b[1m126/689\u001b[0m \u001b[32m━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m30s\u001b[0m 54ms/step - auc: 0.6635 - binary_accuracy: 0.6314 - false_negatives: 300.4762 - false_positives: 433.4524 - loss: 0.6831 - precision: 0.6270 - recall: 0.7066 - true_negatives: 553.9683 - true_positives: 744.1031\n",
      "For batch number 127 the model has a loss of 0.6440318822860718\n",
      "\u001b[1m127/689\u001b[0m \u001b[32m━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m30s\u001b[0m 54ms/step - auc: 0.6637 - binary_accuracy: 0.6315 - false_negatives: 302.8425 - false_positives: 436.5354 - loss: 0.6828 - precision: 0.6271 - recall: 0.7066 - true_negatives: 559.0315 - true_positives: 749.5906\n",
      "For batch number 128 the model has a loss of 0.6438575983047485\n",
      "\u001b[1m128/689\u001b[0m \u001b[32m━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m30s\u001b[0m 54ms/step - auc: 0.6640 - binary_accuracy: 0.6317 - false_negatives: 305.2109 - false_positives: 439.6016 - loss: 0.6824 - precision: 0.6271 - recall: 0.7066 - true_negatives: 564.1016 - true_positives: 755.0859\n",
      "For batch number 129 the model has a loss of 0.6435088515281677\n",
      "\u001b[1m129/689\u001b[0m \u001b[32m━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m30s\u001b[0m 54ms/step - auc: 0.6642 - binary_accuracy: 0.6318 - false_negatives: 307.5736 - false_positives: 442.6589 - loss: 0.6821 - precision: 0.6272 - recall: 0.7066 - true_negatives: 569.1628 - true_positives: 760.6047\n",
      "For batch number 130 the model has a loss of 0.6433708071708679\n",
      "\u001b[1m130/689\u001b[0m \u001b[32m━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m30s\u001b[0m 54ms/step - auc: 0.6645 - binary_accuracy: 0.6320 - false_negatives: 309.9231 - false_positives: 445.6923 - loss: 0.6818 - precision: 0.6273 - recall: 0.7066 - true_negatives: 574.2538 - true_positives: 766.1308\n",
      "For batch number 131 the model has a loss of 0.6429126262664795\n",
      "\u001b[1m131/689\u001b[0m \u001b[32m━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m30s\u001b[0m 54ms/step - auc: 0.6648 - binary_accuracy: 0.6321 - false_negatives: 312.2596 - false_positives: 448.7099 - loss: 0.6816 - precision: 0.6274 - recall: 0.7066 - true_negatives: 579.3511 - true_positives: 771.6794\n",
      "For batch number 132 the model has a loss of 0.642612099647522\n",
      "\u001b[1m132/689\u001b[0m \u001b[32m━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m30s\u001b[0m 54ms/step - auc: 0.6650 - binary_accuracy: 0.6323 - false_negatives: 314.5909 - false_positives: 451.7349 - loss: 0.6813 - precision: 0.6275 - recall: 0.7066 - true_negatives: 584.4545 - true_positives: 777.2197\n",
      "For batch number 133 the model has a loss of 0.6438804864883423\n",
      "\u001b[1m133/689\u001b[0m \u001b[32m━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m30s\u001b[0m 54ms/step - auc: 0.6653 - binary_accuracy: 0.6324 - false_negatives: 316.9474 - false_positives: 454.7669 - loss: 0.6810 - precision: 0.6276 - recall: 0.7066 - true_negatives: 589.5489 - true_positives: 782.7368\n",
      "For batch number 134 the model has a loss of 0.644142746925354\n",
      "\u001b[1m134/689\u001b[0m \u001b[32m━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m30s\u001b[0m 54ms/step - auc: 0.6655 - binary_accuracy: 0.6326 - false_negatives: 319.2985 - false_positives: 457.7985 - loss: 0.6807 - precision: 0.6277 - recall: 0.7066 - true_negatives: 594.6567 - true_positives: 788.2463\n",
      "For batch number 135 the model has a loss of 0.6446367502212524\n",
      "\u001b[1m135/689\u001b[0m \u001b[32m━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m30s\u001b[0m 54ms/step - auc: 0.6658 - binary_accuracy: 0.6327 - false_negatives: 321.6741 - false_positives: 460.8296 - loss: 0.6804 - precision: 0.6277 - recall: 0.7066 - true_negatives: 599.7407 - true_positives: 793.7556\n",
      "For batch number 136 the model has a loss of 0.6459200382232666\n",
      "\u001b[1m136/689\u001b[0m \u001b[32m━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m30s\u001b[0m 54ms/step - auc: 0.6660 - binary_accuracy: 0.6328 - false_negatives: 324.0588 - false_positives: 463.8897 - loss: 0.6802 - precision: 0.6278 - recall: 0.7066 - true_negatives: 604.8015 - true_positives: 799.2500\n",
      "For batch number 137 the model has a loss of 0.6470522880554199\n",
      "\u001b[1m137/689\u001b[0m \u001b[32m━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m29s\u001b[0m 54ms/step - auc: 0.6662 - binary_accuracy: 0.6330 - false_negatives: 326.4818 - false_positives: 466.9635 - loss: 0.6799 - precision: 0.6279 - recall: 0.7065 - true_negatives: 609.8248 - true_positives: 804.7299\n",
      "For batch number 138 the model has a loss of 0.6469020843505859\n",
      "\u001b[1m138/689\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m29s\u001b[0m 54ms/step - auc: 0.6664 - binary_accuracy: 0.6331 - false_negatives: 328.9276 - false_positives: 470.0145 - loss: 0.6797 - precision: 0.6279 - recall: 0.7065 - true_negatives: 614.8333 - true_positives: 810.2246\n",
      "For batch number 139 the model has a loss of 0.647976279258728\n",
      "\u001b[1m139/689\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m29s\u001b[0m 54ms/step - auc: 0.6666 - binary_accuracy: 0.6332 - false_negatives: 331.4316 - false_positives: 473.0719 - loss: 0.6795 - precision: 0.6280 - recall: 0.7064 - true_negatives: 619.7914 - true_positives: 815.7050\n",
      "For batch number 140 the model has a loss of 0.6476601958274841\n",
      "\u001b[1m140/689\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m29s\u001b[0m 54ms/step - auc: 0.6668 - binary_accuracy: 0.6333 - false_negatives: 333.9143 - false_positives: 476.1357 - loss: 0.6792 - precision: 0.6281 - recall: 0.7064 - true_negatives: 624.7928 - true_positives: 821.1572\n",
      "For batch number 141 the model has a loss of 0.6478201150894165\n",
      "\u001b[1m141/689\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m29s\u001b[0m 54ms/step - auc: 0.6670 - binary_accuracy: 0.6334 - false_negatives: 336.3972 - false_positives: 479.1986 - loss: 0.6790 - precision: 0.6281 - recall: 0.7063 - true_negatives: 629.8014 - true_positives: 826.6028\n",
      "For batch number 142 the model has a loss of 0.647575318813324\n",
      "\u001b[1m142/689\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m29s\u001b[0m 54ms/step - auc: 0.6672 - binary_accuracy: 0.6335 - false_negatives: 338.8803 - false_positives: 482.2676 - loss: 0.6788 - precision: 0.6282 - recall: 0.7063 - true_negatives: 634.8169 - true_positives: 832.0352\n",
      "For batch number 143 the model has a loss of 0.6473813652992249\n",
      "\u001b[1m143/689\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m29s\u001b[0m 54ms/step - auc: 0.6674 - binary_accuracy: 0.6336 - false_negatives: 341.3776 - false_positives: 485.3077 - loss: 0.6786 - precision: 0.6283 - recall: 0.7062 - true_negatives: 639.8322 - true_positives: 837.4825\n",
      "For batch number 144 the model has a loss of 0.6470043659210205\n",
      "\u001b[1m144/689\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m29s\u001b[0m 54ms/step - auc: 0.6676 - binary_accuracy: 0.6337 - false_negatives: 343.8680 - false_positives: 488.3403 - loss: 0.6784 - precision: 0.6283 - recall: 0.7062 - true_negatives: 644.8680 - true_positives: 842.9236\n",
      "For batch number 145 the model has a loss of 0.646796703338623\n",
      "\u001b[1m145/689\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m29s\u001b[0m 54ms/step - auc: 0.6678 - binary_accuracy: 0.6338 - false_negatives: 346.3586 - false_positives: 491.3586 - loss: 0.6781 - precision: 0.6284 - recall: 0.7061 - true_negatives: 649.9172 - true_positives: 848.3655\n",
      "For batch number 146 the model has a loss of 0.6464638113975525\n",
      "\u001b[1m146/689\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m29s\u001b[0m 54ms/step - auc: 0.6679 - binary_accuracy: 0.6339 - false_negatives: 348.8356 - false_positives: 494.3767 - loss: 0.6779 - precision: 0.6284 - recall: 0.7061 - true_negatives: 654.9726 - true_positives: 853.8151\n",
      "For batch number 147 the model has a loss of 0.6463760733604431\n",
      "\u001b[1m147/689\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m29s\u001b[0m 54ms/step - auc: 0.6681 - binary_accuracy: 0.6340 - false_negatives: 351.3129 - false_positives: 497.3810 - loss: 0.6777 - precision: 0.6285 - recall: 0.7060 - true_negatives: 660.0272 - true_positives: 859.2789\n",
      "For batch number 148 the model has a loss of 0.6464201807975769\n",
      "\u001b[1m148/689\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m29s\u001b[0m 54ms/step - auc: 0.6683 - binary_accuracy: 0.6341 - false_negatives: 353.7905 - false_positives: 500.3851 - loss: 0.6775 - precision: 0.6286 - recall: 0.7060 - true_negatives: 665.0743 - true_positives: 864.7500\n",
      "For batch number 149 the model has a loss of 0.6461309790611267\n",
      "\u001b[1m149/689\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m29s\u001b[0m 54ms/step - auc: 0.6685 - binary_accuracy: 0.6342 - false_negatives: 356.2617 - false_positives: 503.3893 - loss: 0.6773 - precision: 0.6287 - recall: 0.7059 - true_negatives: 670.1275 - true_positives: 870.2215\n",
      "For batch number 150 the model has a loss of 0.645510733127594\n",
      "\u001b[1m150/689\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m29s\u001b[0m 54ms/step - auc: 0.6687 - binary_accuracy: 0.6343 - false_negatives: 358.7267 - false_positives: 506.3867 - loss: 0.6771 - precision: 0.6287 - recall: 0.7059 - true_negatives: 675.1800 - true_positives: 875.7067\n",
      "For batch number 151 the model has a loss of 0.6456154584884644\n",
      "\u001b[1m151/689\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m29s\u001b[0m 54ms/step - auc: 0.6689 - binary_accuracy: 0.6344 - false_negatives: 361.2053 - false_positives: 509.3775 - loss: 0.6769 - precision: 0.6288 - recall: 0.7059 - true_negatives: 680.2053 - true_positives: 881.2119\n",
      "For batch number 152 the model has a loss of 0.644978404045105\n",
      "\u001b[1m152/689\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m29s\u001b[0m 54ms/step - auc: 0.6691 - binary_accuracy: 0.6345 - false_negatives: 363.6645 - false_positives: 512.3684 - loss: 0.6767 - precision: 0.6289 - recall: 0.7058 - true_negatives: 685.2237 - true_positives: 886.7434\n",
      "For batch number 153 the model has a loss of 0.6444669961929321\n",
      "\u001b[1m153/689\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m29s\u001b[0m 54ms/step - auc: 0.6693 - binary_accuracy: 0.6346 - false_negatives: 366.1111 - false_positives: 515.3726 - loss: 0.6764 - precision: 0.6290 - recall: 0.7058 - true_negatives: 690.2353 - true_positives: 892.2811\n",
      "For batch number 154 the model has a loss of 0.6443365812301636\n",
      "\u001b[1m154/689\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m28s\u001b[0m 54ms/step - auc: 0.6695 - binary_accuracy: 0.6347 - false_negatives: 368.5454 - false_positives: 518.4026 - loss: 0.6762 - precision: 0.6290 - recall: 0.7058 - true_negatives: 695.2402 - true_positives: 897.8117\n",
      "For batch number 155 the model has a loss of 0.6440739631652832\n",
      "\u001b[1m155/689\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m28s\u001b[0m 54ms/step - auc: 0.6697 - binary_accuracy: 0.6348 - false_negatives: 370.9613 - false_positives: 521.4323 - loss: 0.6760 - precision: 0.6291 - recall: 0.7057 - true_negatives: 700.2387 - true_positives: 903.3677\n",
      "For batch number 156 the model has a loss of 0.6441347002983093\n",
      "\u001b[1m156/689\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m28s\u001b[0m 54ms/step - auc: 0.6699 - binary_accuracy: 0.6349 - false_negatives: 373.3654 - false_positives: 524.4808 - loss: 0.6758 - precision: 0.6292 - recall: 0.7057 - true_negatives: 705.2372 - true_positives: 908.9167\n",
      "For batch number 157 the model has a loss of 0.6433335542678833\n",
      "\u001b[1m157/689\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m28s\u001b[0m 54ms/step - auc: 0.6700 - binary_accuracy: 0.6350 - false_negatives: 375.7643 - false_positives: 527.5159 - loss: 0.6756 - precision: 0.6293 - recall: 0.7057 - true_negatives: 710.2612 - true_positives: 914.4586\n",
      "For batch number 158 the model has a loss of 0.6433788537979126\n",
      "\u001b[1m158/689\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m28s\u001b[0m 54ms/step - auc: 0.6702 - binary_accuracy: 0.6351 - false_negatives: 378.1519 - false_positives: 530.5759 - loss: 0.6754 - precision: 0.6293 - recall: 0.7057 - true_negatives: 715.2658 - true_positives: 920.0063\n",
      "For batch number 159 the model has a loss of 0.6438373923301697\n",
      "\u001b[1m159/689\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m28s\u001b[0m 54ms/step - auc: 0.6704 - binary_accuracy: 0.6352 - false_negatives: 380.5346 - false_positives: 533.6352 - loss: 0.6752 - precision: 0.6294 - recall: 0.7057 - true_negatives: 720.2579 - true_positives: 925.5723\n",
      "For batch number 160 the model has a loss of 0.643302857875824\n",
      "\u001b[1m160/689\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m28s\u001b[0m 54ms/step - auc: 0.6706 - binary_accuracy: 0.6353 - false_negatives: 382.9000 - false_positives: 536.6875 - loss: 0.6750 - precision: 0.6295 - recall: 0.7057 - true_negatives: 725.2438 - true_positives: 931.1688\n",
      "For batch number 161 the model has a loss of 0.6422587037086487\n",
      "\u001b[1m161/689\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m28s\u001b[0m 54ms/step - auc: 0.6708 - binary_accuracy: 0.6354 - false_negatives: 385.2360 - false_positives: 539.7329 - loss: 0.6748 - precision: 0.6295 - recall: 0.7057 - true_negatives: 730.2360 - true_positives: 936.7950\n",
      "For batch number 162 the model has a loss of 0.641384482383728\n",
      "\u001b[1m162/689\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m28s\u001b[0m 54ms/step - auc: 0.6710 - binary_accuracy: 0.6356 - false_negatives: 387.5432 - false_positives: 542.7901 - loss: 0.6746 - precision: 0.6296 - recall: 0.7057 - true_negatives: 735.2346 - true_positives: 942.4321\n",
      "For batch number 163 the model has a loss of 0.6418402194976807\n",
      "\u001b[1m163/689\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m28s\u001b[0m 54ms/step - auc: 0.6712 - binary_accuracy: 0.6357 - false_negatives: 389.8712 - false_positives: 545.8282 - loss: 0.6744 - precision: 0.6297 - recall: 0.7057 - true_negatives: 740.2025 - true_positives: 948.0981\n",
      "For batch number 164 the model has a loss of 0.6417083144187927\n",
      "\u001b[1m164/689\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m28s\u001b[0m 54ms/step - auc: 0.6714 - binary_accuracy: 0.6358 - false_negatives: 392.2012 - false_positives: 548.8597 - loss: 0.6742 - precision: 0.6298 - recall: 0.7058 - true_negatives: 745.1585 - true_positives: 953.7805\n",
      "For batch number 165 the model has a loss of 0.6423698663711548\n",
      "\u001b[1m165/689\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m28s\u001b[0m 54ms/step - auc: 0.6716 - binary_accuracy: 0.6359 - false_negatives: 394.5273 - false_positives: 551.9152 - loss: 0.6740 - precision: 0.6298 - recall: 0.7058 - true_negatives: 750.1152 - true_positives: 959.4424\n",
      "For batch number 166 the model has a loss of 0.6421229839324951\n",
      "\u001b[1m166/689\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m28s\u001b[0m 54ms/step - auc: 0.6718 - binary_accuracy: 0.6360 - false_negatives: 396.8434 - false_positives: 554.9819 - loss: 0.6738 - precision: 0.6299 - recall: 0.7058 - true_negatives: 755.0903 - true_positives: 965.0844\n",
      "For batch number 167 the model has a loss of 0.6420251727104187\n",
      "\u001b[1m167/689\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m28s\u001b[0m 54ms/step - auc: 0.6720 - binary_accuracy: 0.6361 - false_negatives: 399.1557 - false_positives: 558.0539 - loss: 0.6736 - precision: 0.6300 - recall: 0.7058 - true_negatives: 760.0778 - true_positives: 970.7126\n",
      "For batch number 168 the model has a loss of 0.6419782638549805\n",
      "\u001b[1m168/689\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m28s\u001b[0m 54ms/step - auc: 0.6721 - binary_accuracy: 0.6362 - false_negatives: 401.4762 - false_positives: 561.1250 - loss: 0.6734 - precision: 0.6300 - recall: 0.7058 - true_negatives: 765.0536 - true_positives: 976.3452\n",
      "For batch number 169 the model has a loss of 0.6421176195144653\n",
      "\u001b[1m169/689\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m28s\u001b[0m 54ms/step - auc: 0.6723 - binary_accuracy: 0.6363 - false_negatives: 403.7988 - false_positives: 564.2071 - loss: 0.6733 - precision: 0.6301 - recall: 0.7059 - true_negatives: 770.0178 - true_positives: 981.9763\n",
      "For batch number 170 the model has a loss of 0.6421716809272766\n",
      "\u001b[1m170/689\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m28s\u001b[0m 54ms/step - auc: 0.6725 - binary_accuracy: 0.6364 - false_negatives: 406.1353 - false_positives: 567.2823 - loss: 0.6731 - precision: 0.6302 - recall: 0.7059 - true_negatives: 774.9647 - true_positives: 987.6177\n",
      "For batch number 171 the model has a loss of 0.6423803567886353\n",
      "\u001b[1m171/689\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m28s\u001b[0m 54ms/step - auc: 0.6727 - binary_accuracy: 0.6365 - false_negatives: 408.4620 - false_positives: 570.3801 - loss: 0.6729 - precision: 0.6302 - recall: 0.7059 - true_negatives: 779.9123 - true_positives: 993.2456\n",
      "For batch number 172 the model has a loss of 0.6422330141067505\n",
      "\u001b[1m172/689\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m28s\u001b[0m 54ms/step - auc: 0.6728 - binary_accuracy: 0.6366 - false_negatives: 410.7965 - false_positives: 573.4709 - loss: 0.6727 - precision: 0.6303 - recall: 0.7059 - true_negatives: 784.8372 - true_positives: 998.8953\n",
      "For batch number 173 the model has a loss of 0.64220130443573\n",
      "\u001b[1m173/689\u001b[0m \u001b[32m━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━\u001b[0m \u001b[1m27s\u001b[0m 54ms/step - auc: 0.6730 - binary_accuracy: 0.6367 - false_negatives: 413.1329 - false_positives: 576.5838 - loss: 0.6725 - precision: 0.6303 - recall: 0.7059 - true_negatives: 789.7515 - true_positives: 1004.5318\n",
      "For batch number 174 the model has a loss of 0.6423661112785339\n",
      "\u001b[1m174/689\u001b[0m \u001b[32m━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━\u001b[0m \u001b[1m27s\u001b[0m 54ms/step - auc: 0.6731 - binary_accuracy: 0.6368 - false_negatives: 415.4598 - false_positives: 579.7184 - loss: 0.6724 - precision: 0.6304 - recall: 0.7059 - true_negatives: 794.6494 - true_positives: 1010.1724\n",
      "For batch number 175 the model has a loss of 0.6420184969902039\n",
      "\u001b[1m175/689\u001b[0m \u001b[32m━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━\u001b[0m \u001b[1m27s\u001b[0m 54ms/step - auc: 0.6733 - binary_accuracy: 0.6369 - false_negatives: 417.7714 - false_positives: 582.8514 - loss: 0.6722 - precision: 0.6304 - recall: 0.7059 - true_negatives: 799.5543 - true_positives: 1015.8229\n",
      "For batch number 176 the model has a loss of 0.642136812210083\n",
      "\u001b[1m176/689\u001b[0m \u001b[32m━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━\u001b[0m \u001b[1m27s\u001b[0m 54ms/step - auc: 0.6735 - binary_accuracy: 0.6369 - false_negatives: 420.0625 - false_positives: 586.0114 - loss: 0.6720 - precision: 0.6305 - recall: 0.7060 - true_negatives: 804.4489 - true_positives: 1021.4773\n",
      "For batch number 177 the model has a loss of 0.6417905688285828\n",
      "\u001b[1m177/689\u001b[0m \u001b[32m━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━\u001b[0m \u001b[1m27s\u001b[0m 54ms/step - auc: 0.6736 - binary_accuracy: 0.6370 - false_negatives: 422.3446 - false_positives: 589.1695 - loss: 0.6719 - precision: 0.6305 - recall: 0.7060 - true_negatives: 809.3560 - true_positives: 1027.1299\n",
      "For batch number 178 the model has a loss of 0.6413522362709045\n",
      "\u001b[1m178/689\u001b[0m \u001b[32m━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━\u001b[0m \u001b[1m27s\u001b[0m 54ms/step - auc: 0.6738 - binary_accuracy: 0.6371 - false_negatives: 424.6180 - false_positives: 592.3259 - loss: 0.6717 - precision: 0.6306 - recall: 0.7060 - true_negatives: 814.2697 - true_positives: 1032.7865\n",
      "For batch number 179 the model has a loss of 0.6407158970832825\n",
      "\u001b[1m179/689\u001b[0m \u001b[32m━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━\u001b[0m \u001b[1m27s\u001b[0m 54ms/step - auc: 0.6740 - binary_accuracy: 0.6372 - false_negatives: 426.8771 - false_positives: 595.4805 - loss: 0.6715 - precision: 0.6306 - recall: 0.7060 - true_negatives: 819.1788 - true_positives: 1038.4637\n",
      "For batch number 180 the model has a loss of 0.6405007243156433\n",
      "\u001b[1m180/689\u001b[0m \u001b[32m━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━\u001b[0m \u001b[1m27s\u001b[0m 54ms/step - auc: 0.6741 - binary_accuracy: 0.6373 - false_negatives: 429.1278 - false_positives: 598.6334 - loss: 0.6713 - precision: 0.6307 - recall: 0.7061 - true_negatives: 824.0778 - true_positives: 1044.1611\n",
      "For batch number 181 the model has a loss of 0.6409578919410706\n",
      "\u001b[1m181/689\u001b[0m \u001b[32m━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━\u001b[0m \u001b[1m27s\u001b[0m 54ms/step - auc: 0.6743 - binary_accuracy: 0.6374 - false_negatives: 431.3812 - false_positives: 601.7900 - loss: 0.6712 - precision: 0.6307 - recall: 0.7061 - true_negatives: 828.9669 - true_positives: 1049.8619\n",
      "For batch number 182 the model has a loss of 0.6398903727531433\n",
      "\u001b[1m182/689\u001b[0m \u001b[32m━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━\u001b[0m \u001b[1m27s\u001b[0m 54ms/step - auc: 0.6745 - binary_accuracy: 0.6375 - false_negatives: 433.6099 - false_positives: 604.9506 - loss: 0.6710 - precision: 0.6308 - recall: 0.7062 - true_negatives: 833.8681 - true_positives: 1055.5714\n",
      "For batch number 183 the model has a loss of 0.6395682692527771\n",
      "\u001b[1m183/689\u001b[0m \u001b[32m━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━\u001b[0m \u001b[1m27s\u001b[0m 54ms/step - auc: 0.6746 - binary_accuracy: 0.6376 - false_negatives: 435.8197 - false_positives: 608.1202 - loss: 0.6708 - precision: 0.6308 - recall: 0.7062 - true_negatives: 838.7650 - true_positives: 1061.2950\n",
      "For batch number 184 the model has a loss of 0.6406096816062927\n",
      "\u001b[1m184/689\u001b[0m \u001b[32m━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━\u001b[0m \u001b[1m27s\u001b[0m 54ms/step - auc: 0.6748 - binary_accuracy: 0.6377 - false_negatives: 438.0272 - false_positives: 611.2989 - loss: 0.6707 - precision: 0.6309 - recall: 0.7063 - true_negatives: 843.6467 - true_positives: 1067.0272\n",
      "For batch number 185 the model has a loss of 0.640451967716217\n",
      "\u001b[1m185/689\u001b[0m \u001b[32m━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━\u001b[0m \u001b[1m27s\u001b[0m 54ms/step - auc: 0.6750 - binary_accuracy: 0.6378 - false_negatives: 440.2270 - false_positives: 614.4865 - loss: 0.6705 - precision: 0.6309 - recall: 0.7063 - true_negatives: 848.5297 - true_positives: 1072.7567\n",
      "For batch number 186 the model has a loss of 0.6407716274261475\n",
      "\u001b[1m186/689\u001b[0m \u001b[32m━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━\u001b[0m \u001b[1m27s\u001b[0m 54ms/step - auc: 0.6751 - binary_accuracy: 0.6378 - false_negatives: 442.4086 - false_positives: 617.6828 - loss: 0.6703 - precision: 0.6309 - recall: 0.7064 - true_negatives: 853.4086 - true_positives: 1078.5000\n",
      "For batch number 187 the model has a loss of 0.640531599521637\n",
      "\u001b[1m187/689\u001b[0m \u001b[32m━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━\u001b[0m \u001b[1m27s\u001b[0m 54ms/step - auc: 0.6753 - binary_accuracy: 0.6379 - false_negatives: 444.5936 - false_positives: 620.8716 - loss: 0.6702 - precision: 0.6310 - recall: 0.7064 - true_negatives: 858.2834 - true_positives: 1084.2513\n",
      "For batch number 188 the model has a loss of 0.6399386525154114\n",
      "\u001b[1m188/689\u001b[0m \u001b[32m━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━\u001b[0m \u001b[1m27s\u001b[0m 54ms/step - auc: 0.6755 - binary_accuracy: 0.6380 - false_negatives: 446.7713 - false_positives: 624.0425 - loss: 0.6700 - precision: 0.6310 - recall: 0.7065 - true_negatives: 863.1542 - true_positives: 1090.0319\n",
      "For batch number 189 the model has a loss of 0.6404707431793213\n",
      "\u001b[1m189/689\u001b[0m \u001b[32m━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━\u001b[0m \u001b[1m27s\u001b[0m 54ms/step - auc: 0.6757 - binary_accuracy: 0.6381 - false_negatives: 448.9524 - false_positives: 627.2064 - loss: 0.6699 - precision: 0.6311 - recall: 0.7065 - true_negatives: 868.0159 - true_positives: 1095.8254\n",
      "For batch number 190 the model has a loss of 0.6402038931846619\n",
      "\u001b[1m190/689\u001b[0m \u001b[32m━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━\u001b[0m \u001b[1m27s\u001b[0m 54ms/step - auc: 0.6758 - binary_accuracy: 0.6382 - false_negatives: 451.1211 - false_positives: 630.3579 - loss: 0.6697 - precision: 0.6312 - recall: 0.7066 - true_negatives: 872.8737 - true_positives: 1101.6473\n",
      "For batch number 191 the model has a loss of 0.6404018402099609\n",
      "\u001b[1m191/689\u001b[0m \u001b[32m━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━\u001b[0m \u001b[1m27s\u001b[0m 54ms/step - auc: 0.6760 - binary_accuracy: 0.6383 - false_negatives: 453.2775 - false_positives: 633.5131 - loss: 0.6696 - precision: 0.6312 - recall: 0.7067 - true_negatives: 877.7277 - true_positives: 1107.4817\n",
      "For batch number 192 the model has a loss of 0.6401216387748718\n",
      "\u001b[1m192/689\u001b[0m \u001b[32m━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━\u001b[0m \u001b[1m26s\u001b[0m 54ms/step - auc: 0.6762 - binary_accuracy: 0.6384 - false_negatives: 455.4271 - false_positives: 636.6614 - loss: 0.6694 - precision: 0.6313 - recall: 0.7067 - true_negatives: 882.5833 - true_positives: 1113.3281\n",
      "For batch number 193 the model has a loss of 0.639938235282898\n",
      "\u001b[1m193/689\u001b[0m \u001b[32m━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━\u001b[0m \u001b[1m26s\u001b[0m 54ms/step - auc: 0.6763 - binary_accuracy: 0.6385 - false_negatives: 457.5648 - false_positives: 639.8083 - loss: 0.6692 - precision: 0.6313 - recall: 0.7068 - true_negatives: 887.4456 - true_positives: 1119.1814\n",
      "For batch number 194 the model has a loss of 0.6404905319213867\n",
      "\u001b[1m194/689\u001b[0m \u001b[32m━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━\u001b[0m \u001b[1m26s\u001b[0m 54ms/step - auc: 0.6765 - binary_accuracy: 0.6386 - false_negatives: 459.7165 - false_positives: 642.9587 - loss: 0.6691 - precision: 0.6314 - recall: 0.7069 - true_negatives: 892.2938 - true_positives: 1125.0309\n",
      "For batch number 195 the model has a loss of 0.6404041647911072\n",
      "\u001b[1m195/689\u001b[0m \u001b[32m━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━\u001b[0m \u001b[1m26s\u001b[0m 54ms/step - auc: 0.6767 - binary_accuracy: 0.6387 - false_negatives: 461.8667 - false_positives: 646.0974 - loss: 0.6690 - precision: 0.6315 - recall: 0.7070 - true_negatives: 897.1436 - true_positives: 1130.8923\n",
      "For batch number 196 the model has a loss of 0.6402695178985596\n",
      "\u001b[1m196/689\u001b[0m \u001b[32m━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━\u001b[0m \u001b[1m26s\u001b[0m 54ms/step - auc: 0.6768 - binary_accuracy: 0.6388 - false_negatives: 464.0153 - false_positives: 649.2296 - loss: 0.6688 - precision: 0.6315 - recall: 0.7070 - true_negatives: 902.0000 - true_positives: 1136.7551\n",
      "For batch number 197 the model has a loss of 0.6400863528251648\n",
      "\u001b[1m197/689\u001b[0m \u001b[32m━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━\u001b[0m \u001b[1m26s\u001b[0m 54ms/step - auc: 0.6770 - binary_accuracy: 0.6389 - false_negatives: 466.1675 - false_positives: 652.3655 - loss: 0.6687 - precision: 0.6316 - recall: 0.7071 - true_negatives: 906.8477 - true_positives: 1142.6193\n",
      "For batch number 198 the model has a loss of 0.6401849389076233\n",
      "\u001b[1m198/689\u001b[0m \u001b[32m━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━\u001b[0m \u001b[1m26s\u001b[0m 54ms/step - auc: 0.6771 - binary_accuracy: 0.6390 - false_negatives: 468.3333 - false_positives: 655.5101 - loss: 0.6685 - precision: 0.6317 - recall: 0.7072 - true_negatives: 911.6768 - true_positives: 1148.4799\n",
      "For batch number 199 the model has a loss of 0.6401887536048889\n",
      "\u001b[1m199/689\u001b[0m \u001b[32m━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━\u001b[0m \u001b[1m26s\u001b[0m 54ms/step - auc: 0.6773 - binary_accuracy: 0.6391 - false_negatives: 470.5025 - false_positives: 658.6533 - loss: 0.6684 - precision: 0.6317 - recall: 0.7072 - true_negatives: 916.5176 - true_positives: 1154.3267\n",
      "For batch number 200 the model has a loss of 0.6397233009338379\n",
      "\u001b[1m200/689\u001b[0m \u001b[32m━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━\u001b[0m \u001b[1m26s\u001b[0m 54ms/step - auc: 0.6774 - binary_accuracy: 0.6392 - false_negatives: 472.6650 - false_positives: 661.7850 - loss: 0.6682 - precision: 0.6318 - recall: 0.7073 - true_negatives: 921.3650 - true_positives: 1160.1851\n",
      "For batch number 201 the model has a loss of 0.6399433612823486\n",
      "\u001b[1m201/689\u001b[0m \u001b[32m━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━\u001b[0m \u001b[1m26s\u001b[0m 54ms/step - auc: 0.6776 - binary_accuracy: 0.6393 - false_negatives: 474.8159 - false_positives: 664.9055 - loss: 0.6681 - precision: 0.6318 - recall: 0.7074 - true_negatives: 926.2239 - true_positives: 1166.0547\n",
      "For batch number 202 the model has a loss of 0.6399109959602356\n",
      "\u001b[1m202/689\u001b[0m \u001b[32m━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━\u001b[0m \u001b[1m26s\u001b[0m 54ms/step - auc: 0.6778 - binary_accuracy: 0.6394 - false_negatives: 476.9703 - false_positives: 668.0396 - loss: 0.6679 - precision: 0.6319 - recall: 0.7074 - true_negatives: 931.0693 - true_positives: 1171.9208\n",
      "For batch number 203 the model has a loss of 0.6397339701652527\n",
      "\u001b[1m203/689\u001b[0m \u001b[32m━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━\u001b[0m \u001b[1m26s\u001b[0m 54ms/step - auc: 0.6779 - binary_accuracy: 0.6395 - false_negatives: 479.1182 - false_positives: 671.1675 - loss: 0.6678 - precision: 0.6320 - recall: 0.7075 - true_negatives: 935.8965 - true_positives: 1177.8177\n",
      "For batch number 204 the model has a loss of 0.639488160610199\n",
      "\u001b[1m204/689\u001b[0m \u001b[32m━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━\u001b[0m \u001b[1m26s\u001b[0m 54ms/step - auc: 0.6781 - binary_accuracy: 0.6396 - false_negatives: 481.2549 - false_positives: 674.2990 - loss: 0.6677 - precision: 0.6320 - recall: 0.7076 - true_negatives: 940.7255 - true_positives: 1183.7206\n",
      "For batch number 205 the model has a loss of 0.639133632183075\n",
      "\u001b[1m205/689\u001b[0m \u001b[32m━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━\u001b[0m \u001b[1m26s\u001b[0m 54ms/step - auc: 0.6782 - binary_accuracy: 0.6397 - false_negatives: 483.3854 - false_positives: 677.4244 - loss: 0.6675 - precision: 0.6321 - recall: 0.7077 - true_negatives: 945.5610 - true_positives: 1189.6293\n",
      "For batch number 206 the model has a loss of 0.6392596364021301\n",
      "\u001b[1m206/689\u001b[0m \u001b[32m━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━\u001b[0m \u001b[1m26s\u001b[0m 54ms/step - auc: 0.6784 - binary_accuracy: 0.6398 - false_negatives: 485.5049 - false_positives: 680.5485 - loss: 0.6674 - precision: 0.6322 - recall: 0.7077 - true_negatives: 950.3981 - true_positives: 1195.5486\n",
      "For batch number 207 the model has a loss of 0.6401198506355286\n",
      "\u001b[1m207/689\u001b[0m \u001b[32m━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━\u001b[0m \u001b[1m26s\u001b[0m 54ms/step - auc: 0.6785 - binary_accuracy: 0.6399 - false_negatives: 487.6135 - false_positives: 683.7150 - loss: 0.6673 - precision: 0.6322 - recall: 0.7078 - true_negatives: 955.2222 - true_positives: 1201.4492\n",
      "For batch number 208 the model has a loss of 0.6399336457252502\n",
      "\u001b[1m208/689\u001b[0m \u001b[32m━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━\u001b[0m \u001b[1m26s\u001b[0m 54ms/step - auc: 0.6787 - binary_accuracy: 0.6400 - false_negatives: 489.7067 - false_positives: 686.8990 - loss: 0.6671 - precision: 0.6323 - recall: 0.7079 - true_negatives: 960.0385 - true_positives: 1207.3557\n",
      "For batch number 209 the model has a loss of 0.6395714282989502\n",
      "\u001b[1m209/689\u001b[0m \u001b[32m━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━\u001b[0m \u001b[1m26s\u001b[0m 54ms/step - auc: 0.6788 - binary_accuracy: 0.6401 - false_negatives: 491.7895 - false_positives: 690.0814 - loss: 0.6670 - precision: 0.6323 - recall: 0.7080 - true_negatives: 964.8613 - true_positives: 1213.2679\n",
      "For batch number 210 the model has a loss of 0.6398536562919617\n",
      "\u001b[1m210/689\u001b[0m \u001b[32m━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━\u001b[0m \u001b[1m25s\u001b[0m 54ms/step - auc: 0.6790 - binary_accuracy: 0.6402 - false_negatives: 493.8714 - false_positives: 693.2667 - loss: 0.6669 - precision: 0.6324 - recall: 0.7081 - true_negatives: 969.6857 - true_positives: 1219.1761\n",
      "For batch number 211 the model has a loss of 0.6400629281997681\n",
      "\u001b[1m211/689\u001b[0m \u001b[32m━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━\u001b[0m \u001b[1m25s\u001b[0m 54ms/step - auc: 0.6791 - binary_accuracy: 0.6402 - false_negatives: 495.9479 - false_positives: 696.4597 - loss: 0.6667 - precision: 0.6324 - recall: 0.7081 - true_negatives: 974.4976 - true_positives: 1225.0947\n",
      "For batch number 212 the model has a loss of 0.640143871307373\n",
      "\u001b[1m212/689\u001b[0m \u001b[32m━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━\u001b[0m \u001b[1m25s\u001b[0m 54ms/step - auc: 0.6793 - binary_accuracy: 0.6403 - false_negatives: 498.0094 - false_positives: 699.6793 - loss: 0.6666 - precision: 0.6325 - recall: 0.7082 - true_negatives: 979.3113 - true_positives: 1231.0000\n",
      "For batch number 213 the model has a loss of 0.640556812286377\n",
      "\u001b[1m213/689\u001b[0m \u001b[32m━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━\u001b[0m \u001b[1m25s\u001b[0m 54ms/step - auc: 0.6794 - binary_accuracy: 0.6404 - false_negatives: 500.0704 - false_positives: 702.9155 - loss: 0.6665 - precision: 0.6325 - recall: 0.7083 - true_negatives: 984.1127 - true_positives: 1236.9014\n",
      "For batch number 214 the model has a loss of 0.6409936547279358\n",
      "\u001b[1m214/689\u001b[0m \u001b[32m━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━\u001b[0m \u001b[1m25s\u001b[0m 54ms/step - auc: 0.6795 - binary_accuracy: 0.6405 - false_negatives: 502.1262 - false_positives: 706.1729 - loss: 0.6664 - precision: 0.6326 - recall: 0.7084 - true_negatives: 988.9159 - true_positives: 1242.7850\n",
      "For batch number 215 the model has a loss of 0.6407213807106018\n",
      "\u001b[1m215/689\u001b[0m \u001b[32m━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━\u001b[0m \u001b[1m25s\u001b[0m 54ms/step - auc: 0.6797 - binary_accuracy: 0.6406 - false_negatives: 504.1768 - false_positives: 709.4326 - loss: 0.6663 - precision: 0.6326 - recall: 0.7085 - true_negatives: 993.7116 - true_positives: 1248.6791\n",
      "For batch number 216 the model has a loss of 0.6410128474235535\n",
      "\u001b[1m216/689\u001b[0m \u001b[32m━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━\u001b[0m \u001b[1m25s\u001b[0m 54ms/step - auc: 0.6798 - binary_accuracy: 0.6407 - false_negatives: 506.2268 - false_positives: 712.7037 - loss: 0.6661 - precision: 0.6326 - recall: 0.7086 - true_negatives: 998.5046 - true_positives: 1254.5648\n",
      "For batch number 217 the model has a loss of 0.6408132314682007\n",
      "\u001b[1m217/689\u001b[0m \u001b[32m━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━\u001b[0m \u001b[1m25s\u001b[0m 54ms/step - auc: 0.6799 - binary_accuracy: 0.6408 - false_negatives: 508.2765 - false_positives: 715.9724 - loss: 0.6660 - precision: 0.6327 - recall: 0.7087 - true_negatives: 1003.2949 - true_positives: 1260.4562\n",
      "For batch number 218 the model has a loss of 0.6402608752250671\n",
      "\u001b[1m218/689\u001b[0m \u001b[32m━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━\u001b[0m \u001b[1m25s\u001b[0m 54ms/step - auc: 0.6801 - binary_accuracy: 0.6408 - false_negatives: 510.3073 - false_positives: 719.2339 - loss: 0.6659 - precision: 0.6327 - recall: 0.7087 - true_negatives: 1008.1009 - true_positives: 1266.3578\n",
      "For batch number 219 the model has a loss of 0.6396764516830444\n",
      "\u001b[1m219/689\u001b[0m \u001b[32m━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━\u001b[0m \u001b[1m25s\u001b[0m 54ms/step - auc: 0.6802 - binary_accuracy: 0.6409 - false_negatives: 512.3425 - false_positives: 722.4886 - loss: 0.6658 - precision: 0.6328 - recall: 0.7088 - true_negatives: 1012.9041 - true_positives: 1272.2649\n",
      "For batch number 220 the model has a loss of 0.6394813060760498\n",
      "\u001b[1m220/689\u001b[0m \u001b[32m━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━\u001b[0m \u001b[1m25s\u001b[0m 54ms/step - auc: 0.6803 - binary_accuracy: 0.6410 - false_negatives: 514.3773 - false_positives: 725.7409 - loss: 0.6657 - precision: 0.6328 - recall: 0.7089 - true_negatives: 1017.7000 - true_positives: 1278.1818\n",
      "For batch number 221 the model has a loss of 0.6386434435844421\n",
      "\u001b[1m221/689\u001b[0m \u001b[32m━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━\u001b[0m \u001b[1m25s\u001b[0m 54ms/step - auc: 0.6805 - binary_accuracy: 0.6411 - false_negatives: 516.3982 - false_positives: 728.9728 - loss: 0.6655 - precision: 0.6328 - recall: 0.7090 - true_negatives: 1022.5113 - true_positives: 1284.1177\n",
      "For batch number 222 the model has a loss of 0.6384332776069641\n",
      "\u001b[1m222/689\u001b[0m \u001b[32m━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━\u001b[0m \u001b[1m25s\u001b[0m 54ms/step - auc: 0.6806 - binary_accuracy: 0.6412 - false_negatives: 518.4279 - false_positives: 732.1982 - loss: 0.6654 - precision: 0.6329 - recall: 0.7091 - true_negatives: 1027.3198 - true_positives: 1290.0541\n",
      "For batch number 223 the model has a loss of 0.6378428936004639\n",
      "\u001b[1m223/689\u001b[0m \u001b[32m━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━\u001b[0m \u001b[1m25s\u001b[0m 54ms/step - auc: 0.6807 - binary_accuracy: 0.6413 - false_negatives: 520.4529 - false_positives: 735.4081 - loss: 0.6653 - precision: 0.6329 - recall: 0.7092 - true_negatives: 1032.1300 - true_positives: 1296.0089\n",
      "For batch number 224 the model has a loss of 0.637495219707489\n",
      "\u001b[1m224/689\u001b[0m \u001b[32m━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━\u001b[0m \u001b[1m25s\u001b[0m 54ms/step - auc: 0.6809 - binary_accuracy: 0.6414 - false_negatives: 522.4643 - false_positives: 738.6339 - loss: 0.6652 - precision: 0.6330 - recall: 0.7093 - true_negatives: 1036.9286 - true_positives: 1301.9733\n",
      "For batch number 225 the model has a loss of 0.6384061574935913\n",
      "\u001b[1m225/689\u001b[0m \u001b[32m━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━\u001b[0m \u001b[1m25s\u001b[0m 54ms/step - auc: 0.6810 - binary_accuracy: 0.6414 - false_negatives: 524.4800 - false_positives: 741.8666 - loss: 0.6651 - precision: 0.6330 - recall: 0.7094 - true_negatives: 1041.7067 - true_positives: 1307.9467\n",
      "For batch number 226 the model has a loss of 0.6376784443855286\n",
      "\u001b[1m226/689\u001b[0m \u001b[32m━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━\u001b[0m \u001b[1m25s\u001b[0m 54ms/step - auc: 0.6811 - binary_accuracy: 0.6415 - false_negatives: 526.4779 - false_positives: 745.1106 - loss: 0.6649 - precision: 0.6331 - recall: 0.7095 - true_negatives: 1046.4779 - true_positives: 1313.9336\n",
      "For batch number 227 the model has a loss of 0.6379413604736328\n",
      "\u001b[1m227/689\u001b[0m \u001b[32m━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━\u001b[0m \u001b[1m25s\u001b[0m 54ms/step - auc: 0.6813 - binary_accuracy: 0.6416 - false_negatives: 528.4626 - false_positives: 748.3921 - loss: 0.6648 - precision: 0.6331 - recall: 0.7096 - true_negatives: 1051.2379 - true_positives: 1319.9075\n",
      "For batch number 228 the model has a loss of 0.6386249661445618\n",
      "\u001b[1m228/689\u001b[0m \u001b[32m━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━\u001b[0m \u001b[1m25s\u001b[0m 54ms/step - auc: 0.6814 - binary_accuracy: 0.6417 - false_negatives: 530.4430 - false_positives: 751.6886 - loss: 0.6647 - precision: 0.6331 - recall: 0.7097 - true_negatives: 1055.9956 - true_positives: 1325.8728\n",
      "For batch number 229 the model has a loss of 0.6380882859230042\n",
      "\u001b[1m229/689\u001b[0m \u001b[32m━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━\u001b[0m \u001b[1m25s\u001b[0m 54ms/step - auc: 0.6815 - binary_accuracy: 0.6418 - false_negatives: 532.4105 - false_positives: 754.9869 - loss: 0.6646 - precision: 0.6332 - recall: 0.7098 - true_negatives: 1060.7511 - true_positives: 1331.8516\n",
      "For batch number 230 the model has a loss of 0.6377013325691223\n",
      "\u001b[1m230/689\u001b[0m \u001b[32m━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━\u001b[0m \u001b[1m24s\u001b[0m 54ms/step - auc: 0.6817 - binary_accuracy: 0.6419 - false_negatives: 534.3652 - false_positives: 758.2869 - loss: 0.6645 - precision: 0.6332 - recall: 0.7099 - true_negatives: 1065.5173 - true_positives: 1337.8304\n",
      "For batch number 231 the model has a loss of 0.6380082368850708\n",
      "\u001b[1m231/689\u001b[0m \u001b[32m━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━\u001b[0m \u001b[1m24s\u001b[0m 54ms/step - auc: 0.6818 - binary_accuracy: 0.6419 - false_negatives: 536.3246 - false_positives: 761.5757 - loss: 0.6644 - precision: 0.6333 - recall: 0.7100 - true_negatives: 1070.2727 - true_positives: 1343.8268\n",
      "For batch number 232 the model has a loss of 0.6382349729537964\n",
      "\u001b[1m232/689\u001b[0m \u001b[32m━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━\u001b[0m \u001b[1m24s\u001b[0m 54ms/step - auc: 0.6820 - binary_accuracy: 0.6420 - false_negatives: 538.2715 - false_positives: 764.8879 - loss: 0.6642 - precision: 0.6333 - recall: 0.7101 - true_negatives: 1075.0344 - true_positives: 1349.8060\n",
      "For batch number 233 the model has a loss of 0.6380404829978943\n",
      "\u001b[1m233/689\u001b[0m \u001b[32m━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━\u001b[0m \u001b[1m24s\u001b[0m 54ms/step - auc: 0.6821 - binary_accuracy: 0.6421 - false_negatives: 540.2275 - false_positives: 768.1888 - loss: 0.6641 - precision: 0.6333 - recall: 0.7102 - true_negatives: 1079.7897 - true_positives: 1355.7939\n",
      "For batch number 234 the model has a loss of 0.6380100250244141\n",
      "\u001b[1m234/689\u001b[0m \u001b[32m━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━\u001b[0m \u001b[1m24s\u001b[0m 54ms/step - auc: 0.6822 - binary_accuracy: 0.6422 - false_negatives: 542.1752 - false_positives: 771.5000 - loss: 0.6640 - precision: 0.6334 - recall: 0.7103 - true_negatives: 1084.5555 - true_positives: 1361.7693\n",
      "For batch number 235 the model has a loss of 0.6379384398460388\n",
      "\u001b[1m235/689\u001b[0m \u001b[32m━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━\u001b[0m \u001b[1m24s\u001b[0m 54ms/step - auc: 0.6823 - binary_accuracy: 0.6423 - false_negatives: 544.1489 - false_positives: 774.8085 - loss: 0.6639 - precision: 0.6334 - recall: 0.7104 - true_negatives: 1089.3234 - true_positives: 1367.7191\n",
      "For batch number 236 the model has a loss of 0.6376623511314392\n",
      "\u001b[1m236/689\u001b[0m \u001b[32m━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━\u001b[0m \u001b[1m24s\u001b[0m 54ms/step - auc: 0.6825 - binary_accuracy: 0.6423 - false_negatives: 546.1356 - false_positives: 778.1017 - loss: 0.6638 - precision: 0.6334 - recall: 0.7105 - true_negatives: 1094.0890 - true_positives: 1373.6737\n",
      "For batch number 237 the model has a loss of 0.6378712058067322\n",
      "\u001b[1m237/689\u001b[0m \u001b[32m━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━\u001b[0m \u001b[1m24s\u001b[0m 54ms/step - auc: 0.6826 - binary_accuracy: 0.6424 - false_negatives: 548.1392 - false_positives: 781.3798 - loss: 0.6637 - precision: 0.6335 - recall: 0.7106 - true_negatives: 1098.8523 - true_positives: 1379.6287\n",
      "For batch number 238 the model has a loss of 0.6375352740287781\n",
      "\u001b[1m238/689\u001b[0m \u001b[32m━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━\u001b[0m \u001b[1m24s\u001b[0m 54ms/step - auc: 0.6827 - binary_accuracy: 0.6425 - false_negatives: 550.1387 - false_positives: 784.6597 - loss: 0.6636 - precision: 0.6335 - recall: 0.7107 - true_negatives: 1103.6177 - true_positives: 1385.5840\n",
      "For batch number 239 the model has a loss of 0.6375420093536377\n",
      "\u001b[1m239/689\u001b[0m \u001b[32m━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━\u001b[0m \u001b[1m24s\u001b[0m 54ms/step - auc: 0.6829 - binary_accuracy: 0.6426 - false_negatives: 552.1548 - false_positives: 787.9247 - loss: 0.6635 - precision: 0.6335 - recall: 0.7108 - true_negatives: 1108.3975 - true_positives: 1391.5231\n",
      "For batch number 240 the model has a loss of 0.6374475955963135\n",
      "\u001b[1m240/689\u001b[0m \u001b[32m━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━\u001b[0m \u001b[1m24s\u001b[0m 54ms/step - auc: 0.6830 - binary_accuracy: 0.6426 - false_negatives: 554.1667 - false_positives: 791.1833 - loss: 0.6634 - precision: 0.6336 - recall: 0.7108 - true_negatives: 1113.1833 - true_positives: 1397.4667\n",
      "For batch number 241 the model has a loss of 0.6369125843048096\n",
      "\u001b[1m241/689\u001b[0m \u001b[32m━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━\u001b[0m \u001b[1m24s\u001b[0m 54ms/step - auc: 0.6831 - binary_accuracy: 0.6427 - false_negatives: 556.1660 - false_positives: 794.4357 - loss: 0.6632 - precision: 0.6336 - recall: 0.7109 - true_negatives: 1117.9834 - true_positives: 1403.4149\n",
      "For batch number 242 the model has a loss of 0.6366543769836426\n",
      "\u001b[1m242/689\u001b[0m \u001b[32m━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━\u001b[0m \u001b[1m24s\u001b[0m 54ms/step - auc: 0.6832 - binary_accuracy: 0.6428 - false_negatives: 558.1611 - false_positives: 797.6777 - loss: 0.6631 - precision: 0.6336 - recall: 0.7110 - true_negatives: 1122.7975 - true_positives: 1409.3636\n",
      "For batch number 243 the model has a loss of 0.6359273195266724\n",
      "\u001b[1m243/689\u001b[0m \u001b[32m━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━\u001b[0m \u001b[1m24s\u001b[0m 55ms/step - auc: 0.6834 - binary_accuracy: 0.6429 - false_negatives: 560.1523 - false_positives: 800.9053 - loss: 0.6630 - precision: 0.6337 - recall: 0.7111 - true_negatives: 1127.6090 - true_positives: 1415.3334\n",
      "For batch number 244 the model has a loss of 0.6356491446495056\n",
      "\u001b[1m244/689\u001b[0m \u001b[32m━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━\u001b[0m \u001b[1m24s\u001b[0m 55ms/step - auc: 0.6835 - binary_accuracy: 0.6430 - false_negatives: 562.1516 - false_positives: 804.1312 - loss: 0.6629 - precision: 0.6337 - recall: 0.7112 - true_negatives: 1132.4263 - true_positives: 1421.2910\n",
      "For batch number 245 the model has a loss of 0.6349696516990662\n",
      "\u001b[1m245/689\u001b[0m \u001b[32m━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━\u001b[0m \u001b[1m24s\u001b[0m 55ms/step - auc: 0.6836 - binary_accuracy: 0.6430 - false_negatives: 564.1347 - false_positives: 807.3510 - loss: 0.6628 - precision: 0.6338 - recall: 0.7113 - true_negatives: 1137.2490 - true_positives: 1427.2653\n",
      "For batch number 246 the model has a loss of 0.6346655488014221\n",
      "\u001b[1m246/689\u001b[0m \u001b[32m━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━\u001b[0m \u001b[1m24s\u001b[0m 55ms/step - auc: 0.6838 - binary_accuracy: 0.6431 - false_negatives: 566.1219 - false_positives: 810.5569 - loss: 0.6627 - precision: 0.6338 - recall: 0.7114 - true_negatives: 1142.0691 - true_positives: 1433.2521\n",
      "For batch number 247 the model has a loss of 0.6346274018287659\n",
      "\u001b[1m247/689\u001b[0m \u001b[32m━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━\u001b[0m \u001b[1m24s\u001b[0m 55ms/step - auc: 0.6839 - binary_accuracy: 0.6432 - false_negatives: 568.0931 - false_positives: 813.7854 - loss: 0.6626 - precision: 0.6339 - recall: 0.7115 - true_negatives: 1146.8907 - true_positives: 1439.2307\n",
      "For batch number 248 the model has a loss of 0.6348053216934204\n",
      "\u001b[1m248/689\u001b[0m \u001b[32m━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━\u001b[0m \u001b[1m24s\u001b[0m 55ms/step - auc: 0.6840 - binary_accuracy: 0.6433 - false_negatives: 570.0645 - false_positives: 817.0161 - loss: 0.6625 - precision: 0.6339 - recall: 0.7116 - true_negatives: 1151.7057 - true_positives: 1445.2137\n",
      "For batch number 249 the model has a loss of 0.6342054009437561\n",
      "\u001b[1m249/689\u001b[0m \u001b[32m━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━\u001b[0m \u001b[1m24s\u001b[0m 55ms/step - auc: 0.6841 - binary_accuracy: 0.6434 - false_negatives: 572.0281 - false_positives: 820.2450 - loss: 0.6623 - precision: 0.6339 - recall: 0.7117 - true_negatives: 1156.5181 - true_positives: 1451.2089\n",
      "For batch number 250 the model has a loss of 0.6350533962249756\n",
      "\u001b[1m250/689\u001b[0m \u001b[32m━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━\u001b[0m \u001b[1m23s\u001b[0m 55ms/step - auc: 0.6843 - binary_accuracy: 0.6435 - false_negatives: 573.9840 - false_positives: 823.4680 - loss: 0.6622 - precision: 0.6340 - recall: 0.7118 - true_negatives: 1161.3199 - true_positives: 1457.2280\n",
      "For batch number 251 the model has a loss of 0.6347799301147461\n",
      "\u001b[1m251/689\u001b[0m \u001b[32m━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━\u001b[0m \u001b[1m23s\u001b[0m 55ms/step - auc: 0.6844 - binary_accuracy: 0.6435 - false_negatives: 575.9323 - false_positives: 826.6972 - loss: 0.6621 - precision: 0.6340 - recall: 0.7119 - true_negatives: 1166.1195 - true_positives: 1463.2510\n",
      "For batch number 252 the model has a loss of 0.6344034075737\n",
      "\u001b[1m252/689\u001b[0m \u001b[32m━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━\u001b[0m \u001b[1m23s\u001b[0m 55ms/step - auc: 0.6845 - binary_accuracy: 0.6436 - false_negatives: 577.8730 - false_positives: 829.9246 - loss: 0.6620 - precision: 0.6341 - recall: 0.7120 - true_negatives: 1170.9246 - true_positives: 1469.2778\n",
      "For batch number 253 the model has a loss of 0.6340229511260986\n",
      "\u001b[1m253/689\u001b[0m \u001b[32m━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━\u001b[0m \u001b[1m23s\u001b[0m 55ms/step - auc: 0.6847 - binary_accuracy: 0.6437 - false_negatives: 579.8024 - false_positives: 833.1620 - loss: 0.6619 - precision: 0.6341 - recall: 0.7121 - true_negatives: 1175.7352 - true_positives: 1475.3004\n",
      "For batch number 254 the model has a loss of 0.6337188482284546\n",
      "\u001b[1m254/689\u001b[0m \u001b[32m━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━\u001b[0m \u001b[1m23s\u001b[0m 55ms/step - auc: 0.6848 - binary_accuracy: 0.6438 - false_negatives: 581.7244 - false_positives: 836.4016 - loss: 0.6618 - precision: 0.6342 - recall: 0.7122 - true_negatives: 1180.5472 - true_positives: 1481.3268\n",
      "For batch number 255 the model has a loss of 0.6335999369621277\n",
      "\u001b[1m255/689\u001b[0m \u001b[32m━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━\u001b[0m \u001b[1m23s\u001b[0m 55ms/step - auc: 0.6849 - binary_accuracy: 0.6439 - false_negatives: 583.6470 - false_positives: 839.6353 - loss: 0.6617 - precision: 0.6342 - recall: 0.7123 - true_negatives: 1185.3647 - true_positives: 1487.3529\n",
      "For batch number 256 the model has a loss of 0.6331403851509094\n",
      "\u001b[1m256/689\u001b[0m \u001b[32m━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━\u001b[0m \u001b[1m23s\u001b[0m 55ms/step - auc: 0.6851 - binary_accuracy: 0.6440 - false_negatives: 585.5664 - false_positives: 842.8711 - loss: 0.6616 - precision: 0.6342 - recall: 0.7124 - true_negatives: 1190.1836 - true_positives: 1493.3789\n",
      "For batch number 257 the model has a loss of 0.6328023672103882\n",
      "\u001b[1m257/689\u001b[0m \u001b[32m━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━\u001b[0m \u001b[1m23s\u001b[0m 55ms/step - auc: 0.6852 - binary_accuracy: 0.6440 - false_negatives: 587.4864 - false_positives: 846.0895 - loss: 0.6615 - precision: 0.6343 - recall: 0.7125 - true_negatives: 1195.0117 - true_positives: 1499.4125\n",
      "For batch number 258 the model has a loss of 0.6322090029716492\n",
      "\u001b[1m258/689\u001b[0m \u001b[32m━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━\u001b[0m \u001b[1m23s\u001b[0m 55ms/step - auc: 0.6853 - binary_accuracy: 0.6441 - false_negatives: 589.3992 - false_positives: 849.3023 - loss: 0.6613 - precision: 0.6343 - recall: 0.7126 - true_negatives: 1199.8450 - true_positives: 1505.4535\n",
      "For batch number 259 the model has a loss of 0.6311021447181702\n",
      "\u001b[1m259/689\u001b[0m \u001b[32m━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━\u001b[0m \u001b[1m23s\u001b[0m 55ms/step - auc: 0.6855 - binary_accuracy: 0.6442 - false_negatives: 591.3011 - false_positives: 852.5020 - loss: 0.6612 - precision: 0.6344 - recall: 0.7127 - true_negatives: 1204.6949 - true_positives: 1511.5020\n",
      "For batch number 260 the model has a loss of 0.6310726404190063\n",
      "\u001b[1m260/689\u001b[0m \u001b[32m━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━\u001b[0m \u001b[1m23s\u001b[0m 55ms/step - auc: 0.6856 - binary_accuracy: 0.6443 - false_negatives: 593.2115 - false_positives: 855.7039 - loss: 0.6611 - precision: 0.6344 - recall: 0.7128 - true_negatives: 1209.5347 - true_positives: 1517.5500\n",
      "For batch number 261 the model has a loss of 0.6307806968688965\n",
      "\u001b[1m261/689\u001b[0m \u001b[32m━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━\u001b[0m \u001b[1m23s\u001b[0m 55ms/step - auc: 0.6857 - binary_accuracy: 0.6444 - false_negatives: 595.1073 - false_positives: 858.9157 - loss: 0.6610 - precision: 0.6345 - recall: 0.7129 - true_negatives: 1214.3870 - true_positives: 1523.5901\n",
      "For batch number 262 the model has a loss of 0.630219042301178\n",
      "\u001b[1m262/689\u001b[0m \u001b[32m━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━\u001b[0m \u001b[1m23s\u001b[0m 55ms/step - auc: 0.6859 - binary_accuracy: 0.6445 - false_negatives: 596.9962 - false_positives: 862.1221 - loss: 0.6609 - precision: 0.6345 - recall: 0.7130 - true_negatives: 1219.2443 - true_positives: 1529.6375\n",
      "For batch number 263 the model has a loss of 0.629859209060669\n",
      "\u001b[1m263/689\u001b[0m \u001b[32m━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━\u001b[0m \u001b[1m23s\u001b[0m 55ms/step - auc: 0.6860 - binary_accuracy: 0.6446 - false_negatives: 598.8784 - false_positives: 865.3270 - loss: 0.6608 - precision: 0.6346 - recall: 0.7131 - true_negatives: 1224.1027 - true_positives: 1535.6920\n",
      "For batch number 264 the model has a loss of 0.6296453475952148\n",
      "\u001b[1m264/689\u001b[0m \u001b[32m━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━\u001b[0m \u001b[1m23s\u001b[0m 55ms/step - auc: 0.6861 - binary_accuracy: 0.6447 - false_negatives: 600.7614 - false_positives: 868.5341 - loss: 0.6606 - precision: 0.6346 - recall: 0.7132 - true_negatives: 1228.9622 - true_positives: 1541.7424\n",
      "For batch number 265 the model has a loss of 0.6296006441116333\n",
      "\u001b[1m265/689\u001b[0m \u001b[32m━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━\u001b[0m \u001b[1m23s\u001b[0m 55ms/step - auc: 0.6863 - binary_accuracy: 0.6447 - false_negatives: 602.6415 - false_positives: 871.7358 - loss: 0.6605 - precision: 0.6347 - recall: 0.7133 - true_negatives: 1233.8151 - true_positives: 1547.8075\n",
      "For batch number 266 the model has a loss of 0.6289603114128113\n",
      "\u001b[1m266/689\u001b[0m \u001b[32m━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━\u001b[0m \u001b[1m23s\u001b[0m 55ms/step - auc: 0.6864 - binary_accuracy: 0.6448 - false_negatives: 604.5113 - false_positives: 874.9211 - loss: 0.6604 - precision: 0.6347 - recall: 0.7134 - true_negatives: 1238.6766 - true_positives: 1553.8910\n",
      "For batch number 267 the model has a loss of 0.6289653778076172\n",
      "\u001b[1m267/689\u001b[0m \u001b[32m━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━\u001b[0m \u001b[1m23s\u001b[0m 55ms/step - auc: 0.6866 - binary_accuracy: 0.6449 - false_negatives: 606.3708 - false_positives: 878.1124 - loss: 0.6603 - precision: 0.6348 - recall: 0.7135 - true_negatives: 1243.5319 - true_positives: 1559.9850\n",
      "For batch number 268 the model has a loss of 0.629056453704834\n",
      "\u001b[1m268/689\u001b[0m \u001b[32m━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━\u001b[0m \u001b[1m23s\u001b[0m 55ms/step - auc: 0.6867 - binary_accuracy: 0.6450 - false_negatives: 608.2239 - false_positives: 881.3171 - loss: 0.6602 - precision: 0.6348 - recall: 0.7136 - true_negatives: 1248.3768 - true_positives: 1566.0820\n",
      "For batch number 269 the model has a loss of 0.628919243812561\n",
      "\u001b[1m269/689\u001b[0m \u001b[32m━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━\u001b[0m \u001b[1m22s\u001b[0m 55ms/step - auc: 0.6868 - binary_accuracy: 0.6451 - false_negatives: 610.0706 - false_positives: 884.5204 - loss: 0.6601 - precision: 0.6349 - recall: 0.7137 - true_negatives: 1253.2156 - true_positives: 1572.1934\n",
      "For batch number 270 the model has a loss of 0.6285409331321716\n",
      "\u001b[1m270/689\u001b[0m \u001b[32m━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━\u001b[0m \u001b[1m22s\u001b[0m 55ms/step - auc: 0.6870 - binary_accuracy: 0.6452 - false_negatives: 611.9111 - false_positives: 887.7222 - loss: 0.6599 - precision: 0.6349 - recall: 0.7138 - true_negatives: 1258.0555 - true_positives: 1578.3112\n",
      "For batch number 271 the model has a loss of 0.6282740235328674\n",
      "\u001b[1m271/689\u001b[0m \u001b[32m━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━\u001b[0m \u001b[1m22s\u001b[0m 55ms/step - auc: 0.6871 - binary_accuracy: 0.6453 - false_negatives: 613.7417 - false_positives: 890.9188 - loss: 0.6598 - precision: 0.6350 - recall: 0.7140 - true_negatives: 1262.8856 - true_positives: 1584.4539\n",
      "For batch number 272 the model has a loss of 0.6285417079925537\n",
      "\u001b[1m272/689\u001b[0m \u001b[32m━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━\u001b[0m \u001b[1m22s\u001b[0m 55ms/step - auc: 0.6872 - binary_accuracy: 0.6454 - false_negatives: 615.5845 - false_positives: 894.1213 - loss: 0.6597 - precision: 0.6350 - recall: 0.7141 - true_negatives: 1267.7059 - true_positives: 1590.5883\n",
      "For batch number 273 the model has a loss of 0.6287612915039062\n",
      "\u001b[1m273/689\u001b[0m \u001b[32m━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━\u001b[0m \u001b[1m22s\u001b[0m 55ms/step - auc: 0.6874 - binary_accuracy: 0.6455 - false_negatives: 617.4542 - false_positives: 897.3187 - loss: 0.6596 - precision: 0.6351 - recall: 0.7142 - true_negatives: 1272.5238 - true_positives: 1596.7032\n",
      "For batch number 274 the model has a loss of 0.6281461119651794\n",
      "\u001b[1m274/689\u001b[0m \u001b[32m━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━\u001b[0m \u001b[1m22s\u001b[0m 55ms/step - auc: 0.6875 - binary_accuracy: 0.6455 - false_negatives: 619.3138 - false_positives: 900.5073 - loss: 0.6595 - precision: 0.6351 - recall: 0.7143 - true_negatives: 1277.3503 - true_positives: 1602.8285\n",
      "For batch number 275 the model has a loss of 0.6285325288772583\n",
      "\u001b[1m275/689\u001b[0m \u001b[32m━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━\u001b[0m \u001b[1m22s\u001b[0m 55ms/step - auc: 0.6876 - binary_accuracy: 0.6456 - false_negatives: 621.1746 - false_positives: 903.6909 - loss: 0.6594 - precision: 0.6352 - recall: 0.7144 - true_negatives: 1282.1782 - true_positives: 1608.9564\n",
      "For batch number 276 the model has a loss of 0.6281235814094543\n",
      "\u001b[1m276/689\u001b[0m \u001b[32m━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━\u001b[0m \u001b[1m22s\u001b[0m 55ms/step - auc: 0.6878 - binary_accuracy: 0.6457 - false_negatives: 623.0290 - false_positives: 906.8768 - loss: 0.6593 - precision: 0.6352 - recall: 0.7145 - true_negatives: 1287.0145 - true_positives: 1615.0797\n",
      "For batch number 277 the model has a loss of 0.6284337043762207\n",
      "\u001b[1m277/689\u001b[0m \u001b[32m━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━\u001b[0m \u001b[1m22s\u001b[0m 55ms/step - auc: 0.6879 - binary_accuracy: 0.6458 - false_negatives: 624.8809 - false_positives: 910.0686 - loss: 0.6591 - precision: 0.6353 - recall: 0.7146 - true_negatives: 1291.8519 - true_positives: 1621.1986\n",
      "For batch number 278 the model has a loss of 0.6285852789878845\n",
      "\u001b[1m278/689\u001b[0m \u001b[32m━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━\u001b[0m \u001b[1m22s\u001b[0m 55ms/step - auc: 0.6880 - binary_accuracy: 0.6459 - false_negatives: 626.7230 - false_positives: 913.2626 - loss: 0.6590 - precision: 0.6353 - recall: 0.7147 - true_negatives: 1296.6942 - true_positives: 1627.3202\n",
      "For batch number 279 the model has a loss of 0.6287198066711426\n",
      "\u001b[1m279/689\u001b[0m \u001b[32m━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━\u001b[0m \u001b[1m22s\u001b[0m 55ms/step - auc: 0.6882 - binary_accuracy: 0.6460 - false_negatives: 628.5627 - false_positives: 916.4623 - loss: 0.6589 - precision: 0.6354 - recall: 0.7148 - true_negatives: 1301.5413 - true_positives: 1633.4337\n",
      "For batch number 280 the model has a loss of 0.6283360719680786\n",
      "\u001b[1m280/689\u001b[0m \u001b[32m━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━\u001b[0m \u001b[1m22s\u001b[0m 55ms/step - auc: 0.6883 - binary_accuracy: 0.6461 - false_negatives: 630.3893 - false_positives: 919.6572 - loss: 0.6588 - precision: 0.6354 - recall: 0.7149 - true_negatives: 1306.4000 - true_positives: 1639.5536\n",
      "For batch number 281 the model has a loss of 0.627704381942749\n",
      "\u001b[1m281/689\u001b[0m \u001b[32m━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━\u001b[0m \u001b[1m22s\u001b[0m 55ms/step - auc: 0.6884 - binary_accuracy: 0.6462 - false_negatives: 632.2135 - false_positives: 922.8398 - loss: 0.6587 - precision: 0.6355 - recall: 0.7151 - true_negatives: 1311.2668 - true_positives: 1645.6797\n",
      "For batch number 282 the model has a loss of 0.6273625493049622\n",
      "\u001b[1m282/689\u001b[0m \u001b[32m━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━\u001b[0m \u001b[1m22s\u001b[0m 55ms/step - auc: 0.6886 - binary_accuracy: 0.6462 - false_negatives: 634.0390 - false_positives: 926.0106 - loss: 0.6586 - precision: 0.6355 - recall: 0.7152 - true_negatives: 1316.1348 - true_positives: 1651.8156\n",
      "For batch number 283 the model has a loss of 0.6271880269050598\n",
      "\u001b[1m283/689\u001b[0m \u001b[32m━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━\u001b[0m \u001b[1m22s\u001b[0m 55ms/step - auc: 0.6887 - binary_accuracy: 0.6463 - false_negatives: 635.8622 - false_positives: 929.1767 - loss: 0.6585 - precision: 0.6356 - recall: 0.7153 - true_negatives: 1321.0000 - true_positives: 1657.9612\n",
      "For batch number 284 the model has a loss of 0.62656170129776\n",
      "\u001b[1m284/689\u001b[0m \u001b[32m━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━\u001b[0m \u001b[1m22s\u001b[0m 55ms/step - auc: 0.6888 - binary_accuracy: 0.6464 - false_negatives: 637.6725 - false_positives: 932.3380 - loss: 0.6584 - precision: 0.6356 - recall: 0.7154 - true_negatives: 1325.8802 - true_positives: 1664.1091\n",
      "For batch number 285 the model has a loss of 0.627261221408844\n",
      "\u001b[1m285/689\u001b[0m \u001b[32m━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━\u001b[0m \u001b[1m22s\u001b[0m 55ms/step - auc: 0.6890 - binary_accuracy: 0.6465 - false_negatives: 639.4982 - false_positives: 935.4982 - loss: 0.6583 - precision: 0.6357 - recall: 0.7155 - true_negatives: 1330.7544 - true_positives: 1670.2491\n",
      "For batch number 286 the model has a loss of 0.6264869570732117\n",
      "\u001b[1m286/689\u001b[0m \u001b[32m━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━\u001b[0m \u001b[1m22s\u001b[0m 55ms/step - auc: 0.6891 - binary_accuracy: 0.6466 - false_negatives: 641.3112 - false_positives: 938.6608 - loss: 0.6582 - precision: 0.6357 - recall: 0.7156 - true_negatives: 1335.6294 - true_positives: 1676.3986\n",
      "For batch number 287 the model has a loss of 0.6263265013694763\n",
      "\u001b[1m287/689\u001b[0m \u001b[32m━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━\u001b[0m \u001b[1m21s\u001b[0m 55ms/step - auc: 0.6892 - binary_accuracy: 0.6467 - false_negatives: 643.1185 - false_positives: 941.8223 - loss: 0.6580 - precision: 0.6358 - recall: 0.7157 - true_negatives: 1340.5052 - true_positives: 1682.5540\n",
      "For batch number 288 the model has a loss of 0.6259434223175049\n",
      "\u001b[1m288/689\u001b[0m \u001b[32m━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━\u001b[0m \u001b[1m21s\u001b[0m 55ms/step - auc: 0.6894 - binary_accuracy: 0.6468 - false_negatives: 644.9271 - false_positives: 944.9757 - loss: 0.6579 - precision: 0.6358 - recall: 0.7158 - true_negatives: 1345.3820 - true_positives: 1688.7153\n",
      "For batch number 289 the model has a loss of 0.6259471774101257\n",
      "\u001b[1m289/689\u001b[0m \u001b[32m━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━\u001b[0m \u001b[1m21s\u001b[0m 55ms/step - auc: 0.6895 - binary_accuracy: 0.6469 - false_negatives: 646.7266 - false_positives: 948.1281 - loss: 0.6578 - precision: 0.6359 - recall: 0.7160 - true_negatives: 1350.2837 - true_positives: 1694.8616\n",
      "For batch number 290 the model has a loss of 0.625466525554657\n",
      "\u001b[1m290/689\u001b[0m \u001b[32m━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━\u001b[0m \u001b[1m21s\u001b[0m 55ms/step - auc: 0.6897 - binary_accuracy: 0.6470 - false_negatives: 648.5138 - false_positives: 951.2793 - loss: 0.6577 - precision: 0.6359 - recall: 0.7161 - true_negatives: 1355.1897 - true_positives: 1701.0172\n",
      "For batch number 291 the model has a loss of 0.625002384185791\n",
      "\u001b[1m291/689\u001b[0m \u001b[32m━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━\u001b[0m \u001b[1m21s\u001b[0m 55ms/step - auc: 0.6898 - binary_accuracy: 0.6471 - false_negatives: 650.2921 - false_positives: 954.4296 - loss: 0.6576 - precision: 0.6360 - recall: 0.7162 - true_negatives: 1360.1100 - true_positives: 1707.1683\n",
      "For batch number 292 the model has a loss of 0.6245548725128174\n",
      "\u001b[1m292/689\u001b[0m \u001b[32m━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━\u001b[0m \u001b[1m21s\u001b[0m 55ms/step - auc: 0.6899 - binary_accuracy: 0.6472 - false_negatives: 652.0651 - false_positives: 957.5685 - loss: 0.6575 - precision: 0.6360 - recall: 0.7163 - true_negatives: 1365.0343 - true_positives: 1713.3322\n",
      "For batch number 293 the model has a loss of 0.624152660369873\n",
      "\u001b[1m293/689\u001b[0m \u001b[32m━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━\u001b[0m \u001b[1m21s\u001b[0m 55ms/step - auc: 0.6901 - binary_accuracy: 0.6473 - false_negatives: 653.8293 - false_positives: 960.7031 - loss: 0.6574 - precision: 0.6361 - recall: 0.7164 - true_negatives: 1369.9624 - true_positives: 1719.5051\n",
      "For batch number 294 the model has a loss of 0.6246485710144043\n",
      "\u001b[1m294/689\u001b[0m \u001b[32m━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━\u001b[0m \u001b[1m21s\u001b[0m 55ms/step - auc: 0.6902 - binary_accuracy: 0.6474 - false_negatives: 655.5986 - false_positives: 963.8333 - loss: 0.6573 - precision: 0.6362 - recall: 0.7165 - true_negatives: 1374.8810 - true_positives: 1725.6871\n",
      "For batch number 295 the model has a loss of 0.6240431666374207\n",
      "\u001b[1m295/689\u001b[0m \u001b[32m━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━\u001b[0m \u001b[1m21s\u001b[0m 55ms/step - auc: 0.6903 - binary_accuracy: 0.6475 - false_negatives: 657.3661 - false_positives: 966.9594 - loss: 0.6571 - precision: 0.6362 - recall: 0.7167 - true_negatives: 1379.8033 - true_positives: 1731.8712\n",
      "For batch number 296 the model has a loss of 0.6241649985313416\n",
      "\u001b[1m296/689\u001b[0m \u001b[32m━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━\u001b[0m \u001b[1m21s\u001b[0m 55ms/step - auc: 0.6905 - binary_accuracy: 0.6475 - false_negatives: 659.1385 - false_positives: 970.0980 - loss: 0.6570 - precision: 0.6363 - recall: 0.7168 - true_negatives: 1384.7196 - true_positives: 1738.0439\n",
      "For batch number 297 the model has a loss of 0.6239131689071655\n",
      "\u001b[1m297/689\u001b[0m \u001b[32m━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━\u001b[0m \u001b[1m21s\u001b[0m 55ms/step - auc: 0.6906 - binary_accuracy: 0.6476 - false_negatives: 660.9023 - false_positives: 973.2391 - loss: 0.6569 - precision: 0.6363 - recall: 0.7169 - true_negatives: 1389.6465 - true_positives: 1744.2122\n",
      "For batch number 298 the model has a loss of 0.6249166131019592\n",
      "\u001b[1m298/689\u001b[0m \u001b[32m━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━\u001b[0m \u001b[1m21s\u001b[0m 55ms/step - auc: 0.6907 - binary_accuracy: 0.6477 - false_negatives: 662.6644 - false_positives: 976.3759 - loss: 0.6568 - precision: 0.6364 - recall: 0.7170 - true_negatives: 1394.5771 - true_positives: 1750.3826\n",
      "For batch number 299 the model has a loss of 0.6241064667701721\n",
      "\u001b[1m299/689\u001b[0m \u001b[32m━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━\u001b[0m \u001b[1m21s\u001b[0m 55ms/step - auc: 0.6909 - binary_accuracy: 0.6478 - false_negatives: 664.4147 - false_positives: 979.5084 - loss: 0.6567 - precision: 0.6364 - recall: 0.7171 - true_negatives: 1399.5150 - true_positives: 1756.5619\n",
      "For batch number 300 the model has a loss of 0.623673141002655\n",
      "\u001b[1m300/689\u001b[0m \u001b[32m━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━\u001b[0m \u001b[1m21s\u001b[0m 55ms/step - auc: 0.6910 - binary_accuracy: 0.6479 - false_negatives: 666.1667 - false_positives: 982.6200 - loss: 0.6566 - precision: 0.6365 - recall: 0.7172 - true_negatives: 1404.4534 - true_positives: 1762.7600\n",
      "For batch number 301 the model has a loss of 0.6232941746711731\n",
      "\u001b[1m301/689\u001b[0m \u001b[32m━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━\u001b[0m \u001b[1m21s\u001b[0m 55ms/step - auc: 0.6912 - binary_accuracy: 0.6480 - false_negatives: 667.9169 - false_positives: 985.7276 - loss: 0.6565 - precision: 0.6365 - recall: 0.7174 - true_negatives: 1409.3854 - true_positives: 1768.9701\n",
      "For batch number 302 the model has a loss of 0.6229763031005859\n",
      "\u001b[1m302/689\u001b[0m \u001b[32m━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━\u001b[0m \u001b[1m21s\u001b[0m 55ms/step - auc: 0.6913 - binary_accuracy: 0.6481 - false_negatives: 669.6622 - false_positives: 988.8344 - loss: 0.6564 - precision: 0.6366 - recall: 0.7175 - true_negatives: 1414.3146 - true_positives: 1775.1887\n",
      "For batch number 303 the model has a loss of 0.6228399872779846\n",
      "\u001b[1m303/689\u001b[0m \u001b[32m━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━\u001b[0m \u001b[1m21s\u001b[0m 55ms/step - auc: 0.6914 - binary_accuracy: 0.6482 - false_negatives: 671.3994 - false_positives: 991.9439 - loss: 0.6563 - precision: 0.6367 - recall: 0.7176 - true_negatives: 1419.2476 - true_positives: 1781.4093\n",
      "For batch number 304 the model has a loss of 0.6229304075241089\n",
      "\u001b[1m304/689\u001b[0m \u001b[32m━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━\u001b[0m \u001b[1m21s\u001b[0m 55ms/step - auc: 0.6916 - binary_accuracy: 0.6483 - false_negatives: 673.1513 - false_positives: 995.0493 - loss: 0.6562 - precision: 0.6367 - recall: 0.7177 - true_negatives: 1424.1710 - true_positives: 1787.6283\n",
      "For batch number 305 the model has a loss of 0.6226761341094971\n",
      "\u001b[1m305/689\u001b[0m \u001b[32m━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━\u001b[0m \u001b[1m20s\u001b[0m 55ms/step - auc: 0.6917 - binary_accuracy: 0.6484 - false_negatives: 674.8918 - false_positives: 998.1606 - loss: 0.6560 - precision: 0.6368 - recall: 0.7178 - true_negatives: 1429.1049 - true_positives: 1793.8427\n",
      "For batch number 306 the model has a loss of 0.6227023601531982\n",
      "\u001b[1m306/689\u001b[0m \u001b[32m━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━\u001b[0m \u001b[1m20s\u001b[0m 55ms/step - auc: 0.6918 - binary_accuracy: 0.6485 - false_negatives: 676.6242 - false_positives: 1001.2843 - loss: 0.6559 - precision: 0.6368 - recall: 0.7179 - true_negatives: 1434.0491 - true_positives: 1800.0425\n",
      "For batch number 307 the model has a loss of 0.6225725412368774\n",
      "\u001b[1m307/689\u001b[0m \u001b[32m━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━\u001b[0m \u001b[1m20s\u001b[0m 55ms/step - auc: 0.6920 - binary_accuracy: 0.6486 - false_negatives: 678.3453 - false_positives: 1004.4072 - loss: 0.6558 - precision: 0.6369 - recall: 0.7181 - true_negatives: 1439.0000 - true_positives: 1806.2476\n",
      "For batch number 308 the model has a loss of 0.6221010088920593\n",
      "\u001b[1m308/689\u001b[0m \u001b[32m━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━\u001b[0m \u001b[1m20s\u001b[0m 55ms/step - auc: 0.6921 - binary_accuracy: 0.6487 - false_negatives: 680.0617 - false_positives: 1007.5162 - loss: 0.6557 - precision: 0.6370 - recall: 0.7182 - true_negatives: 1443.9642 - true_positives: 1812.4578\n",
      "For batch number 309 the model has a loss of 0.6218201518058777\n",
      "\u001b[1m309/689\u001b[0m \u001b[32m━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━\u001b[0m \u001b[1m20s\u001b[0m 55ms/step - auc: 0.6922 - binary_accuracy: 0.6488 - false_negatives: 681.7734 - false_positives: 1010.6213 - loss: 0.6556 - precision: 0.6370 - recall: 0.7183 - true_negatives: 1448.9320 - true_positives: 1818.6731\n",
      "For batch number 310 the model has a loss of 0.6213688254356384\n",
      "\u001b[1m310/689\u001b[0m \u001b[32m━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━\u001b[0m \u001b[1m20s\u001b[0m 55ms/step - auc: 0.6924 - binary_accuracy: 0.6489 - false_negatives: 683.4774 - false_positives: 1013.7129 - loss: 0.6555 - precision: 0.6371 - recall: 0.7184 - true_negatives: 1453.9032 - true_positives: 1824.9065\n",
      "For batch number 311 the model has a loss of 0.6210037469863892\n",
      "\u001b[1m311/689\u001b[0m \u001b[32m━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━\u001b[0m \u001b[1m20s\u001b[0m 55ms/step - auc: 0.6925 - binary_accuracy: 0.6490 - false_negatives: 685.1704 - false_positives: 1016.7942 - loss: 0.6554 - precision: 0.6371 - recall: 0.7185 - true_negatives: 1458.8713 - true_positives: 1831.1639\n",
      "For batch number 312 the model has a loss of 0.6207043528556824\n",
      "\u001b[1m312/689\u001b[0m \u001b[32m━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━\u001b[0m \u001b[1m20s\u001b[0m 55ms/step - auc: 0.6926 - binary_accuracy: 0.6491 - false_negatives: 686.8558 - false_positives: 1019.8750 - loss: 0.6553 - precision: 0.6372 - recall: 0.7187 - true_negatives: 1463.8397 - true_positives: 1837.4294\n",
      "For batch number 313 the model has a loss of 0.6211273670196533\n",
      "\u001b[1m313/689\u001b[0m \u001b[32m━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━\u001b[0m \u001b[1m20s\u001b[0m 55ms/step - auc: 0.6928 - binary_accuracy: 0.6492 - false_negatives: 688.5367 - false_positives: 1022.9521 - loss: 0.6552 - precision: 0.6373 - recall: 0.7188 - true_negatives: 1468.8019 - true_positives: 1843.7092\n",
      "For batch number 314 the model has a loss of 0.6204843521118164\n",
      "\u001b[1m314/689\u001b[0m \u001b[32m━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━\u001b[0m \u001b[1m20s\u001b[0m 55ms/step - auc: 0.6929 - binary_accuracy: 0.6493 - false_negatives: 690.2070 - false_positives: 1026.0223 - loss: 0.6551 - precision: 0.6373 - recall: 0.7189 - true_negatives: 1473.7771 - true_positives: 1849.9937\n",
      "For batch number 315 the model has a loss of 0.6206733584403992\n",
      "\u001b[1m315/689\u001b[0m \u001b[32m━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━\u001b[0m \u001b[1m20s\u001b[0m 55ms/step - auc: 0.6931 - binary_accuracy: 0.6494 - false_negatives: 691.8730 - false_positives: 1029.0984 - loss: 0.6549 - precision: 0.6374 - recall: 0.7191 - true_negatives: 1478.7491 - true_positives: 1856.2794\n",
      "For batch number 316 the model has a loss of 0.6205676794052124\n",
      "\u001b[1m316/689\u001b[0m \u001b[32m━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━\u001b[0m \u001b[1m20s\u001b[0m 55ms/step - auc: 0.6932 - binary_accuracy: 0.6495 - false_negatives: 693.5348 - false_positives: 1032.1741 - loss: 0.6548 - precision: 0.6375 - recall: 0.7192 - true_negatives: 1483.7184 - true_positives: 1862.5728\n",
      "For batch number 317 the model has a loss of 0.6201956868171692\n",
      "\u001b[1m317/689\u001b[0m \u001b[32m━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━\u001b[0m \u001b[1m20s\u001b[0m 55ms/step - auc: 0.6933 - binary_accuracy: 0.6496 - false_negatives: 695.1924 - false_positives: 1035.2429 - loss: 0.6547 - precision: 0.6375 - recall: 0.7193 - true_negatives: 1488.6908 - true_positives: 1868.8738\n",
      "For batch number 318 the model has a loss of 0.6200311779975891\n",
      "\u001b[1m318/689\u001b[0m \u001b[32m━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━\u001b[0m \u001b[1m20s\u001b[0m 55ms/step - auc: 0.6935 - binary_accuracy: 0.6497 - false_negatives: 696.8459 - false_positives: 1038.3051 - loss: 0.6546 - precision: 0.6376 - recall: 0.7194 - true_negatives: 1493.6730 - true_positives: 1875.1761\n",
      "For batch number 319 the model has a loss of 0.6198896765708923\n",
      "\u001b[1m319/689\u001b[0m \u001b[32m━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━\u001b[0m \u001b[1m20s\u001b[0m 55ms/step - auc: 0.6936 - binary_accuracy: 0.6498 - false_negatives: 698.4953 - false_positives: 1041.3605 - loss: 0.6545 - precision: 0.6377 - recall: 0.7196 - true_negatives: 1498.6520 - true_positives: 1881.4922\n",
      "For batch number 320 the model has a loss of 0.6196389198303223\n",
      "\u001b[1m320/689\u001b[0m \u001b[32m━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━\u001b[0m \u001b[1m20s\u001b[0m 55ms/step - auc: 0.6938 - binary_accuracy: 0.6499 - false_negatives: 700.1375 - false_positives: 1044.4156 - loss: 0.6544 - precision: 0.6377 - recall: 0.7197 - true_negatives: 1503.6250 - true_positives: 1887.8219\n",
      "For batch number 321 the model has a loss of 0.6198112368583679\n",
      "\u001b[1m321/689\u001b[0m \u001b[32m━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━\u001b[0m \u001b[1m20s\u001b[0m 55ms/step - auc: 0.6939 - binary_accuracy: 0.6500 - false_negatives: 701.7726 - false_positives: 1047.4705 - loss: 0.6543 - precision: 0.6378 - recall: 0.7198 - true_negatives: 1508.6044 - true_positives: 1894.1526\n",
      "For batch number 322 the model has a loss of 0.6200469136238098\n",
      "\u001b[1m322/689\u001b[0m \u001b[32m━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━\u001b[0m \u001b[1m20s\u001b[0m 55ms/step - auc: 0.6940 - binary_accuracy: 0.6501 - false_negatives: 703.4069 - false_positives: 1050.5342 - loss: 0.6542 - precision: 0.6379 - recall: 0.7200 - true_negatives: 1513.5807 - true_positives: 1900.4783\n",
      "For batch number 323 the model has a loss of 0.6202263832092285\n",
      "\u001b[1m323/689\u001b[0m \u001b[32m━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━\u001b[0m \u001b[1m20s\u001b[0m 55ms/step - auc: 0.6942 - binary_accuracy: 0.6502 - false_negatives: 705.0433 - false_positives: 1053.6038 - loss: 0.6541 - precision: 0.6379 - recall: 0.7201 - true_negatives: 1518.5573 - true_positives: 1906.7957\n",
      "For batch number 324 the model has a loss of 0.6207866668701172\n",
      "\u001b[1m324/689\u001b[0m \u001b[32m━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━\u001b[0m \u001b[1m19s\u001b[0m 55ms/step - auc: 0.6943 - binary_accuracy: 0.6503 - false_negatives: 706.6790 - false_positives: 1056.6821 - loss: 0.6540 - precision: 0.6380 - recall: 0.7202 - true_negatives: 1523.5278 - true_positives: 1913.1111\n",
      "For batch number 325 the model has a loss of 0.620451807975769\n",
      "\u001b[1m325/689\u001b[0m \u001b[32m━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━\u001b[0m \u001b[1m19s\u001b[0m 55ms/step - auc: 0.6944 - binary_accuracy: 0.6504 - false_negatives: 708.3077 - false_positives: 1059.7570 - loss: 0.6539 - precision: 0.6380 - recall: 0.7204 - true_negatives: 1528.5046 - true_positives: 1919.4308\n",
      "For batch number 326 the model has a loss of 0.6206408739089966\n",
      "\u001b[1m326/689\u001b[0m \u001b[32m━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━\u001b[0m \u001b[1m19s\u001b[0m 55ms/step - auc: 0.6946 - binary_accuracy: 0.6505 - false_negatives: 709.9294 - false_positives: 1062.8252 - loss: 0.6538 - precision: 0.6381 - recall: 0.7205 - true_negatives: 1533.4785 - true_positives: 1925.7668\n",
      "For batch number 327 the model has a loss of 0.6203140616416931\n",
      "\u001b[1m327/689\u001b[0m \u001b[32m━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━\u001b[0m \u001b[1m19s\u001b[0m 55ms/step - auc: 0.6947 - binary_accuracy: 0.6506 - false_negatives: 711.5413 - false_positives: 1065.8960 - loss: 0.6537 - precision: 0.6382 - recall: 0.7206 - true_negatives: 1538.4587 - true_positives: 1932.1040\n",
      "For batch number 328 the model has a loss of 0.6204856038093567\n",
      "\u001b[1m328/689\u001b[0m \u001b[32m━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━\u001b[0m \u001b[1m19s\u001b[0m 55ms/step - auc: 0.6948 - binary_accuracy: 0.6507 - false_negatives: 713.1464 - false_positives: 1068.9664 - loss: 0.6536 - precision: 0.6382 - recall: 0.7207 - true_negatives: 1543.4481 - true_positives: 1938.4390\n",
      "For batch number 329 the model has a loss of 0.6212416887283325\n",
      "\u001b[1m329/689\u001b[0m \u001b[32m━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━\u001b[0m \u001b[1m19s\u001b[0m 55ms/step - auc: 0.6950 - binary_accuracy: 0.6508 - false_negatives: 714.7447 - false_positives: 1072.0516 - loss: 0.6535 - precision: 0.6383 - recall: 0.7209 - true_negatives: 1548.4437 - true_positives: 1944.7599\n",
      "For batch number 330 the model has a loss of 0.6209979057312012\n",
      "\u001b[1m330/689\u001b[0m \u001b[32m━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━\u001b[0m \u001b[1m19s\u001b[0m 55ms/step - auc: 0.6951 - binary_accuracy: 0.6509 - false_negatives: 716.3333 - false_positives: 1075.1425 - loss: 0.6534 - precision: 0.6384 - recall: 0.7210 - true_negatives: 1553.4454 - true_positives: 1951.0787\n",
      "For batch number 331 the model has a loss of 0.6204360127449036\n",
      "\u001b[1m331/689\u001b[0m \u001b[32m━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━\u001b[0m \u001b[1m19s\u001b[0m 55ms/step - auc: 0.6952 - binary_accuracy: 0.6510 - false_negatives: 717.9124 - false_positives: 1078.2266 - loss: 0.6533 - precision: 0.6384 - recall: 0.7211 - true_negatives: 1558.4441 - true_positives: 1957.4169\n",
      "For batch number 332 the model has a loss of 0.6197627782821655\n",
      "\u001b[1m332/689\u001b[0m \u001b[32m━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━\u001b[0m \u001b[1m19s\u001b[0m 55ms/step - auc: 0.6954 - binary_accuracy: 0.6512 - false_negatives: 719.4819 - false_positives: 1081.2982 - loss: 0.6532 - precision: 0.6385 - recall: 0.7213 - true_negatives: 1563.4458 - true_positives: 1963.7740\n",
      "For batch number 333 the model has a loss of 0.6198905110359192\n",
      "\u001b[1m333/689\u001b[0m \u001b[32m━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━\u001b[0m \u001b[1m19s\u001b[0m 55ms/step - auc: 0.6955 - binary_accuracy: 0.6513 - false_negatives: 721.0450 - false_positives: 1084.3634 - loss: 0.6531 - precision: 0.6386 - recall: 0.7214 - true_negatives: 1568.4474 - true_positives: 1970.1442\n",
      "For batch number 334 the model has a loss of 0.6193750500679016\n",
      "\u001b[1m334/689\u001b[0m \u001b[32m━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━\u001b[0m \u001b[1m19s\u001b[0m 55ms/step - auc: 0.6956 - binary_accuracy: 0.6514 - false_negatives: 722.5988 - false_positives: 1087.4221 - loss: 0.6530 - precision: 0.6386 - recall: 0.7216 - true_negatives: 1573.4521 - true_positives: 1976.5270\n",
      "For batch number 335 the model has a loss of 0.6189659237861633\n",
      "\u001b[1m335/689\u001b[0m \u001b[32m━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━\u001b[0m \u001b[1m19s\u001b[0m 55ms/step - auc: 0.6958 - binary_accuracy: 0.6515 - false_negatives: 724.1433 - false_positives: 1090.4836 - loss: 0.6529 - precision: 0.6387 - recall: 0.7217 - true_negatives: 1578.4657 - true_positives: 1982.9075\n",
      "For batch number 336 the model has a loss of 0.6190873384475708\n",
      "\u001b[1m336/689\u001b[0m \u001b[32m━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━\u001b[0m \u001b[1m19s\u001b[0m 55ms/step - auc: 0.6959 - binary_accuracy: 0.6516 - false_negatives: 725.6845 - false_positives: 1093.5387 - loss: 0.6528 - precision: 0.6388 - recall: 0.7218 - true_negatives: 1583.4733 - true_positives: 1989.3036\n",
      "For batch number 337 the model has a loss of 0.6191771030426025\n",
      "\u001b[1m337/689\u001b[0m \u001b[32m━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━\u001b[0m \u001b[1m19s\u001b[0m 55ms/step - auc: 0.6960 - binary_accuracy: 0.6517 - false_negatives: 727.2285 - false_positives: 1096.5935 - loss: 0.6527 - precision: 0.6388 - recall: 0.7220 - true_negatives: 1588.4836 - true_positives: 1995.6943\n",
      "For batch number 338 the model has a loss of 0.6195151805877686\n",
      "\u001b[1m338/689\u001b[0m \u001b[32m━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━\u001b[0m \u001b[1m19s\u001b[0m 55ms/step - auc: 0.6962 - binary_accuracy: 0.6518 - false_negatives: 728.7663 - false_positives: 1099.6450 - loss: 0.6526 - precision: 0.6389 - recall: 0.7221 - true_negatives: 1593.5000 - true_positives: 2002.0887\n",
      "For batch number 339 the model has a loss of 0.6195159554481506\n",
      "\u001b[1m339/689\u001b[0m \u001b[32m━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━\u001b[0m \u001b[1m19s\u001b[0m 55ms/step - auc: 0.6963 - binary_accuracy: 0.6519 - false_negatives: 730.3009 - false_positives: 1102.6962 - loss: 0.6525 - precision: 0.6390 - recall: 0.7223 - true_negatives: 1598.5073 - true_positives: 2008.4956\n",
      "For batch number 340 the model has a loss of 0.6198287606239319\n",
      "\u001b[1m340/689\u001b[0m \u001b[32m━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━\u001b[0m \u001b[1m19s\u001b[0m 55ms/step - auc: 0.6964 - binary_accuracy: 0.6520 - false_negatives: 731.8323 - false_positives: 1105.7529 - loss: 0.6524 - precision: 0.6390 - recall: 0.7224 - true_negatives: 1603.5059 - true_positives: 2014.9088\n",
      "For batch number 341 the model has a loss of 0.6196838617324829\n",
      "\u001b[1m341/689\u001b[0m \u001b[32m━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━\u001b[0m \u001b[1m19s\u001b[0m 55ms/step - auc: 0.6965 - binary_accuracy: 0.6521 - false_negatives: 733.3578 - false_positives: 1108.8093 - loss: 0.6523 - precision: 0.6391 - recall: 0.7225 - true_negatives: 1608.4956 - true_positives: 2021.3373\n",
      "For batch number 342 the model has a loss of 0.6192489266395569\n",
      "\u001b[1m342/689\u001b[0m \u001b[32m━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━\u001b[0m \u001b[1m18s\u001b[0m 55ms/step - auc: 0.6967 - binary_accuracy: 0.6522 - false_negatives: 734.8743 - false_positives: 1111.8684 - loss: 0.6522 - precision: 0.6392 - recall: 0.7227 - true_negatives: 1613.4883 - true_positives: 2027.7690\n",
      "For batch number 343 the model has a loss of 0.6190463304519653\n",
      "\u001b[1m343/689\u001b[0m \u001b[32m━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━\u001b[0m \u001b[1m18s\u001b[0m 55ms/step - auc: 0.6968 - binary_accuracy: 0.6523 - false_negatives: 736.3819 - false_positives: 1114.9301 - loss: 0.6521 - precision: 0.6392 - recall: 0.7228 - true_negatives: 1618.4811 - true_positives: 2034.2070\n",
      "For batch number 344 the model has a loss of 0.6186510324478149\n",
      "\u001b[1m344/689\u001b[0m \u001b[32m━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━\u001b[0m \u001b[1m18s\u001b[0m 55ms/step - auc: 0.6969 - binary_accuracy: 0.6524 - false_negatives: 737.8837 - false_positives: 1117.9884 - loss: 0.6520 - precision: 0.6393 - recall: 0.7230 - true_negatives: 1623.4855 - true_positives: 2040.6425\n",
      "For batch number 345 the model has a loss of 0.6183004379272461\n",
      "\u001b[1m345/689\u001b[0m \u001b[32m━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━\u001b[0m \u001b[1m18s\u001b[0m 55ms/step - auc: 0.6971 - binary_accuracy: 0.6525 - false_negatives: 739.3826 - false_positives: 1121.0464 - loss: 0.6519 - precision: 0.6394 - recall: 0.7231 - true_negatives: 1628.4899 - true_positives: 2047.0812\n",
      "For batch number 346 the model has a loss of 0.618043839931488\n",
      "\u001b[1m346/689\u001b[0m \u001b[32m━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━\u001b[0m \u001b[1m18s\u001b[0m 55ms/step - auc: 0.6972 - binary_accuracy: 0.6527 - false_negatives: 740.8728 - false_positives: 1124.1099 - loss: 0.6518 - precision: 0.6394 - recall: 0.7233 - true_negatives: 1633.5029 - true_positives: 2053.5144\n",
      "For batch number 347 the model has a loss of 0.6180753111839294\n",
      "\u001b[1m347/689\u001b[0m \u001b[32m━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━\u001b[0m \u001b[1m18s\u001b[0m 55ms/step - auc: 0.6973 - binary_accuracy: 0.6528 - false_negatives: 742.3602 - false_positives: 1127.1758 - loss: 0.6517 - precision: 0.6395 - recall: 0.7234 - true_negatives: 1638.5187 - true_positives: 2059.9453\n",
      "For batch number 348 the model has a loss of 0.6183598041534424\n",
      "\u001b[1m348/689\u001b[0m \u001b[32m━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━\u001b[0m \u001b[1m18s\u001b[0m 55ms/step - auc: 0.6975 - binary_accuracy: 0.6529 - false_negatives: 743.8448 - false_positives: 1130.2500 - loss: 0.6516 - precision: 0.6395 - recall: 0.7235 - true_negatives: 1643.5374 - true_positives: 2066.3679\n",
      "For batch number 349 the model has a loss of 0.617757260799408\n",
      "\u001b[1m349/689\u001b[0m \u001b[32m━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━\u001b[0m \u001b[1m18s\u001b[0m 55ms/step - auc: 0.6976 - binary_accuracy: 0.6530 - false_negatives: 745.3209 - false_positives: 1133.3124 - loss: 0.6515 - precision: 0.6396 - recall: 0.7237 - true_negatives: 1648.5587 - true_positives: 2072.8081\n",
      "For batch number 350 the model has a loss of 0.6174697279930115\n",
      "\u001b[1m350/689\u001b[0m \u001b[32m━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━\u001b[0m \u001b[1m18s\u001b[0m 55ms/step - auc: 0.6977 - binary_accuracy: 0.6531 - false_negatives: 746.7886 - false_positives: 1136.3800 - loss: 0.6514 - precision: 0.6397 - recall: 0.7238 - true_negatives: 1653.5829 - true_positives: 2079.2485\n",
      "For batch number 351 the model has a loss of 0.6174230575561523\n",
      "\u001b[1m351/689\u001b[0m \u001b[32m━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━\u001b[0m \u001b[1m18s\u001b[0m 55ms/step - auc: 0.6978 - binary_accuracy: 0.6532 - false_negatives: 748.2479 - false_positives: 1139.4502 - loss: 0.6513 - precision: 0.6397 - recall: 0.7240 - true_negatives: 1658.6154 - true_positives: 2085.6865\n",
      "For batch number 352 the model has a loss of 0.6175116300582886\n",
      "\u001b[1m352/689\u001b[0m \u001b[32m━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━\u001b[0m \u001b[1m18s\u001b[0m 55ms/step - auc: 0.6980 - binary_accuracy: 0.6533 - false_negatives: 749.7074 - false_positives: 1142.5171 - loss: 0.6512 - precision: 0.6398 - recall: 0.7241 - true_negatives: 1663.6421 - true_positives: 2092.1335\n",
      "For batch number 353 the model has a loss of 0.6169297099113464\n",
      "\u001b[1m353/689\u001b[0m \u001b[32m━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━\u001b[0m \u001b[1m18s\u001b[0m 55ms/step - auc: 0.6981 - binary_accuracy: 0.6534 - false_negatives: 751.1586 - false_positives: 1145.5751 - loss: 0.6511 - precision: 0.6399 - recall: 0.7243 - true_negatives: 1668.6742 - true_positives: 2098.5920\n",
      "For batch number 354 the model has a loss of 0.6161229014396667\n",
      "\u001b[1m354/689\u001b[0m \u001b[32m━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━\u001b[0m \u001b[1m18s\u001b[0m 55ms/step - auc: 0.6982 - binary_accuracy: 0.6535 - false_negatives: 752.6045 - false_positives: 1148.6243 - loss: 0.6510 - precision: 0.6399 - recall: 0.7244 - true_negatives: 1673.7062 - true_positives: 2105.0649\n",
      "For batch number 355 the model has a loss of 0.6155138611793518\n",
      "\u001b[1m355/689\u001b[0m \u001b[32m━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━\u001b[0m \u001b[1m18s\u001b[0m 55ms/step - auc: 0.6984 - binary_accuracy: 0.6536 - false_negatives: 754.0422 - false_positives: 1151.6732 - loss: 0.6509 - precision: 0.6400 - recall: 0.7246 - true_negatives: 1678.7465 - true_positives: 2111.5381\n",
      "For batch number 356 the model has a loss of 0.6154735684394836\n",
      "\u001b[1m356/689\u001b[0m \u001b[32m━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━\u001b[0m \u001b[1m18s\u001b[0m 55ms/step - auc: 0.6985 - binary_accuracy: 0.6537 - false_negatives: 755.4719 - false_positives: 1154.7191 - loss: 0.6508 - precision: 0.6401 - recall: 0.7247 - true_negatives: 1683.7781 - true_positives: 2118.0310\n",
      "For batch number 357 the model has a loss of 0.615449845790863\n",
      "\u001b[1m357/689\u001b[0m \u001b[32m━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━\u001b[0m \u001b[1m18s\u001b[0m 55ms/step - auc: 0.6986 - binary_accuracy: 0.6538 - false_negatives: 756.8936 - false_positives: 1157.7731 - loss: 0.6507 - precision: 0.6401 - recall: 0.7249 - true_negatives: 1688.8096 - true_positives: 2124.5239\n",
      "For batch number 358 the model has a loss of 0.6151050925254822\n",
      "\u001b[1m358/689\u001b[0m \u001b[32m━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━\u001b[0m \u001b[1m18s\u001b[0m 55ms/step - auc: 0.6987 - binary_accuracy: 0.6540 - false_negatives: 758.3073 - false_positives: 1160.8157 - loss: 0.6506 - precision: 0.6402 - recall: 0.7250 - true_negatives: 1693.8380 - true_positives: 2131.0391\n",
      "For batch number 359 the model has a loss of 0.6146002411842346\n",
      "\u001b[1m359/689\u001b[0m \u001b[32m━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━\u001b[0m \u001b[1m18s\u001b[0m 55ms/step - auc: 0.6989 - binary_accuracy: 0.6541 - false_negatives: 759.7131 - false_positives: 1163.8496 - loss: 0.6505 - precision: 0.6403 - recall: 0.7252 - true_negatives: 1698.8635 - true_positives: 2137.5737\n",
      "For batch number 360 the model has a loss of 0.6143240332603455\n",
      "\u001b[1m360/689\u001b[0m \u001b[32m━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━\u001b[0m \u001b[1m18s\u001b[0m 55ms/step - auc: 0.6990 - binary_accuracy: 0.6542 - false_negatives: 761.1111 - false_positives: 1166.8833 - loss: 0.6504 - precision: 0.6404 - recall: 0.7253 - true_negatives: 1703.8861 - true_positives: 2144.1194\n",
      "For batch number 361 the model has a loss of 0.6138251423835754\n",
      "\u001b[1m361/689\u001b[0m \u001b[32m━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━\u001b[0m \u001b[1m17s\u001b[0m 55ms/step - auc: 0.6991 - binary_accuracy: 0.6543 - false_negatives: 762.5042 - false_positives: 1169.9058 - loss: 0.6503 - precision: 0.6404 - recall: 0.7255 - true_negatives: 1708.9114 - true_positives: 2150.6787\n",
      "For batch number 362 the model has a loss of 0.6132407784461975\n",
      "\u001b[1m362/689\u001b[0m \u001b[32m━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━\u001b[0m \u001b[1m17s\u001b[0m 55ms/step - auc: 0.6993 - binary_accuracy: 0.6544 - false_negatives: 763.8895 - false_positives: 1172.9254 - loss: 0.6502 - precision: 0.6405 - recall: 0.7256 - true_negatives: 1713.9392 - true_positives: 2157.2458\n",
      "For batch number 363 the model has a loss of 0.6129960417747498\n",
      "\u001b[1m363/689\u001b[0m \u001b[32m━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━\u001b[0m \u001b[1m17s\u001b[0m 55ms/step - auc: 0.6994 - binary_accuracy: 0.6545 - false_negatives: 765.2700 - false_positives: 1175.9421 - loss: 0.6501 - precision: 0.6406 - recall: 0.7258 - true_negatives: 1718.9697 - true_positives: 2163.8181\n",
      "For batch number 364 the model has a loss of 0.6125049591064453\n",
      "\u001b[1m364/689\u001b[0m \u001b[32m━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━\u001b[0m \u001b[1m17s\u001b[0m 55ms/step - auc: 0.6995 - binary_accuracy: 0.6546 - false_negatives: 766.6456 - false_positives: 1178.9587 - loss: 0.6500 - precision: 0.6406 - recall: 0.7260 - true_negatives: 1724.0083 - true_positives: 2170.3875\n",
      "For batch number 365 the model has a loss of 0.612358033657074\n",
      "\u001b[1m365/689\u001b[0m \u001b[32m━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━\u001b[0m \u001b[1m17s\u001b[0m 55ms/step - auc: 0.6997 - binary_accuracy: 0.6547 - false_negatives: 768.0164 - false_positives: 1181.9808 - loss: 0.6499 - precision: 0.6407 - recall: 0.7261 - true_negatives: 1729.0520 - true_positives: 2176.9507\n",
      "For batch number 366 the model has a loss of 0.6125898361206055\n",
      "\u001b[1m366/689\u001b[0m \u001b[32m━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━\u001b[0m \u001b[1m17s\u001b[0m 55ms/step - auc: 0.6998 - binary_accuracy: 0.6549 - false_negatives: 769.3853 - false_positives: 1185.0055 - loss: 0.6498 - precision: 0.6408 - recall: 0.7263 - true_negatives: 1734.0956 - true_positives: 2183.5137\n",
      "For batch number 367 the model has a loss of 0.6126641035079956\n",
      "\u001b[1m367/689\u001b[0m \u001b[32m━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━\u001b[0m \u001b[1m17s\u001b[0m 55ms/step - auc: 0.6999 - binary_accuracy: 0.6550 - false_negatives: 770.7493 - false_positives: 1188.0327 - loss: 0.6497 - precision: 0.6409 - recall: 0.7264 - true_negatives: 1739.1417 - true_positives: 2190.0764\n",
      "For batch number 368 the model has a loss of 0.612907350063324\n",
      "\u001b[1m368/689\u001b[0m \u001b[32m━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━\u001b[0m \u001b[1m17s\u001b[0m 55ms/step - auc: 0.7001 - binary_accuracy: 0.6551 - false_negatives: 772.1087 - false_positives: 1191.0625 - loss: 0.6496 - precision: 0.6409 - recall: 0.7266 - true_negatives: 1744.1902 - true_positives: 2196.6387\n",
      "For batch number 369 the model has a loss of 0.612308919429779\n",
      "\u001b[1m369/689\u001b[0m \u001b[32m━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━\u001b[0m \u001b[1m17s\u001b[0m 55ms/step - auc: 0.7002 - binary_accuracy: 0.6552 - false_negatives: 773.4634 - false_positives: 1194.0786 - loss: 0.6495 - precision: 0.6410 - recall: 0.7267 - true_negatives: 1749.2357 - true_positives: 2203.2222\n",
      "For batch number 370 the model has a loss of 0.6118462681770325\n",
      "\u001b[1m370/689\u001b[0m \u001b[32m━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━\u001b[0m \u001b[1m17s\u001b[0m 55ms/step - auc: 0.7003 - binary_accuracy: 0.6553 - false_negatives: 774.8135 - false_positives: 1197.0892 - loss: 0.6494 - precision: 0.6411 - recall: 0.7269 - true_negatives: 1754.2865 - true_positives: 2209.8108\n",
      "For batch number 371 the model has a loss of 0.6120024919509888\n",
      "\u001b[1m371/689\u001b[0m \u001b[32m━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━\u001b[0m \u001b[1m17s\u001b[0m 55ms/step - auc: 0.7005 - binary_accuracy: 0.6554 - false_negatives: 776.1644 - false_positives: 1200.1078 - loss: 0.6493 - precision: 0.6411 - recall: 0.7270 - true_negatives: 1759.3369 - true_positives: 2216.3909\n",
      "For batch number 372 the model has a loss of 0.6115095615386963\n",
      "\u001b[1m372/689\u001b[0m \u001b[32m━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━\u001b[0m \u001b[1m17s\u001b[0m 55ms/step - auc: 0.7006 - binary_accuracy: 0.6555 - false_negatives: 777.5081 - false_positives: 1203.1237 - loss: 0.6492 - precision: 0.6412 - recall: 0.7272 - true_negatives: 1764.3844 - true_positives: 2222.9839\n",
      "For batch number 373 the model has a loss of 0.6113829016685486\n",
      "\u001b[1m373/689\u001b[0m \u001b[32m━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━\u001b[0m \u001b[1m17s\u001b[0m 55ms/step - auc: 0.7007 - binary_accuracy: 0.6557 - false_negatives: 778.8499 - false_positives: 1206.1421 - loss: 0.6491 - precision: 0.6413 - recall: 0.7274 - true_negatives: 1769.4290 - true_positives: 2229.5791\n",
      "For batch number 374 the model has a loss of 0.6111241579055786\n",
      "\u001b[1m374/689\u001b[0m \u001b[32m━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━\u001b[0m \u001b[1m17s\u001b[0m 55ms/step - auc: 0.7009 - binary_accuracy: 0.6558 - false_negatives: 780.1845 - false_positives: 1209.1577 - loss: 0.6490 - precision: 0.6414 - recall: 0.7275 - true_negatives: 1774.4813 - true_positives: 2236.1765\n",
      "For batch number 375 the model has a loss of 0.6107751727104187\n",
      "\u001b[1m375/689\u001b[0m \u001b[32m━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━\u001b[0m \u001b[1m17s\u001b[0m 55ms/step - auc: 0.7010 - binary_accuracy: 0.6559 - false_negatives: 781.5120 - false_positives: 1212.1627 - loss: 0.6489 - precision: 0.6414 - recall: 0.7277 - true_negatives: 1779.5280 - true_positives: 2242.7974\n",
      "For batch number 376 the model has a loss of 0.6108282208442688\n",
      "\u001b[1m376/689\u001b[0m \u001b[32m━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━\u001b[0m \u001b[1m17s\u001b[0m 55ms/step - auc: 0.7011 - binary_accuracy: 0.6560 - false_negatives: 782.8325 - false_positives: 1215.1782 - loss: 0.6488 - precision: 0.6415 - recall: 0.7278 - true_negatives: 1784.5665 - true_positives: 2249.4229\n",
      "For batch number 377 the model has a loss of 0.6110571026802063\n",
      "\u001b[1m377/689\u001b[0m \u001b[32m━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━\u001b[0m \u001b[1m17s\u001b[0m 55ms/step - auc: 0.7013 - binary_accuracy: 0.6561 - false_negatives: 784.1486 - false_positives: 1218.1830 - loss: 0.6487 - precision: 0.6416 - recall: 0.7280 - true_negatives: 1789.6074 - true_positives: 2256.0610\n",
      "For batch number 378 the model has a loss of 0.6109589338302612\n",
      "\u001b[1m378/689\u001b[0m \u001b[32m━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━\u001b[0m \u001b[1m17s\u001b[0m 55ms/step - auc: 0.7014 - binary_accuracy: 0.6562 - false_negatives: 785.4576 - false_positives: 1221.1904 - loss: 0.6486 - precision: 0.6416 - recall: 0.7282 - true_negatives: 1794.6534 - true_positives: 2262.6985\n",
      "For batch number 379 the model has a loss of 0.6107410192489624\n",
      "\u001b[1m379/689\u001b[0m \u001b[32m━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━\u001b[0m \u001b[1m16s\u001b[0m 55ms/step - auc: 0.7015 - binary_accuracy: 0.6563 - false_negatives: 786.7625 - false_positives: 1224.2006 - loss: 0.6485 - precision: 0.6417 - recall: 0.7283 - true_negatives: 1799.7045 - true_positives: 2269.3325\n",
      "For batch number 380 the model has a loss of 0.610204815864563\n",
      "\u001b[1m380/689\u001b[0m \u001b[32m━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━\u001b[0m \u001b[1m16s\u001b[0m 55ms/step - auc: 0.7017 - binary_accuracy: 0.6565 - false_negatives: 788.0605 - false_positives: 1227.2000 - loss: 0.6484 - precision: 0.6418 - recall: 0.7285 - true_negatives: 1804.7552 - true_positives: 2275.9841\n",
      "For batch number 381 the model has a loss of 0.6100574135780334\n",
      "\u001b[1m381/689\u001b[0m \u001b[32m━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━\u001b[0m \u001b[1m16s\u001b[0m 55ms/step - auc: 0.7018 - binary_accuracy: 0.6566 - false_negatives: 789.3543 - false_positives: 1230.1995 - loss: 0.6483 - precision: 0.6419 - recall: 0.7286 - true_negatives: 1809.8083 - true_positives: 2282.6377\n",
      "For batch number 382 the model has a loss of 0.6095551252365112\n",
      "\u001b[1m382/689\u001b[0m \u001b[32m━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━\u001b[0m \u001b[1m16s\u001b[0m 55ms/step - auc: 0.7019 - binary_accuracy: 0.6567 - false_negatives: 790.6414 - false_positives: 1233.1963 - loss: 0.6482 - precision: 0.6419 - recall: 0.7288 - true_negatives: 1814.8717 - true_positives: 2289.2905\n",
      "For batch number 383 the model has a loss of 0.6089759469032288\n",
      "\u001b[1m383/689\u001b[0m \u001b[32m━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━\u001b[0m \u001b[1m16s\u001b[0m 55ms/step - auc: 0.7021 - binary_accuracy: 0.6568 - false_negatives: 791.9217 - false_positives: 1236.1906 - loss: 0.6481 - precision: 0.6420 - recall: 0.7290 - true_negatives: 1819.9425 - true_positives: 2295.9451\n",
      "For batch number 384 the model has a loss of 0.6087924838066101\n",
      "\u001b[1m384/689\u001b[0m \u001b[32m━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━\u001b[0m \u001b[1m16s\u001b[0m 55ms/step - auc: 0.7022 - binary_accuracy: 0.6569 - false_negatives: 793.1953 - false_positives: 1239.1875 - loss: 0.6480 - precision: 0.6421 - recall: 0.7291 - true_negatives: 1825.0182 - true_positives: 2302.5989\n",
      "For batch number 385 the model has a loss of 0.608469545841217\n",
      "\u001b[1m385/689\u001b[0m \u001b[32m━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━\u001b[0m \u001b[1m16s\u001b[0m 55ms/step - auc: 0.7023 - binary_accuracy: 0.6570 - false_negatives: 794.4623 - false_positives: 1242.1818 - loss: 0.6479 - precision: 0.6422 - recall: 0.7293 - true_negatives: 1830.0935 - true_positives: 2309.2625\n",
      "For batch number 386 the model has a loss of 0.6085677146911621\n",
      "\u001b[1m386/689\u001b[0m \u001b[32m━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━\u001b[0m \u001b[1m16s\u001b[0m 55ms/step - auc: 0.7025 - binary_accuracy: 0.6572 - false_negatives: 795.7254 - false_positives: 1245.1710 - loss: 0.6478 - precision: 0.6422 - recall: 0.7294 - true_negatives: 1835.1683 - true_positives: 2315.9353\n",
      "For batch number 387 the model has a loss of 0.6082655787467957\n",
      "\u001b[1m387/689\u001b[0m \u001b[32m━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━\u001b[0m \u001b[1m16s\u001b[0m 55ms/step - auc: 0.7026 - binary_accuracy: 0.6573 - false_negatives: 796.9845 - false_positives: 1248.1602 - loss: 0.6477 - precision: 0.6423 - recall: 0.7296 - true_negatives: 1840.2559 - true_positives: 2322.5994\n",
      "For batch number 388 the model has a loss of 0.6083570122718811\n",
      "\u001b[1m388/689\u001b[0m \u001b[32m━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━\u001b[0m \u001b[1m16s\u001b[0m 55ms/step - auc: 0.7027 - binary_accuracy: 0.6574 - false_negatives: 798.2397 - false_positives: 1251.1417 - loss: 0.6476 - precision: 0.6424 - recall: 0.7298 - true_negatives: 1845.3453 - true_positives: 2329.2732\n",
      "For batch number 389 the model has a loss of 0.6090677976608276\n",
      "\u001b[1m389/689\u001b[0m \u001b[32m━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━\u001b[0m \u001b[1m16s\u001b[0m 55ms/step - auc: 0.7029 - binary_accuracy: 0.6575 - false_negatives: 799.4910 - false_positives: 1254.1337 - loss: 0.6475 - precision: 0.6425 - recall: 0.7299 - true_negatives: 1850.4370 - true_positives: 2335.9382\n",
      "For batch number 390 the model has a loss of 0.608841061592102\n",
      "\u001b[1m390/689\u001b[0m \u001b[32m━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━\u001b[0m \u001b[1m16s\u001b[0m 55ms/step - auc: 0.7030 - binary_accuracy: 0.6576 - false_negatives: 800.7359 - false_positives: 1257.1307 - loss: 0.6474 - precision: 0.6425 - recall: 0.7301 - true_negatives: 1855.5231 - true_positives: 2342.6104\n",
      "For batch number 391 the model has a loss of 0.6085851788520813\n",
      "\u001b[1m391/689\u001b[0m \u001b[32m━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━\u001b[0m \u001b[1m16s\u001b[0m 55ms/step - auc: 0.7031 - binary_accuracy: 0.6577 - false_negatives: 801.9796 - false_positives: 1260.1151 - loss: 0.6473 - precision: 0.6426 - recall: 0.7303 - true_negatives: 1860.6086 - true_positives: 2349.2966\n",
      "For batch number 392 the model has a loss of 0.6082332134246826\n",
      "\u001b[1m392/689\u001b[0m \u001b[32m━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━\u001b[0m \u001b[1m16s\u001b[0m 55ms/step - auc: 0.7033 - binary_accuracy: 0.6579 - false_negatives: 803.2194 - false_positives: 1263.0918 - loss: 0.6472 - precision: 0.6427 - recall: 0.7304 - true_negatives: 1865.6990 - true_positives: 2355.9897\n",
      "For batch number 393 the model has a loss of 0.6079898476600647\n",
      "\u001b[1m393/689\u001b[0m \u001b[32m━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━\u001b[0m \u001b[1m16s\u001b[0m 55ms/step - auc: 0.7034 - binary_accuracy: 0.6580 - false_negatives: 804.4580 - false_positives: 1266.0560 - loss: 0.6471 - precision: 0.6428 - recall: 0.7306 - true_negatives: 1870.7811 - true_positives: 2362.7048\n",
      "For batch number 394 the model has a loss of 0.6077823638916016\n",
      "\u001b[1m394/689\u001b[0m \u001b[32m━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━\u001b[0m \u001b[1m16s\u001b[0m 55ms/step - auc: 0.7036 - binary_accuracy: 0.6581 - false_negatives: 805.6904 - false_positives: 1269.0280 - loss: 0.6470 - precision: 0.6428 - recall: 0.7307 - true_negatives: 1875.8655 - true_positives: 2369.4163\n",
      "For batch number 395 the model has a loss of 0.6074501276016235\n",
      "\u001b[1m395/689\u001b[0m \u001b[32m━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━\u001b[0m \u001b[1m16s\u001b[0m 55ms/step - auc: 0.7037 - binary_accuracy: 0.6582 - false_negatives: 806.9164 - false_positives: 1272.0050 - loss: 0.6469 - precision: 0.6429 - recall: 0.7309 - true_negatives: 1880.9545 - true_positives: 2376.1240\n",
      "For batch number 396 the model has a loss of 0.607378363609314\n",
      "\u001b[1m396/689\u001b[0m \u001b[32m━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━\u001b[0m \u001b[1m16s\u001b[0m 55ms/step - auc: 0.7038 - binary_accuracy: 0.6583 - false_negatives: 808.1566 - false_positives: 1274.9697 - loss: 0.6468 - precision: 0.6430 - recall: 0.7311 - true_negatives: 1886.0505 - true_positives: 2382.8232\n",
      "For batch number 397 the model has a loss of 0.6067662239074707\n",
      "\u001b[1m397/689\u001b[0m \u001b[32m━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━\u001b[0m \u001b[1m15s\u001b[0m 55ms/step - auc: 0.7040 - binary_accuracy: 0.6584 - false_negatives: 809.3929 - false_positives: 1277.9270 - loss: 0.6467 - precision: 0.6431 - recall: 0.7312 - true_negatives: 1891.1411 - true_positives: 2389.5391\n",
      "For batch number 398 the model has a loss of 0.6062547564506531\n",
      "\u001b[1m398/689\u001b[0m \u001b[32m━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━\u001b[0m \u001b[1m15s\u001b[0m 55ms/step - auc: 0.7041 - binary_accuracy: 0.6586 - false_negatives: 810.6231 - false_positives: 1280.8768 - loss: 0.6466 - precision: 0.6431 - recall: 0.7314 - true_negatives: 1896.2312 - true_positives: 2396.2688\n",
      "For batch number 399 the model has a loss of 0.6064109802246094\n",
      "\u001b[1m399/689\u001b[0m \u001b[32m━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━\u001b[0m \u001b[1m15s\u001b[0m 55ms/step - auc: 0.7042 - binary_accuracy: 0.6587 - false_negatives: 811.8496 - false_positives: 1283.8246 - loss: 0.6465 - precision: 0.6432 - recall: 0.7316 - true_negatives: 1901.3182 - true_positives: 2403.0076\n",
      "For batch number 400 the model has a loss of 0.6058652400970459\n",
      "\u001b[1m400/689\u001b[0m \u001b[32m━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━\u001b[0m \u001b[1m15s\u001b[0m 55ms/step - auc: 0.7044 - binary_accuracy: 0.6588 - false_negatives: 813.0700 - false_positives: 1286.7675 - loss: 0.6464 - precision: 0.6433 - recall: 0.7317 - true_negatives: 1906.4075 - true_positives: 2409.7549\n",
      "For batch number 401 the model has a loss of 0.605526864528656\n",
      "\u001b[1m401/689\u001b[0m \u001b[32m━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━\u001b[0m \u001b[1m15s\u001b[0m 55ms/step - auc: 0.7045 - binary_accuracy: 0.6589 - false_negatives: 814.2843 - false_positives: 1289.7133 - loss: 0.6463 - precision: 0.6434 - recall: 0.7319 - true_negatives: 1911.4912 - true_positives: 2416.5112\n",
      "For batch number 402 the model has a loss of 0.6053098440170288\n",
      "\u001b[1m402/689\u001b[0m \u001b[32m━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━\u001b[0m \u001b[1m15s\u001b[0m 55ms/step - auc: 0.7046 - binary_accuracy: 0.6590 - false_negatives: 815.4926 - false_positives: 1292.6666 - loss: 0.6462 - precision: 0.6435 - recall: 0.7321 - true_negatives: 1916.5870 - true_positives: 2423.2537\n",
      "For batch number 403 the model has a loss of 0.605692982673645\n",
      "\u001b[1m403/689\u001b[0m \u001b[32m━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━\u001b[0m \u001b[1m15s\u001b[0m 55ms/step - auc: 0.7048 - binary_accuracy: 0.6592 - false_negatives: 816.7022 - false_positives: 1295.6154 - loss: 0.6461 - precision: 0.6435 - recall: 0.7322 - true_negatives: 1921.6799 - true_positives: 2430.0024\n",
      "For batch number 404 the model has a loss of 0.6055890917778015\n",
      "\u001b[1m404/689\u001b[0m \u001b[32m━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━\u001b[0m \u001b[1m15s\u001b[0m 55ms/step - auc: 0.7049 - binary_accuracy: 0.6593 - false_negatives: 817.9109 - false_positives: 1298.5668 - loss: 0.6460 - precision: 0.6436 - recall: 0.7324 - true_negatives: 1926.7698 - true_positives: 2436.7524\n",
      "For batch number 405 the model has a loss of 0.6051713228225708\n",
      "\u001b[1m405/689\u001b[0m \u001b[32m━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━\u001b[0m \u001b[1m15s\u001b[0m 55ms/step - auc: 0.7050 - binary_accuracy: 0.6594 - false_negatives: 819.1136 - false_positives: 1301.5186 - loss: 0.6459 - precision: 0.6437 - recall: 0.7325 - true_negatives: 1931.8643 - true_positives: 2443.5037\n",
      "For batch number 406 the model has a loss of 0.6052711009979248\n",
      "\u001b[1m406/689\u001b[0m \u001b[32m━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━\u001b[0m \u001b[1m15s\u001b[0m 55ms/step - auc: 0.7052 - binary_accuracy: 0.6595 - false_negatives: 820.3152 - false_positives: 1304.4680 - loss: 0.6458 - precision: 0.6438 - recall: 0.7327 - true_negatives: 1936.9581 - true_positives: 2450.2585\n",
      "For batch number 407 the model has a loss of 0.6052742600440979\n",
      "\u001b[1m407/689\u001b[0m \u001b[32m━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━\u001b[0m \u001b[1m15s\u001b[0m 55ms/step - auc: 0.7053 - binary_accuracy: 0.6596 - false_negatives: 821.5110 - false_positives: 1307.4226 - loss: 0.6457 - precision: 0.6438 - recall: 0.7329 - true_negatives: 1942.0565 - true_positives: 2457.0098\n",
      "For batch number 408 the model has a loss of 0.6047682762145996\n",
      "\u001b[1m408/689\u001b[0m \u001b[32m━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━\u001b[0m \u001b[1m15s\u001b[0m 55ms/step - auc: 0.7054 - binary_accuracy: 0.6597 - false_negatives: 822.7034 - false_positives: 1310.3701 - loss: 0.6456 - precision: 0.6439 - recall: 0.7330 - true_negatives: 1947.1617 - true_positives: 2463.7646\n",
      "For batch number 409 the model has a loss of 0.6043760776519775\n",
      "\u001b[1m409/689\u001b[0m \u001b[32m━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━\u001b[0m \u001b[1m15s\u001b[0m 55ms/step - auc: 0.7056 - binary_accuracy: 0.6599 - false_negatives: 823.8900 - false_positives: 1313.3179 - loss: 0.6455 - precision: 0.6440 - recall: 0.7332 - true_negatives: 1952.2665 - true_positives: 2470.5256\n",
      "For batch number 410 the model has a loss of 0.6046795845031738\n",
      "\u001b[1m410/689\u001b[0m \u001b[32m━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━\u001b[0m \u001b[1m15s\u001b[0m 55ms/step - auc: 0.7057 - binary_accuracy: 0.6600 - false_negatives: 825.0781 - false_positives: 1316.2659 - loss: 0.6454 - precision: 0.6441 - recall: 0.7334 - true_negatives: 1957.3683 - true_positives: 2477.2878\n",
      "For batch number 411 the model has a loss of 0.6041547060012817\n",
      "\u001b[1m411/689\u001b[0m \u001b[32m━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━\u001b[0m \u001b[1m15s\u001b[0m 55ms/step - auc: 0.7059 - binary_accuracy: 0.6601 - false_negatives: 826.2603 - false_positives: 1319.2117 - loss: 0.6453 - precision: 0.6441 - recall: 0.7335 - true_negatives: 1962.4745 - true_positives: 2484.0535\n",
      "For batch number 412 the model has a loss of 0.6042582392692566\n",
      "\u001b[1m412/689\u001b[0m \u001b[32m━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━\u001b[0m \u001b[1m15s\u001b[0m 55ms/step - auc: 0.7060 - binary_accuracy: 0.6602 - false_negatives: 827.4418 - false_positives: 1322.1530 - loss: 0.6452 - precision: 0.6442 - recall: 0.7337 - true_negatives: 1967.5825 - true_positives: 2490.8228\n",
      "For batch number 413 the model has a loss of 0.6039345264434814\n",
      "\u001b[1m413/689\u001b[0m \u001b[32m━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━\u001b[0m \u001b[1m15s\u001b[0m 55ms/step - auc: 0.7061 - binary_accuracy: 0.6603 - false_negatives: 828.6174 - false_positives: 1325.0896 - loss: 0.6451 - precision: 0.6443 - recall: 0.7338 - true_negatives: 1972.6901 - true_positives: 2497.6030\n",
      "For batch number 414 the model has a loss of 0.6034138798713684\n",
      "\u001b[1m414/689\u001b[0m \u001b[32m━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━\u001b[0m \u001b[1m15s\u001b[0m 55ms/step - auc: 0.7063 - binary_accuracy: 0.6604 - false_negatives: 829.7874 - false_positives: 1328.0242 - loss: 0.6450 - precision: 0.6444 - recall: 0.7340 - true_negatives: 1977.8019 - true_positives: 2504.3865\n",
      "For batch number 415 the model has a loss of 0.6030669212341309\n",
      "\u001b[1m415/689\u001b[0m \u001b[32m━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━\u001b[0m \u001b[1m15s\u001b[0m 55ms/step - auc: 0.7064 - binary_accuracy: 0.6606 - false_negatives: 830.9542 - false_positives: 1330.9542 - loss: 0.6449 - precision: 0.6444 - recall: 0.7342 - true_negatives: 1982.9156 - true_positives: 2511.1760\n",
      "For batch number 416 the model has a loss of 0.6026590466499329\n",
      "\u001b[1m416/689\u001b[0m \u001b[32m━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━\u001b[0m \u001b[1m15s\u001b[0m 55ms/step - auc: 0.7065 - binary_accuracy: 0.6607 - false_negatives: 832.1154 - false_positives: 1333.8846 - loss: 0.6448 - precision: 0.6445 - recall: 0.7343 - true_negatives: 1988.0337 - true_positives: 2517.9663\n",
      "For batch number 417 the model has a loss of 0.6024606227874756\n",
      "\u001b[1m417/689\u001b[0m \u001b[32m━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━\u001b[0m \u001b[1m14s\u001b[0m 55ms/step - auc: 0.7067 - binary_accuracy: 0.6608 - false_negatives: 833.2710 - false_positives: 1336.8177 - loss: 0.6447 - precision: 0.6446 - recall: 0.7345 - true_negatives: 1993.1487 - true_positives: 2524.7627\n",
      "For batch number 418 the model has a loss of 0.6018208265304565\n",
      "\u001b[1m418/689\u001b[0m \u001b[32m━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━\u001b[0m \u001b[1m14s\u001b[0m 55ms/step - auc: 0.7068 - binary_accuracy: 0.6609 - false_negatives: 834.4211 - false_positives: 1339.7465 - loss: 0.6446 - precision: 0.6447 - recall: 0.7347 - true_negatives: 1998.2632 - true_positives: 2531.5693\n",
      "For batch number 419 the model has a loss of 0.6017398834228516\n",
      "\u001b[1m419/689\u001b[0m \u001b[32m━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━\u001b[0m \u001b[1m14s\u001b[0m 55ms/step - auc: 0.7069 - binary_accuracy: 0.6610 - false_negatives: 835.5680 - false_positives: 1342.6682 - loss: 0.6445 - precision: 0.6448 - recall: 0.7348 - true_negatives: 2003.3748 - true_positives: 2538.3889\n",
      "For batch number 420 the model has a loss of 0.6016310453414917\n",
      "\u001b[1m420/689\u001b[0m \u001b[32m━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━\u001b[0m \u001b[1m14s\u001b[0m 55ms/step - auc: 0.7071 - binary_accuracy: 0.6612 - false_negatives: 836.7095 - false_positives: 1345.5881 - loss: 0.6444 - precision: 0.6448 - recall: 0.7350 - true_negatives: 2008.4880 - true_positives: 2545.2144\n",
      "For batch number 421 the model has a loss of 0.6011129021644592\n",
      "\u001b[1m421/689\u001b[0m \u001b[32m━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━\u001b[0m \u001b[1m14s\u001b[0m 55ms/step - auc: 0.7072 - binary_accuracy: 0.6613 - false_negatives: 837.8456 - false_positives: 1348.5012 - loss: 0.6443 - precision: 0.6449 - recall: 0.7352 - true_negatives: 2013.6105 - true_positives: 2552.0427\n",
      "For batch number 422 the model has a loss of 0.6010885238647461\n",
      "\u001b[1m422/689\u001b[0m \u001b[32m━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━\u001b[0m \u001b[1m14s\u001b[0m 55ms/step - auc: 0.7073 - binary_accuracy: 0.6614 - false_negatives: 838.9763 - false_positives: 1351.4194 - loss: 0.6442 - precision: 0.6450 - recall: 0.7353 - true_negatives: 2018.7369 - true_positives: 2558.8672\n",
      "For batch number 423 the model has a loss of 0.6006461977958679\n",
      "\u001b[1m423/689\u001b[0m \u001b[32m━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━\u001b[0m \u001b[1m14s\u001b[0m 55ms/step - auc: 0.7075 - binary_accuracy: 0.6615 - false_negatives: 840.1040 - false_positives: 1354.3286 - loss: 0.6441 - precision: 0.6451 - recall: 0.7355 - true_negatives: 2023.8652 - true_positives: 2565.7021\n",
      "For batch number 424 the model has a loss of 0.6006066799163818\n",
      "\u001b[1m424/689\u001b[0m \u001b[32m━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━\u001b[0m \u001b[1m14s\u001b[0m 55ms/step - auc: 0.7076 - binary_accuracy: 0.6616 - false_negatives: 841.2335 - false_positives: 1357.2358 - loss: 0.6440 - precision: 0.6452 - recall: 0.7357 - true_negatives: 2028.9906 - true_positives: 2572.5400\n",
      "For batch number 425 the model has a loss of 0.6002854108810425\n",
      "\u001b[1m425/689\u001b[0m \u001b[32m━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━\u001b[0m \u001b[1m14s\u001b[0m 55ms/step - auc: 0.7078 - binary_accuracy: 0.6618 - false_negatives: 842.3600 - false_positives: 1360.1365 - loss: 0.6438 - precision: 0.6452 - recall: 0.7358 - true_negatives: 2034.1152 - true_positives: 2579.3882\n",
      "For batch number 426 the model has a loss of 0.6002862453460693\n",
      "\u001b[1m426/689\u001b[0m \u001b[32m━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━\u001b[0m \u001b[1m14s\u001b[0m 55ms/step - auc: 0.7079 - binary_accuracy: 0.6619 - false_negatives: 843.4836 - false_positives: 1363.0399 - loss: 0.6437 - precision: 0.6453 - recall: 0.7360 - true_negatives: 2039.2488 - true_positives: 2586.2278\n",
      "For batch number 427 the model has a loss of 0.6002005934715271\n",
      "\u001b[1m427/689\u001b[0m \u001b[32m━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━\u001b[0m \u001b[1m14s\u001b[0m 55ms/step - auc: 0.7080 - binary_accuracy: 0.6620 - false_negatives: 844.6042 - false_positives: 1365.9414 - loss: 0.6436 - precision: 0.6454 - recall: 0.7362 - true_negatives: 2044.3794 - true_positives: 2593.0750\n",
      "For batch number 428 the model has a loss of 0.6005270481109619\n",
      "\u001b[1m428/689\u001b[0m \u001b[32m━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━\u001b[0m \u001b[1m14s\u001b[0m 55ms/step - auc: 0.7082 - binary_accuracy: 0.6621 - false_negatives: 845.7313 - false_positives: 1368.8387 - loss: 0.6435 - precision: 0.6455 - recall: 0.7363 - true_negatives: 2049.5071 - true_positives: 2599.9229\n",
      "For batch number 429 the model has a loss of 0.6007425785064697\n",
      "\u001b[1m429/689\u001b[0m \u001b[32m━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━\u001b[0m \u001b[1m14s\u001b[0m 55ms/step - auc: 0.7083 - binary_accuracy: 0.6622 - false_negatives: 846.8625 - false_positives: 1371.7366 - loss: 0.6434 - precision: 0.6456 - recall: 0.7365 - true_negatives: 2054.6340 - true_positives: 2606.7668\n",
      "For batch number 430 the model has a loss of 0.600397527217865\n",
      "\u001b[1m430/689\u001b[0m \u001b[32m━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━\u001b[0m \u001b[1m14s\u001b[0m 55ms/step - auc: 0.7084 - binary_accuracy: 0.6623 - false_negatives: 847.9883 - false_positives: 1374.6349 - loss: 0.6433 - precision: 0.6456 - recall: 0.7366 - true_negatives: 2059.7605 - true_positives: 2613.6162\n",
      "For batch number 431 the model has a loss of 0.6006178855895996\n",
      "\u001b[1m431/689\u001b[0m \u001b[32m━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━\u001b[0m \u001b[1m14s\u001b[0m 55ms/step - auc: 0.7086 - binary_accuracy: 0.6625 - false_negatives: 849.1160 - false_positives: 1377.5337 - loss: 0.6432 - precision: 0.6457 - recall: 0.7368 - true_negatives: 2064.8816 - true_positives: 2620.4688\n",
      "For batch number 432 the model has a loss of 0.6009299755096436\n",
      "\u001b[1m432/689\u001b[0m \u001b[32m━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━\u001b[0m \u001b[1m14s\u001b[0m 55ms/step - auc: 0.7087 - binary_accuracy: 0.6626 - false_negatives: 850.2500 - false_positives: 1380.4352 - loss: 0.6431 - precision: 0.6458 - recall: 0.7370 - true_negatives: 2069.9932 - true_positives: 2627.3218\n",
      "For batch number 433 the model has a loss of 0.6006686091423035\n",
      "\u001b[1m433/689\u001b[0m \u001b[32m━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━\u001b[0m \u001b[1m14s\u001b[0m 55ms/step - auc: 0.7088 - binary_accuracy: 0.6627 - false_negatives: 851.3834 - false_positives: 1383.3348 - loss: 0.6430 - precision: 0.6459 - recall: 0.7371 - true_negatives: 2075.1062 - true_positives: 2634.1755\n",
      "For batch number 434 the model has a loss of 0.6012044548988342\n",
      "\u001b[1m434/689\u001b[0m \u001b[32m━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━\u001b[0m \u001b[1m14s\u001b[0m 55ms/step - auc: 0.7090 - binary_accuracy: 0.6628 - false_negatives: 852.5161 - false_positives: 1386.2373 - loss: 0.6430 - precision: 0.6459 - recall: 0.7373 - true_negatives: 2080.2212 - true_positives: 2641.0254\n",
      "For batch number 435 the model has a loss of 0.6009137630462646\n",
      "\u001b[1m435/689\u001b[0m \u001b[32m━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━\u001b[0m \u001b[1m14s\u001b[0m 55ms/step - auc: 0.7091 - binary_accuracy: 0.6629 - false_negatives: 853.6460 - false_positives: 1389.1403 - loss: 0.6429 - precision: 0.6460 - recall: 0.7375 - true_negatives: 2085.3357 - true_positives: 2647.8782\n",
      "For batch number 436 the model has a loss of 0.6012429594993591\n",
      "\u001b[1m436/689\u001b[0m \u001b[32m━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━\u001b[0m \u001b[1m13s\u001b[0m 55ms/step - auc: 0.7092 - binary_accuracy: 0.6630 - false_negatives: 854.7752 - false_positives: 1392.0436 - loss: 0.6428 - precision: 0.6461 - recall: 0.7376 - true_negatives: 2090.4495 - true_positives: 2654.7317\n",
      "For batch number 437 the model has a loss of 0.6007372736930847\n",
      "\u001b[1m437/689\u001b[0m \u001b[32m━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━\u001b[0m \u001b[1m13s\u001b[0m 55ms/step - auc: 0.7094 - binary_accuracy: 0.6632 - false_negatives: 855.8993 - false_positives: 1394.9427 - loss: 0.6427 - precision: 0.6462 - recall: 0.7378 - true_negatives: 2095.5652 - true_positives: 2661.5928\n",
      "For batch number 438 the model has a loss of 0.6011832356452942\n",
      "\u001b[1m438/689\u001b[0m \u001b[32m━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━\u001b[0m \u001b[1m13s\u001b[0m 55ms/step - auc: 0.7095 - binary_accuracy: 0.6633 - false_negatives: 857.0228 - false_positives: 1397.8516 - loss: 0.6426 - precision: 0.6463 - recall: 0.7379 - true_negatives: 2100.6804 - true_positives: 2668.4453\n",
      "For batch number 439 the model has a loss of 0.6007162928581238\n",
      "\u001b[1m439/689\u001b[0m \u001b[32m━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━\u001b[0m \u001b[1m13s\u001b[0m 55ms/step - auc: 0.7096 - binary_accuracy: 0.6634 - false_negatives: 858.1412 - false_positives: 1400.7562 - loss: 0.6425 - precision: 0.6463 - recall: 0.7381 - true_negatives: 2105.7996 - true_positives: 2675.3030\n",
      "For batch number 440 the model has a loss of 0.600119411945343\n",
      "\u001b[1m440/689\u001b[0m \u001b[32m━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━\u001b[0m \u001b[1m13s\u001b[0m 55ms/step - auc: 0.7098 - binary_accuracy: 0.6635 - false_negatives: 859.2568 - false_positives: 1403.6500 - loss: 0.6424 - precision: 0.6464 - recall: 0.7383 - true_negatives: 2110.9226 - true_positives: 2682.1704\n",
      "For batch number 441 the model has a loss of 0.5998226404190063\n",
      "\u001b[1m441/689\u001b[0m \u001b[32m━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━\u001b[0m \u001b[1m13s\u001b[0m 55ms/step - auc: 0.7099 - binary_accuracy: 0.6636 - false_negatives: 860.3674 - false_positives: 1406.5442 - loss: 0.6423 - precision: 0.6465 - recall: 0.7384 - true_negatives: 2116.0566 - true_positives: 2689.0317\n",
      "For batch number 442 the model has a loss of 0.5997666120529175\n",
      "\u001b[1m442/689\u001b[0m \u001b[32m━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━\u001b[0m \u001b[1m13s\u001b[0m 55ms/step - auc: 0.7100 - binary_accuracy: 0.6637 - false_negatives: 861.4796 - false_positives: 1409.4298 - loss: 0.6422 - precision: 0.6466 - recall: 0.7386 - true_negatives: 2121.1855 - true_positives: 2695.9050\n",
      "For batch number 443 the model has a loss of 0.5998147130012512\n",
      "\u001b[1m443/689\u001b[0m \u001b[32m━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━\u001b[0m \u001b[1m13s\u001b[0m 55ms/step - auc: 0.7101 - binary_accuracy: 0.6639 - false_negatives: 862.5914 - false_positives: 1412.3206 - loss: 0.6421 - precision: 0.6466 - recall: 0.7387 - true_negatives: 2126.3184 - true_positives: 2702.7698\n",
      "For batch number 444 the model has a loss of 0.5994246602058411\n",
      "\u001b[1m444/689\u001b[0m \u001b[32m━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━\u001b[0m \u001b[1m13s\u001b[0m 55ms/step - auc: 0.7103 - binary_accuracy: 0.6640 - false_negatives: 863.7004 - false_positives: 1415.2095 - loss: 0.6420 - precision: 0.6467 - recall: 0.7389 - true_negatives: 2131.4526 - true_positives: 2709.6375\n",
      "For batch number 445 the model has a loss of 0.5989575982093811\n",
      "\u001b[1m445/689\u001b[0m \u001b[32m━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━\u001b[0m \u001b[1m13s\u001b[0m 55ms/step - auc: 0.7104 - binary_accuracy: 0.6641 - false_negatives: 864.8045 - false_positives: 1418.0944 - loss: 0.6419 - precision: 0.6468 - recall: 0.7390 - true_negatives: 2136.5933 - true_positives: 2716.5078\n",
      "For batch number 446 the model has a loss of 0.599024772644043\n",
      "\u001b[1m446/689\u001b[0m \u001b[32m━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━\u001b[0m \u001b[1m13s\u001b[0m 55ms/step - auc: 0.7105 - binary_accuracy: 0.6642 - false_negatives: 865.9081 - false_positives: 1420.9821 - loss: 0.6418 - precision: 0.6469 - recall: 0.7392 - true_negatives: 2141.7354 - true_positives: 2723.3745\n",
      "For batch number 447 the model has a loss of 0.5988882184028625\n",
      "\u001b[1m447/689\u001b[0m \u001b[32m━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━\u001b[0m \u001b[1m13s\u001b[0m 55ms/step - auc: 0.7107 - binary_accuracy: 0.6643 - false_negatives: 867.0090 - false_positives: 1423.8590 - loss: 0.6417 - precision: 0.6469 - recall: 0.7394 - true_negatives: 2146.8838 - true_positives: 2730.2483\n",
      "For batch number 448 the model has a loss of 0.5987090468406677\n",
      "\u001b[1m448/689\u001b[0m \u001b[32m━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━\u001b[0m \u001b[1m13s\u001b[0m 55ms/step - auc: 0.7108 - binary_accuracy: 0.6644 - false_negatives: 868.1049 - false_positives: 1426.7344 - loss: 0.6416 - precision: 0.6470 - recall: 0.7395 - true_negatives: 2152.0291 - true_positives: 2737.1316\n",
      "For batch number 449 the model has a loss of 0.598809003829956\n",
      "\u001b[1m449/689\u001b[0m \u001b[32m━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━\u001b[0m \u001b[1m13s\u001b[0m 55ms/step - auc: 0.7109 - binary_accuracy: 0.6645 - false_negatives: 869.1982 - false_positives: 1429.6058 - loss: 0.6415 - precision: 0.6471 - recall: 0.7397 - true_negatives: 2157.1782 - true_positives: 2744.0178\n",
      "For batch number 450 the model has a loss of 0.5982556939125061\n",
      "\u001b[1m450/689\u001b[0m \u001b[32m━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━\u001b[0m \u001b[1m13s\u001b[0m 55ms/step - auc: 0.7111 - binary_accuracy: 0.6647 - false_negatives: 870.2889 - false_positives: 1432.4711 - loss: 0.6414 - precision: 0.6472 - recall: 0.7398 - true_negatives: 2162.3354 - true_positives: 2750.9045\n",
      "For batch number 451 the model has a loss of 0.5978904366493225\n",
      "\u001b[1m451/689\u001b[0m \u001b[32m━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━\u001b[0m \u001b[1m13s\u001b[0m 55ms/step - auc: 0.7112 - binary_accuracy: 0.6648 - false_negatives: 871.3770 - false_positives: 1435.3348 - loss: 0.6413 - precision: 0.6473 - recall: 0.7400 - true_negatives: 2167.4988 - true_positives: 2757.7893\n",
      "For batch number 452 the model has a loss of 0.5976143479347229\n",
      "\u001b[1m452/689\u001b[0m \u001b[32m━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━\u001b[0m \u001b[1m13s\u001b[0m 55ms/step - auc: 0.7113 - binary_accuracy: 0.6649 - false_negatives: 872.4602 - false_positives: 1438.1903 - loss: 0.6412 - precision: 0.6473 - recall: 0.7402 - true_negatives: 2172.6682 - true_positives: 2764.6814\n",
      "For batch number 453 the model has a loss of 0.5975179672241211\n",
      "\u001b[1m453/689\u001b[0m \u001b[32m━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━\u001b[0m \u001b[1m13s\u001b[0m 55ms/step - auc: 0.7115 - binary_accuracy: 0.6650 - false_negatives: 873.5408 - false_positives: 1441.0375 - loss: 0.6411 - precision: 0.6474 - recall: 0.7403 - true_negatives: 2177.8433 - true_positives: 2771.5784\n",
      "For batch number 454 the model has a loss of 0.5972444415092468\n",
      "\u001b[1m454/689\u001b[0m \u001b[32m━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━\u001b[0m \u001b[1m12s\u001b[0m 55ms/step - auc: 0.7116 - binary_accuracy: 0.6651 - false_negatives: 874.6190 - false_positives: 1443.8833 - loss: 0.6410 - precision: 0.6475 - recall: 0.7405 - true_negatives: 2183.0220 - true_positives: 2778.4758\n",
      "For batch number 455 the model has a loss of 0.5967457294464111\n",
      "\u001b[1m455/689\u001b[0m \u001b[32m━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━\u001b[0m \u001b[1m12s\u001b[0m 55ms/step - auc: 0.7117 - binary_accuracy: 0.6652 - false_negatives: 875.6923 - false_positives: 1446.7230 - loss: 0.6409 - precision: 0.6476 - recall: 0.7406 - true_negatives: 2188.2043 - true_positives: 2785.3801\n",
      "For batch number 456 the model has a loss of 0.5964549779891968\n",
      "\u001b[1m456/689\u001b[0m \u001b[32m━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━\u001b[0m \u001b[1m12s\u001b[0m 55ms/step - auc: 0.7119 - binary_accuracy: 0.6654 - false_negatives: 876.7697 - false_positives: 1449.5526 - loss: 0.6408 - precision: 0.6477 - recall: 0.7408 - true_negatives: 2193.3794 - true_positives: 2792.2983\n",
      "For batch number 457 the model has a loss of 0.5961117148399353\n",
      "\u001b[1m457/689\u001b[0m \u001b[32m━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━\u001b[0m \u001b[1m12s\u001b[0m 55ms/step - auc: 0.7120 - binary_accuracy: 0.6655 - false_negatives: 877.8425 - false_positives: 1452.3785 - loss: 0.6407 - precision: 0.6477 - recall: 0.7410 - true_negatives: 2198.5623 - true_positives: 2799.2166\n",
      "For batch number 458 the model has a loss of 0.596533477306366\n",
      "\u001b[1m458/689\u001b[0m \u001b[32m━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━\u001b[0m \u001b[1m12s\u001b[0m 55ms/step - auc: 0.7122 - binary_accuracy: 0.6656 - false_negatives: 878.9149 - false_positives: 1455.2009 - loss: 0.6406 - precision: 0.6478 - recall: 0.7411 - true_negatives: 2203.7468 - true_positives: 2806.1375\n",
      "For batch number 459 the model has a loss of 0.5962200164794922\n",
      "\u001b[1m459/689\u001b[0m \u001b[32m━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━\u001b[0m \u001b[1m12s\u001b[0m 55ms/step - auc: 0.7123 - binary_accuracy: 0.6657 - false_negatives: 879.9847 - false_positives: 1458.0217 - loss: 0.6405 - precision: 0.6479 - recall: 0.7413 - true_negatives: 2208.9412 - true_positives: 2813.0522\n",
      "For batch number 460 the model has a loss of 0.5956238508224487\n",
      "\u001b[1m460/689\u001b[0m \u001b[32m━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━\u001b[0m \u001b[1m12s\u001b[0m 55ms/step - auc: 0.7124 - binary_accuracy: 0.6658 - false_negatives: 881.0500 - false_positives: 1460.8369 - loss: 0.6404 - precision: 0.6480 - recall: 0.7414 - true_negatives: 2214.1458 - true_positives: 2819.9673\n",
      "For batch number 461 the model has a loss of 0.5953770279884338\n",
      "\u001b[1m461/689\u001b[0m \u001b[32m━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━\u001b[0m \u001b[1m12s\u001b[0m 55ms/step - auc: 0.7126 - binary_accuracy: 0.6660 - false_negatives: 882.1107 - false_positives: 1463.6486 - loss: 0.6403 - precision: 0.6481 - recall: 0.7416 - true_negatives: 2219.3513 - true_positives: 2826.8894\n",
      "For batch number 462 the model has a loss of 0.5950312614440918\n",
      "\u001b[1m462/689\u001b[0m \u001b[32m━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━\u001b[0m \u001b[1m12s\u001b[0m 55ms/step - auc: 0.7127 - binary_accuracy: 0.6661 - false_negatives: 883.1688 - false_positives: 1466.4589 - loss: 0.6403 - precision: 0.6481 - recall: 0.7417 - true_negatives: 2224.5649 - true_positives: 2833.8074\n",
      "For batch number 463 the model has a loss of 0.5946411490440369\n",
      "\u001b[1m463/689\u001b[0m \u001b[32m━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━\u001b[0m \u001b[1m12s\u001b[0m 55ms/step - auc: 0.7128 - binary_accuracy: 0.6662 - false_negatives: 884.2225 - false_positives: 1469.2700 - loss: 0.6402 - precision: 0.6482 - recall: 0.7419 - true_negatives: 2229.7839 - true_positives: 2840.7236\n",
      "For batch number 464 the model has a loss of 0.5944479703903198\n",
      "\u001b[1m464/689\u001b[0m \u001b[32m━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━\u001b[0m \u001b[1m12s\u001b[0m 55ms/step - auc: 0.7130 - binary_accuracy: 0.6663 - false_negatives: 885.2737 - false_positives: 1472.0732 - loss: 0.6401 - precision: 0.6483 - recall: 0.7421 - true_negatives: 2235.0000 - true_positives: 2847.6531\n",
      "For batch number 465 the model has a loss of 0.5942995548248291\n",
      "\u001b[1m465/689\u001b[0m \u001b[32m━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━\u001b[0m \u001b[1m12s\u001b[0m 55ms/step - auc: 0.7131 - binary_accuracy: 0.6664 - false_negatives: 886.3226 - false_positives: 1474.8817 - loss: 0.6400 - precision: 0.6484 - recall: 0.7422 - true_negatives: 2240.2129 - true_positives: 2854.5828\n",
      "For batch number 466 the model has a loss of 0.5941061973571777\n",
      "\u001b[1m466/689\u001b[0m \u001b[32m━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━\u001b[0m \u001b[1m12s\u001b[0m 55ms/step - auc: 0.7132 - binary_accuracy: 0.6665 - false_negatives: 887.3669 - false_positives: 1477.6953 - loss: 0.6399 - precision: 0.6485 - recall: 0.7424 - true_negatives: 2245.4207 - true_positives: 2861.5171\n",
      "For batch number 467 the model has a loss of 0.5939121246337891\n",
      "\u001b[1m467/689\u001b[0m \u001b[32m━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━\u001b[0m \u001b[1m12s\u001b[0m 55ms/step - auc: 0.7134 - binary_accuracy: 0.6667 - false_negatives: 888.4069 - false_positives: 1480.5096 - loss: 0.6398 - precision: 0.6486 - recall: 0.7425 - true_negatives: 2250.6316 - true_positives: 2868.4519\n",
      "For batch number 468 the model has a loss of 0.5935177206993103\n",
      "\u001b[1m468/689\u001b[0m \u001b[32m━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━\u001b[0m \u001b[1m12s\u001b[0m 55ms/step - auc: 0.7135 - binary_accuracy: 0.6668 - false_negatives: 889.4423 - false_positives: 1483.3206 - loss: 0.6397 - precision: 0.6486 - recall: 0.7427 - true_negatives: 2255.8484 - true_positives: 2875.3889\n",
      "For batch number 469 the model has a loss of 0.5934314727783203\n",
      "\u001b[1m469/689\u001b[0m \u001b[32m━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━\u001b[0m \u001b[1m12s\u001b[0m 55ms/step - auc: 0.7136 - binary_accuracy: 0.6669 - false_negatives: 890.4733 - false_positives: 1486.1343 - loss: 0.6396 - precision: 0.6487 - recall: 0.7429 - true_negatives: 2261.0576 - true_positives: 2882.3347\n",
      "For batch number 470 the model has a loss of 0.5937363505363464\n",
      "\u001b[1m470/689\u001b[0m \u001b[32m━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━\u001b[0m \u001b[1m12s\u001b[0m 55ms/step - auc: 0.7138 - binary_accuracy: 0.6670 - false_negatives: 891.5021 - false_positives: 1488.9553 - loss: 0.6395 - precision: 0.6488 - recall: 0.7430 - true_negatives: 2266.2659 - true_positives: 2889.2766\n",
      "For batch number 471 the model has a loss of 0.5933426022529602\n",
      "\u001b[1m471/689\u001b[0m \u001b[32m━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━\u001b[0m \u001b[1m12s\u001b[0m 55ms/step - auc: 0.7139 - binary_accuracy: 0.6671 - false_negatives: 892.5266 - false_positives: 1491.7771 - loss: 0.6394 - precision: 0.6489 - recall: 0.7432 - true_negatives: 2271.4734 - true_positives: 2896.2229\n",
      "For batch number 472 the model has a loss of 0.5927016139030457\n",
      "\u001b[1m472/689\u001b[0m \u001b[32m━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━\u001b[0m \u001b[1m12s\u001b[0m 55ms/step - auc: 0.7140 - binary_accuracy: 0.6672 - false_negatives: 893.5466 - false_positives: 1494.5911 - loss: 0.6393 - precision: 0.6490 - recall: 0.7433 - true_negatives: 2276.6865 - true_positives: 2903.1758\n",
      "For batch number 473 the model has a loss of 0.5924731492996216\n",
      "\u001b[1m473/689\u001b[0m \u001b[32m━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━\u001b[0m \u001b[1m11s\u001b[0m 55ms/step - auc: 0.7142 - binary_accuracy: 0.6674 - false_negatives: 894.5624 - false_positives: 1497.4081 - loss: 0.6392 - precision: 0.6490 - recall: 0.7435 - true_negatives: 2281.9006 - true_positives: 2910.1289\n",
      "For batch number 474 the model has a loss of 0.5923688411712646\n",
      "\u001b[1m474/689\u001b[0m \u001b[32m━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━\u001b[0m \u001b[1m11s\u001b[0m 55ms/step - auc: 0.7143 - binary_accuracy: 0.6675 - false_negatives: 895.5759 - false_positives: 1500.2279 - loss: 0.6391 - precision: 0.6491 - recall: 0.7436 - true_negatives: 2287.1160 - true_positives: 2917.0801\n",
      "For batch number 475 the model has a loss of 0.5923316478729248\n",
      "\u001b[1m475/689\u001b[0m \u001b[32m━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━\u001b[0m \u001b[1m11s\u001b[0m 55ms/step - auc: 0.7144 - binary_accuracy: 0.6676 - false_negatives: 896.5873 - false_positives: 1503.0378 - loss: 0.6390 - precision: 0.6492 - recall: 0.7438 - true_negatives: 2292.3347 - true_positives: 2924.0400\n",
      "For batch number 476 the model has a loss of 0.5918223261833191\n",
      "\u001b[1m476/689\u001b[0m \u001b[32m━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━\u001b[0m \u001b[1m11s\u001b[0m 55ms/step - auc: 0.7146 - binary_accuracy: 0.6677 - false_negatives: 897.5945 - false_positives: 1505.8403 - loss: 0.6389 - precision: 0.6493 - recall: 0.7440 - true_negatives: 2297.5588 - true_positives: 2931.0063\n",
      "For batch number 477 the model has a loss of 0.5916822552680969\n",
      "\u001b[1m477/689\u001b[0m \u001b[32m━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━\u001b[0m \u001b[1m11s\u001b[0m 55ms/step - auc: 0.7147 - binary_accuracy: 0.6678 - false_negatives: 898.5996 - false_positives: 1508.6373 - loss: 0.6388 - precision: 0.6494 - recall: 0.7441 - true_negatives: 2302.7861 - true_positives: 2937.9771\n",
      "For batch number 478 the model has a loss of 0.5916031002998352\n",
      "\u001b[1m478/689\u001b[0m \u001b[32m━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━\u001b[0m \u001b[1m11s\u001b[0m 55ms/step - auc: 0.7148 - binary_accuracy: 0.6679 - false_negatives: 899.6025 - false_positives: 1511.4309 - loss: 0.6387 - precision: 0.6494 - recall: 0.7443 - true_negatives: 2308.0125 - true_positives: 2944.9539\n",
      "For batch number 479 the model has a loss of 0.591547966003418\n",
      "\u001b[1m479/689\u001b[0m \u001b[32m━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━\u001b[0m \u001b[1m11s\u001b[0m 55ms/step - auc: 0.7150 - binary_accuracy: 0.6681 - false_negatives: 900.6013 - false_positives: 1514.2360 - loss: 0.6386 - precision: 0.6495 - recall: 0.7444 - true_negatives: 2313.2441 - true_positives: 2951.9187\n",
      "For batch number 480 the model has a loss of 0.5913944840431213\n",
      "\u001b[1m480/689\u001b[0m \u001b[32m━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━\u001b[0m \u001b[1m11s\u001b[0m 55ms/step - auc: 0.7151 - binary_accuracy: 0.6682 - false_negatives: 901.5979 - false_positives: 1517.0437 - loss: 0.6385 - precision: 0.6496 - recall: 0.7446 - true_negatives: 2318.4771 - true_positives: 2958.8813\n",
      "For batch number 481 the model has a loss of 0.5910816192626953\n",
      "\u001b[1m481/689\u001b[0m \u001b[32m━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━\u001b[0m \u001b[1m11s\u001b[0m 55ms/step - auc: 0.7152 - binary_accuracy: 0.6683 - false_negatives: 902.5905 - false_positives: 1519.8483 - loss: 0.6384 - precision: 0.6497 - recall: 0.7448 - true_negatives: 2323.7068 - true_positives: 2965.8545\n",
      "For batch number 482 the model has a loss of 0.5907125473022461\n",
      "\u001b[1m482/689\u001b[0m \u001b[32m━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━\u001b[0m \u001b[1m11s\u001b[0m 55ms/step - auc: 0.7154 - binary_accuracy: 0.6684 - false_negatives: 903.5789 - false_positives: 1522.6494 - loss: 0.6383 - precision: 0.6498 - recall: 0.7449 - true_negatives: 2328.9399 - true_positives: 2972.8320\n",
      "For batch number 483 the model has a loss of 0.5907219648361206\n",
      "\u001b[1m483/689\u001b[0m \u001b[32m━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━\u001b[0m \u001b[1m11s\u001b[0m 55ms/step - auc: 0.7155 - binary_accuracy: 0.6685 - false_negatives: 904.5652 - false_positives: 1525.4473 - loss: 0.6382 - precision: 0.6498 - recall: 0.7451 - true_negatives: 2334.1719 - true_positives: 2979.8157\n",
      "For batch number 484 the model has a loss of 0.5903553366661072\n",
      "\u001b[1m484/689\u001b[0m \u001b[32m━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━\u001b[0m \u001b[1m11s\u001b[0m 55ms/step - auc: 0.7156 - binary_accuracy: 0.6686 - false_negatives: 905.5475 - false_positives: 1528.2417 - loss: 0.6381 - precision: 0.6499 - recall: 0.7452 - true_negatives: 2339.4050 - true_positives: 2986.8057\n",
      "For batch number 485 the model has a loss of 0.5910822749137878\n",
      "\u001b[1m485/689\u001b[0m \u001b[32m━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━\u001b[0m \u001b[1m11s\u001b[0m 55ms/step - auc: 0.7158 - binary_accuracy: 0.6688 - false_negatives: 906.5278 - false_positives: 1531.0392 - loss: 0.6380 - precision: 0.6500 - recall: 0.7454 - true_negatives: 2344.6392 - true_positives: 2993.7937\n",
      "For batch number 486 the model has a loss of 0.5904691815376282\n",
      "\u001b[1m486/689\u001b[0m \u001b[32m━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━\u001b[0m \u001b[1m11s\u001b[0m 55ms/step - auc: 0.7159 - binary_accuracy: 0.6689 - false_negatives: 907.5041 - false_positives: 1533.8313 - loss: 0.6379 - precision: 0.6501 - recall: 0.7455 - true_negatives: 2349.8765 - true_positives: 3000.7881\n",
      "For batch number 487 the model has a loss of 0.5902425646781921\n",
      "\u001b[1m487/689\u001b[0m \u001b[32m━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━\u001b[0m \u001b[1m11s\u001b[0m 55ms/step - auc: 0.7160 - binary_accuracy: 0.6690 - false_negatives: 908.4805 - false_positives: 1536.6160 - loss: 0.6378 - precision: 0.6502 - recall: 0.7457 - true_negatives: 2355.1108 - true_positives: 3007.7927\n",
      "For batch number 488 the model has a loss of 0.5899150371551514\n",
      "\u001b[1m488/689\u001b[0m \u001b[32m━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━\u001b[0m \u001b[1m11s\u001b[0m 55ms/step - auc: 0.7162 - binary_accuracy: 0.6691 - false_negatives: 909.4529 - false_positives: 1539.4016 - loss: 0.6377 - precision: 0.6502 - recall: 0.7459 - true_negatives: 2360.3503 - true_positives: 3014.7952\n",
      "For batch number 489 the model has a loss of 0.5899804830551147\n",
      "\u001b[1m489/689\u001b[0m \u001b[32m━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━\u001b[0m \u001b[1m11s\u001b[0m 55ms/step - auc: 0.7163 - binary_accuracy: 0.6692 - false_negatives: 910.4233 - false_positives: 1542.1799 - loss: 0.6376 - precision: 0.6503 - recall: 0.7460 - true_negatives: 2365.5889 - true_positives: 3021.8079\n",
      "For batch number 490 the model has a loss of 0.5902718901634216\n",
      "\u001b[1m490/689\u001b[0m \u001b[32m━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━\u001b[0m \u001b[1m11s\u001b[0m 55ms/step - auc: 0.7164 - binary_accuracy: 0.6693 - false_negatives: 911.3939 - false_positives: 1544.9551 - loss: 0.6375 - precision: 0.6504 - recall: 0.7462 - true_negatives: 2370.8264 - true_positives: 3028.8245\n",
      "For batch number 491 the model has a loss of 0.589961051940918\n",
      "\u001b[1m491/689\u001b[0m \u001b[32m━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━\u001b[0m \u001b[1m10s\u001b[0m 55ms/step - auc: 0.7166 - binary_accuracy: 0.6695 - false_negatives: 912.3605 - false_positives: 1547.7312 - loss: 0.6374 - precision: 0.6505 - recall: 0.7463 - true_negatives: 2376.0632 - true_positives: 3035.8452\n",
      "For batch number 492 the model has a loss of 0.5897276401519775\n",
      "\u001b[1m492/689\u001b[0m \u001b[32m━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━\u001b[0m \u001b[1m10s\u001b[0m 55ms/step - auc: 0.7167 - binary_accuracy: 0.6696 - false_negatives: 913.3252 - false_positives: 1550.5082 - loss: 0.6373 - precision: 0.6506 - recall: 0.7465 - true_negatives: 2381.2988 - true_positives: 3042.8679\n",
      "For batch number 493 the model has a loss of 0.5893934369087219\n",
      "\u001b[1m493/689\u001b[0m \u001b[32m━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━\u001b[0m \u001b[1m10s\u001b[0m 55ms/step - auc: 0.7168 - binary_accuracy: 0.6697 - false_negatives: 914.2860 - false_positives: 1553.2839 - loss: 0.6372 - precision: 0.6506 - recall: 0.7467 - true_negatives: 2386.5437 - true_positives: 3049.8865\n",
      "For batch number 494 the model has a loss of 0.5898578763008118\n",
      "\u001b[1m494/689\u001b[0m \u001b[32m━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━\u001b[0m \u001b[1m10s\u001b[0m 55ms/step - auc: 0.7170 - binary_accuracy: 0.6698 - false_negatives: 915.2449 - false_positives: 1556.0547 - loss: 0.6371 - precision: 0.6507 - recall: 0.7468 - true_negatives: 2391.7896 - true_positives: 3056.9109\n",
      "For batch number 495 the model has a loss of 0.5897943377494812\n",
      "\u001b[1m495/689\u001b[0m \u001b[32m━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━\u001b[0m \u001b[1m10s\u001b[0m 55ms/step - auc: 0.7171 - binary_accuracy: 0.6699 - false_negatives: 916.2040 - false_positives: 1558.8344 - loss: 0.6370 - precision: 0.6508 - recall: 0.7470 - true_negatives: 2397.0322 - true_positives: 3063.9292\n",
      "For batch number 496 the model has a loss of 0.5894086360931396\n",
      "\u001b[1m496/689\u001b[0m \u001b[32m━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━\u001b[0m \u001b[1m10s\u001b[0m 55ms/step - auc: 0.7172 - binary_accuracy: 0.6700 - false_negatives: 917.1593 - false_positives: 1561.6129 - loss: 0.6369 - precision: 0.6509 - recall: 0.7471 - true_negatives: 2402.2822 - true_positives: 3070.9456\n",
      "For batch number 497 the model has a loss of 0.5896762609481812\n",
      "\u001b[1m497/689\u001b[0m \u001b[32m━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━\u001b[0m \u001b[1m10s\u001b[0m 55ms/step - auc: 0.7174 - binary_accuracy: 0.6702 - false_negatives: 918.1127 - false_positives: 1564.3823 - loss: 0.6368 - precision: 0.6510 - recall: 0.7473 - true_negatives: 2407.5393 - true_positives: 3077.9658\n",
      "For batch number 498 the model has a loss of 0.5894246697425842\n",
      "\u001b[1m498/689\u001b[0m \u001b[32m━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━\u001b[0m \u001b[1m10s\u001b[0m 55ms/step - auc: 0.7175 - binary_accuracy: 0.6703 - false_negatives: 919.0643 - false_positives: 1567.1506 - loss: 0.6367 - precision: 0.6510 - recall: 0.7474 - true_negatives: 2412.8013 - true_positives: 3084.9839\n",
      "For batch number 499 the model has a loss of 0.5891194343566895\n",
      "\u001b[1m499/689\u001b[0m \u001b[32m━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━\u001b[0m \u001b[1m10s\u001b[0m 55ms/step - auc: 0.7176 - binary_accuracy: 0.6704 - false_negatives: 920.0140 - false_positives: 1569.9178 - loss: 0.6366 - precision: 0.6511 - recall: 0.7476 - true_negatives: 2418.0601 - true_positives: 3092.0081\n",
      "For batch number 500 the model has a loss of 0.5885812640190125\n",
      "\u001b[1m500/689\u001b[0m \u001b[32m━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━\u001b[0m \u001b[1m10s\u001b[0m 55ms/step - auc: 0.7178 - binary_accuracy: 0.6705 - false_negatives: 920.9600 - false_positives: 1572.6820 - loss: 0.6365 - precision: 0.6512 - recall: 0.7477 - true_negatives: 2423.3240 - true_positives: 3099.0339\n",
      "For batch number 501 the model has a loss of 0.5881783366203308\n",
      "\u001b[1m501/689\u001b[0m \u001b[32m━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━\u001b[0m \u001b[1m10s\u001b[0m 55ms/step - auc: 0.7179 - binary_accuracy: 0.6706 - false_negatives: 921.9042 - false_positives: 1575.4431 - loss: 0.6364 - precision: 0.6513 - recall: 0.7479 - true_negatives: 2428.5928 - true_positives: 3106.0598\n",
      "For batch number 502 the model has a loss of 0.5880222320556641\n",
      "\u001b[1m502/689\u001b[0m \u001b[32m━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━\u001b[0m \u001b[1m10s\u001b[0m 55ms/step - auc: 0.7180 - binary_accuracy: 0.6707 - false_negatives: 922.8466 - false_positives: 1578.1992 - loss: 0.6363 - precision: 0.6514 - recall: 0.7481 - true_negatives: 2433.8586 - true_positives: 3113.0957\n",
      "For batch number 503 the model has a loss of 0.5876830816268921\n",
      "\u001b[1m503/689\u001b[0m \u001b[32m━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━\u001b[0m \u001b[1m10s\u001b[0m 55ms/step - auc: 0.7182 - binary_accuracy: 0.6709 - false_negatives: 923.7853 - false_positives: 1580.9563 - loss: 0.6362 - precision: 0.6514 - recall: 0.7482 - true_negatives: 2439.1233 - true_positives: 3120.1353\n",
      "For batch number 504 the model has a loss of 0.5871869921684265\n",
      "\u001b[1m504/689\u001b[0m \u001b[32m━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━\u001b[0m \u001b[1m10s\u001b[0m 55ms/step - auc: 0.7183 - binary_accuracy: 0.6710 - false_negatives: 924.7222 - false_positives: 1583.7043 - loss: 0.6362 - precision: 0.6515 - recall: 0.7484 - true_negatives: 2444.3909 - true_positives: 3127.1826\n",
      "For batch number 505 the model has a loss of 0.5869286060333252\n",
      "\u001b[1m505/689\u001b[0m \u001b[32m━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━\u001b[0m \u001b[1m10s\u001b[0m 55ms/step - auc: 0.7184 - binary_accuracy: 0.6711 - false_negatives: 925.6574 - false_positives: 1586.4535 - loss: 0.6361 - precision: 0.6516 - recall: 0.7485 - true_negatives: 2449.6633 - true_positives: 3134.2258\n",
      "For batch number 506 the model has a loss of 0.586704671382904\n",
      "\u001b[1m506/689\u001b[0m \u001b[32m━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━\u001b[0m \u001b[1m10s\u001b[0m 55ms/step - auc: 0.7186 - binary_accuracy: 0.6712 - false_negatives: 926.5929 - false_positives: 1589.1996 - loss: 0.6360 - precision: 0.6517 - recall: 0.7487 - true_negatives: 2454.9348 - true_positives: 3141.2727\n",
      "For batch number 507 the model has a loss of 0.5864312052726746\n",
      "\u001b[1m507/689\u001b[0m \u001b[32m━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━\u001b[0m \u001b[1m10s\u001b[0m 55ms/step - auc: 0.7187 - binary_accuracy: 0.6713 - false_negatives: 927.5247 - false_positives: 1591.9388 - loss: 0.6359 - precision: 0.6518 - recall: 0.7488 - true_negatives: 2460.2051 - true_positives: 3148.3313\n",
      "For batch number 508 the model has a loss of 0.5867598652839661\n",
      "\u001b[1m508/689\u001b[0m \u001b[32m━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━\u001b[0m \u001b[1m10s\u001b[0m 55ms/step - auc: 0.7188 - binary_accuracy: 0.6714 - false_negatives: 928.4587 - false_positives: 1594.6752 - loss: 0.6358 - precision: 0.6518 - recall: 0.7490 - true_negatives: 2465.4685 - true_positives: 3155.3977\n",
      "For batch number 509 the model has a loss of 0.5871353149414062\n",
      "\u001b[1m509/689\u001b[0m \u001b[32m━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━\u001b[0m \u001b[1m9s\u001b[0m 55ms/step - auc: 0.7189 - binary_accuracy: 0.6716 - false_negatives: 929.3910 - false_positives: 1597.4204 - loss: 0.6357 - precision: 0.6519 - recall: 0.7492 - true_negatives: 2470.7329 - true_positives: 3162.4558 \n",
      "For batch number 510 the model has a loss of 0.5875977873802185\n",
      "\u001b[1m510/689\u001b[0m \u001b[32m━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━\u001b[0m \u001b[1m9s\u001b[0m 55ms/step - auc: 0.7191 - binary_accuracy: 0.6717 - false_negatives: 930.3216 - false_positives: 1600.1666 - loss: 0.6356 - precision: 0.6520 - recall: 0.7493 - true_negatives: 2475.9961 - true_positives: 3169.5156\n",
      "For batch number 511 the model has a loss of 0.587154746055603\n",
      "\u001b[1m511/689\u001b[0m \u001b[32m━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━\u001b[0m \u001b[1m9s\u001b[0m 55ms/step - auc: 0.7192 - binary_accuracy: 0.6718 - false_negatives: 931.2485 - false_positives: 1602.9100 - loss: 0.6355 - precision: 0.6521 - recall: 0.7495 - true_negatives: 2481.2661 - true_positives: 3176.5754\n",
      "For batch number 512 the model has a loss of 0.5867329835891724\n",
      "\u001b[1m512/689\u001b[0m \u001b[32m━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━\u001b[0m \u001b[1m9s\u001b[0m 55ms/step - auc: 0.7193 - binary_accuracy: 0.6719 - false_negatives: 932.1738 - false_positives: 1605.6465 - loss: 0.6354 - precision: 0.6522 - recall: 0.7496 - true_negatives: 2486.5352 - true_positives: 3183.6445\n",
      "For batch number 513 the model has a loss of 0.5863913297653198\n",
      "\u001b[1m513/689\u001b[0m \u001b[32m━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━\u001b[0m \u001b[1m9s\u001b[0m 55ms/step - auc: 0.7195 - binary_accuracy: 0.6720 - false_negatives: 933.0955 - false_positives: 1608.3899 - loss: 0.6353 - precision: 0.6522 - recall: 0.7498 - true_negatives: 2491.8013 - true_positives: 3190.7134\n",
      "For batch number 514 the model has a loss of 0.5862175226211548\n",
      "\u001b[1m514/689\u001b[0m \u001b[32m━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━\u001b[0m \u001b[1m9s\u001b[0m 55ms/step - auc: 0.7196 - binary_accuracy: 0.6721 - false_negatives: 934.0156 - false_positives: 1611.1343 - loss: 0.6352 - precision: 0.6523 - recall: 0.7499 - true_negatives: 2497.0740 - true_positives: 3197.7764\n",
      "For batch number 515 the model has a loss of 0.5863291621208191\n",
      "\u001b[1m515/689\u001b[0m \u001b[32m━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━\u001b[0m \u001b[1m9s\u001b[0m 55ms/step - auc: 0.7197 - binary_accuracy: 0.6722 - false_negatives: 934.9340 - false_positives: 1613.8854 - loss: 0.6351 - precision: 0.6524 - recall: 0.7501 - true_negatives: 2502.3552 - true_positives: 3204.8252\n",
      "For batch number 516 the model has a loss of 0.5860694050788879\n",
      "\u001b[1m516/689\u001b[0m \u001b[32m━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━\u001b[0m \u001b[1m9s\u001b[0m 55ms/step - auc: 0.7199 - binary_accuracy: 0.6724 - false_negatives: 935.8508 - false_positives: 1616.6395 - loss: 0.6350 - precision: 0.6525 - recall: 0.7502 - true_negatives: 2507.6377 - true_positives: 3211.8721\n",
      "For batch number 517 the model has a loss of 0.5862160325050354\n",
      "\u001b[1m517/689\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m9s\u001b[0m 55ms/step - auc: 0.7200 - binary_accuracy: 0.6725 - false_negatives: 936.7698 - false_positives: 1619.3907 - loss: 0.6349 - precision: 0.6526 - recall: 0.7504 - true_negatives: 2512.9226 - true_positives: 3218.9167\n",
      "For batch number 518 the model has a loss of 0.5860350131988525\n",
      "\u001b[1m518/689\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m9s\u001b[0m 55ms/step - auc: 0.7201 - binary_accuracy: 0.6726 - false_negatives: 937.6853 - false_positives: 1622.1428 - loss: 0.6348 - precision: 0.6526 - recall: 0.7505 - true_negatives: 2518.2124 - true_positives: 3225.9595\n",
      "For batch number 519 the model has a loss of 0.5862079858779907\n",
      "\u001b[1m519/689\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m9s\u001b[0m 55ms/step - auc: 0.7203 - binary_accuracy: 0.6727 - false_negatives: 938.6031 - false_positives: 1624.8901 - loss: 0.6347 - precision: 0.6527 - recall: 0.7507 - true_negatives: 2523.5029 - true_positives: 3233.0039\n",
      "For batch number 520 the model has a loss of 0.5859770178794861\n",
      "\u001b[1m520/689\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m9s\u001b[0m 55ms/step - auc: 0.7204 - binary_accuracy: 0.6728 - false_negatives: 939.5173 - false_positives: 1627.6346 - loss: 0.6346 - precision: 0.6528 - recall: 0.7508 - true_negatives: 2528.8040 - true_positives: 3240.0442\n",
      "For batch number 521 the model has a loss of 0.5856727361679077\n",
      "\u001b[1m521/689\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m9s\u001b[0m 55ms/step - auc: 0.7205 - binary_accuracy: 0.6729 - false_negatives: 940.4299 - false_positives: 1630.3743 - loss: 0.6345 - precision: 0.6529 - recall: 0.7510 - true_negatives: 2534.1094 - true_positives: 3247.0864\n",
      "For batch number 522 the model has a loss of 0.5852676630020142\n",
      "\u001b[1m522/689\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m9s\u001b[0m 56ms/step - auc: 0.7206 - binary_accuracy: 0.6730 - false_negatives: 941.3410 - false_positives: 1633.1073 - loss: 0.6344 - precision: 0.6529 - recall: 0.7511 - true_negatives: 2539.4158 - true_positives: 3254.1360\n",
      "For batch number 523 the model has a loss of 0.5851262807846069\n",
      "\u001b[1m523/689\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m9s\u001b[0m 56ms/step - auc: 0.7208 - binary_accuracy: 0.6731 - false_negatives: 942.2486 - false_positives: 1635.8394 - loss: 0.6343 - precision: 0.6530 - recall: 0.7513 - true_negatives: 2544.7341 - true_positives: 3261.1777\n",
      "For batch number 524 the model has a loss of 0.5849683880805969\n",
      "\u001b[1m524/689\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m9s\u001b[0m 56ms/step - auc: 0.7209 - binary_accuracy: 0.6733 - false_negatives: 943.1546 - false_positives: 1638.5745 - loss: 0.6342 - precision: 0.6531 - recall: 0.7515 - true_negatives: 2550.0554 - true_positives: 3268.2156\n",
      "For batch number 525 the model has a loss of 0.5848808288574219\n",
      "\u001b[1m525/689\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m9s\u001b[0m 56ms/step - auc: 0.7210 - binary_accuracy: 0.6734 - false_negatives: 944.0590 - false_positives: 1641.3086 - loss: 0.6342 - precision: 0.6532 - recall: 0.7516 - true_negatives: 2555.3828 - true_positives: 3275.2495\n",
      "For batch number 526 the model has a loss of 0.5845379829406738\n",
      "\u001b[1m526/689\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m9s\u001b[0m 56ms/step - auc: 0.7212 - binary_accuracy: 0.6735 - false_negatives: 944.9601 - false_positives: 1644.0380 - loss: 0.6341 - precision: 0.6533 - recall: 0.7518 - true_negatives: 2560.7109 - true_positives: 3282.2908\n",
      "For batch number 527 the model has a loss of 0.5840869545936584\n",
      "\u001b[1m527/689\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m8s\u001b[0m 56ms/step - auc: 0.7213 - binary_accuracy: 0.6736 - false_negatives: 945.8577 - false_positives: 1646.7590 - loss: 0.6340 - precision: 0.6533 - recall: 0.7519 - true_negatives: 2566.0437 - true_positives: 3289.3396\n",
      "For batch number 528 the model has a loss of 0.583966851234436\n",
      "\u001b[1m528/689\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m8s\u001b[0m 56ms/step - auc: 0.7214 - binary_accuracy: 0.6737 - false_negatives: 946.7557 - false_positives: 1649.4791 - loss: 0.6339 - precision: 0.6534 - recall: 0.7521 - true_negatives: 2571.3806 - true_positives: 3296.3845\n",
      "For batch number 529 the model has a loss of 0.5837985277175903\n",
      "\u001b[1m529/689\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m8s\u001b[0m 56ms/step - auc: 0.7215 - binary_accuracy: 0.6738 - false_negatives: 947.6541 - false_positives: 1652.1947 - loss: 0.6338 - precision: 0.6535 - recall: 0.7522 - true_negatives: 2576.7222 - true_positives: 3303.4292\n",
      "For batch number 530 the model has a loss of 0.5834243297576904\n",
      "\u001b[1m530/689\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m8s\u001b[0m 56ms/step - auc: 0.7217 - binary_accuracy: 0.6739 - false_negatives: 948.5510 - false_positives: 1654.9076 - loss: 0.6337 - precision: 0.6536 - recall: 0.7524 - true_negatives: 2582.0659 - true_positives: 3310.4756\n",
      "For batch number 531 the model has a loss of 0.5829779505729675\n",
      "\u001b[1m531/689\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m8s\u001b[0m 56ms/step - auc: 0.7218 - binary_accuracy: 0.6741 - false_negatives: 949.4445 - false_positives: 1657.6158 - loss: 0.6336 - precision: 0.6536 - recall: 0.7525 - true_negatives: 2587.4180 - true_positives: 3317.5217\n",
      "For batch number 532 the model has a loss of 0.5826059579849243\n",
      "\u001b[1m532/689\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m8s\u001b[0m 56ms/step - auc: 0.7219 - binary_accuracy: 0.6742 - false_negatives: 950.3402 - false_positives: 1660.3158 - loss: 0.6335 - precision: 0.6537 - recall: 0.7527 - true_negatives: 2592.7688 - true_positives: 3324.5752\n",
      "For batch number 533 the model has a loss of 0.5822070837020874\n",
      "\u001b[1m533/689\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m8s\u001b[0m 56ms/step - auc: 0.7221 - binary_accuracy: 0.6743 - false_negatives: 951.2327 - false_positives: 1663.0112 - loss: 0.6334 - precision: 0.6538 - recall: 0.7528 - true_negatives: 2598.1238 - true_positives: 3331.6323\n",
      "For batch number 534 the model has a loss of 0.5821767449378967\n",
      "\u001b[1m534/689\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m8s\u001b[0m 56ms/step - auc: 0.7222 - binary_accuracy: 0.6744 - false_negatives: 952.1273 - false_positives: 1665.7023 - loss: 0.6333 - precision: 0.6539 - recall: 0.7530 - true_negatives: 2603.4795 - true_positives: 3338.6909\n",
      "For batch number 535 the model has a loss of 0.5828787684440613\n",
      "\u001b[1m535/689\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m8s\u001b[0m 56ms/step - auc: 0.7223 - binary_accuracy: 0.6745 - false_negatives: 953.0224 - false_positives: 1668.3981 - loss: 0.6332 - precision: 0.6540 - recall: 0.7531 - true_negatives: 2608.8354 - true_positives: 3345.7439\n",
      "For batch number 536 the model has a loss of 0.5828222036361694\n",
      "\u001b[1m536/689\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m8s\u001b[0m 56ms/step - auc: 0.7224 - binary_accuracy: 0.6746 - false_negatives: 953.9160 - false_positives: 1671.0970 - loss: 0.6331 - precision: 0.6540 - recall: 0.7533 - true_negatives: 2614.1941 - true_positives: 3352.7930\n",
      "For batch number 537 the model has a loss of 0.5827625393867493\n",
      "\u001b[1m537/689\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m8s\u001b[0m 56ms/step - auc: 0.7226 - binary_accuracy: 0.6747 - false_negatives: 954.8119 - false_positives: 1673.7914 - loss: 0.6330 - precision: 0.6541 - recall: 0.7534 - true_negatives: 2619.5513 - true_positives: 3359.8455\n",
      "For batch number 538 the model has a loss of 0.5826905369758606\n",
      "\u001b[1m538/689\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m8s\u001b[0m 56ms/step - auc: 0.7227 - binary_accuracy: 0.6748 - false_negatives: 955.7082 - false_positives: 1676.4833 - loss: 0.6329 - precision: 0.6542 - recall: 0.7535 - true_negatives: 2624.9089 - true_positives: 3366.8997\n",
      "For batch number 539 the model has a loss of 0.5824078917503357\n",
      "\u001b[1m539/689\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m8s\u001b[0m 56ms/step - auc: 0.7228 - binary_accuracy: 0.6750 - false_negatives: 956.6067 - false_positives: 1679.1688 - loss: 0.6328 - precision: 0.6543 - recall: 0.7537 - true_negatives: 2630.2671 - true_positives: 3373.9573\n",
      "For batch number 540 the model has a loss of 0.5823655724525452\n",
      "\u001b[1m540/689\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m8s\u001b[0m 56ms/step - auc: 0.7229 - binary_accuracy: 0.6751 - false_negatives: 957.5093 - false_positives: 1681.8556 - loss: 0.6327 - precision: 0.6544 - recall: 0.7538 - true_negatives: 2635.6240 - true_positives: 3381.0112\n",
      "For batch number 541 the model has a loss of 0.5822310447692871\n",
      "\u001b[1m541/689\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m8s\u001b[0m 56ms/step - auc: 0.7231 - binary_accuracy: 0.6752 - false_negatives: 958.4103 - false_positives: 1684.5378 - loss: 0.6326 - precision: 0.6544 - recall: 0.7540 - true_negatives: 2640.9797 - true_positives: 3388.0720\n",
      "For batch number 542 the model has a loss of 0.5824960470199585\n",
      "\u001b[1m542/689\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m8s\u001b[0m 56ms/step - auc: 0.7232 - binary_accuracy: 0.6753 - false_negatives: 959.3099 - false_positives: 1687.2251 - loss: 0.6325 - precision: 0.6545 - recall: 0.7541 - true_negatives: 2646.3376 - true_positives: 3395.1272\n",
      "For batch number 543 the model has a loss of 0.5824220180511475\n",
      "\u001b[1m543/689\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m8s\u001b[0m 56ms/step - auc: 0.7233 - binary_accuracy: 0.6754 - false_negatives: 960.2100 - false_positives: 1689.9116 - loss: 0.6325 - precision: 0.6546 - recall: 0.7543 - true_negatives: 2651.6887 - true_positives: 3402.1897\n",
      "For batch number 544 the model has a loss of 0.5821036696434021\n",
      "\u001b[1m544/689\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m8s\u001b[0m 56ms/step - auc: 0.7234 - binary_accuracy: 0.6755 - false_negatives: 961.1066 - false_positives: 1692.5992 - loss: 0.6324 - precision: 0.6547 - recall: 0.7544 - true_negatives: 2657.0386 - true_positives: 3409.2556\n",
      "For batch number 545 the model has a loss of 0.5823233723640442\n",
      "\u001b[1m545/689\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m7s\u001b[0m 56ms/step - auc: 0.7236 - binary_accuracy: 0.6756 - false_negatives: 962.0018 - false_positives: 1695.2881 - loss: 0.6323 - precision: 0.6547 - recall: 0.7546 - true_negatives: 2662.3889 - true_positives: 3416.3210\n",
      "For batch number 546 the model has a loss of 0.5822097063064575\n",
      "\u001b[1m546/689\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m7s\u001b[0m 56ms/step - auc: 0.7237 - binary_accuracy: 0.6757 - false_negatives: 962.8956 - false_positives: 1697.9762 - loss: 0.6322 - precision: 0.6548 - recall: 0.7547 - true_negatives: 2667.7417 - true_positives: 3423.3865\n",
      "For batch number 547 the model has a loss of 0.582448422908783\n",
      "\u001b[1m547/689\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m7s\u001b[0m 56ms/step - auc: 0.7238 - binary_accuracy: 0.6758 - false_negatives: 963.7897 - false_positives: 1700.6672 - loss: 0.6321 - precision: 0.6549 - recall: 0.7549 - true_negatives: 2673.0950 - true_positives: 3430.4480\n",
      "For batch number 548 the model has a loss of 0.5825505256652832\n",
      "\u001b[1m548/689\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m7s\u001b[0m 56ms/step - auc: 0.7239 - binary_accuracy: 0.6760 - false_negatives: 964.6879 - false_positives: 1703.3558 - loss: 0.6320 - precision: 0.6550 - recall: 0.7550 - true_negatives: 2678.4470 - true_positives: 3437.5090\n",
      "For batch number 549 the model has a loss of 0.5825695991516113\n",
      "\u001b[1m549/689\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m7s\u001b[0m 56ms/step - auc: 0.7241 - binary_accuracy: 0.6761 - false_negatives: 965.5865 - false_positives: 1706.0400 - loss: 0.6319 - precision: 0.6551 - recall: 0.7551 - true_negatives: 2683.7979 - true_positives: 3444.5757\n",
      "For batch number 550 the model has a loss of 0.5826159715652466\n",
      "\u001b[1m550/689\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m7s\u001b[0m 56ms/step - auc: 0.7242 - binary_accuracy: 0.6762 - false_negatives: 966.4891 - false_positives: 1708.7236 - loss: 0.6318 - precision: 0.6551 - recall: 0.7553 - true_negatives: 2689.1455 - true_positives: 3451.6418\n",
      "For batch number 551 the model has a loss of 0.5825434327125549\n",
      "\u001b[1m551/689\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m7s\u001b[0m 56ms/step - auc: 0.7243 - binary_accuracy: 0.6763 - false_negatives: 967.3939 - false_positives: 1711.4011 - loss: 0.6317 - precision: 0.6552 - recall: 0.7554 - true_negatives: 2694.4900 - true_positives: 3458.7151\n",
      "For batch number 552 the model has a loss of 0.5825440883636475\n",
      "\u001b[1m552/689\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m7s\u001b[0m 56ms/step - auc: 0.7244 - binary_accuracy: 0.6764 - false_negatives: 968.3007 - false_positives: 1714.0725 - loss: 0.6316 - precision: 0.6553 - recall: 0.7556 - true_negatives: 2699.8352 - true_positives: 3465.7917\n",
      "For batch number 553 the model has a loss of 0.5825791358947754\n",
      "\u001b[1m553/689\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m7s\u001b[0m 56ms/step - auc: 0.7246 - binary_accuracy: 0.6765 - false_negatives: 969.2079 - false_positives: 1716.7487 - loss: 0.6316 - precision: 0.6554 - recall: 0.7557 - true_negatives: 2705.1772 - true_positives: 3472.8662\n",
      "For batch number 554 the model has a loss of 0.5822404623031616\n",
      "\u001b[1m554/689\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m7s\u001b[0m 56ms/step - auc: 0.7247 - binary_accuracy: 0.6766 - false_negatives: 970.1137 - false_positives: 1719.4205 - loss: 0.6315 - precision: 0.6554 - recall: 0.7559 - true_negatives: 2710.5254 - true_positives: 3479.9404\n",
      "For batch number 555 the model has a loss of 0.5819335579872131\n",
      "\u001b[1m555/689\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m7s\u001b[0m 56ms/step - auc: 0.7248 - binary_accuracy: 0.6767 - false_negatives: 971.0180 - false_positives: 1722.0883 - loss: 0.6314 - precision: 0.6555 - recall: 0.7560 - true_negatives: 2715.8757 - true_positives: 3487.0181\n",
      "For batch number 556 the model has a loss of 0.5823456645011902\n",
      "\u001b[1m556/689\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m7s\u001b[0m 56ms/step - auc: 0.7249 - binary_accuracy: 0.6768 - false_negatives: 971.9208 - false_positives: 1724.7554 - loss: 0.6313 - precision: 0.6556 - recall: 0.7561 - true_negatives: 2721.2266 - true_positives: 3494.0972\n",
      "For batch number 557 the model has a loss of 0.5817935466766357\n",
      "\u001b[1m557/689\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m7s\u001b[0m 56ms/step - auc: 0.7250 - binary_accuracy: 0.6769 - false_negatives: 972.8223 - false_positives: 1727.4183 - loss: 0.6312 - precision: 0.6557 - recall: 0.7563 - true_negatives: 2726.5818 - true_positives: 3501.1777\n",
      "For batch number 558 the model has a loss of 0.5817911028862\n",
      "\u001b[1m558/689\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m7s\u001b[0m 56ms/step - auc: 0.7252 - binary_accuracy: 0.6770 - false_negatives: 973.7205 - false_positives: 1730.0878 - loss: 0.6311 - precision: 0.6557 - recall: 0.7564 - true_negatives: 2731.9390 - true_positives: 3508.2527\n",
      "For batch number 559 the model has a loss of 0.5815209746360779\n",
      "\u001b[1m559/689\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m7s\u001b[0m 56ms/step - auc: 0.7253 - binary_accuracy: 0.6772 - false_negatives: 974.6154 - false_positives: 1732.7603 - loss: 0.6310 - precision: 0.6558 - recall: 0.7566 - true_negatives: 2737.2969 - true_positives: 3515.3274\n",
      "For batch number 560 the model has a loss of 0.5815297961235046\n",
      "\u001b[1m560/689\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m7s\u001b[0m 56ms/step - auc: 0.7254 - binary_accuracy: 0.6773 - false_negatives: 975.5107 - false_positives: 1735.4303 - loss: 0.6309 - precision: 0.6559 - recall: 0.7567 - true_negatives: 2742.6572 - true_positives: 3522.4019\n",
      "For batch number 561 the model has a loss of 0.5816396474838257\n",
      "\u001b[1m561/689\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m7s\u001b[0m 56ms/step - auc: 0.7255 - binary_accuracy: 0.6774 - false_negatives: 976.4064 - false_positives: 1738.1088 - loss: 0.6308 - precision: 0.6560 - recall: 0.7568 - true_negatives: 2748.0178 - true_positives: 3529.4670\n",
      "For batch number 562 the model has a loss of 0.5813784599304199\n",
      "\u001b[1m562/689\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m7s\u001b[0m 56ms/step - auc: 0.7256 - binary_accuracy: 0.6775 - false_negatives: 977.2990 - false_positives: 1740.7882 - loss: 0.6308 - precision: 0.6560 - recall: 0.7570 - true_negatives: 2753.3826 - true_positives: 3536.5303\n",
      "For batch number 563 the model has a loss of 0.5816150307655334\n",
      "\u001b[1m563/689\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m7s\u001b[0m 56ms/step - auc: 0.7258 - binary_accuracy: 0.6776 - false_negatives: 978.1901 - false_positives: 1743.4742 - loss: 0.6307 - precision: 0.6561 - recall: 0.7571 - true_negatives: 2758.7441 - true_positives: 3543.5916\n",
      "For batch number 564 the model has a loss of 0.5815776586532593\n",
      "\u001b[1m564/689\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m6s\u001b[0m 56ms/step - auc: 0.7259 - binary_accuracy: 0.6777 - false_negatives: 979.0833 - false_positives: 1746.1543 - loss: 0.6306 - precision: 0.6562 - recall: 0.7573 - true_negatives: 2764.1099 - true_positives: 3550.6526\n",
      "For batch number 565 the model has a loss of 0.5817136764526367\n",
      "\u001b[1m565/689\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m6s\u001b[0m 56ms/step - auc: 0.7260 - binary_accuracy: 0.6778 - false_negatives: 979.9770 - false_positives: 1748.8372 - loss: 0.6305 - precision: 0.6563 - recall: 0.7574 - true_negatives: 2769.4797 - true_positives: 3557.7063\n",
      "For batch number 566 the model has a loss of 0.5820468068122864\n",
      "\u001b[1m566/689\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m6s\u001b[0m 56ms/step - auc: 0.7261 - binary_accuracy: 0.6779 - false_negatives: 980.8710 - false_positives: 1751.5159 - loss: 0.6304 - precision: 0.6563 - recall: 0.7575 - true_negatives: 2774.8533 - true_positives: 3564.7598\n",
      "For batch number 567 the model has a loss of 0.581924557685852\n",
      "\u001b[1m567/689\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m6s\u001b[0m 56ms/step - auc: 0.7262 - binary_accuracy: 0.6780 - false_negatives: 981.7654 - false_positives: 1754.1887 - loss: 0.6303 - precision: 0.6564 - recall: 0.7577 - true_negatives: 2780.2239 - true_positives: 3571.8218\n",
      "For batch number 568 the model has a loss of 0.5818469524383545\n",
      "\u001b[1m568/689\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m6s\u001b[0m 56ms/step - auc: 0.7264 - binary_accuracy: 0.6781 - false_negatives: 982.6584 - false_positives: 1756.8574 - loss: 0.6302 - precision: 0.6565 - recall: 0.7578 - true_negatives: 2785.5986 - true_positives: 3578.8855\n",
      "For batch number 569 the model has a loss of 0.5817515254020691\n",
      "\u001b[1m569/689\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m6s\u001b[0m 56ms/step - auc: 0.7265 - binary_accuracy: 0.6782 - false_negatives: 983.5501 - false_positives: 1759.5308 - loss: 0.6302 - precision: 0.6566 - recall: 0.7579 - true_negatives: 2790.9753 - true_positives: 3585.9438\n",
      "For batch number 570 the model has a loss of 0.5813825726509094\n",
      "\u001b[1m570/689\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m6s\u001b[0m 56ms/step - auc: 0.7266 - binary_accuracy: 0.6783 - false_negatives: 984.4386 - false_positives: 1762.1965 - loss: 0.6301 - precision: 0.6566 - recall: 0.7581 - true_negatives: 2796.3596 - true_positives: 3593.0054\n",
      "For batch number 571 the model has a loss of 0.5815010666847229\n",
      "\u001b[1m571/689\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m6s\u001b[0m 56ms/step - auc: 0.7267 - binary_accuracy: 0.6784 - false_negatives: 985.3275 - false_positives: 1764.8582 - loss: 0.6300 - precision: 0.6567 - recall: 0.7582 - true_negatives: 2801.7461 - true_positives: 3600.0684\n",
      "For batch number 572 the model has a loss of 0.581723690032959\n",
      "\u001b[1m572/689\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m6s\u001b[0m 56ms/step - auc: 0.7268 - binary_accuracy: 0.6785 - false_negatives: 986.2150 - false_positives: 1767.5140 - loss: 0.6299 - precision: 0.6568 - recall: 0.7584 - true_negatives: 2807.1277 - true_positives: 3607.1433\n",
      "For batch number 573 the model has a loss of 0.581540584564209\n",
      "\u001b[1m573/689\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m6s\u001b[0m 56ms/step - auc: 0.7269 - binary_accuracy: 0.6786 - false_negatives: 987.0995 - false_positives: 1770.1675 - loss: 0.6298 - precision: 0.6569 - recall: 0.7585 - true_negatives: 2812.5132 - true_positives: 3614.2200\n",
      "For batch number 574 the model has a loss of 0.5811254382133484\n",
      "\u001b[1m574/689\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m6s\u001b[0m 56ms/step - auc: 0.7271 - binary_accuracy: 0.6787 - false_negatives: 987.9808 - false_positives: 1772.8170 - loss: 0.6297 - precision: 0.6569 - recall: 0.7586 - true_negatives: 2817.9060 - true_positives: 3621.2961\n",
      "For batch number 575 the model has a loss of 0.5811107754707336\n",
      "\u001b[1m575/689\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m6s\u001b[0m 56ms/step - auc: 0.7272 - binary_accuracy: 0.6789 - false_negatives: 988.8591 - false_positives: 1775.4696 - loss: 0.6296 - precision: 0.6570 - recall: 0.7588 - true_negatives: 2823.2991 - true_positives: 3628.3721\n",
      "For batch number 576 the model has a loss of 0.5808802843093872\n",
      "\u001b[1m576/689\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m6s\u001b[0m 56ms/step - auc: 0.7273 - binary_accuracy: 0.6790 - false_negatives: 989.7344 - false_positives: 1778.1233 - loss: 0.6296 - precision: 0.6571 - recall: 0.7589 - true_negatives: 2828.6926 - true_positives: 3635.4497\n",
      "For batch number 577 the model has a loss of 0.580757737159729\n",
      "\u001b[1m577/689\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m6s\u001b[0m 56ms/step - auc: 0.7274 - binary_accuracy: 0.6791 - false_negatives: 990.6100 - false_positives: 1780.7765 - loss: 0.6295 - precision: 0.6572 - recall: 0.7590 - true_negatives: 2834.0884 - true_positives: 3642.5251\n",
      "For batch number 578 the model has a loss of 0.5804302096366882\n",
      "\u001b[1m578/689\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m6s\u001b[0m 56ms/step - auc: 0.7275 - binary_accuracy: 0.6792 - false_negatives: 991.4827 - false_positives: 1783.4257 - loss: 0.6294 - precision: 0.6572 - recall: 0.7592 - true_negatives: 2839.4827 - true_positives: 3649.6089\n",
      "For batch number 579 the model has a loss of 0.5804288387298584\n",
      "\u001b[1m579/689\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m6s\u001b[0m 56ms/step - auc: 0.7276 - binary_accuracy: 0.6793 - false_negatives: 992.3541 - false_positives: 1786.0725 - loss: 0.6293 - precision: 0.6573 - recall: 0.7593 - true_negatives: 2844.8792 - true_positives: 3656.6943\n",
      "For batch number 580 the model has a loss of 0.5803278088569641\n",
      "\u001b[1m580/689\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m6s\u001b[0m 56ms/step - auc: 0.7278 - binary_accuracy: 0.6794 - false_negatives: 993.2241 - false_positives: 1788.7137 - loss: 0.6292 - precision: 0.6574 - recall: 0.7595 - true_negatives: 2850.2793 - true_positives: 3663.7827\n",
      "For batch number 581 the model has a loss of 0.5801296234130859\n",
      "\u001b[1m581/689\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m6s\u001b[0m 56ms/step - auc: 0.7279 - binary_accuracy: 0.6795 - false_negatives: 994.0964 - false_positives: 1791.3459 - loss: 0.6291 - precision: 0.6575 - recall: 0.7596 - true_negatives: 2855.6765 - true_positives: 3670.8813\n",
      "For batch number 582 the model has a loss of 0.5800065994262695\n",
      "\u001b[1m582/689\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m5s\u001b[0m 56ms/step - auc: 0.7280 - binary_accuracy: 0.6796 - false_negatives: 994.9673 - false_positives: 1793.9742 - loss: 0.6291 - precision: 0.6575 - recall: 0.7597 - true_negatives: 2861.0723 - true_positives: 3677.9863\n",
      "For batch number 583 the model has a loss of 0.5802708268165588\n",
      "\u001b[1m583/689\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m5s\u001b[0m 56ms/step - auc: 0.7281 - binary_accuracy: 0.6797 - false_negatives: 995.8405 - false_positives: 1796.6003 - loss: 0.6290 - precision: 0.6576 - recall: 0.7599 - true_negatives: 2866.4717 - true_positives: 3685.0874\n",
      "For batch number 584 the model has a loss of 0.5799668431282043\n",
      "\u001b[1m584/689\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m5s\u001b[0m 56ms/step - auc: 0.7282 - binary_accuracy: 0.6798 - false_negatives: 996.7106 - false_positives: 1799.2227 - loss: 0.6289 - precision: 0.6577 - recall: 0.7600 - true_negatives: 2871.8801 - true_positives: 3692.1865\n",
      "For batch number 585 the model has a loss of 0.5797204971313477\n",
      "\u001b[1m585/689\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m5s\u001b[0m 56ms/step - auc: 0.7283 - binary_accuracy: 0.6799 - false_negatives: 997.5795 - false_positives: 1801.8445 - loss: 0.6288 - precision: 0.6578 - recall: 0.7601 - true_negatives: 2877.2905 - true_positives: 3699.2854\n",
      "For batch number 586 the model has a loss of 0.5796133875846863\n",
      "\u001b[1m586/689\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m5s\u001b[0m 56ms/step - auc: 0.7285 - binary_accuracy: 0.6800 - false_negatives: 998.4471 - false_positives: 1804.4607 - loss: 0.6287 - precision: 0.6578 - recall: 0.7603 - true_negatives: 2882.6980 - true_positives: 3706.3943\n",
      "For batch number 587 the model has a loss of 0.5792230367660522\n",
      "\u001b[1m587/689\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m5s\u001b[0m 56ms/step - auc: 0.7286 - binary_accuracy: 0.6801 - false_negatives: 999.3118 - false_positives: 1807.0732 - loss: 0.6286 - precision: 0.6579 - recall: 0.7604 - true_negatives: 2888.1074 - true_positives: 3713.5076\n",
      "For batch number 588 the model has a loss of 0.5792081356048584\n",
      "\u001b[1m588/689\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m5s\u001b[0m 56ms/step - auc: 0.7287 - binary_accuracy: 0.6802 - false_negatives: 1000.1752 - false_positives: 1809.6871 - loss: 0.6285 - precision: 0.6580 - recall: 0.7605 - true_negatives: 2893.5154 - true_positives: 3720.6226\n",
      "For batch number 589 the model has a loss of 0.5794227719306946\n",
      "\u001b[1m589/689\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m5s\u001b[0m 56ms/step - auc: 0.7288 - binary_accuracy: 0.6803 - false_negatives: 1001.0374 - false_positives: 1812.2954 - loss: 0.6285 - precision: 0.6581 - recall: 0.7607 - true_negatives: 2898.9253 - true_positives: 3727.7419\n",
      "For batch number 590 the model has a loss of 0.5798193216323853\n",
      "\u001b[1m590/689\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m5s\u001b[0m 56ms/step - auc: 0.7289 - binary_accuracy: 0.6804 - false_negatives: 1001.9000 - false_positives: 1814.9017 - loss: 0.6284 - precision: 0.6581 - recall: 0.7608 - true_negatives: 2904.3340 - true_positives: 3734.8645\n",
      "For batch number 591 the model has a loss of 0.5796749591827393\n",
      "\u001b[1m591/689\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m5s\u001b[0m 56ms/step - auc: 0.7290 - binary_accuracy: 0.6805 - false_negatives: 1002.7614 - false_positives: 1817.5043 - loss: 0.6283 - precision: 0.6582 - recall: 0.7609 - true_negatives: 2909.7444 - true_positives: 3741.9897\n",
      "For batch number 592 the model has a loss of 0.5796005129814148\n",
      "\u001b[1m592/689\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m5s\u001b[0m 56ms/step - auc: 0.7291 - binary_accuracy: 0.6806 - false_negatives: 1003.6233 - false_positives: 1820.1064 - loss: 0.6282 - precision: 0.6583 - recall: 0.7611 - true_negatives: 2915.1570 - true_positives: 3749.1133\n",
      "For batch number 593 the model has a loss of 0.5796631574630737\n",
      "\u001b[1m593/689\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m5s\u001b[0m 56ms/step - auc: 0.7293 - binary_accuracy: 0.6808 - false_negatives: 1004.4840 - false_positives: 1822.7065 - loss: 0.6281 - precision: 0.6584 - recall: 0.7612 - true_negatives: 2920.5732 - true_positives: 3756.2361\n",
      "For batch number 594 the model has a loss of 0.5795243382453918\n",
      "\u001b[1m594/689\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m5s\u001b[0m 56ms/step - auc: 0.7294 - binary_accuracy: 0.6809 - false_negatives: 1005.3434 - false_positives: 1825.3064 - loss: 0.6281 - precision: 0.6585 - recall: 0.7613 - true_negatives: 2925.9883 - true_positives: 3763.3621\n",
      "For batch number 595 the model has a loss of 0.579457700252533\n",
      "\u001b[1m595/689\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m5s\u001b[0m 56ms/step - auc: 0.7295 - binary_accuracy: 0.6810 - false_negatives: 1006.2017 - false_positives: 1827.9109 - loss: 0.6280 - precision: 0.6585 - recall: 0.7615 - true_negatives: 2931.4050 - true_positives: 3770.4824\n",
      "For batch number 596 the model has a loss of 0.5792997479438782\n",
      "\u001b[1m596/689\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m5s\u001b[0m 56ms/step - auc: 0.7296 - binary_accuracy: 0.6811 - false_negatives: 1007.0571 - false_positives: 1830.5201 - loss: 0.6279 - precision: 0.6586 - recall: 0.7616 - true_negatives: 2936.8223 - true_positives: 3777.6006\n",
      "For batch number 597 the model has a loss of 0.5792255997657776\n",
      "\u001b[1m597/689\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m5s\u001b[0m 56ms/step - auc: 0.7297 - binary_accuracy: 0.6812 - false_negatives: 1007.9129 - false_positives: 1833.1290 - loss: 0.6278 - precision: 0.6587 - recall: 0.7617 - true_negatives: 2942.2395 - true_positives: 3784.7185\n",
      "For batch number 598 the model has a loss of 0.5792431831359863\n",
      "\u001b[1m598/689\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m5s\u001b[0m 56ms/step - auc: 0.7298 - binary_accuracy: 0.6813 - false_negatives: 1008.7676 - false_positives: 1835.7374 - loss: 0.6277 - precision: 0.6588 - recall: 0.7619 - true_negatives: 2947.6572 - true_positives: 3791.8379\n",
      "For batch number 599 the model has a loss of 0.5790467262268066\n",
      "\u001b[1m599/689\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m5s\u001b[0m 56ms/step - auc: 0.7299 - binary_accuracy: 0.6814 - false_negatives: 1009.6194 - false_positives: 1838.3489 - loss: 0.6276 - precision: 0.6588 - recall: 0.7620 - true_negatives: 2953.0752 - true_positives: 3798.9565\n",
      "For batch number 600 the model has a loss of 0.5789163112640381\n",
      "\u001b[1m600/689\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m4s\u001b[0m 56ms/step - auc: 0.7301 - binary_accuracy: 0.6815 - false_negatives: 1010.4700 - false_positives: 1840.9634 - loss: 0.6276 - precision: 0.6589 - recall: 0.7621 - true_negatives: 2958.4951 - true_positives: 3806.0718\n",
      "For batch number 601 the model has a loss of 0.5784607529640198\n",
      "\u001b[1m601/689\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m4s\u001b[0m 56ms/step - auc: 0.7302 - binary_accuracy: 0.6816 - false_negatives: 1011.3178 - false_positives: 1843.5724 - loss: 0.6275 - precision: 0.6590 - recall: 0.7623 - true_negatives: 2963.9167 - true_positives: 3813.1931\n",
      "For batch number 602 the model has a loss of 0.5781610608100891\n",
      "\u001b[1m602/689\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m4s\u001b[0m 56ms/step - auc: 0.7303 - binary_accuracy: 0.6817 - false_negatives: 1012.1628 - false_positives: 1846.1794 - loss: 0.6274 - precision: 0.6590 - recall: 0.7624 - true_negatives: 2969.3423 - true_positives: 3820.3157\n",
      "For batch number 603 the model has a loss of 0.5781021118164062\n",
      "\u001b[1m603/689\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m4s\u001b[0m 56ms/step - auc: 0.7304 - binary_accuracy: 0.6818 - false_negatives: 1013.0050 - false_positives: 1848.7860 - loss: 0.6273 - precision: 0.6591 - recall: 0.7625 - true_negatives: 2974.7695 - true_positives: 3827.4395\n",
      "For batch number 604 the model has a loss of 0.5777822732925415\n",
      "\u001b[1m604/689\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m4s\u001b[0m 56ms/step - auc: 0.7305 - binary_accuracy: 0.6819 - false_negatives: 1013.8460 - false_positives: 1851.3875 - loss: 0.6272 - precision: 0.6592 - recall: 0.7627 - true_negatives: 2980.1921 - true_positives: 3834.5745\n",
      "For batch number 605 the model has a loss of 0.5777697563171387\n",
      "\u001b[1m605/689\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m4s\u001b[0m 56ms/step - auc: 0.7306 - binary_accuracy: 0.6820 - false_negatives: 1014.6860 - false_positives: 1853.9835 - loss: 0.6272 - precision: 0.6593 - recall: 0.7628 - true_negatives: 2985.6133 - true_positives: 3841.7173\n",
      "For batch number 606 the model has a loss of 0.5774247050285339\n",
      "\u001b[1m606/689\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m4s\u001b[0m 56ms/step - auc: 0.7307 - binary_accuracy: 0.6821 - false_negatives: 1015.5231 - false_positives: 1856.5792 - loss: 0.6271 - precision: 0.6593 - recall: 0.7629 - true_negatives: 2991.0378 - true_positives: 3848.8596\n",
      "For batch number 607 the model has a loss of 0.577545702457428\n",
      "\u001b[1m607/689\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m4s\u001b[0m 56ms/step - auc: 0.7308 - binary_accuracy: 0.6822 - false_negatives: 1016.3591 - false_positives: 1859.1714 - loss: 0.6270 - precision: 0.6594 - recall: 0.7631 - true_negatives: 2996.4597 - true_positives: 3856.0098\n",
      "For batch number 608 the model has a loss of 0.5778906345367432\n",
      "\u001b[1m608/689\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m4s\u001b[0m 56ms/step - auc: 0.7310 - binary_accuracy: 0.6823 - false_negatives: 1017.1974 - false_positives: 1861.7648 - loss: 0.6269 - precision: 0.6595 - recall: 0.7632 - true_negatives: 3001.8816 - true_positives: 3863.1562\n",
      "For batch number 609 the model has a loss of 0.5777527093887329\n",
      "\u001b[1m609/689\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m4s\u001b[0m 56ms/step - auc: 0.7311 - binary_accuracy: 0.6824 - false_negatives: 1018.0328 - false_positives: 1864.3563 - loss: 0.6268 - precision: 0.6596 - recall: 0.7633 - true_negatives: 3007.3005 - true_positives: 3870.3103\n",
      "For batch number 610 the model has a loss of 0.5775149464607239\n",
      "\u001b[1m610/689\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m4s\u001b[0m 56ms/step - auc: 0.7312 - binary_accuracy: 0.6825 - false_negatives: 1018.8656 - false_positives: 1866.9475 - loss: 0.6268 - precision: 0.6596 - recall: 0.7635 - true_negatives: 3012.7212 - true_positives: 3877.4656\n",
      "For batch number 611 the model has a loss of 0.5774215459823608\n",
      "\u001b[1m611/689\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m4s\u001b[0m 56ms/step - auc: 0.7313 - binary_accuracy: 0.6826 - false_negatives: 1019.6972 - false_positives: 1869.5400 - loss: 0.6267 - precision: 0.6597 - recall: 0.7636 - true_negatives: 3018.1440 - true_positives: 3884.6187\n",
      "For batch number 612 the model has a loss of 0.577316164970398\n",
      "\u001b[1m612/689\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m4s\u001b[0m 56ms/step - auc: 0.7314 - binary_accuracy: 0.6827 - false_negatives: 1020.5278 - false_positives: 1872.1340 - loss: 0.6266 - precision: 0.6598 - recall: 0.7637 - true_negatives: 3023.5654 - true_positives: 3891.7729\n",
      "For batch number 613 the model has a loss of 0.5773216485977173\n",
      "\u001b[1m613/689\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m4s\u001b[0m 56ms/step - auc: 0.7315 - binary_accuracy: 0.6828 - false_negatives: 1021.3572 - false_positives: 1874.7308 - loss: 0.6265 - precision: 0.6599 - recall: 0.7639 - true_negatives: 3028.9871 - true_positives: 3898.9250\n",
      "For batch number 614 the model has a loss of 0.5773590207099915\n",
      "\u001b[1m614/689\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m4s\u001b[0m 56ms/step - auc: 0.7316 - binary_accuracy: 0.6829 - false_negatives: 1022.1857 - false_positives: 1877.3257 - loss: 0.6264 - precision: 0.6599 - recall: 0.7640 - true_negatives: 3034.4072 - true_positives: 3906.0815\n",
      "For batch number 615 the model has a loss of 0.5772753357887268\n",
      "\u001b[1m615/689\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m4s\u001b[0m 56ms/step - auc: 0.7317 - binary_accuracy: 0.6830 - false_negatives: 1023.0146 - false_positives: 1879.9187 - loss: 0.6263 - precision: 0.6600 - recall: 0.7641 - true_negatives: 3039.8308 - true_positives: 3913.2358\n",
      "For batch number 616 the model has a loss of 0.5770950317382812\n",
      "\u001b[1m616/689\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m4s\u001b[0m 56ms/step - auc: 0.7318 - binary_accuracy: 0.6831 - false_negatives: 1023.8425 - false_positives: 1882.5033 - loss: 0.6263 - precision: 0.6601 - recall: 0.7642 - true_negatives: 3045.2517 - true_positives: 3920.4026\n",
      "For batch number 617 the model has a loss of 0.5770519375801086\n",
      "\u001b[1m617/689\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m4s\u001b[0m 56ms/step - auc: 0.7320 - binary_accuracy: 0.6832 - false_negatives: 1024.6742 - false_positives: 1885.0842 - loss: 0.6262 - precision: 0.6602 - recall: 0.7644 - true_negatives: 3050.6743 - true_positives: 3927.5674\n",
      "For batch number 618 the model has a loss of 0.5770098567008972\n",
      "\u001b[1m618/689\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m3s\u001b[0m 56ms/step - auc: 0.7321 - binary_accuracy: 0.6833 - false_negatives: 1025.5065 - false_positives: 1887.6635 - loss: 0.6261 - precision: 0.6602 - recall: 0.7645 - true_negatives: 3056.0891 - true_positives: 3934.7412\n",
      "For batch number 619 the model has a loss of 0.5771758556365967\n",
      "\u001b[1m619/689\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m3s\u001b[0m 56ms/step - auc: 0.7322 - binary_accuracy: 0.6834 - false_negatives: 1026.3376 - false_positives: 1890.2439 - loss: 0.6260 - precision: 0.6603 - recall: 0.7646 - true_negatives: 3061.5024 - true_positives: 3941.9160\n",
      "For batch number 620 the model has a loss of 0.5770706534385681\n",
      "\u001b[1m620/689\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m3s\u001b[0m 56ms/step - auc: 0.7323 - binary_accuracy: 0.6835 - false_negatives: 1027.1677 - false_positives: 1892.8290 - loss: 0.6260 - precision: 0.6604 - recall: 0.7648 - true_negatives: 3066.9160 - true_positives: 3949.0872\n",
      "For batch number 621 the model has a loss of 0.576727032661438\n",
      "\u001b[1m621/689\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m3s\u001b[0m 56ms/step - auc: 0.7324 - binary_accuracy: 0.6836 - false_negatives: 1027.9968 - false_positives: 1895.4106 - loss: 0.6259 - precision: 0.6605 - recall: 0.7649 - true_negatives: 3072.3318 - true_positives: 3956.2610\n",
      "For batch number 622 the model has a loss of 0.5765460133552551\n",
      "\u001b[1m622/689\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m3s\u001b[0m 56ms/step - auc: 0.7325 - binary_accuracy: 0.6837 - false_negatives: 1028.8231 - false_positives: 1897.9935 - loss: 0.6258 - precision: 0.6605 - recall: 0.7650 - true_negatives: 3077.7476 - true_positives: 3963.4358\n",
      "For batch number 623 the model has a loss of 0.5764058232307434\n",
      "\u001b[1m623/689\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m3s\u001b[0m 56ms/step - auc: 0.7326 - binary_accuracy: 0.6838 - false_negatives: 1029.6500 - false_positives: 1900.5763 - loss: 0.6257 - precision: 0.6606 - recall: 0.7652 - true_negatives: 3083.1621 - true_positives: 3970.6116\n",
      "For batch number 624 the model has a loss of 0.5761375427246094\n",
      "\u001b[1m624/689\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m3s\u001b[0m 56ms/step - auc: 0.7327 - binary_accuracy: 0.6839 - false_negatives: 1030.4744 - false_positives: 1903.1587 - loss: 0.6256 - precision: 0.6607 - recall: 0.7653 - true_negatives: 3088.5754 - true_positives: 3977.7917\n",
      "For batch number 625 the model has a loss of 0.5762014985084534\n",
      "\u001b[1m625/689\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m3s\u001b[0m 56ms/step - auc: 0.7328 - binary_accuracy: 0.6840 - false_negatives: 1031.2976 - false_positives: 1905.7393 - loss: 0.6256 - precision: 0.6608 - recall: 0.7654 - true_negatives: 3093.9873 - true_positives: 3984.9761\n",
      "For batch number 626 the model has a loss of 0.5761781334877014\n",
      "\u001b[1m626/689\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m3s\u001b[0m 56ms/step - auc: 0.7329 - binary_accuracy: 0.6841 - false_negatives: 1032.1198 - false_positives: 1908.3195 - loss: 0.6255 - precision: 0.6608 - recall: 0.7655 - true_negatives: 3099.4009 - true_positives: 3992.1597\n",
      "For batch number 627 the model has a loss of 0.5760846138000488\n",
      "\u001b[1m627/689\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m3s\u001b[0m 56ms/step - auc: 0.7330 - binary_accuracy: 0.6842 - false_negatives: 1032.9410 - false_positives: 1910.8964 - loss: 0.6254 - precision: 0.6609 - recall: 0.7657 - true_negatives: 3104.8149 - true_positives: 3999.3477\n",
      "For batch number 628 the model has a loss of 0.5758471488952637\n",
      "\u001b[1m628/689\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m3s\u001b[0m 56ms/step - auc: 0.7331 - binary_accuracy: 0.6843 - false_negatives: 1033.7611 - false_positives: 1913.4681 - loss: 0.6253 - precision: 0.6610 - recall: 0.7658 - true_negatives: 3110.2261 - true_positives: 4006.5447\n",
      "For batch number 629 the model has a loss of 0.5757043361663818\n",
      "\u001b[1m629/689\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m3s\u001b[0m 56ms/step - auc: 0.7333 - binary_accuracy: 0.6844 - false_negatives: 1034.5819 - false_positives: 1916.0366 - loss: 0.6252 - precision: 0.6610 - recall: 0.7659 - true_negatives: 3115.6328 - true_positives: 4013.7488\n",
      "For batch number 630 the model has a loss of 0.5755153298377991\n",
      "\u001b[1m630/689\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m3s\u001b[0m 56ms/step - auc: 0.7334 - binary_accuracy: 0.6845 - false_negatives: 1035.4016 - false_positives: 1918.6079 - loss: 0.6252 - precision: 0.6611 - recall: 0.7661 - true_negatives: 3121.0381 - true_positives: 4020.9524\n",
      "For batch number 631 the model has a loss of 0.5754368305206299\n",
      "\u001b[1m631/689\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m3s\u001b[0m 56ms/step - auc: 0.7335 - binary_accuracy: 0.6846 - false_negatives: 1036.2203 - false_positives: 1921.1791 - loss: 0.6251 - precision: 0.6612 - recall: 0.7662 - true_negatives: 3126.4407 - true_positives: 4028.1602\n",
      "For batch number 632 the model has a loss of 0.5753734707832336\n",
      "\u001b[1m632/689\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m3s\u001b[0m 56ms/step - auc: 0.7336 - binary_accuracy: 0.6847 - false_negatives: 1037.0364 - false_positives: 1923.7548 - loss: 0.6250 - precision: 0.6613 - recall: 0.7663 - true_negatives: 3131.8401 - true_positives: 4035.3687\n",
      "For batch number 633 the model has a loss of 0.5753065943717957\n",
      "\u001b[1m633/689\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m3s\u001b[0m 56ms/step - auc: 0.7337 - binary_accuracy: 0.6848 - false_negatives: 1037.8530 - false_positives: 1926.3270 - loss: 0.6249 - precision: 0.6613 - recall: 0.7664 - true_negatives: 3137.2371 - true_positives: 4042.5830\n",
      "For batch number 634 the model has a loss of 0.5750296711921692\n",
      "\u001b[1m634/689\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m3s\u001b[0m 56ms/step - auc: 0.7338 - binary_accuracy: 0.6849 - false_negatives: 1038.6688 - false_positives: 1928.8943 - loss: 0.6248 - precision: 0.6614 - recall: 0.7666 - true_negatives: 3142.6309 - true_positives: 4049.8059\n",
      "For batch number 635 the model has a loss of 0.5749441385269165\n",
      "\u001b[1m635/689\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m3s\u001b[0m 56ms/step - auc: 0.7339 - binary_accuracy: 0.6850 - false_negatives: 1039.4835 - false_positives: 1931.4614 - loss: 0.6248 - precision: 0.6615 - recall: 0.7667 - true_negatives: 3148.0220 - true_positives: 4057.0330\n",
      "For batch number 636 the model has a loss of 0.5748263001441956\n",
      "\u001b[1m636/689\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m2s\u001b[0m 56ms/step - auc: 0.7340 - binary_accuracy: 0.6851 - false_negatives: 1040.2957 - false_positives: 1934.0299 - loss: 0.6247 - precision: 0.6616 - recall: 0.7668 - true_negatives: 3153.4182 - true_positives: 4064.2563\n",
      "For batch number 637 the model has a loss of 0.574522852897644\n",
      "\u001b[1m637/689\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m2s\u001b[0m 56ms/step - auc: 0.7341 - binary_accuracy: 0.6852 - false_negatives: 1041.1052 - false_positives: 1936.5966 - loss: 0.6246 - precision: 0.6616 - recall: 0.7669 - true_negatives: 3158.8147 - true_positives: 4071.4834\n",
      "For batch number 638 the model has a loss of 0.574489414691925\n",
      "\u001b[1m638/689\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m2s\u001b[0m 56ms/step - auc: 0.7342 - binary_accuracy: 0.6853 - false_negatives: 1041.9138 - false_positives: 1939.1599 - loss: 0.6245 - precision: 0.6617 - recall: 0.7671 - true_negatives: 3164.2068 - true_positives: 4078.7195\n",
      "For batch number 639 the model has a loss of 0.574489176273346\n",
      "\u001b[1m639/689\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m2s\u001b[0m 56ms/step - auc: 0.7343 - binary_accuracy: 0.6854 - false_negatives: 1042.7214 - false_positives: 1941.7324 - loss: 0.6245 - precision: 0.6618 - recall: 0.7672 - true_negatives: 3169.5962 - true_positives: 4085.9500\n",
      "For batch number 640 the model has a loss of 0.5743304491043091\n",
      "\u001b[1m640/689\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m2s\u001b[0m 56ms/step - auc: 0.7344 - binary_accuracy: 0.6855 - false_negatives: 1043.5266 - false_positives: 1944.3079 - loss: 0.6244 - precision: 0.6619 - recall: 0.7673 - true_negatives: 3174.9829 - true_positives: 4093.1829\n",
      "For batch number 641 the model has a loss of 0.574169397354126\n",
      "\u001b[1m641/689\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m2s\u001b[0m 56ms/step - auc: 0.7345 - binary_accuracy: 0.6856 - false_negatives: 1044.3323 - false_positives: 1946.8845 - loss: 0.6243 - precision: 0.6619 - recall: 0.7674 - true_negatives: 3180.3713 - true_positives: 4100.4116\n",
      "For batch number 642 the model has a loss of 0.5741399526596069\n",
      "\u001b[1m642/689\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m2s\u001b[0m 56ms/step - auc: 0.7346 - binary_accuracy: 0.6857 - false_negatives: 1045.1355 - false_positives: 1949.4689 - loss: 0.6242 - precision: 0.6620 - recall: 0.7676 - true_negatives: 3185.7571 - true_positives: 4107.6387\n",
      "For batch number 643 the model has a loss of 0.5738638639450073\n",
      "\u001b[1m643/689\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m2s\u001b[0m 56ms/step - auc: 0.7348 - binary_accuracy: 0.6858 - false_negatives: 1045.9393 - false_positives: 1952.0482 - loss: 0.6241 - precision: 0.6621 - recall: 0.7677 - true_negatives: 3191.1445 - true_positives: 4114.8677\n",
      "For batch number 644 the model has a loss of 0.5736413598060608\n",
      "\u001b[1m644/689\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m2s\u001b[0m 56ms/step - auc: 0.7349 - binary_accuracy: 0.6859 - false_negatives: 1046.7422 - false_positives: 1954.6273 - loss: 0.6241 - precision: 0.6621 - recall: 0.7678 - true_negatives: 3196.5327 - true_positives: 4122.0977\n",
      "For batch number 645 the model has a loss of 0.5734754204750061\n",
      "\u001b[1m645/689\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m2s\u001b[0m 56ms/step - auc: 0.7350 - binary_accuracy: 0.6860 - false_negatives: 1047.5442 - false_positives: 1957.2031 - loss: 0.6240 - precision: 0.6622 - recall: 0.7680 - true_negatives: 3201.9177 - true_positives: 4129.3350\n",
      "For batch number 646 the model has a loss of 0.5732082724571228\n",
      "\u001b[1m646/689\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m2s\u001b[0m 56ms/step - auc: 0.7351 - binary_accuracy: 0.6861 - false_negatives: 1048.3452 - false_positives: 1959.7802 - loss: 0.6239 - precision: 0.6623 - recall: 0.7681 - true_negatives: 3207.3003 - true_positives: 4136.5742\n",
      "For batch number 647 the model has a loss of 0.5730075836181641\n",
      "\u001b[1m647/689\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m2s\u001b[0m 56ms/step - auc: 0.7352 - binary_accuracy: 0.6862 - false_negatives: 1049.1438 - false_positives: 1962.3617 - loss: 0.6238 - precision: 0.6624 - recall: 0.7682 - true_negatives: 3212.6848 - true_positives: 4143.8101\n",
      "For batch number 648 the model has a loss of 0.5728907585144043\n",
      "\u001b[1m648/689\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m2s\u001b[0m 56ms/step - auc: 0.7353 - binary_accuracy: 0.6863 - false_negatives: 1049.9398 - false_positives: 1964.9491 - loss: 0.6237 - precision: 0.6624 - recall: 0.7683 - true_negatives: 3218.0693 - true_positives: 4151.0415\n",
      "For batch number 649 the model has a loss of 0.5725244283676147\n",
      "\u001b[1m649/689\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m2s\u001b[0m 56ms/step - auc: 0.7354 - binary_accuracy: 0.6864 - false_negatives: 1050.7334 - false_positives: 1967.5363 - loss: 0.6237 - precision: 0.6625 - recall: 0.7685 - true_negatives: 3223.4575 - true_positives: 4158.2729\n",
      "For batch number 650 the model has a loss of 0.5724157094955444\n",
      "\u001b[1m650/689\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m2s\u001b[0m 56ms/step - auc: 0.7355 - binary_accuracy: 0.6865 - false_negatives: 1051.5247 - false_positives: 1970.1184 - loss: 0.6236 - precision: 0.6626 - recall: 0.7686 - true_negatives: 3228.8416 - true_positives: 4165.5156\n",
      "For batch number 651 the model has a loss of 0.5724185109138489\n",
      "\u001b[1m651/689\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m2s\u001b[0m 56ms/step - auc: 0.7356 - binary_accuracy: 0.6866 - false_negatives: 1052.3134 - false_positives: 1972.7097 - loss: 0.6235 - precision: 0.6626 - recall: 0.7687 - true_negatives: 3234.2244 - true_positives: 4172.7529\n",
      "For batch number 652 the model has a loss of 0.5720739960670471\n",
      "\u001b[1m652/689\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m2s\u001b[0m 56ms/step - auc: 0.7357 - binary_accuracy: 0.6867 - false_negatives: 1053.0997 - false_positives: 1975.2960 - loss: 0.6234 - precision: 0.6627 - recall: 0.7688 - true_negatives: 3239.6074 - true_positives: 4179.9971\n",
      "For batch number 653 the model has a loss of 0.5720483064651489\n",
      "\u001b[1m653/689\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m2s\u001b[0m 56ms/step - auc: 0.7358 - binary_accuracy: 0.6868 - false_negatives: 1053.8837 - false_positives: 1977.8837 - loss: 0.6234 - precision: 0.6628 - recall: 0.7690 - true_negatives: 3244.9954 - true_positives: 4187.2373\n",
      "For batch number 654 the model has a loss of 0.5717647671699524\n",
      "\u001b[1m654/689\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m1s\u001b[0m 56ms/step - auc: 0.7359 - binary_accuracy: 0.6869 - false_negatives: 1054.6652 - false_positives: 1980.4694 - loss: 0.6233 - precision: 0.6628 - recall: 0.7691 - true_negatives: 3250.3853 - true_positives: 4194.4800\n",
      "For batch number 655 the model has a loss of 0.5714671015739441\n",
      "\u001b[1m655/689\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m1s\u001b[0m 56ms/step - auc: 0.7360 - binary_accuracy: 0.6870 - false_negatives: 1055.4442 - false_positives: 1983.0549 - loss: 0.6232 - precision: 0.6629 - recall: 0.7692 - true_negatives: 3255.7803 - true_positives: 4201.7207\n",
      "For batch number 656 the model has a loss of 0.5711885094642639\n",
      "\u001b[1m656/689\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m1s\u001b[0m 56ms/step - auc: 0.7361 - binary_accuracy: 0.6871 - false_negatives: 1056.2211 - false_positives: 1985.6372 - loss: 0.6231 - precision: 0.6630 - recall: 0.7693 - true_negatives: 3261.1753 - true_positives: 4208.9663\n",
      "For batch number 657 the model has a loss of 0.5709465146064758\n",
      "\u001b[1m657/689\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m1s\u001b[0m 56ms/step - auc: 0.7362 - binary_accuracy: 0.6872 - false_negatives: 1056.9955 - false_positives: 1988.2162 - loss: 0.6230 - precision: 0.6631 - recall: 0.7695 - true_negatives: 3266.5737 - true_positives: 4216.2148\n",
      "For batch number 658 the model has a loss of 0.5708287954330444\n",
      "\u001b[1m658/689\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m1s\u001b[0m 56ms/step - auc: 0.7363 - binary_accuracy: 0.6873 - false_negatives: 1057.7690 - false_positives: 1990.7964 - loss: 0.6230 - precision: 0.6631 - recall: 0.7696 - true_negatives: 3271.9741 - true_positives: 4223.4604\n",
      "For batch number 659 the model has a loss of 0.5704658627510071\n",
      "\u001b[1m659/689\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m1s\u001b[0m 56ms/step - auc: 0.7364 - binary_accuracy: 0.6874 - false_negatives: 1058.5402 - false_positives: 1993.3748 - loss: 0.6229 - precision: 0.6632 - recall: 0.7697 - true_negatives: 3277.3809 - true_positives: 4230.7041\n",
      "For batch number 660 the model has a loss of 0.5701810121536255\n",
      "\u001b[1m660/689\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m1s\u001b[0m 56ms/step - auc: 0.7365 - binary_accuracy: 0.6875 - false_negatives: 1059.3091 - false_positives: 1995.9561 - loss: 0.6228 - precision: 0.6633 - recall: 0.7698 - true_negatives: 3282.7878 - true_positives: 4237.9468\n",
      "For batch number 661 the model has a loss of 0.5698297619819641\n",
      "\u001b[1m661/689\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m1s\u001b[0m 56ms/step - auc: 0.7366 - binary_accuracy: 0.6876 - false_negatives: 1060.0757 - false_positives: 1998.5386 - loss: 0.6227 - precision: 0.6633 - recall: 0.7700 - true_negatives: 3288.1997 - true_positives: 4245.1860\n",
      "For batch number 662 the model has a loss of 0.569675862789154\n",
      "\u001b[1m662/689\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m1s\u001b[0m 56ms/step - auc: 0.7367 - binary_accuracy: 0.6877 - false_negatives: 1060.8398 - false_positives: 2001.1194 - loss: 0.6226 - precision: 0.6634 - recall: 0.7701 - true_negatives: 3293.6147 - true_positives: 4252.4258\n",
      "For batch number 663 the model has a loss of 0.5693540573120117\n",
      "\u001b[1m663/689\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m1s\u001b[0m 56ms/step - auc: 0.7369 - binary_accuracy: 0.6878 - false_negatives: 1061.6018 - false_positives: 2003.6984 - loss: 0.6226 - precision: 0.6635 - recall: 0.7702 - true_negatives: 3299.0347 - true_positives: 4259.6650\n",
      "For batch number 664 the model has a loss of 0.5687961578369141\n",
      "\u001b[1m664/689\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m1s\u001b[0m 56ms/step - auc: 0.7370 - binary_accuracy: 0.6879 - false_negatives: 1062.3615 - false_positives: 2006.2711 - loss: 0.6225 - precision: 0.6635 - recall: 0.7703 - true_negatives: 3304.4592 - true_positives: 4266.9082\n",
      "For batch number 665 the model has a loss of 0.5687403678894043\n",
      "\u001b[1m665/689\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m1s\u001b[0m 56ms/step - auc: 0.7371 - binary_accuracy: 0.6880 - false_negatives: 1063.1204 - false_positives: 2008.8406 - loss: 0.6224 - precision: 0.6636 - recall: 0.7705 - true_negatives: 3309.8811 - true_positives: 4274.1577\n",
      "For batch number 666 the model has a loss of 0.5684153437614441\n",
      "\u001b[1m666/689\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m1s\u001b[0m 56ms/step - auc: 0.7372 - binary_accuracy: 0.6881 - false_negatives: 1063.8768 - false_positives: 2011.4084 - loss: 0.6223 - precision: 0.6637 - recall: 0.7706 - true_negatives: 3315.3093 - true_positives: 4281.4053\n",
      "For batch number 667 the model has a loss of 0.5687431693077087\n",
      "\u001b[1m667/689\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m1s\u001b[0m 56ms/step - auc: 0.7373 - binary_accuracy: 0.6882 - false_negatives: 1064.6342 - false_positives: 2013.9775 - loss: 0.6222 - precision: 0.6638 - recall: 0.7707 - true_negatives: 3320.7390 - true_positives: 4288.6494\n",
      "For batch number 668 the model has a loss of 0.5684568285942078\n",
      "\u001b[1m668/689\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m1s\u001b[0m 56ms/step - auc: 0.7374 - binary_accuracy: 0.6883 - false_negatives: 1065.3893 - false_positives: 2016.5404 - loss: 0.6222 - precision: 0.6638 - recall: 0.7708 - true_negatives: 3326.1736 - true_positives: 4295.8965\n",
      "For batch number 669 the model has a loss of 0.5682228803634644\n",
      "\u001b[1m669/689\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m1s\u001b[0m 56ms/step - auc: 0.7375 - binary_accuracy: 0.6883 - false_negatives: 1066.1436 - false_positives: 2019.0986 - loss: 0.6221 - precision: 0.6639 - recall: 0.7709 - true_negatives: 3331.6128 - true_positives: 4303.1450\n",
      "For batch number 670 the model has a loss of 0.5678989887237549\n",
      "\u001b[1m670/689\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m1s\u001b[0m 56ms/step - auc: 0.7376 - binary_accuracy: 0.6884 - false_negatives: 1066.8955 - false_positives: 2021.6537 - loss: 0.6220 - precision: 0.6640 - recall: 0.7711 - true_negatives: 3337.0613 - true_positives: 4310.3896\n",
      "For batch number 671 the model has a loss of 0.5677763819694519\n",
      "\u001b[1m671/689\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m1s\u001b[0m 56ms/step - auc: 0.7377 - binary_accuracy: 0.6885 - false_negatives: 1067.6469 - false_positives: 2024.2057 - loss: 0.6219 - precision: 0.6640 - recall: 0.7712 - true_negatives: 3342.5098 - true_positives: 4317.6377\n",
      "For batch number 672 the model has a loss of 0.5675262808799744\n",
      "\u001b[1m672/689\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 56ms/step - auc: 0.7378 - binary_accuracy: 0.6886 - false_negatives: 1068.3959 - false_positives: 2026.7560 - loss: 0.6218 - precision: 0.6641 - recall: 0.7713 - true_negatives: 3347.9644 - true_positives: 4324.8838\n",
      "For batch number 673 the model has a loss of 0.5674752593040466\n",
      "\u001b[1m673/689\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 56ms/step - auc: 0.7379 - binary_accuracy: 0.6887 - false_negatives: 1069.1456 - false_positives: 2029.3046 - loss: 0.6217 - precision: 0.6642 - recall: 0.7714 - true_negatives: 3353.4204 - true_positives: 4332.1294\n",
      "For batch number 674 the model has a loss of 0.5671884417533875\n",
      "\u001b[1m674/689\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 56ms/step - auc: 0.7380 - binary_accuracy: 0.6888 - false_negatives: 1069.8947 - false_positives: 2031.8486 - loss: 0.6217 - precision: 0.6642 - recall: 0.7716 - true_negatives: 3358.8843 - true_positives: 4339.3726\n",
      "For batch number 675 the model has a loss of 0.5667622685432434\n",
      "\u001b[1m675/689\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 56ms/step - auc: 0.7381 - binary_accuracy: 0.6889 - false_negatives: 1070.6429 - false_positives: 2034.3867 - loss: 0.6216 - precision: 0.6643 - recall: 0.7717 - true_negatives: 3364.3525 - true_positives: 4346.6177\n",
      "For batch number 676 the model has a loss of 0.5666111707687378\n",
      "\u001b[1m676/689\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 56ms/step - auc: 0.7382 - binary_accuracy: 0.6890 - false_negatives: 1071.3905 - false_positives: 2036.9231 - loss: 0.6215 - precision: 0.6644 - recall: 0.7718 - true_negatives: 3369.8210 - true_positives: 4353.8652\n",
      "For batch number 677 the model has a loss of 0.5664410591125488\n",
      "\u001b[1m677/689\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 56ms/step - auc: 0.7383 - binary_accuracy: 0.6891 - false_negatives: 1072.1418 - false_positives: 2039.4520 - loss: 0.6214 - precision: 0.6645 - recall: 0.7719 - true_negatives: 3375.2881 - true_positives: 4361.1182\n",
      "For batch number 678 the model has a loss of 0.5663269758224487\n",
      "\u001b[1m678/689\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 56ms/step - auc: 0.7384 - binary_accuracy: 0.6892 - false_negatives: 1072.8923 - false_positives: 2041.9852 - loss: 0.6213 - precision: 0.6645 - recall: 0.7721 - true_negatives: 3380.7581 - true_positives: 4368.3643\n",
      "For batch number 679 the model has a loss of 0.5660641193389893\n",
      "\u001b[1m679/689\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 56ms/step - auc: 0.7385 - binary_accuracy: 0.6893 - false_negatives: 1073.6421 - false_positives: 2044.5155 - loss: 0.6213 - precision: 0.6646 - recall: 0.7722 - true_negatives: 3386.2312 - true_positives: 4375.6113\n",
      "For batch number 680 the model has a loss of 0.5658761858940125\n",
      "\u001b[1m680/689\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 56ms/step - auc: 0.7386 - binary_accuracy: 0.6894 - false_negatives: 1074.3927 - false_positives: 2047.0441 - loss: 0.6212 - precision: 0.6647 - recall: 0.7723 - true_negatives: 3391.7073 - true_positives: 4382.8560\n",
      "For batch number 681 the model has a loss of 0.5654987692832947\n",
      "\u001b[1m681/689\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 56ms/step - auc: 0.7387 - binary_accuracy: 0.6895 - false_negatives: 1075.1410 - false_positives: 2049.5698 - loss: 0.6211 - precision: 0.6647 - recall: 0.7724 - true_negatives: 3397.1895 - true_positives: 4390.0996\n",
      "For batch number 682 the model has a loss of 0.5657114386558533\n",
      "\u001b[1m682/689\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 56ms/step - auc: 0.7388 - binary_accuracy: 0.6896 - false_negatives: 1075.8900 - false_positives: 2052.0923 - loss: 0.6210 - precision: 0.6648 - recall: 0.7725 - true_negatives: 3402.6746 - true_positives: 4397.3433\n",
      "For batch number 683 the model has a loss of 0.5653467774391174\n",
      "\u001b[1m683/689\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 56ms/step - auc: 0.7389 - binary_accuracy: 0.6897 - false_negatives: 1076.6368 - false_positives: 2054.6135 - loss: 0.6209 - precision: 0.6649 - recall: 0.7727 - true_negatives: 3408.1641 - true_positives: 4404.5854\n",
      "For batch number 684 the model has a loss of 0.5652402639389038\n",
      "\u001b[1m684/689\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 56ms/step - auc: 0.7390 - binary_accuracy: 0.6898 - false_negatives: 1077.3845 - false_positives: 2057.1345 - loss: 0.6209 - precision: 0.6649 - recall: 0.7728 - true_negatives: 3413.6492 - true_positives: 4411.8320\n",
      "For batch number 685 the model has a loss of 0.5650264024734497\n",
      "\u001b[1m685/689\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 56ms/step - auc: 0.7391 - binary_accuracy: 0.6899 - false_negatives: 1078.1299 - false_positives: 2059.6511 - loss: 0.6208 - precision: 0.6650 - recall: 0.7729 - true_negatives: 3419.1372 - true_positives: 4419.0815\n",
      "For batch number 686 the model has a loss of 0.5649157166481018\n",
      "\u001b[1m686/689\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 56ms/step - auc: 0.7393 - binary_accuracy: 0.6900 - false_negatives: 1078.8746 - false_positives: 2062.1663 - loss: 0.6207 - precision: 0.6651 - recall: 0.7730 - true_negatives: 3424.6282 - true_positives: 4426.3311\n",
      "For batch number 687 the model has a loss of 0.5645995140075684\n",
      "\u001b[1m687/689\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 56ms/step - auc: 0.7394 - binary_accuracy: 0.6901 - false_negatives: 1079.6172 - false_positives: 2064.6797 - loss: 0.6206 - precision: 0.6651 - recall: 0.7731 - true_negatives: 3430.1223 - true_positives: 4433.5806\n",
      "For batch number 688 the model has a loss of 0.5642452239990234\n",
      "\u001b[1m688/689\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 56ms/step - auc: 0.7395 - binary_accuracy: 0.6902 - false_negatives: 1080.3575 - false_positives: 2067.1934 - loss: 0.6205 - precision: 0.6652 - recall: 0.7733 - true_negatives: 3435.6206 - true_positives: 4440.8286\n",
      "For batch number 689 the model has a loss of 0.5639389753341675\n",
      "\u001b[1m689/689\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 56ms/step - auc: 0.7396 - binary_accuracy: 0.6903 - false_negatives: 1081.0973 - false_positives: 2069.7024 - loss: 0.6204 - precision: 0.6653 - recall: 0.7734 - true_negatives: 3441.1189 - true_positives: 4448.0786\n",
      "For epoch number 1 the model has a loss of 0.5639389753341675\n",
      "\u001b[1m689/689\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m43s\u001b[0m 60ms/step - auc: 0.7397 - binary_accuracy: 0.6904 - false_negatives: 1081.8348 - false_positives: 2072.2043 - loss: 0.6204 - precision: 0.6654 - recall: 0.7735 - true_negatives: 3446.6016 - true_positives: 4455.3071 - val_auc: 0.8665 - val_binary_accuracy: 0.6069 - val_false_negatives: 9.0000 - val_false_positives: 1074.0000 - val_loss: 0.7799 - val_precision: 0.5627 - val_recall: 0.9935 - val_true_negatives: 290.0000 - val_true_positives: 1382.0000\n",
      "Epoch 2/5\n",
      "\n",
      "For batch number 1 the model has a loss of 0.39581355452537537\n",
      "\u001b[1m  1/689\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m55s\u001b[0m 81ms/step - auc: 0.9563 - binary_accuracy: 0.8125 - false_negatives: 1.0000 - false_positives: 5.0000 - loss: 0.3958 - precision: 0.7222 - recall: 0.9286 - true_negatives: 13.0000 - true_positives: 13.0000\n",
      "For batch number 2 the model has a loss of 0.34738123416900635\n",
      "\u001b[1m  2/689\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m39s\u001b[0m 57ms/step - auc: 0.9679 - binary_accuracy: 0.8359 - false_negatives: 1.0000 - false_positives: 6.5000 - loss: 0.3716 - precision: 0.7530 - recall: 0.9476 - true_negatives: 19.5000 - true_positives: 21.0000\n",
      "For batch number 3 the model has a loss of 0.3766263723373413\n",
      "\u001b[1m  3/689\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m38s\u001b[0m 57ms/step - auc: 0.9646 - binary_accuracy: 0.8490 - false_negatives: 1.0000 - false_positives: 8.0000 - loss: 0.3733 - precision: 0.7687 - recall: 0.9577 - true_negatives: 26.3333 - true_positives: 28.6667\n",
      "For batch number 4 the model has a loss of 0.41677334904670715\n",
      "\u001b[1m  4/689\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m39s\u001b[0m 58ms/step - auc: 0.9593 - binary_accuracy: 0.8555 - false_negatives: 1.5000 - false_positives: 9.2500 - loss: 0.3841 - precision: 0.7843 - recall: 0.9571 - true_negatives: 31.7500 - true_positives: 37.5000\n",
      "For batch number 5 the model has a loss of 0.403261661529541\n",
      "\u001b[1m  5/689\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m39s\u001b[0m 58ms/step - auc: 0.9563 - binary_accuracy: 0.8594 - false_negatives: 1.8000 - false_positives: 10.8000 - loss: 0.3880 - precision: 0.7924 - recall: 0.9584 - true_negatives: 37.4000 - true_positives: 46.0000\n",
      "For batch number 6 the model has a loss of 0.3677479922771454\n",
      "\u001b[1m  6/689\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m39s\u001b[0m 57ms/step - auc: 0.9565 - binary_accuracy: 0.8637 - false_negatives: 2.0000 - false_positives: 12.1667 - loss: 0.3846 - precision: 0.7994 - recall: 0.9603 - true_negatives: 43.5000 - true_positives: 54.3333\n",
      "For batch number 7 the model has a loss of 0.3426132798194885\n",
      "\u001b[1m  7/689\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m38s\u001b[0m 57ms/step - auc: 0.9571 - binary_accuracy: 0.8685 - false_negatives: 2.1429 - false_positives: 13.2857 - loss: 0.3786 - precision: 0.8071 - recall: 0.9624 - true_negatives: 49.4286 - true_positives: 63.1429\n",
      "For batch number 8 the model has a loss of 0.37493348121643066\n",
      "\u001b[1m  8/689\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m40s\u001b[0m 59ms/step - auc: 0.9572 - binary_accuracy: 0.8723 - false_negatives: 2.3750 - false_positives: 14.3750 - loss: 0.3781 - precision: 0.8135 - recall: 0.9634 - true_negatives: 55.3750 - true_positives: 71.8750\n",
      "For batch number 9 the model has a loss of 0.420310378074646\n",
      "\u001b[1m  9/689\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m39s\u001b[0m 58ms/step - auc: 0.9563 - binary_accuracy: 0.8737 - false_negatives: 2.8889 - false_positives: 15.6667 - loss: 0.3828 - precision: 0.8175 - recall: 0.9624 - true_negatives: 61.2222 - true_positives: 80.2222\n",
      "For batch number 10 the model has a loss of 0.42354997992515564\n",
      "\u001b[1m 10/689\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m39s\u001b[0m 58ms/step - auc: 0.9554 - binary_accuracy: 0.8742 - false_negatives: 3.4000 - false_positives: 17.2000 - loss: 0.3869 - precision: 0.8193 - recall: 0.9614 - true_negatives: 67.4000 - true_positives: 88.0000\n",
      "For batch number 11 the model has a loss of 0.4329410493373871\n",
      "\u001b[1m 11/689\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m39s\u001b[0m 58ms/step - auc: 0.9544 - binary_accuracy: 0.8745 - false_negatives: 4.0000 - false_positives: 18.6364 - loss: 0.3911 - precision: 0.8214 - recall: 0.9600 - true_negatives: 73.3636 - true_positives: 96.0000\n",
      "For batch number 12 the model has a loss of 0.43438637256622314\n",
      "\u001b[1m 12/689\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m39s\u001b[0m 58ms/step - auc: 0.9532 - binary_accuracy: 0.8748 - false_negatives: 4.6667 - false_positives: 20.0000 - loss: 0.3947 - precision: 0.8235 - recall: 0.9585 - true_negatives: 79.2500 - true_positives: 104.0833\n",
      "For batch number 13 the model has a loss of 0.4587324857711792\n",
      "\u001b[1m 13/689\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m40s\u001b[0m 59ms/step - auc: 0.9519 - binary_accuracy: 0.8751 - false_negatives: 5.3077 - false_positives: 21.3077 - loss: 0.3996 - precision: 0.8256 - recall: 0.9572 - true_negatives: 85.1538 - true_positives: 112.2308\n",
      "For batch number 14 the model has a loss of 0.46417516469955444\n",
      "\u001b[1m 14/689\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m40s\u001b[0m 60ms/step - auc: 0.9506 - binary_accuracy: 0.8750 - false_negatives: 6.0000 - false_positives: 22.7857 - loss: 0.4042 - precision: 0.8268 - recall: 0.9558 - true_negatives: 90.8571 - true_positives: 120.3571\n",
      "For batch number 15 the model has a loss of 0.45435744524002075\n",
      "\u001b[1m 15/689\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m40s\u001b[0m 59ms/step - auc: 0.9496 - binary_accuracy: 0.8750 - false_negatives: 6.6000 - false_positives: 24.2667 - loss: 0.4076 - precision: 0.8278 - recall: 0.9548 - true_negatives: 96.8667 - true_positives: 128.2667\n",
      "For batch number 16 the model has a loss of 0.4506703019142151\n",
      "\u001b[1m 16/689\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m39s\u001b[0m 59ms/step - auc: 0.9487 - binary_accuracy: 0.8749 - false_negatives: 7.1875 - false_positives: 25.8125 - loss: 0.4103 - precision: 0.8284 - recall: 0.9539 - true_negatives: 103.0625 - true_positives: 135.9375\n",
      "For batch number 17 the model has a loss of 0.4465690851211548\n",
      "\u001b[1m 17/689\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m39s\u001b[0m 59ms/step - auc: 0.9479 - binary_accuracy: 0.8748 - false_negatives: 7.7647 - false_positives: 27.3529 - loss: 0.4124 - precision: 0.8289 - recall: 0.9531 - true_negatives: 109.1176 - true_positives: 143.7647\n",
      "For batch number 18 the model has a loss of 0.4430428445339203\n",
      "\u001b[1m 18/689\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m39s\u001b[0m 59ms/step - auc: 0.9472 - binary_accuracy: 0.8746 - false_negatives: 8.3333 - false_positives: 28.9444 - loss: 0.4141 - precision: 0.8292 - recall: 0.9523 - true_negatives: 115.3333 - true_positives: 151.3889\n",
      "For batch number 19 the model has a loss of 0.4406520426273346\n",
      "\u001b[1m 19/689\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m39s\u001b[0m 59ms/step - auc: 0.9466 - binary_accuracy: 0.8743 - false_negatives: 8.9474 - false_positives: 30.5263 - loss: 0.4155 - precision: 0.8294 - recall: 0.9515 - true_negatives: 121.5789 - true_positives: 158.9474\n",
      "For batch number 20 the model has a loss of 0.4368497431278229\n",
      "\u001b[1m 20/689\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m39s\u001b[0m 60ms/step - auc: 0.9461 - binary_accuracy: 0.8739 - false_negatives: 9.5500 - false_positives: 32.2500 - loss: 0.4166 - precision: 0.8292 - recall: 0.9507 - true_negatives: 127.8000 - true_positives: 166.4000\n",
      "For batch number 21 the model has a loss of 0.4308728277683258\n",
      "\u001b[1m 21/689\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m39s\u001b[0m 59ms/step - auc: 0.9457 - binary_accuracy: 0.8736 - false_negatives: 10.0952 - false_positives: 33.9524 - loss: 0.4173 - precision: 0.8290 - recall: 0.9502 - true_negatives: 134.1905 - true_positives: 173.7619\n",
      "For batch number 22 the model has a loss of 0.4243534207344055\n",
      "\u001b[1m 22/689\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m39s\u001b[0m 59ms/step - auc: 0.9454 - binary_accuracy: 0.8733 - false_negatives: 10.6818 - false_positives: 35.5909 - loss: 0.4176 - precision: 0.8290 - recall: 0.9495 - true_negatives: 140.5000 - true_positives: 181.2273\n",
      "For batch number 23 the model has a loss of 0.42411670088768005\n",
      "\u001b[1m 23/689\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m39s\u001b[0m 59ms/step - auc: 0.9451 - binary_accuracy: 0.8730 - false_negatives: 11.2609 - false_positives: 37.3043 - loss: 0.4179 - precision: 0.8289 - recall: 0.9490 - true_negatives: 146.6522 - true_positives: 188.7826\n",
      "For batch number 24 the model has a loss of 0.4204140603542328\n",
      "\u001b[1m 24/689\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m39s\u001b[0m 59ms/step - auc: 0.9448 - binary_accuracy: 0.8727 - false_negatives: 11.8333 - false_positives: 39.0000 - loss: 0.4180 - precision: 0.8287 - recall: 0.9485 - true_negatives: 152.8750 - true_positives: 196.2917\n",
      "For batch number 25 the model has a loss of 0.42220306396484375\n",
      "\u001b[1m 25/689\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m39s\u001b[0m 59ms/step - auc: 0.9445 - binary_accuracy: 0.8723 - false_negatives: 12.4000 - false_positives: 40.8000 - loss: 0.4181 - precision: 0.8283 - recall: 0.9480 - true_negatives: 159.1200 - true_positives: 203.6800\n",
      "For batch number 26 the model has a loss of 0.4222450256347656\n",
      "\u001b[1m 26/689\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m38s\u001b[0m 59ms/step - auc: 0.9442 - binary_accuracy: 0.8718 - false_negatives: 12.9615 - false_positives: 42.6538 - loss: 0.4183 - precision: 0.8279 - recall: 0.9475 - true_negatives: 165.3462 - true_positives: 211.0385\n",
      "For batch number 27 the model has a loss of 0.4328854978084564\n",
      "\u001b[1m 27/689\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m38s\u001b[0m 59ms/step - auc: 0.9438 - binary_accuracy: 0.8714 - false_negatives: 13.5556 - false_positives: 44.4815 - loss: 0.4188 - precision: 0.8274 - recall: 0.9470 - true_negatives: 171.5926 - true_positives: 218.3704\n",
      "For batch number 28 the model has a loss of 0.42428073287010193\n",
      "\u001b[1m 28/689\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m38s\u001b[0m 58ms/step - auc: 0.9435 - binary_accuracy: 0.8711 - false_negatives: 14.1071 - false_positives: 46.2500 - loss: 0.4190 - precision: 0.8271 - recall: 0.9466 - true_negatives: 177.8929 - true_positives: 225.7500\n",
      "For batch number 29 the model has a loss of 0.42164990305900574\n",
      "\u001b[1m 29/689\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m38s\u001b[0m 58ms/step - auc: 0.9433 - binary_accuracy: 0.8708 - false_negatives: 14.6552 - false_positives: 48.0345 - loss: 0.4191 - precision: 0.8268 - recall: 0.9463 - true_negatives: 184.1724 - true_positives: 233.1379\n",
      "For batch number 30 the model has a loss of 0.41656357049942017\n",
      "\u001b[1m 30/689\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m38s\u001b[0m 58ms/step - auc: 0.9432 - binary_accuracy: 0.8705 - false_negatives: 15.1667 - false_positives: 49.8000 - loss: 0.4190 - precision: 0.8266 - recall: 0.9460 - true_negatives: 190.4333 - true_positives: 240.6000\n",
      "For batch number 31 the model has a loss of 0.4181172251701355\n",
      "\u001b[1m 31/689\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m38s\u001b[0m 58ms/step - auc: 0.9429 - binary_accuracy: 0.8704 - false_negatives: 15.6452 - false_positives: 51.5484 - loss: 0.4190 - precision: 0.8264 - recall: 0.9458 - true_negatives: 196.6452 - true_positives: 248.1613\n",
      "For batch number 32 the model has a loss of 0.41506654024124146\n",
      "\u001b[1m 32/689\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m38s\u001b[0m 59ms/step - auc: 0.9427 - binary_accuracy: 0.8702 - false_negatives: 16.1250 - false_positives: 53.2812 - loss: 0.4189 - precision: 0.8262 - recall: 0.9457 - true_negatives: 202.8125 - true_positives: 255.7812\n",
      "For batch number 33 the model has a loss of 0.4151570498943329\n",
      "\u001b[1m 33/689\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m38s\u001b[0m 59ms/step - auc: 0.9424 - binary_accuracy: 0.8699 - false_negatives: 16.7273 - false_positives: 55.0000 - loss: 0.4188 - precision: 0.8261 - recall: 0.9453 - true_negatives: 208.9394 - true_positives: 263.3333\n",
      "For batch number 34 the model has a loss of 0.41719093918800354\n",
      "\u001b[1m 34/689\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m38s\u001b[0m 59ms/step - auc: 0.9421 - binary_accuracy: 0.8697 - false_negatives: 17.2941 - false_positives: 56.7941 - loss: 0.4187 - precision: 0.8258 - recall: 0.9450 - true_negatives: 215.0882 - true_positives: 270.8235\n",
      "For batch number 35 the model has a loss of 0.41283732652664185\n",
      "\u001b[1m 35/689\u001b[0m \u001b[32m━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m38s\u001b[0m 59ms/step - auc: 0.9418 - binary_accuracy: 0.8695 - false_negatives: 17.8571 - false_positives: 58.5143 - loss: 0.4186 - precision: 0.8256 - recall: 0.9447 - true_negatives: 221.2286 - true_positives: 278.4000\n",
      "For batch number 36 the model has a loss of 0.42332708835601807\n",
      "\u001b[1m 36/689\u001b[0m \u001b[32m━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m38s\u001b[0m 59ms/step - auc: 0.9415 - binary_accuracy: 0.8692 - false_negatives: 18.4722 - false_positives: 60.1944 - loss: 0.4187 - precision: 0.8256 - recall: 0.9444 - true_negatives: 227.2222 - true_positives: 286.1111\n",
      "For batch number 37 the model has a loss of 0.42602968215942383\n",
      "\u001b[1m 37/689\u001b[0m \u001b[32m━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m38s\u001b[0m 58ms/step - auc: 0.9412 - binary_accuracy: 0.8690 - false_negatives: 19.1622 - false_positives: 61.8378 - loss: 0.4189 - precision: 0.8256 - recall: 0.9440 - true_negatives: 233.1892 - true_positives: 293.8108\n",
      "For batch number 38 the model has a loss of 0.42669305205345154\n",
      "\u001b[1m 38/689\u001b[0m \u001b[32m━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m38s\u001b[0m 58ms/step - auc: 0.9408 - binary_accuracy: 0.8688 - false_negatives: 19.8421 - false_positives: 63.4737 - loss: 0.4191 - precision: 0.8256 - recall: 0.9436 - true_negatives: 239.1053 - true_positives: 301.5789\n",
      "For batch number 39 the model has a loss of 0.4237673282623291\n",
      "\u001b[1m 39/689\u001b[0m \u001b[32m━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m37s\u001b[0m 58ms/step - auc: 0.9405 - binary_accuracy: 0.8686 - false_negatives: 20.4872 - false_positives: 65.1026 - loss: 0.4192 - precision: 0.8256 - recall: 0.9432 - true_negatives: 245.0000 - true_positives: 309.4102\n",
      "For batch number 40 the model has a loss of 0.4261733591556549\n",
      "\u001b[1m 40/689\u001b[0m \u001b[32m━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m37s\u001b[0m 58ms/step - auc: 0.9402 - binary_accuracy: 0.8685 - false_negatives: 21.1750 - false_positives: 66.6750 - loss: 0.4194 - precision: 0.8257 - recall: 0.9429 - true_negatives: 250.9000 - true_positives: 317.2500\n",
      "For batch number 41 the model has a loss of 0.43024230003356934\n",
      "\u001b[1m 41/689\u001b[0m \u001b[32m━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m37s\u001b[0m 58ms/step - auc: 0.9399 - binary_accuracy: 0.8683 - false_negatives: 21.9024 - false_positives: 68.2195 - loss: 0.4196 - precision: 0.8259 - recall: 0.9425 - true_negatives: 256.7561 - true_positives: 325.1219\n",
      "For batch number 42 the model has a loss of 0.4283228814601898\n",
      "\u001b[1m 42/689\u001b[0m \u001b[32m━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m37s\u001b[0m 58ms/step - auc: 0.9395 - binary_accuracy: 0.8682 - false_negatives: 22.6190 - false_positives: 69.7619 - loss: 0.4199 - precision: 0.8260 - recall: 0.9421 - true_negatives: 262.7381 - true_positives: 332.8810\n",
      "For batch number 43 the model has a loss of 0.4321863353252411\n",
      "\u001b[1m 43/689\u001b[0m \u001b[32m━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m37s\u001b[0m 58ms/step - auc: 0.9392 - binary_accuracy: 0.8681 - false_negatives: 23.3256 - false_positives: 71.2326 - loss: 0.4201 - precision: 0.8262 - recall: 0.9417 - true_negatives: 268.6512 - true_positives: 340.7907\n",
      "For batch number 44 the model has a loss of 0.4309888780117035\n",
      "\u001b[1m 44/689\u001b[0m \u001b[32m━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m37s\u001b[0m 58ms/step - auc: 0.9389 - binary_accuracy: 0.8680 - false_negatives: 24.0227 - false_positives: 72.7045 - loss: 0.4204 - precision: 0.8264 - recall: 0.9414 - true_negatives: 274.5682 - true_positives: 348.7046\n",
      "For batch number 45 the model has a loss of 0.43149256706237793\n",
      "\u001b[1m 45/689\u001b[0m \u001b[32m━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m37s\u001b[0m 58ms/step - auc: 0.9385 - binary_accuracy: 0.8679 - false_negatives: 24.7111 - false_positives: 74.2667 - loss: 0.4206 - precision: 0.8265 - recall: 0.9411 - true_negatives: 280.4889 - true_positives: 356.5333\n",
      "For batch number 46 the model has a loss of 0.4302102029323578\n",
      "\u001b[1m 46/689\u001b[0m \u001b[32m━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m37s\u001b[0m 58ms/step - auc: 0.9382 - binary_accuracy: 0.8678 - false_negatives: 25.3696 - false_positives: 75.8913 - loss: 0.4208 - precision: 0.8265 - recall: 0.9408 - true_negatives: 286.4348 - true_positives: 364.3044\n",
      "For batch number 47 the model has a loss of 0.4284737706184387\n",
      "\u001b[1m 47/689\u001b[0m \u001b[32m━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m37s\u001b[0m 58ms/step - auc: 0.9379 - binary_accuracy: 0.8676 - false_negatives: 26.0000 - false_positives: 77.5532 - loss: 0.4210 - precision: 0.8265 - recall: 0.9406 - true_negatives: 292.3192 - true_positives: 372.1277\n",
      "For batch number 48 the model has a loss of 0.4285100996494293\n",
      "\u001b[1m 48/689\u001b[0m \u001b[32m━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m37s\u001b[0m 58ms/step - auc: 0.9375 - binary_accuracy: 0.8675 - false_negatives: 26.6250 - false_positives: 79.2500 - loss: 0.4212 - precision: 0.8264 - recall: 0.9404 - true_negatives: 298.2292 - true_positives: 379.8958\n",
      "For batch number 49 the model has a loss of 0.42800620198249817\n",
      "\u001b[1m 49/689\u001b[0m \u001b[32m━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m37s\u001b[0m 58ms/step - auc: 0.9372 - binary_accuracy: 0.8673 - false_negatives: 27.2449 - false_positives: 80.9184 - loss: 0.4213 - precision: 0.8264 - recall: 0.9402 - true_negatives: 304.0816 - true_positives: 387.7551\n",
      "For batch number 50 the model has a loss of 0.4256805181503296\n",
      "\u001b[1m 50/689\u001b[0m \u001b[32m━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m37s\u001b[0m 58ms/step - auc: 0.9369 - binary_accuracy: 0.8672 - false_negatives: 27.8400 - false_positives: 82.5800 - loss: 0.4214 - precision: 0.8264 - recall: 0.9400 - true_negatives: 310.0000 - true_positives: 395.5800\n",
      "For batch number 51 the model has a loss of 0.42163291573524475\n",
      "\u001b[1m 51/689\u001b[0m \u001b[32m━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m37s\u001b[0m 58ms/step - auc: 0.9366 - binary_accuracy: 0.8672 - false_negatives: 28.4510 - false_positives: 84.1765 - loss: 0.4214 - precision: 0.8265 - recall: 0.9398 - true_negatives: 315.9804 - true_positives: 403.3922\n",
      "For batch number 52 the model has a loss of 0.41922345757484436\n",
      "\u001b[1m 52/689\u001b[0m \u001b[32m━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m36s\u001b[0m 58ms/step - auc: 0.9364 - binary_accuracy: 0.8671 - false_negatives: 29.0385 - false_positives: 85.7885 - loss: 0.4214 - precision: 0.8265 - recall: 0.9397 - true_negatives: 321.9038 - true_positives: 411.2692\n",
      "For batch number 53 the model has a loss of 0.4216924011707306\n",
      "\u001b[1m 53/689\u001b[0m \u001b[32m━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m36s\u001b[0m 58ms/step - auc: 0.9362 - binary_accuracy: 0.8670 - false_negatives: 29.6415 - false_positives: 87.4151 - loss: 0.4214 - precision: 0.8265 - recall: 0.9395 - true_negatives: 327.8302 - true_positives: 419.1132\n",
      "For batch number 54 the model has a loss of 0.41993021965026855\n",
      "\u001b[1m 54/689\u001b[0m \u001b[32m━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m36s\u001b[0m 58ms/step - auc: 0.9360 - binary_accuracy: 0.8669 - false_negatives: 30.2222 - false_positives: 89.0370 - loss: 0.4213 - precision: 0.8266 - recall: 0.9394 - true_negatives: 333.7963 - true_positives: 426.9445\n",
      "For batch number 55 the model has a loss of 0.41832566261291504\n",
      "\u001b[1m 55/689\u001b[0m \u001b[32m━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m36s\u001b[0m 58ms/step - auc: 0.9358 - binary_accuracy: 0.8669 - false_negatives: 30.7818 - false_positives: 90.6727 - loss: 0.4213 - precision: 0.8266 - recall: 0.9393 - true_negatives: 339.7818 - true_positives: 434.7636\n",
      "For batch number 56 the model has a loss of 0.41889214515686035\n",
      "\u001b[1m 56/689\u001b[0m \u001b[32m━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m36s\u001b[0m 58ms/step - auc: 0.9356 - binary_accuracy: 0.8668 - false_negatives: 31.3393 - false_positives: 92.3571 - loss: 0.4212 - precision: 0.8265 - recall: 0.9392 - true_negatives: 345.7500 - true_positives: 442.5536\n",
      "For batch number 57 the model has a loss of 0.41659194231033325\n",
      "\u001b[1m 57/689\u001b[0m \u001b[32m━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m36s\u001b[0m 58ms/step - auc: 0.9354 - binary_accuracy: 0.8667 - false_negatives: 31.8947 - false_positives: 94.0175 - loss: 0.4212 - precision: 0.8265 - recall: 0.9391 - true_negatives: 351.7368 - true_positives: 450.3509\n",
      "For batch number 58 the model has a loss of 0.41751229763031006\n",
      "\u001b[1m 58/689\u001b[0m \u001b[32m━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m36s\u001b[0m 58ms/step - auc: 0.9352 - binary_accuracy: 0.8666 - false_negatives: 32.4310 - false_positives: 95.7069 - loss: 0.4211 - precision: 0.8265 - recall: 0.9390 - true_negatives: 357.7242 - true_positives: 458.1379\n",
      "For batch number 59 the model has a loss of 0.41807594895362854\n",
      "\u001b[1m 59/689\u001b[0m \u001b[32m━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m36s\u001b[0m 58ms/step - auc: 0.9350 - binary_accuracy: 0.8666 - false_negatives: 32.9492 - false_positives: 97.4576 - loss: 0.4210 - precision: 0.8264 - recall: 0.9389 - true_negatives: 363.7119 - true_positives: 465.8813\n",
      "For batch number 60 the model has a loss of 0.4173502027988434\n",
      "\u001b[1m 60/689\u001b[0m \u001b[32m━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m36s\u001b[0m 58ms/step - auc: 0.9347 - binary_accuracy: 0.8664 - false_negatives: 33.4667 - false_positives: 99.2500 - loss: 0.4210 - precision: 0.8263 - recall: 0.9389 - true_negatives: 369.6833 - true_positives: 473.6000\n",
      "For batch number 61 the model has a loss of 0.42053163051605225\n",
      "\u001b[1m 61/689\u001b[0m \u001b[32m━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m36s\u001b[0m 58ms/step - auc: 0.9345 - binary_accuracy: 0.8663 - false_negatives: 33.9836 - false_positives: 101.0328 - loss: 0.4210 - precision: 0.8262 - recall: 0.9388 - true_negatives: 375.6393 - true_positives: 481.3443\n",
      "For batch number 62 the model has a loss of 0.41988497972488403\n",
      "\u001b[1m 62/689\u001b[0m \u001b[32m━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m36s\u001b[0m 58ms/step - auc: 0.9343 - binary_accuracy: 0.8662 - false_negatives: 34.4839 - false_positives: 102.8387 - loss: 0.4210 - precision: 0.8261 - recall: 0.9388 - true_negatives: 381.5807 - true_positives: 489.0968\n",
      "For batch number 63 the model has a loss of 0.4201154410839081\n",
      "\u001b[1m 63/689\u001b[0m \u001b[32m━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m36s\u001b[0m 58ms/step - auc: 0.9341 - binary_accuracy: 0.8661 - false_negatives: 34.9683 - false_positives: 104.6825 - loss: 0.4209 - precision: 0.8259 - recall: 0.9388 - true_negatives: 387.5238 - true_positives: 496.8254\n",
      "For batch number 64 the model has a loss of 0.42060673236846924\n",
      "\u001b[1m 64/689\u001b[0m \u001b[32m━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m36s\u001b[0m 58ms/step - auc: 0.9339 - binary_accuracy: 0.8660 - false_negatives: 35.4688 - false_positives: 106.5312 - loss: 0.4209 - precision: 0.8258 - recall: 0.9387 - true_negatives: 393.4375 - true_positives: 504.5625\n",
      "For batch number 65 the model has a loss of 0.4184548258781433\n",
      "\u001b[1m 65/689\u001b[0m \u001b[32m━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m36s\u001b[0m 58ms/step - auc: 0.9337 - binary_accuracy: 0.8659 - false_negatives: 35.9692 - false_positives: 108.3385 - loss: 0.4209 - precision: 0.8257 - recall: 0.9387 - true_negatives: 399.3692 - true_positives: 512.3231\n",
      "For batch number 66 the model has a loss of 0.4161716103553772\n",
      "\u001b[1m 66/689\u001b[0m \u001b[32m━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m36s\u001b[0m 58ms/step - auc: 0.9335 - binary_accuracy: 0.8658 - false_negatives: 36.4545 - false_positives: 110.1061 - loss: 0.4208 - precision: 0.8256 - recall: 0.9387 - true_negatives: 405.3333 - true_positives: 520.1061\n",
      "For batch number 67 the model has a loss of 0.41867750883102417\n",
      "\u001b[1m 67/689\u001b[0m \u001b[32m━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m36s\u001b[0m 58ms/step - auc: 0.9333 - binary_accuracy: 0.8658 - false_negatives: 36.9851 - false_positives: 111.8209 - loss: 0.4208 - precision: 0.8255 - recall: 0.9387 - true_negatives: 411.2836 - true_positives: 527.9105\n",
      "For batch number 68 the model has a loss of 0.4169222414493561\n",
      "\u001b[1m 68/689\u001b[0m \u001b[32m━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m36s\u001b[0m 58ms/step - auc: 0.9331 - binary_accuracy: 0.8657 - false_negatives: 37.5147 - false_positives: 113.5147 - loss: 0.4207 - precision: 0.8255 - recall: 0.9386 - true_negatives: 417.2647 - true_positives: 535.7059\n",
      "For batch number 69 the model has a loss of 0.41634872555732727\n",
      "\u001b[1m 69/689\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m35s\u001b[0m 58ms/step - auc: 0.9330 - binary_accuracy: 0.8657 - false_negatives: 38.0580 - false_positives: 115.1884 - loss: 0.4207 - precision: 0.8255 - recall: 0.9386 - true_negatives: 423.2464 - true_positives: 543.5073\n",
      "For batch number 70 the model has a loss of 0.4156809449195862\n",
      "\u001b[1m 70/689\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m35s\u001b[0m 58ms/step - auc: 0.9328 - binary_accuracy: 0.8656 - false_negatives: 38.6143 - false_positives: 116.8286 - loss: 0.4206 - precision: 0.8255 - recall: 0.9385 - true_negatives: 429.2571 - true_positives: 551.3000\n",
      "For batch number 71 the model has a loss of 0.4153307378292084\n",
      "\u001b[1m 71/689\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m35s\u001b[0m 58ms/step - auc: 0.9326 - binary_accuracy: 0.8656 - false_negatives: 39.1690 - false_positives: 118.4225 - loss: 0.4205 - precision: 0.8255 - recall: 0.9384 - true_negatives: 435.2676 - true_positives: 559.1409\n",
      "For batch number 72 the model has a loss of 0.4156217575073242\n",
      "\u001b[1m 72/689\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m35s\u001b[0m 58ms/step - auc: 0.9325 - binary_accuracy: 0.8656 - false_negatives: 39.7222 - false_positives: 119.9861 - loss: 0.4205 - precision: 0.8256 - recall: 0.9384 - true_negatives: 441.3333 - true_positives: 566.9583\n",
      "For batch number 73 the model has a loss of 0.41903871297836304\n",
      "\u001b[1m 73/689\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m35s\u001b[0m 58ms/step - auc: 0.9323 - binary_accuracy: 0.8656 - false_negatives: 40.2877 - false_positives: 121.5616 - loss: 0.4204 - precision: 0.8256 - recall: 0.9383 - true_negatives: 447.3836 - true_positives: 574.7672\n",
      "For batch number 74 the model has a loss of 0.4216036796569824\n",
      "\u001b[1m 74/689\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m35s\u001b[0m 58ms/step - auc: 0.9321 - binary_accuracy: 0.8656 - false_negatives: 40.8649 - false_positives: 123.1757 - loss: 0.4205 - precision: 0.8256 - recall: 0.9382 - true_negatives: 453.4324 - true_positives: 582.5270\n",
      "For batch number 75 the model has a loss of 0.4212161898612976\n",
      "\u001b[1m 75/689\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m35s\u001b[0m 58ms/step - auc: 0.9320 - binary_accuracy: 0.8655 - false_negatives: 41.4400 - false_positives: 124.8000 - loss: 0.4205 - precision: 0.8256 - recall: 0.9382 - true_negatives: 459.5200 - true_positives: 590.2400\n",
      "For batch number 76 the model has a loss of 0.4209802448749542\n",
      "\u001b[1m 76/689\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m35s\u001b[0m 58ms/step - auc: 0.9318 - binary_accuracy: 0.8655 - false_negatives: 42.0263 - false_positives: 126.4079 - loss: 0.4205 - precision: 0.8256 - recall: 0.9381 - true_negatives: 465.5789 - true_positives: 597.9868\n",
      "For batch number 77 the model has a loss of 0.42025861144065857\n",
      "\u001b[1m 77/689\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m35s\u001b[0m 58ms/step - auc: 0.9317 - binary_accuracy: 0.8655 - false_negatives: 42.6104 - false_positives: 128.0130 - loss: 0.4205 - precision: 0.8257 - recall: 0.9380 - true_negatives: 471.6623 - true_positives: 605.7143\n",
      "For batch number 78 the model has a loss of 0.42011183500289917\n",
      "\u001b[1m 78/689\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m35s\u001b[0m 58ms/step - auc: 0.9315 - binary_accuracy: 0.8654 - false_negatives: 43.1923 - false_positives: 129.6410 - loss: 0.4205 - precision: 0.8257 - recall: 0.9379 - true_negatives: 477.7692 - true_positives: 613.3975\n",
      "For batch number 79 the model has a loss of 0.4200195074081421\n",
      "\u001b[1m 79/689\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m35s\u001b[0m 58ms/step - auc: 0.9314 - binary_accuracy: 0.8654 - false_negatives: 43.7722 - false_positives: 131.2532 - loss: 0.4205 - precision: 0.8257 - recall: 0.9378 - true_negatives: 483.8734 - true_positives: 621.1013\n",
      "For batch number 80 the model has a loss of 0.4199647903442383\n",
      "\u001b[1m 80/689\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m35s\u001b[0m 58ms/step - auc: 0.9312 - binary_accuracy: 0.8654 - false_negatives: 44.3625 - false_positives: 132.8625 - loss: 0.4205 - precision: 0.8257 - recall: 0.9378 - true_negatives: 489.9625 - true_positives: 628.8125\n",
      "For batch number 81 the model has a loss of 0.4182846248149872\n",
      "\u001b[1m 81/689\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m35s\u001b[0m 58ms/step - auc: 0.9311 - binary_accuracy: 0.8654 - false_negatives: 44.9506 - false_positives: 134.4444 - loss: 0.4204 - precision: 0.8257 - recall: 0.9377 - true_negatives: 496.0370 - true_positives: 636.5679\n",
      "For batch number 82 the model has a loss of 0.41923603415489197\n",
      "\u001b[1m 82/689\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m35s\u001b[0m 58ms/step - auc: 0.9310 - binary_accuracy: 0.8653 - false_negatives: 45.5366 - false_positives: 136.0610 - loss: 0.4204 - precision: 0.8257 - recall: 0.9376 - true_negatives: 502.0976 - true_positives: 644.3049\n",
      "For batch number 83 the model has a loss of 0.41987285017967224\n",
      "\u001b[1m 83/689\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m35s\u001b[0m 58ms/step - auc: 0.9309 - binary_accuracy: 0.8653 - false_negatives: 46.1084 - false_positives: 137.7349 - loss: 0.4204 - precision: 0.8257 - recall: 0.9376 - true_negatives: 508.1566 - true_positives: 652.0000\n",
      "For batch number 84 the model has a loss of 0.4185299277305603\n",
      "\u001b[1m 84/689\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m35s\u001b[0m 58ms/step - auc: 0.9308 - binary_accuracy: 0.8652 - false_negatives: 46.6667 - false_positives: 139.4167 - loss: 0.4204 - precision: 0.8257 - recall: 0.9375 - true_negatives: 514.1786 - true_positives: 659.7381\n",
      "For batch number 85 the model has a loss of 0.4172554910182953\n",
      "\u001b[1m 85/689\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m35s\u001b[0m 58ms/step - auc: 0.9306 - binary_accuracy: 0.8652 - false_negatives: 47.2118 - false_positives: 141.1059 - loss: 0.4203 - precision: 0.8256 - recall: 0.9375 - true_negatives: 520.2000 - true_positives: 667.4824\n",
      "For batch number 86 the model has a loss of 0.41545602679252625\n",
      "\u001b[1m 86/689\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m35s\u001b[0m 58ms/step - auc: 0.9306 - binary_accuracy: 0.8652 - false_negatives: 47.7558 - false_positives: 142.7907 - loss: 0.4203 - precision: 0.8256 - recall: 0.9374 - true_negatives: 526.2442 - true_positives: 675.2093\n",
      "For batch number 87 the model has a loss of 0.41398119926452637\n",
      "\u001b[1m 87/689\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m35s\u001b[0m 58ms/step - auc: 0.9305 - binary_accuracy: 0.8651 - false_negatives: 48.2874 - false_positives: 144.4483 - loss: 0.4202 - precision: 0.8256 - recall: 0.9374 - true_negatives: 532.2529 - true_positives: 683.0115\n",
      "For batch number 88 the model has a loss of 0.41331946849823\n",
      "\u001b[1m 88/689\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m35s\u001b[0m 58ms/step - auc: 0.9304 - binary_accuracy: 0.8651 - false_negatives: 48.8182 - false_positives: 146.1136 - loss: 0.4201 - precision: 0.8256 - recall: 0.9374 - true_negatives: 538.2614 - true_positives: 690.8068\n",
      "For batch number 89 the model has a loss of 0.4148554503917694\n",
      "\u001b[1m 89/689\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m35s\u001b[0m 58ms/step - auc: 0.9303 - binary_accuracy: 0.8651 - false_negatives: 49.3483 - false_positives: 147.8090 - loss: 0.4201 - precision: 0.8255 - recall: 0.9373 - true_negatives: 544.3146 - true_positives: 698.5281\n",
      "For batch number 90 the model has a loss of 0.41522932052612305\n",
      "\u001b[1m 90/689\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m35s\u001b[0m 58ms/step - auc: 0.9302 - binary_accuracy: 0.8650 - false_negatives: 49.8667 - false_positives: 149.5444 - loss: 0.4200 - precision: 0.8255 - recall: 0.9373 - true_negatives: 550.4000 - true_positives: 706.1889\n",
      "For batch number 91 the model has a loss of 0.4136034846305847\n",
      "\u001b[1m 91/689\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m34s\u001b[0m 58ms/step - auc: 0.9301 - binary_accuracy: 0.8650 - false_negatives: 50.3956 - false_positives: 151.2418 - loss: 0.4200 - precision: 0.8255 - recall: 0.9373 - true_negatives: 556.5165 - true_positives: 713.8461\n",
      "For batch number 92 the model has a loss of 0.4133140742778778\n",
      "\u001b[1m 92/689\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m34s\u001b[0m 58ms/step - auc: 0.9300 - binary_accuracy: 0.8649 - false_negatives: 50.9239 - false_positives: 152.9674 - loss: 0.4199 - precision: 0.8254 - recall: 0.9373 - true_negatives: 562.6304 - true_positives: 721.4783\n",
      "For batch number 93 the model has a loss of 0.4131282567977905\n",
      "\u001b[1m 93/689\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m34s\u001b[0m 58ms/step - auc: 0.9299 - binary_accuracy: 0.8649 - false_negatives: 51.4409 - false_positives: 154.6882 - loss: 0.4198 - precision: 0.8253 - recall: 0.9372 - true_negatives: 568.7419 - true_positives: 729.1290\n",
      "For batch number 94 the model has a loss of 0.4156390130519867\n",
      "\u001b[1m 94/689\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m34s\u001b[0m 59ms/step - auc: 0.9298 - binary_accuracy: 0.8649 - false_negatives: 51.9681 - false_positives: 156.4468 - loss: 0.4198 - precision: 0.8253 - recall: 0.9372 - true_negatives: 574.8298 - true_positives: 736.7553\n",
      "For batch number 95 the model has a loss of 0.4164625108242035\n",
      "\u001b[1m 95/689\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m34s\u001b[0m 58ms/step - auc: 0.9297 - binary_accuracy: 0.8648 - false_negatives: 52.4947 - false_positives: 158.2105 - loss: 0.4197 - precision: 0.8252 - recall: 0.9372 - true_negatives: 580.8947 - true_positives: 744.4000\n",
      "For batch number 96 the model has a loss of 0.41542279720306396\n",
      "\u001b[1m 96/689\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m34s\u001b[0m 58ms/step - auc: 0.9296 - binary_accuracy: 0.8648 - false_negatives: 53.0104 - false_positives: 159.9688 - loss: 0.4197 - precision: 0.8251 - recall: 0.9372 - true_negatives: 586.9479 - true_positives: 752.0729\n",
      "For batch number 97 the model has a loss of 0.4175153076648712\n",
      "\u001b[1m 97/689\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m34s\u001b[0m 59ms/step - auc: 0.9295 - binary_accuracy: 0.8647 - false_negatives: 53.5258 - false_positives: 161.7835 - loss: 0.4197 - precision: 0.8250 - recall: 0.9372 - true_negatives: 593.0206 - true_positives: 759.6701\n",
      "For batch number 98 the model has a loss of 0.4179518222808838\n",
      "\u001b[1m 98/689\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m34s\u001b[0m 59ms/step - auc: 0.9294 - binary_accuracy: 0.8646 - false_negatives: 54.0306 - false_positives: 163.6225 - loss: 0.4196 - precision: 0.8249 - recall: 0.9371 - true_negatives: 599.1122 - true_positives: 767.2347\n",
      "For batch number 99 the model has a loss of 0.41598010063171387\n",
      "\u001b[1m 99/689\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m34s\u001b[0m 59ms/step - auc: 0.9293 - binary_accuracy: 0.8646 - false_negatives: 54.5253 - false_positives: 165.4646 - loss: 0.4196 - precision: 0.8248 - recall: 0.9371 - true_negatives: 605.2121 - true_positives: 774.7980\n",
      "For batch number 100 the model has a loss of 0.4135545492172241\n",
      "\u001b[1m100/689\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m34s\u001b[0m 59ms/step - auc: 0.9292 - binary_accuracy: 0.8645 - false_negatives: 55.0100 - false_positives: 167.2700 - loss: 0.4195 - precision: 0.8247 - recall: 0.9371 - true_negatives: 611.3200 - true_positives: 782.4000\n",
      "For batch number 101 the model has a loss of 0.41169148683547974\n",
      "\u001b[1m101/689\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m34s\u001b[0m 59ms/step - auc: 0.9291 - binary_accuracy: 0.8645 - false_negatives: 55.4851 - false_positives: 169.0396 - loss: 0.4195 - precision: 0.8247 - recall: 0.9371 - true_negatives: 617.4653 - true_positives: 790.0099\n",
      "For batch number 102 the model has a loss of 0.41083624958992004\n",
      "\u001b[1m102/689\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m34s\u001b[0m 58ms/step - auc: 0.9291 - binary_accuracy: 0.8645 - false_negatives: 55.9510 - false_positives: 170.8137 - loss: 0.4194 - precision: 0.8246 - recall: 0.9371 - true_negatives: 623.6177 - true_positives: 797.6177\n",
      "For batch number 103 the model has a loss of 0.4114072024822235\n",
      "\u001b[1m103/689\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m34s\u001b[0m 58ms/step - auc: 0.9290 - binary_accuracy: 0.8644 - false_negatives: 56.4175 - false_positives: 172.6116 - loss: 0.4193 - precision: 0.8245 - recall: 0.9372 - true_negatives: 629.7961 - true_positives: 805.1747\n",
      "For batch number 104 the model has a loss of 0.412370890378952\n",
      "\u001b[1m104/689\u001b[0m \u001b[32m━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m34s\u001b[0m 59ms/step - auc: 0.9289 - binary_accuracy: 0.8644 - false_negatives: 56.8846 - false_positives: 174.4135 - loss: 0.4192 - precision: 0.8244 - recall: 0.9372 - true_negatives: 636.0000 - true_positives: 812.7019\n",
      "For batch number 105 the model has a loss of 0.4116876423358917\n",
      "\u001b[1m105/689\u001b[0m \u001b[32m━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m34s\u001b[0m 59ms/step - auc: 0.9288 - binary_accuracy: 0.8643 - false_negatives: 57.3429 - false_positives: 176.2095 - loss: 0.4192 - precision: 0.8243 - recall: 0.9372 - true_negatives: 642.2191 - true_positives: 820.2286\n",
      "For batch number 106 the model has a loss of 0.4120507836341858\n",
      "\u001b[1m106/689\u001b[0m \u001b[32m━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m34s\u001b[0m 59ms/step - auc: 0.9288 - binary_accuracy: 0.8643 - false_negatives: 57.8019 - false_positives: 177.9906 - loss: 0.4191 - precision: 0.8243 - recall: 0.9372 - true_negatives: 648.4245 - true_positives: 827.7830\n",
      "For batch number 107 the model has a loss of 0.4100041687488556\n",
      "\u001b[1m107/689\u001b[0m \u001b[32m━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m34s\u001b[0m 58ms/step - auc: 0.9287 - binary_accuracy: 0.8643 - false_negatives: 58.2523 - false_positives: 179.7477 - loss: 0.4190 - precision: 0.8242 - recall: 0.9372 - true_negatives: 654.6729 - true_positives: 835.3271\n",
      "For batch number 108 the model has a loss of 0.40933796763420105\n",
      "\u001b[1m108/689\u001b[0m \u001b[32m━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m34s\u001b[0m 59ms/step - auc: 0.9286 - binary_accuracy: 0.8643 - false_negatives: 58.7037 - false_positives: 181.4722 - loss: 0.4189 - precision: 0.8241 - recall: 0.9372 - true_negatives: 660.9537 - true_positives: 842.8704\n",
      "For batch number 109 the model has a loss of 0.4093075394630432\n",
      "\u001b[1m109/689\u001b[0m \u001b[32m━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m33s\u001b[0m 59ms/step - auc: 0.9286 - binary_accuracy: 0.8643 - false_negatives: 59.1651 - false_positives: 183.1835 - loss: 0.4188 - precision: 0.8241 - recall: 0.9372 - true_negatives: 667.2752 - true_positives: 850.3762\n",
      "For batch number 110 the model has a loss of 0.41054773330688477\n",
      "\u001b[1m110/689\u001b[0m \u001b[32m━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m33s\u001b[0m 59ms/step - auc: 0.9285 - binary_accuracy: 0.8643 - false_negatives: 59.6727 - false_positives: 184.8636 - loss: 0.4188 - precision: 0.8241 - recall: 0.9372 - true_negatives: 673.5909 - true_positives: 857.8727\n",
      "For batch number 111 the model has a loss of 0.4095144271850586\n",
      "\u001b[1m111/689\u001b[0m \u001b[32m━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m33s\u001b[0m 58ms/step - auc: 0.9284 - binary_accuracy: 0.8642 - false_negatives: 60.1712 - false_positives: 186.5315 - loss: 0.4187 - precision: 0.8240 - recall: 0.9372 - true_negatives: 679.9640 - true_positives: 865.3333\n",
      "For batch number 112 the model has a loss of 0.4098803400993347\n",
      "\u001b[1m112/689\u001b[0m \u001b[32m━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m33s\u001b[0m 58ms/step - auc: 0.9284 - binary_accuracy: 0.8642 - false_negatives: 60.6964 - false_positives: 188.1875 - loss: 0.4186 - precision: 0.8240 - recall: 0.9372 - true_negatives: 686.3571 - true_positives: 872.7589\n",
      "For batch number 113 the model has a loss of 0.4084882140159607\n",
      "\u001b[1m113/689\u001b[0m \u001b[32m━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m33s\u001b[0m 58ms/step - auc: 0.9283 - binary_accuracy: 0.8642 - false_negatives: 61.2124 - false_positives: 189.8142 - loss: 0.4185 - precision: 0.8240 - recall: 0.9372 - true_negatives: 692.7699 - true_positives: 880.2036\n",
      "For batch number 114 the model has a loss of 0.4073459804058075\n",
      "\u001b[1m114/689\u001b[0m \u001b[32m━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m33s\u001b[0m 58ms/step - auc: 0.9283 - binary_accuracy: 0.8642 - false_negatives: 61.7281 - false_positives: 191.4211 - loss: 0.4184 - precision: 0.8240 - recall: 0.9372 - true_negatives: 699.2018 - true_positives: 887.6491\n",
      "For batch number 115 the model has a loss of 0.4065638780593872\n",
      "\u001b[1m115/689\u001b[0m \u001b[32m━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m33s\u001b[0m 58ms/step - auc: 0.9282 - binary_accuracy: 0.8643 - false_negatives: 62.2348 - false_positives: 193.0087 - loss: 0.4183 - precision: 0.8240 - recall: 0.9371 - true_negatives: 705.6087 - true_positives: 895.1478\n",
      "For batch number 116 the model has a loss of 0.4085880219936371\n",
      "\u001b[1m116/689\u001b[0m \u001b[32m━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m33s\u001b[0m 58ms/step - auc: 0.9281 - binary_accuracy: 0.8643 - false_negatives: 62.7586 - false_positives: 194.6121 - loss: 0.4182 - precision: 0.8239 - recall: 0.9371 - true_negatives: 711.9828 - true_positives: 902.6465\n",
      "For batch number 117 the model has a loss of 0.40834736824035645\n",
      "\u001b[1m117/689\u001b[0m \u001b[32m━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m33s\u001b[0m 58ms/step - auc: 0.9281 - binary_accuracy: 0.8643 - false_negatives: 63.2735 - false_positives: 196.2393 - loss: 0.4181 - precision: 0.8239 - recall: 0.9371 - true_negatives: 718.3761 - true_positives: 910.1111\n",
      "For batch number 118 the model has a loss of 0.4090670049190521\n",
      "\u001b[1m118/689\u001b[0m \u001b[32m━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m33s\u001b[0m 58ms/step - auc: 0.9280 - binary_accuracy: 0.8643 - false_negatives: 63.7881 - false_positives: 197.8559 - loss: 0.4181 - precision: 0.8239 - recall: 0.9371 - true_negatives: 724.7542 - true_positives: 917.6017\n",
      "For batch number 119 the model has a loss of 0.4101056456565857\n",
      "\u001b[1m119/689\u001b[0m \u001b[32m━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m33s\u001b[0m 58ms/step - auc: 0.9280 - binary_accuracy: 0.8643 - false_negatives: 64.3193 - false_positives: 199.4958 - loss: 0.4180 - precision: 0.8239 - recall: 0.9371 - true_negatives: 731.1260 - true_positives: 925.0588\n",
      "For batch number 120 the model has a loss of 0.4111897051334381\n",
      "\u001b[1m120/689\u001b[0m \u001b[32m━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m33s\u001b[0m 58ms/step - auc: 0.9279 - binary_accuracy: 0.8642 - false_negatives: 64.8500 - false_positives: 201.1583 - loss: 0.4179 - precision: 0.8239 - recall: 0.9370 - true_negatives: 737.5000 - true_positives: 932.4916\n",
      "For batch number 121 the model has a loss of 0.4106103181838989\n",
      "\u001b[1m121/689\u001b[0m \u001b[32m━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m33s\u001b[0m 58ms/step - auc: 0.9278 - binary_accuracy: 0.8642 - false_negatives: 65.3802 - false_positives: 202.8264 - loss: 0.4179 - precision: 0.8238 - recall: 0.9370 - true_negatives: 743.8926 - true_positives: 939.9008\n",
      "For batch number 122 the model has a loss of 0.4105372428894043\n",
      "\u001b[1m122/689\u001b[0m \u001b[32m━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m33s\u001b[0m 58ms/step - auc: 0.9278 - binary_accuracy: 0.8642 - false_negatives: 65.9180 - false_positives: 204.5000 - loss: 0.4178 - precision: 0.8238 - recall: 0.9370 - true_negatives: 750.2787 - true_positives: 947.3033\n",
      "For batch number 123 the model has a loss of 0.41238534450531006\n",
      "\u001b[1m123/689\u001b[0m \u001b[32m━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m33s\u001b[0m 58ms/step - auc: 0.9277 - binary_accuracy: 0.8642 - false_negatives: 66.4634 - false_positives: 206.1707 - loss: 0.4178 - precision: 0.8237 - recall: 0.9370 - true_negatives: 756.6667 - true_positives: 954.6992\n",
      "For batch number 124 the model has a loss of 0.4130707383155823\n",
      "\u001b[1m124/689\u001b[0m \u001b[32m━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m32s\u001b[0m 58ms/step - auc: 0.9276 - binary_accuracy: 0.8642 - false_negatives: 67.0000 - false_positives: 207.8871 - loss: 0.4177 - precision: 0.8237 - recall: 0.9369 - true_negatives: 763.0806 - true_positives: 962.0323\n",
      "For batch number 125 the model has a loss of 0.4122815728187561\n",
      "\u001b[1m125/689\u001b[0m \u001b[32m━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m32s\u001b[0m 58ms/step - auc: 0.9276 - binary_accuracy: 0.8641 - false_negatives: 67.5360 - false_positives: 209.5840 - loss: 0.4177 - precision: 0.8236 - recall: 0.9369 - true_negatives: 769.4880 - true_positives: 969.3920\n",
      "For batch number 126 the model has a loss of 0.41146737337112427\n",
      "\u001b[1m126/689\u001b[0m \u001b[32m━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m32s\u001b[0m 58ms/step - auc: 0.9275 - binary_accuracy: 0.8641 - false_negatives: 68.0635 - false_positives: 211.2857 - loss: 0.4176 - precision: 0.8236 - recall: 0.9369 - true_negatives: 775.9207 - true_positives: 976.7302\n",
      "For batch number 127 the model has a loss of 0.41176745295524597\n",
      "\u001b[1m127/689\u001b[0m \u001b[32m━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m32s\u001b[0m 58ms/step - auc: 0.9274 - binary_accuracy: 0.8641 - false_negatives: 68.6063 - false_positives: 212.9685 - loss: 0.4176 - precision: 0.8235 - recall: 0.9369 - true_negatives: 782.3228 - true_positives: 984.1024\n",
      "For batch number 128 the model has a loss of 0.41063982248306274\n",
      "\u001b[1m128/689\u001b[0m \u001b[32m━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m32s\u001b[0m 58ms/step - auc: 0.9274 - binary_accuracy: 0.8641 - false_negatives: 69.1406 - false_positives: 214.6484 - loss: 0.4175 - precision: 0.8235 - recall: 0.9368 - true_negatives: 788.7578 - true_positives: 991.4531\n",
      "For batch number 129 the model has a loss of 0.40897396206855774\n",
      "\u001b[1m129/689\u001b[0m \u001b[32m━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m32s\u001b[0m 58ms/step - auc: 0.9273 - binary_accuracy: 0.8641 - false_negatives: 69.6667 - false_positives: 216.3023 - loss: 0.4175 - precision: 0.8235 - recall: 0.9368 - true_negatives: 795.2325 - true_positives: 998.7985\n",
      "For batch number 130 the model has a loss of 0.40824759006500244\n",
      "\u001b[1m130/689\u001b[0m \u001b[32m━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m32s\u001b[0m 58ms/step - auc: 0.9273 - binary_accuracy: 0.8641 - false_negatives: 70.1923 - false_positives: 217.9385 - loss: 0.4174 - precision: 0.8234 - recall: 0.9368 - true_negatives: 801.7077 - true_positives: 1006.1616\n",
      "For batch number 131 the model has a loss of 0.40899911522865295\n",
      "\u001b[1m131/689\u001b[0m \u001b[32m━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m32s\u001b[0m 58ms/step - auc: 0.9272 - binary_accuracy: 0.8641 - false_negatives: 70.7176 - false_positives: 219.5954 - loss: 0.4173 - precision: 0.8234 - recall: 0.9368 - true_negatives: 808.1909 - true_positives: 1013.4962\n",
      "For batch number 132 the model has a loss of 0.40825945138931274\n",
      "\u001b[1m132/689\u001b[0m \u001b[32m━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m32s\u001b[0m 58ms/step - auc: 0.9272 - binary_accuracy: 0.8641 - false_negatives: 71.2348 - false_positives: 221.2273 - loss: 0.4173 - precision: 0.8234 - recall: 0.9367 - true_negatives: 814.6970 - true_positives: 1020.8409\n",
      "For batch number 133 the model has a loss of 0.4067729413509369\n",
      "\u001b[1m133/689\u001b[0m \u001b[32m━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m32s\u001b[0m 58ms/step - auc: 0.9272 - binary_accuracy: 0.8641 - false_negatives: 71.7444 - false_positives: 222.8346 - loss: 0.4172 - precision: 0.8234 - recall: 0.9367 - true_negatives: 821.2180 - true_positives: 1028.2030\n",
      "For batch number 134 the model has a loss of 0.4060155153274536\n",
      "\u001b[1m134/689\u001b[0m \u001b[32m━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m32s\u001b[0m 58ms/step - auc: 0.9271 - binary_accuracy: 0.8641 - false_negatives: 72.2463 - false_positives: 224.4328 - loss: 0.4171 - precision: 0.8234 - recall: 0.9367 - true_negatives: 827.7687 - true_positives: 1035.5522\n",
      "For batch number 135 the model has a loss of 0.40489545464515686\n",
      "\u001b[1m135/689\u001b[0m \u001b[32m━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m32s\u001b[0m 58ms/step - auc: 0.9271 - binary_accuracy: 0.8641 - false_negatives: 72.7630 - false_positives: 226.0074 - loss: 0.4170 - precision: 0.8234 - recall: 0.9367 - true_negatives: 834.3037 - true_positives: 1042.9259\n",
      "For batch number 136 the model has a loss of 0.40401676297187805\n",
      "\u001b[1m136/689\u001b[0m \u001b[32m━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m32s\u001b[0m 58ms/step - auc: 0.9270 - binary_accuracy: 0.8641 - false_negatives: 73.2794 - false_positives: 227.5735 - loss: 0.4169 - precision: 0.8234 - recall: 0.9367 - true_negatives: 840.8456 - true_positives: 1050.3015\n",
      "For batch number 137 the model has a loss of 0.40540552139282227\n",
      "\u001b[1m137/689\u001b[0m \u001b[32m━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m32s\u001b[0m 58ms/step - auc: 0.9270 - binary_accuracy: 0.8641 - false_negatives: 73.8029 - false_positives: 229.1241 - loss: 0.4168 - precision: 0.8234 - recall: 0.9367 - true_negatives: 847.4015 - true_positives: 1057.6715\n",
      "For batch number 138 the model has a loss of 0.4047955572605133\n",
      "\u001b[1m138/689\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m32s\u001b[0m 58ms/step - auc: 0.9270 - binary_accuracy: 0.8642 - false_negatives: 74.3406 - false_positives: 230.6522 - loss: 0.4168 - precision: 0.8234 - recall: 0.9367 - true_negatives: 853.9565 - true_positives: 1065.0508\n",
      "For batch number 139 the model has a loss of 0.4048056900501251\n",
      "\u001b[1m139/689\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m32s\u001b[0m 58ms/step - auc: 0.9269 - binary_accuracy: 0.8642 - false_negatives: 74.8849 - false_positives: 232.1727 - loss: 0.4167 - precision: 0.8234 - recall: 0.9366 - true_negatives: 860.4892 - true_positives: 1072.4532\n",
      "For batch number 140 the model has a loss of 0.40490612387657166\n",
      "\u001b[1m140/689\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m32s\u001b[0m 58ms/step - auc: 0.9269 - binary_accuracy: 0.8642 - false_negatives: 75.4286 - false_positives: 233.6929 - loss: 0.4166 - precision: 0.8234 - recall: 0.9366 - true_negatives: 867.0286 - true_positives: 1079.8500\n",
      "For batch number 141 the model has a loss of 0.4049043655395508\n",
      "\u001b[1m141/689\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m31s\u001b[0m 58ms/step - auc: 0.9269 - binary_accuracy: 0.8642 - false_negatives: 75.9787 - false_positives: 235.2128 - loss: 0.4165 - precision: 0.8234 - recall: 0.9366 - true_negatives: 873.5886 - true_positives: 1087.2198\n",
      "For batch number 142 the model has a loss of 0.40360909700393677\n",
      "\u001b[1m142/689\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m31s\u001b[0m 58ms/step - auc: 0.9268 - binary_accuracy: 0.8642 - false_negatives: 76.5211 - false_positives: 236.7254 - loss: 0.4164 - precision: 0.8234 - recall: 0.9365 - true_negatives: 880.1620 - true_positives: 1094.5916\n",
      "For batch number 143 the model has a loss of 0.40259337425231934\n",
      "\u001b[1m143/689\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m31s\u001b[0m 59ms/step - auc: 0.9268 - binary_accuracy: 0.8643 - false_negatives: 77.0629 - false_positives: 238.2238 - loss: 0.4163 - precision: 0.8235 - recall: 0.9365 - true_negatives: 886.7273 - true_positives: 1101.9860\n",
      "For batch number 144 the model has a loss of 0.40264177322387695\n",
      "\u001b[1m144/689\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m31s\u001b[0m 59ms/step - auc: 0.9268 - binary_accuracy: 0.8643 - false_negatives: 77.6042 - false_positives: 239.7222 - loss: 0.4162 - precision: 0.8235 - recall: 0.9365 - true_negatives: 893.2917 - true_positives: 1109.3820\n",
      "For batch number 145 the model has a loss of 0.4020162522792816\n",
      "\u001b[1m145/689\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m31s\u001b[0m 58ms/step - auc: 0.9268 - binary_accuracy: 0.8643 - false_negatives: 78.1379 - false_positives: 241.2276 - loss: 0.4161 - precision: 0.8235 - recall: 0.9365 - true_negatives: 899.8690 - true_positives: 1116.7655\n",
      "For batch number 146 the model has a loss of 0.4015616774559021\n",
      "\u001b[1m146/689\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m31s\u001b[0m 59ms/step - auc: 0.9267 - binary_accuracy: 0.8643 - false_negatives: 78.6644 - false_positives: 242.7329 - loss: 0.4160 - precision: 0.8235 - recall: 0.9365 - true_negatives: 906.4384 - true_positives: 1124.1644\n",
      "For batch number 147 the model has a loss of 0.4016253352165222\n",
      "\u001b[1m147/689\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m31s\u001b[0m 59ms/step - auc: 0.9267 - binary_accuracy: 0.8644 - false_negatives: 79.1905 - false_positives: 244.2381 - loss: 0.4159 - precision: 0.8235 - recall: 0.9365 - true_negatives: 913.0068 - true_positives: 1131.5646\n",
      "For batch number 148 the model has a loss of 0.4005963206291199\n",
      "\u001b[1m148/689\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m31s\u001b[0m 59ms/step - auc: 0.9267 - binary_accuracy: 0.8644 - false_negatives: 79.7095 - false_positives: 245.7297 - loss: 0.4158 - precision: 0.8236 - recall: 0.9364 - true_negatives: 919.5743 - true_positives: 1138.9865\n",
      "For batch number 149 the model has a loss of 0.40138524770736694\n",
      "\u001b[1m149/689\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m31s\u001b[0m 59ms/step - auc: 0.9266 - binary_accuracy: 0.8644 - false_negatives: 80.2483 - false_positives: 247.2081 - loss: 0.4157 - precision: 0.8236 - recall: 0.9364 - true_negatives: 926.1141 - true_positives: 1146.4296\n",
      "For batch number 150 the model has a loss of 0.4003702402114868\n",
      "\u001b[1m150/689\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m31s\u001b[0m 59ms/step - auc: 0.9266 - binary_accuracy: 0.8645 - false_negatives: 80.7800 - false_positives: 248.6800 - loss: 0.4156 - precision: 0.8236 - recall: 0.9364 - true_negatives: 932.6733 - true_positives: 1153.8667\n",
      "For batch number 151 the model has a loss of 0.39976760745048523\n",
      "\u001b[1m151/689\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m31s\u001b[0m 59ms/step - auc: 0.9266 - binary_accuracy: 0.8645 - false_negatives: 81.3046 - false_positives: 250.1589 - loss: 0.4155 - precision: 0.8237 - recall: 0.9364 - true_negatives: 939.2252 - true_positives: 1161.3113\n",
      "For batch number 152 the model has a loss of 0.39957863092422485\n",
      "\u001b[1m152/689\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m31s\u001b[0m 59ms/step - auc: 0.9266 - binary_accuracy: 0.8645 - false_negatives: 81.8355 - false_positives: 251.6447 - loss: 0.4154 - precision: 0.8237 - recall: 0.9364 - true_negatives: 945.7632 - true_positives: 1168.7566\n",
      "For batch number 153 the model has a loss of 0.3990817070007324\n",
      "\u001b[1m153/689\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m31s\u001b[0m 59ms/step - auc: 0.9266 - binary_accuracy: 0.8645 - false_negatives: 82.3595 - false_positives: 253.1242 - loss: 0.4153 - precision: 0.8237 - recall: 0.9364 - true_negatives: 952.3072 - true_positives: 1176.2091\n",
      "For batch number 154 the model has a loss of 0.3991069793701172\n",
      "\u001b[1m154/689\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m31s\u001b[0m 58ms/step - auc: 0.9265 - binary_accuracy: 0.8646 - false_negatives: 82.8766 - false_positives: 254.6104 - loss: 0.4152 - precision: 0.8238 - recall: 0.9364 - true_negatives: 958.8766 - true_positives: 1183.6364\n",
      "For batch number 155 the model has a loss of 0.3981688618659973\n",
      "\u001b[1m155/689\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m31s\u001b[0m 58ms/step - auc: 0.9265 - binary_accuracy: 0.8646 - false_negatives: 83.3871 - false_positives: 256.0903 - loss: 0.4151 - precision: 0.8238 - recall: 0.9363 - true_negatives: 965.4516 - true_positives: 1191.0709\n",
      "For batch number 156 the model has a loss of 0.3985426127910614\n",
      "\u001b[1m156/689\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m31s\u001b[0m 59ms/step - auc: 0.9265 - binary_accuracy: 0.8646 - false_negatives: 83.9103 - false_positives: 257.5705 - loss: 0.4150 - precision: 0.8238 - recall: 0.9363 - true_negatives: 972.0064 - true_positives: 1198.5128\n",
      "For batch number 157 the model has a loss of 0.39777690172195435\n",
      "\u001b[1m157/689\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m31s\u001b[0m 59ms/step - auc: 0.9265 - binary_accuracy: 0.8647 - false_negatives: 84.4268 - false_positives: 259.0510 - loss: 0.4149 - precision: 0.8238 - recall: 0.9363 - true_negatives: 978.5732 - true_positives: 1205.9491\n",
      "For batch number 158 the model has a loss of 0.39685186743736267\n",
      "\u001b[1m158/689\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m31s\u001b[0m 59ms/step - auc: 0.9265 - binary_accuracy: 0.8647 - false_negatives: 84.9367 - false_positives: 260.5190 - loss: 0.4148 - precision: 0.8239 - recall: 0.9363 - true_negatives: 985.1519 - true_positives: 1213.3925\n",
      "For batch number 159 the model has a loss of 0.39746198058128357\n",
      "\u001b[1m159/689\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m31s\u001b[0m 59ms/step - auc: 0.9264 - binary_accuracy: 0.8647 - false_negatives: 85.4403 - false_positives: 262.0126 - loss: 0.4147 - precision: 0.8239 - recall: 0.9363 - true_negatives: 991.7170 - true_positives: 1220.8302\n",
      "For batch number 160 the model has a loss of 0.39711129665374756\n",
      "\u001b[1m160/689\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m30s\u001b[0m 59ms/step - auc: 0.9264 - binary_accuracy: 0.8648 - false_negatives: 85.9437 - false_positives: 263.5063 - loss: 0.4145 - precision: 0.8239 - recall: 0.9363 - true_negatives: 998.2625 - true_positives: 1228.2875\n",
      "For batch number 161 the model has a loss of 0.39604973793029785\n",
      "\u001b[1m161/689\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m30s\u001b[0m 59ms/step - auc: 0.9264 - binary_accuracy: 0.8648 - false_negatives: 86.4410 - false_positives: 264.9938 - loss: 0.4144 - precision: 0.8240 - recall: 0.9363 - true_negatives: 1004.8074 - true_positives: 1235.7578\n",
      "For batch number 162 the model has a loss of 0.3951919376850128\n",
      "\u001b[1m162/689\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m30s\u001b[0m 59ms/step - auc: 0.9264 - binary_accuracy: 0.8648 - false_negatives: 86.9321 - false_positives: 266.4753 - loss: 0.4143 - precision: 0.8240 - recall: 0.9363 - true_negatives: 1011.3765 - true_positives: 1243.2161\n",
      "For batch number 163 the model has a loss of 0.3943626582622528\n",
      "\u001b[1m163/689\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m30s\u001b[0m 59ms/step - auc: 0.9264 - binary_accuracy: 0.8649 - false_negatives: 87.4172 - false_positives: 267.9571 - loss: 0.4142 - precision: 0.8240 - recall: 0.9363 - true_negatives: 1017.9387 - true_positives: 1250.6871\n",
      "For batch number 164 the model has a loss of 0.39488592743873596\n",
      "\u001b[1m164/689\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m30s\u001b[0m 59ms/step - auc: 0.9263 - binary_accuracy: 0.8649 - false_negatives: 87.9024 - false_positives: 269.4268 - loss: 0.4141 - precision: 0.8241 - recall: 0.9363 - true_negatives: 1024.4878 - true_positives: 1258.1830\n",
      "For batch number 165 the model has a loss of 0.395420640707016\n",
      "\u001b[1m165/689\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m30s\u001b[0m 59ms/step - auc: 0.9263 - binary_accuracy: 0.8650 - false_negatives: 88.3939 - false_positives: 270.8970 - loss: 0.4140 - precision: 0.8241 - recall: 0.9363 - true_negatives: 1031.0303 - true_positives: 1265.6788\n",
      "For batch number 166 the model has a loss of 0.3957483172416687\n",
      "\u001b[1m166/689\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m30s\u001b[0m 59ms/step - auc: 0.9263 - binary_accuracy: 0.8650 - false_negatives: 88.8916 - false_positives: 272.3916 - loss: 0.4138 - precision: 0.8241 - recall: 0.9363 - true_negatives: 1037.5723 - true_positives: 1273.1445\n",
      "For batch number 167 the model has a loss of 0.39481809735298157\n",
      "\u001b[1m167/689\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m30s\u001b[0m 59ms/step - auc: 0.9263 - binary_accuracy: 0.8650 - false_negatives: 89.3832 - false_positives: 273.8802 - loss: 0.4137 - precision: 0.8241 - recall: 0.9363 - true_negatives: 1044.1138 - true_positives: 1280.6228\n",
      "For batch number 168 the model has a loss of 0.3951890468597412\n",
      "\u001b[1m168/689\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m30s\u001b[0m 59ms/step - auc: 0.9263 - binary_accuracy: 0.8650 - false_negatives: 89.8929 - false_positives: 275.3690 - loss: 0.4136 - precision: 0.8242 - recall: 0.9363 - true_negatives: 1050.6488 - true_positives: 1288.0892\n",
      "For batch number 169 the model has a loss of 0.3952277898788452\n",
      "\u001b[1m169/689\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m30s\u001b[0m 59ms/step - auc: 0.9263 - binary_accuracy: 0.8651 - false_negatives: 90.4083 - false_positives: 276.8580 - loss: 0.4135 - precision: 0.8242 - recall: 0.9363 - true_negatives: 1057.1835 - true_positives: 1295.5503\n",
      "For batch number 170 the model has a loss of 0.39465364813804626\n",
      "\u001b[1m170/689\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m30s\u001b[0m 59ms/step - auc: 0.9263 - binary_accuracy: 0.8651 - false_negatives: 90.9176 - false_positives: 278.3529 - loss: 0.4134 - precision: 0.8242 - recall: 0.9363 - true_negatives: 1063.7059 - true_positives: 1303.0236\n",
      "For batch number 171 the model has a loss of 0.3950890302658081\n",
      "\u001b[1m171/689\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m30s\u001b[0m 59ms/step - auc: 0.9262 - binary_accuracy: 0.8651 - false_negatives: 91.4269 - false_positives: 279.8421 - loss: 0.4133 - precision: 0.8242 - recall: 0.9363 - true_negatives: 1070.2397 - true_positives: 1310.4912\n",
      "For batch number 172 the model has a loss of 0.3943779170513153\n",
      "\u001b[1m172/689\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m30s\u001b[0m 59ms/step - auc: 0.9262 - binary_accuracy: 0.8652 - false_negatives: 91.9360 - false_positives: 281.3314 - loss: 0.4132 - precision: 0.8243 - recall: 0.9363 - true_negatives: 1076.7791 - true_positives: 1317.9535\n",
      "For batch number 173 the model has a loss of 0.39355045557022095\n",
      "\u001b[1m173/689\u001b[0m \u001b[32m━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━\u001b[0m \u001b[1m30s\u001b[0m 59ms/step - auc: 0.9262 - binary_accuracy: 0.8652 - false_negatives: 92.4393 - false_positives: 282.8092 - loss: 0.4131 - precision: 0.8243 - recall: 0.9363 - true_negatives: 1083.3584 - true_positives: 1325.3931\n",
      "For batch number 174 the model has a loss of 0.393869549036026\n",
      "\u001b[1m174/689\u001b[0m \u001b[32m━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━\u001b[0m \u001b[1m30s\u001b[0m 59ms/step - auc: 0.9262 - binary_accuracy: 0.8652 - false_negatives: 92.9368 - false_positives: 284.2931 - loss: 0.4130 - precision: 0.8243 - recall: 0.9363 - true_negatives: 1089.9368 - true_positives: 1332.8334\n",
      "For batch number 175 the model has a loss of 0.3926764726638794\n",
      "\u001b[1m175/689\u001b[0m \u001b[32m━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━\u001b[0m \u001b[1m30s\u001b[0m 59ms/step - auc: 0.9262 - binary_accuracy: 0.8653 - false_negatives: 93.4286 - false_positives: 285.7600 - loss: 0.4128 - precision: 0.8244 - recall: 0.9363 - true_negatives: 1096.5258 - true_positives: 1340.2858\n",
      "For batch number 176 the model has a loss of 0.3928227722644806\n",
      "\u001b[1m176/689\u001b[0m \u001b[32m━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━\u001b[0m \u001b[1m30s\u001b[0m 59ms/step - auc: 0.9262 - binary_accuracy: 0.8653 - false_negatives: 93.9205 - false_positives: 287.2386 - loss: 0.4127 - precision: 0.8244 - recall: 0.9363 - true_negatives: 1103.1079 - true_positives: 1347.7329\n",
      "For batch number 177 the model has a loss of 0.39315518736839294\n",
      "\u001b[1m177/689\u001b[0m \u001b[32m━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━\u001b[0m \u001b[1m29s\u001b[0m 59ms/step - auc: 0.9262 - binary_accuracy: 0.8653 - false_negatives: 94.4124 - false_positives: 288.7175 - loss: 0.4126 - precision: 0.8244 - recall: 0.9363 - true_negatives: 1109.6892 - true_positives: 1355.1808\n",
      "For batch number 178 the model has a loss of 0.39282679557800293\n",
      "\u001b[1m178/689\u001b[0m \u001b[32m━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━\u001b[0m \u001b[1m29s\u001b[0m 59ms/step - auc: 0.9262 - binary_accuracy: 0.8654 - false_negatives: 94.8989 - false_positives: 290.2079 - loss: 0.4125 - precision: 0.8244 - recall: 0.9363 - true_negatives: 1116.2753 - true_positives: 1362.6179\n",
      "For batch number 179 the model has a loss of 0.39285537600517273\n",
      "\u001b[1m179/689\u001b[0m \u001b[32m━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━\u001b[0m \u001b[1m29s\u001b[0m 59ms/step - auc: 0.9261 - binary_accuracy: 0.8654 - false_negatives: 95.3799 - false_positives: 291.7151 - loss: 0.4124 - precision: 0.8245 - recall: 0.9363 - true_negatives: 1122.8547 - true_positives: 1370.0503\n",
      "For batch number 180 the model has a loss of 0.3921264111995697\n",
      "\u001b[1m180/689\u001b[0m \u001b[32m━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━\u001b[0m \u001b[1m29s\u001b[0m 59ms/step - auc: 0.9261 - binary_accuracy: 0.8654 - false_negatives: 95.8556 - false_positives: 293.2222 - loss: 0.4123 - precision: 0.8245 - recall: 0.9363 - true_negatives: 1129.4222 - true_positives: 1377.5000\n",
      "For batch number 181 the model has a loss of 0.3918755352497101\n",
      "\u001b[1m181/689\u001b[0m \u001b[32m━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━\u001b[0m \u001b[1m29s\u001b[0m 59ms/step - auc: 0.9261 - binary_accuracy: 0.8655 - false_negatives: 96.3260 - false_positives: 294.7238 - loss: 0.4122 - precision: 0.8245 - recall: 0.9363 - true_negatives: 1135.9779 - true_positives: 1384.9724\n",
      "For batch number 182 the model has a loss of 0.3913252651691437\n",
      "\u001b[1m182/689\u001b[0m \u001b[32m━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━\u001b[0m \u001b[1m29s\u001b[0m 59ms/step - auc: 0.9261 - binary_accuracy: 0.8655 - false_negatives: 96.7912 - false_positives: 296.2143 - loss: 0.4121 - precision: 0.8245 - recall: 0.9363 - true_negatives: 1142.5494 - true_positives: 1392.4451\n",
      "For batch number 183 the model has a loss of 0.3907165825366974\n",
      "\u001b[1m183/689\u001b[0m \u001b[32m━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━\u001b[0m \u001b[1m29s\u001b[0m 59ms/step - auc: 0.9261 - binary_accuracy: 0.8655 - false_negatives: 97.2514 - false_positives: 297.7049 - loss: 0.4119 - precision: 0.8245 - recall: 0.9363 - true_negatives: 1149.1147 - true_positives: 1399.9290\n",
      "For batch number 184 the model has a loss of 0.3907468318939209\n",
      "\u001b[1m184/689\u001b[0m \u001b[32m━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━\u001b[0m \u001b[1m29s\u001b[0m 59ms/step - auc: 0.9261 - binary_accuracy: 0.8656 - false_negatives: 97.7174 - false_positives: 299.1902 - loss: 0.4118 - precision: 0.8246 - recall: 0.9363 - true_negatives: 1155.6902 - true_positives: 1407.4022\n",
      "For batch number 185 the model has a loss of 0.3915053904056549\n",
      "\u001b[1m185/689\u001b[0m \u001b[32m━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━\u001b[0m \u001b[1m29s\u001b[0m 59ms/step - auc: 0.9261 - binary_accuracy: 0.8656 - false_negatives: 98.1892 - false_positives: 300.6703 - loss: 0.4117 - precision: 0.8246 - recall: 0.9363 - true_negatives: 1162.2811 - true_positives: 1414.8595\n",
      "For batch number 186 the model has a loss of 0.3925676941871643\n",
      "\u001b[1m186/689\u001b[0m \u001b[32m━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━\u001b[0m \u001b[1m29s\u001b[0m 59ms/step - auc: 0.9261 - binary_accuracy: 0.8656 - false_negatives: 98.6613 - false_positives: 302.1505 - loss: 0.4116 - precision: 0.8246 - recall: 0.9364 - true_negatives: 1168.8763 - true_positives: 1422.3118\n",
      "For batch number 187 the model has a loss of 0.3917486369609833\n",
      "\u001b[1m187/689\u001b[0m \u001b[32m━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━\u001b[0m \u001b[1m29s\u001b[0m 59ms/step - auc: 0.9261 - binary_accuracy: 0.8657 - false_negatives: 99.1283 - false_positives: 303.6257 - loss: 0.4115 - precision: 0.8247 - recall: 0.9364 - true_negatives: 1175.4760 - true_positives: 1429.7700\n",
      "For batch number 188 the model has a loss of 0.3920493423938751\n",
      "\u001b[1m188/689\u001b[0m \u001b[32m━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━\u001b[0m \u001b[1m29s\u001b[0m 59ms/step - auc: 0.9261 - binary_accuracy: 0.8657 - false_negatives: 99.6064 - false_positives: 305.0904 - loss: 0.4114 - precision: 0.8247 - recall: 0.9364 - true_negatives: 1182.0638 - true_positives: 1437.2394\n",
      "For batch number 189 the model has a loss of 0.39201751351356506\n",
      "\u001b[1m189/689\u001b[0m \u001b[32m━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━\u001b[0m \u001b[1m29s\u001b[0m 59ms/step - auc: 0.9261 - binary_accuracy: 0.8657 - false_negatives: 100.0847 - false_positives: 306.5609 - loss: 0.4113 - precision: 0.8247 - recall: 0.9364 - true_negatives: 1188.6349 - true_positives: 1444.7196\n",
      "For batch number 190 the model has a loss of 0.39238429069519043\n",
      "\u001b[1m190/689\u001b[0m \u001b[32m━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━\u001b[0m \u001b[1m29s\u001b[0m 59ms/step - auc: 0.9261 - binary_accuracy: 0.8658 - false_negatives: 100.5684 - false_positives: 308.0158 - loss: 0.4112 - precision: 0.8247 - recall: 0.9364 - true_negatives: 1195.2053 - true_positives: 1452.2106\n",
      "For batch number 191 the model has a loss of 0.3912697732448578\n",
      "\u001b[1m191/689\u001b[0m \u001b[32m━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━\u001b[0m \u001b[1m29s\u001b[0m 59ms/step - auc: 0.9261 - binary_accuracy: 0.8658 - false_negatives: 101.0471 - false_positives: 309.4555 - loss: 0.4111 - precision: 0.8248 - recall: 0.9364 - true_negatives: 1201.7854 - true_positives: 1459.7120\n",
      "For batch number 192 the model has a loss of 0.39085066318511963\n",
      "\u001b[1m192/689\u001b[0m \u001b[32m━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━\u001b[0m \u001b[1m29s\u001b[0m 59ms/step - auc: 0.9261 - binary_accuracy: 0.8658 - false_negatives: 101.5260 - false_positives: 310.8854 - loss: 0.4110 - precision: 0.8248 - recall: 0.9364 - true_negatives: 1208.3802 - true_positives: 1467.2084\n",
      "For batch number 193 the model has a loss of 0.39133498072624207\n",
      "\u001b[1m193/689\u001b[0m \u001b[32m━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━\u001b[0m \u001b[1m29s\u001b[0m 59ms/step - auc: 0.9261 - binary_accuracy: 0.8659 - false_negatives: 102.0052 - false_positives: 312.3109 - loss: 0.4109 - precision: 0.8249 - recall: 0.9364 - true_negatives: 1214.9689 - true_positives: 1474.7150\n",
      "For batch number 194 the model has a loss of 0.3914964497089386\n",
      "\u001b[1m194/689\u001b[0m \u001b[32m━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━\u001b[0m \u001b[1m29s\u001b[0m 59ms/step - auc: 0.9261 - binary_accuracy: 0.8659 - false_negatives: 102.4948 - false_positives: 313.7320 - loss: 0.4108 - precision: 0.8249 - recall: 0.9364 - true_negatives: 1221.5515 - true_positives: 1482.2217\n",
      "For batch number 195 the model has a loss of 0.39116916060447693\n",
      "\u001b[1m195/689\u001b[0m \u001b[32m━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━\u001b[0m \u001b[1m28s\u001b[0m 59ms/step - auc: 0.9261 - binary_accuracy: 0.8660 - false_negatives: 102.9795 - false_positives: 315.1538 - loss: 0.4107 - precision: 0.8249 - recall: 0.9364 - true_negatives: 1228.1384 - true_positives: 1489.7281\n",
      "For batch number 196 the model has a loss of 0.39124706387519836\n",
      "\u001b[1m196/689\u001b[0m \u001b[32m━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━\u001b[0m \u001b[1m28s\u001b[0m 59ms/step - auc: 0.9261 - binary_accuracy: 0.8660 - false_negatives: 103.4643 - false_positives: 316.5714 - loss: 0.4106 - precision: 0.8250 - recall: 0.9364 - true_negatives: 1234.7245 - true_positives: 1497.2397\n",
      "For batch number 197 the model has a loss of 0.3912869691848755\n",
      "\u001b[1m197/689\u001b[0m \u001b[32m━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━\u001b[0m \u001b[1m28s\u001b[0m 59ms/step - auc: 0.9261 - binary_accuracy: 0.8660 - false_negatives: 103.9543 - false_positives: 317.9797 - loss: 0.4105 - precision: 0.8250 - recall: 0.9364 - true_negatives: 1241.2994 - true_positives: 1504.7665\n",
      "For batch number 198 the model has a loss of 0.3907381296157837\n",
      "\u001b[1m198/689\u001b[0m \u001b[32m━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━\u001b[0m \u001b[1m28s\u001b[0m 59ms/step - auc: 0.9260 - binary_accuracy: 0.8661 - false_negatives: 104.4444 - false_positives: 319.3839 - loss: 0.4104 - precision: 0.8251 - recall: 0.9364 - true_negatives: 1247.8636 - true_positives: 1512.3081\n",
      "For batch number 199 the model has a loss of 0.3910599648952484\n",
      "\u001b[1m199/689\u001b[0m \u001b[32m━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━\u001b[0m \u001b[1m28s\u001b[0m 59ms/step - auc: 0.9260 - binary_accuracy: 0.8661 - false_negatives: 104.9347 - false_positives: 320.8040 - loss: 0.4103 - precision: 0.8251 - recall: 0.9364 - true_negatives: 1254.4221 - true_positives: 1519.8392\n",
      "For batch number 200 the model has a loss of 0.3903551995754242\n",
      "\u001b[1m200/689\u001b[0m \u001b[32m━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━\u001b[0m \u001b[1m28s\u001b[0m 59ms/step - auc: 0.9260 - binary_accuracy: 0.8662 - false_negatives: 105.4200 - false_positives: 322.2250 - loss: 0.4102 - precision: 0.8252 - recall: 0.9364 - true_negatives: 1260.9800 - true_positives: 1527.3750\n",
      "For batch number 201 the model has a loss of 0.39072996377944946\n",
      "\u001b[1m201/689\u001b[0m \u001b[32m━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━\u001b[0m \u001b[1m28s\u001b[0m 59ms/step - auc: 0.9260 - binary_accuracy: 0.8662 - false_negatives: 105.9055 - false_positives: 323.6517 - loss: 0.4101 - precision: 0.8252 - recall: 0.9364 - true_negatives: 1267.5374 - true_positives: 1534.9055\n",
      "For batch number 202 the model has a loss of 0.38953453302383423\n",
      "\u001b[1m202/689\u001b[0m \u001b[32m━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━\u001b[0m \u001b[1m28s\u001b[0m 59ms/step - auc: 0.9260 - binary_accuracy: 0.8662 - false_negatives: 106.3861 - false_positives: 325.0644 - loss: 0.4100 - precision: 0.8252 - recall: 0.9364 - true_negatives: 1274.1188 - true_positives: 1542.4307\n",
      "For batch number 203 the model has a loss of 0.38859832286834717\n",
      "\u001b[1m203/689\u001b[0m \u001b[32m━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━\u001b[0m \u001b[1m28s\u001b[0m 59ms/step - auc: 0.9260 - binary_accuracy: 0.8663 - false_negatives: 106.8670 - false_positives: 326.4630 - loss: 0.4099 - precision: 0.8253 - recall: 0.9364 - true_negatives: 1280.6897 - true_positives: 1549.9803\n",
      "For batch number 204 the model has a loss of 0.3877802789211273\n",
      "\u001b[1m204/689\u001b[0m \u001b[32m━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━\u001b[0m \u001b[1m28s\u001b[0m 58ms/step - auc: 0.9260 - binary_accuracy: 0.8663 - false_negatives: 107.3431 - false_positives: 327.8578 - loss: 0.4098 - precision: 0.8253 - recall: 0.9365 - true_negatives: 1287.2500 - true_positives: 1557.5491\n",
      "For batch number 205 the model has a loss of 0.3873463571071625\n",
      "\u001b[1m205/689\u001b[0m \u001b[32m━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━\u001b[0m \u001b[1m28s\u001b[0m 58ms/step - auc: 0.9261 - binary_accuracy: 0.8664 - false_negatives: 107.8146 - false_positives: 329.2585 - loss: 0.4097 - precision: 0.8254 - recall: 0.9365 - true_negatives: 1293.7952 - true_positives: 1565.1317\n",
      "For batch number 206 the model has a loss of 0.387650728225708\n",
      "\u001b[1m206/689\u001b[0m \u001b[32m━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━\u001b[0m \u001b[1m28s\u001b[0m 58ms/step - auc: 0.9261 - binary_accuracy: 0.8664 - false_negatives: 108.2816 - false_positives: 330.6845 - loss: 0.4096 - precision: 0.8254 - recall: 0.9365 - true_negatives: 1300.3398 - true_positives: 1572.6942\n",
      "For batch number 207 the model has a loss of 0.38793760538101196\n",
      "\u001b[1m207/689\u001b[0m \u001b[32m━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━\u001b[0m \u001b[1m28s\u001b[0m 58ms/step - auc: 0.9261 - binary_accuracy: 0.8665 - false_negatives: 108.7488 - false_positives: 332.1159 - loss: 0.4095 - precision: 0.8255 - recall: 0.9365 - true_negatives: 1306.8744 - true_positives: 1580.2609\n",
      "For batch number 208 the model has a loss of 0.3881857693195343\n",
      "\u001b[1m208/689\u001b[0m \u001b[32m━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━\u001b[0m \u001b[1m28s\u001b[0m 58ms/step - auc: 0.9261 - binary_accuracy: 0.8665 - false_negatives: 109.2115 - false_positives: 333.5769 - loss: 0.4094 - precision: 0.8255 - recall: 0.9365 - true_negatives: 1313.4087 - true_positives: 1587.8029\n",
      "For batch number 209 the model has a loss of 0.38806313276290894\n",
      "\u001b[1m209/689\u001b[0m \u001b[32m━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━\u001b[0m \u001b[1m28s\u001b[0m 58ms/step - auc: 0.9261 - binary_accuracy: 0.8665 - false_negatives: 109.6699 - false_positives: 335.0431 - loss: 0.4093 - precision: 0.8255 - recall: 0.9365 - true_negatives: 1319.9569 - true_positives: 1595.3302\n",
      "For batch number 210 the model has a loss of 0.38752612471580505\n",
      "\u001b[1m210/689\u001b[0m \u001b[32m━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━\u001b[0m \u001b[1m28s\u001b[0m 58ms/step - auc: 0.9261 - binary_accuracy: 0.8666 - false_negatives: 110.1238 - false_positives: 336.5095 - loss: 0.4092 - precision: 0.8256 - recall: 0.9365 - true_negatives: 1326.5048 - true_positives: 1602.8619\n",
      "For batch number 211 the model has a loss of 0.3886995315551758\n",
      "\u001b[1m211/689\u001b[0m \u001b[32m━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━\u001b[0m \u001b[1m27s\u001b[0m 58ms/step - auc: 0.9261 - binary_accuracy: 0.8666 - false_negatives: 110.5782 - false_positives: 337.9858 - loss: 0.4091 - precision: 0.8256 - recall: 0.9365 - true_negatives: 1333.0474 - true_positives: 1610.3887\n",
      "For batch number 212 the model has a loss of 0.3894944190979004\n",
      "\u001b[1m212/689\u001b[0m \u001b[32m━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━\u001b[0m \u001b[1m27s\u001b[0m 58ms/step - auc: 0.9261 - binary_accuracy: 0.8666 - false_negatives: 111.0330 - false_positives: 339.4764 - loss: 0.4090 - precision: 0.8256 - recall: 0.9365 - true_negatives: 1339.5754 - true_positives: 1617.9150\n",
      "For batch number 213 the model has a loss of 0.3897041976451874\n",
      "\u001b[1m213/689\u001b[0m \u001b[32m━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━\u001b[0m \u001b[1m27s\u001b[0m 58ms/step - auc: 0.9261 - binary_accuracy: 0.8666 - false_negatives: 111.4836 - false_positives: 340.9812 - loss: 0.4089 - precision: 0.8256 - recall: 0.9366 - true_negatives: 1346.1080 - true_positives: 1625.4272\n",
      "For batch number 214 the model has a loss of 0.3901996314525604\n",
      "\u001b[1m214/689\u001b[0m \u001b[32m━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━\u001b[0m \u001b[1m27s\u001b[0m 58ms/step - auc: 0.9261 - binary_accuracy: 0.8667 - false_negatives: 111.9346 - false_positives: 342.5140 - loss: 0.4088 - precision: 0.8256 - recall: 0.9366 - true_negatives: 1352.6449 - true_positives: 1632.9065\n",
      "For batch number 215 the model has a loss of 0.39001163840293884\n",
      "\u001b[1m215/689\u001b[0m \u001b[32m━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━\u001b[0m \u001b[1m27s\u001b[0m 58ms/step - auc: 0.9261 - binary_accuracy: 0.8667 - false_negatives: 112.3814 - false_positives: 344.0512 - loss: 0.4087 - precision: 0.8256 - recall: 0.9366 - true_negatives: 1359.1814 - true_positives: 1640.3860\n",
      "For batch number 216 the model has a loss of 0.3908195495605469\n",
      "\u001b[1m216/689\u001b[0m \u001b[32m━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━\u001b[0m \u001b[1m27s\u001b[0m 58ms/step - auc: 0.9260 - binary_accuracy: 0.8667 - false_negatives: 112.8287 - false_positives: 345.5833 - loss: 0.4086 - precision: 0.8257 - recall: 0.9366 - true_negatives: 1365.7177 - true_positives: 1647.8704\n",
      "For batch number 217 the model has a loss of 0.3902208209037781\n",
      "\u001b[1m217/689\u001b[0m \u001b[32m━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━\u001b[0m \u001b[1m27s\u001b[0m 58ms/step - auc: 0.9260 - binary_accuracy: 0.8667 - false_negatives: 113.2719 - false_positives: 347.1060 - loss: 0.4085 - precision: 0.8257 - recall: 0.9366 - true_negatives: 1372.2534 - true_positives: 1655.3687\n",
      "For batch number 218 the model has a loss of 0.38945502042770386\n",
      "\u001b[1m218/689\u001b[0m \u001b[32m━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━\u001b[0m \u001b[1m27s\u001b[0m 58ms/step - auc: 0.9260 - binary_accuracy: 0.8668 - false_negatives: 113.7110 - false_positives: 348.6193 - loss: 0.4084 - precision: 0.8257 - recall: 0.9366 - true_negatives: 1378.8027 - true_positives: 1662.8669\n",
      "For batch number 219 the model has a loss of 0.3890635073184967\n",
      "\u001b[1m219/689\u001b[0m \u001b[32m━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━\u001b[0m \u001b[1m27s\u001b[0m 58ms/step - auc: 0.9260 - binary_accuracy: 0.8668 - false_negatives: 114.1461 - false_positives: 350.1233 - loss: 0.4084 - precision: 0.8257 - recall: 0.9367 - true_negatives: 1385.3562 - true_positives: 1670.3744\n",
      "For batch number 220 the model has a loss of 0.3886116147041321\n",
      "\u001b[1m220/689\u001b[0m \u001b[32m━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━\u001b[0m \u001b[1m27s\u001b[0m 58ms/step - auc: 0.9260 - binary_accuracy: 0.8668 - false_negatives: 114.5773 - false_positives: 351.6227 - loss: 0.4083 - precision: 0.8257 - recall: 0.9367 - true_negatives: 1391.9227 - true_positives: 1677.8773\n",
      "For batch number 221 the model has a loss of 0.38829925656318665\n",
      "\u001b[1m221/689\u001b[0m \u001b[32m━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━\u001b[0m \u001b[1m27s\u001b[0m 58ms/step - auc: 0.9260 - binary_accuracy: 0.8669 - false_negatives: 115.0045 - false_positives: 353.1222 - loss: 0.4082 - precision: 0.8258 - recall: 0.9367 - true_negatives: 1398.4886 - true_positives: 1685.3846\n",
      "For batch number 222 the model has a loss of 0.38832762837409973\n",
      "\u001b[1m222/689\u001b[0m \u001b[32m━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━\u001b[0m \u001b[1m27s\u001b[0m 58ms/step - auc: 0.9260 - binary_accuracy: 0.8669 - false_negatives: 115.4324 - false_positives: 354.6081 - loss: 0.4081 - precision: 0.8258 - recall: 0.9367 - true_negatives: 1405.0496 - true_positives: 1692.9099\n",
      "For batch number 223 the model has a loss of 0.3879031538963318\n",
      "\u001b[1m223/689\u001b[0m \u001b[32m━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━\u001b[0m \u001b[1m27s\u001b[0m 58ms/step - auc: 0.9260 - binary_accuracy: 0.8669 - false_negatives: 115.8565 - false_positives: 356.0942 - loss: 0.4080 - precision: 0.8258 - recall: 0.9367 - true_negatives: 1411.5964 - true_positives: 1700.4529\n",
      "For batch number 224 the model has a loss of 0.3877507746219635\n",
      "\u001b[1m224/689\u001b[0m \u001b[32m━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━\u001b[0m \u001b[1m27s\u001b[0m 58ms/step - auc: 0.9260 - binary_accuracy: 0.8670 - false_negatives: 116.2812 - false_positives: 357.5804 - loss: 0.4079 - precision: 0.8258 - recall: 0.9368 - true_negatives: 1418.1473 - true_positives: 1707.9911\n",
      "For batch number 225 the model has a loss of 0.38849732279777527\n",
      "\u001b[1m225/689\u001b[0m \u001b[32m━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━\u001b[0m \u001b[1m27s\u001b[0m 58ms/step - auc: 0.9260 - binary_accuracy: 0.8670 - false_negatives: 116.7156 - false_positives: 359.0533 - loss: 0.4078 - precision: 0.8258 - recall: 0.9368 - true_negatives: 1424.6934 - true_positives: 1715.5377\n",
      "For batch number 226 the model has a loss of 0.38817426562309265\n",
      "\u001b[1m226/689\u001b[0m \u001b[32m━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━\u001b[0m \u001b[1m27s\u001b[0m 58ms/step - auc: 0.9260 - binary_accuracy: 0.8670 - false_negatives: 117.1504 - false_positives: 360.5221 - loss: 0.4077 - precision: 0.8259 - recall: 0.9368 - true_negatives: 1431.2478 - true_positives: 1723.0796\n",
      "For batch number 227 the model has a loss of 0.3877008557319641\n",
      "\u001b[1m227/689\u001b[0m \u001b[32m━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━\u001b[0m \u001b[1m26s\u001b[0m 58ms/step - auc: 0.9260 - binary_accuracy: 0.8671 - false_negatives: 117.5815 - false_positives: 361.9868 - loss: 0.4076 - precision: 0.8259 - recall: 0.9368 - true_negatives: 1437.8193 - true_positives: 1730.6123\n",
      "For batch number 228 the model has a loss of 0.388396680355072\n",
      "\u001b[1m228/689\u001b[0m \u001b[32m━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━\u001b[0m \u001b[1m26s\u001b[0m 58ms/step - auc: 0.9260 - binary_accuracy: 0.8671 - false_negatives: 118.0088 - false_positives: 363.4737 - loss: 0.4076 - precision: 0.8259 - recall: 0.9368 - true_negatives: 1444.3860 - true_positives: 1738.1316\n",
      "For batch number 229 the model has a loss of 0.38836634159088135\n",
      "\u001b[1m229/689\u001b[0m \u001b[32m━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━\u001b[0m \u001b[1m26s\u001b[0m 58ms/step - auc: 0.9260 - binary_accuracy: 0.8671 - false_negatives: 118.4367 - false_positives: 364.9607 - loss: 0.4075 - precision: 0.8259 - recall: 0.9368 - true_negatives: 1450.9695 - true_positives: 1745.6332\n",
      "For batch number 230 the model has a loss of 0.3888063132762909\n",
      "\u001b[1m230/689\u001b[0m \u001b[32m━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━\u001b[0m \u001b[1m26s\u001b[0m 58ms/step - auc: 0.9260 - binary_accuracy: 0.8672 - false_negatives: 118.8652 - false_positives: 366.4565 - loss: 0.4074 - precision: 0.8260 - recall: 0.9369 - true_negatives: 1457.5565 - true_positives: 1753.1217\n",
      "For batch number 231 the model has a loss of 0.38832467794418335\n",
      "\u001b[1m231/689\u001b[0m \u001b[32m━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━\u001b[0m \u001b[1m26s\u001b[0m 58ms/step - auc: 0.9260 - binary_accuracy: 0.8672 - false_negatives: 119.2944 - false_positives: 367.9481 - loss: 0.4073 - precision: 0.8260 - recall: 0.9369 - true_negatives: 1464.1472 - true_positives: 1760.6104\n",
      "For batch number 232 the model has a loss of 0.3886339068412781\n",
      "\u001b[1m232/689\u001b[0m \u001b[32m━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━\u001b[0m \u001b[1m26s\u001b[0m 58ms/step - auc: 0.9260 - binary_accuracy: 0.8672 - false_negatives: 119.7371 - false_positives: 369.4310 - loss: 0.4072 - precision: 0.8260 - recall: 0.9369 - true_negatives: 1470.7284 - true_positives: 1768.1034\n",
      "For batch number 233 the model has a loss of 0.3886803090572357\n",
      "\u001b[1m233/689\u001b[0m \u001b[32m━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━\u001b[0m \u001b[1m26s\u001b[0m 58ms/step - auc: 0.9260 - binary_accuracy: 0.8673 - false_negatives: 120.1845 - false_positives: 370.9142 - loss: 0.4072 - precision: 0.8260 - recall: 0.9369 - true_negatives: 1477.3047 - true_positives: 1775.5966\n",
      "For batch number 234 the model has a loss of 0.3886931538581848\n",
      "\u001b[1m234/689\u001b[0m \u001b[32m━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━\u001b[0m \u001b[1m26s\u001b[0m 58ms/step - auc: 0.9260 - binary_accuracy: 0.8673 - false_negatives: 120.6367 - false_positives: 372.3932 - loss: 0.4071 - precision: 0.8260 - recall: 0.9369 - true_negatives: 1483.8846 - true_positives: 1783.0854\n",
      "For batch number 235 the model has a loss of 0.3891272246837616\n",
      "\u001b[1m235/689\u001b[0m \u001b[32m━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━\u001b[0m \u001b[1m26s\u001b[0m 58ms/step - auc: 0.9260 - binary_accuracy: 0.8673 - false_negatives: 121.0936 - false_positives: 373.8681 - loss: 0.4070 - precision: 0.8261 - recall: 0.9369 - true_negatives: 1490.4681 - true_positives: 1790.5702\n",
      "For batch number 236 the model has a loss of 0.3885214030742645\n",
      "\u001b[1m236/689\u001b[0m \u001b[32m━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━\u001b[0m \u001b[1m26s\u001b[0m 58ms/step - auc: 0.9260 - binary_accuracy: 0.8673 - false_negatives: 121.5466 - false_positives: 375.3390 - loss: 0.4069 - precision: 0.8261 - recall: 0.9369 - true_negatives: 1497.0466 - true_positives: 1798.0677\n",
      "For batch number 237 the model has a loss of 0.3878241777420044\n",
      "\u001b[1m237/689\u001b[0m \u001b[32m━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━\u001b[0m \u001b[1m26s\u001b[0m 58ms/step - auc: 0.9260 - binary_accuracy: 0.8674 - false_negatives: 121.9958 - false_positives: 376.8059 - loss: 0.4068 - precision: 0.8261 - recall: 0.9370 - true_negatives: 1503.6329 - true_positives: 1805.5654\n",
      "For batch number 238 the model has a loss of 0.3873232305049896\n",
      "\u001b[1m238/689\u001b[0m \u001b[32m━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━\u001b[0m \u001b[1m26s\u001b[0m 58ms/step - auc: 0.9260 - binary_accuracy: 0.8674 - false_negatives: 122.4412 - false_positives: 378.2689 - loss: 0.4068 - precision: 0.8261 - recall: 0.9370 - true_negatives: 1510.2227 - true_positives: 1813.0673\n",
      "For batch number 239 the model has a loss of 0.3866454064846039\n",
      "\u001b[1m239/689\u001b[0m \u001b[32m━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━\u001b[0m \u001b[1m26s\u001b[0m 58ms/step - auc: 0.9260 - binary_accuracy: 0.8674 - false_negatives: 122.8828 - false_positives: 379.7238 - loss: 0.4067 - precision: 0.8262 - recall: 0.9370 - true_negatives: 1516.8242 - true_positives: 1820.5691\n",
      "For batch number 240 the model has a loss of 0.38630154728889465\n",
      "\u001b[1m240/689\u001b[0m \u001b[32m━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━\u001b[0m \u001b[1m26s\u001b[0m 58ms/step - auc: 0.9260 - binary_accuracy: 0.8675 - false_negatives: 123.3208 - false_positives: 381.1792 - loss: 0.4066 - precision: 0.8262 - recall: 0.9370 - true_negatives: 1523.4250 - true_positives: 1828.0750\n",
      "For batch number 241 the model has a loss of 0.3853459358215332\n",
      "\u001b[1m241/689\u001b[0m \u001b[32m━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━\u001b[0m \u001b[1m26s\u001b[0m 58ms/step - auc: 0.9260 - binary_accuracy: 0.8675 - false_negatives: 123.7552 - false_positives: 382.6307 - loss: 0.4065 - precision: 0.8262 - recall: 0.9370 - true_negatives: 1530.0332 - true_positives: 1835.5809\n",
      "For batch number 242 the model has a loss of 0.38503846526145935\n",
      "\u001b[1m242/689\u001b[0m \u001b[32m━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━\u001b[0m \u001b[1m26s\u001b[0m 58ms/step - auc: 0.9260 - binary_accuracy: 0.8675 - false_negatives: 124.1901 - false_positives: 384.0868 - loss: 0.4064 - precision: 0.8262 - recall: 0.9370 - true_negatives: 1536.6447 - true_positives: 1843.0785\n",
      "For batch number 243 the model has a loss of 0.3851008117198944\n",
      "\u001b[1m243/689\u001b[0m \u001b[32m━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━\u001b[0m \u001b[1m26s\u001b[0m 58ms/step - auc: 0.9260 - binary_accuracy: 0.8676 - false_negatives: 124.6296 - false_positives: 385.5350 - loss: 0.4063 - precision: 0.8263 - recall: 0.9371 - true_negatives: 1543.2428 - true_positives: 1850.5927\n",
      "For batch number 244 the model has a loss of 0.3845474123954773\n",
      "\u001b[1m244/689\u001b[0m \u001b[32m━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━\u001b[0m \u001b[1m25s\u001b[0m 58ms/step - auc: 0.9260 - binary_accuracy: 0.8676 - false_negatives: 125.0656 - false_positives: 386.9795 - loss: 0.4062 - precision: 0.8263 - recall: 0.9371 - true_negatives: 1549.8279 - true_positives: 1858.1271\n",
      "For batch number 245 the model has a loss of 0.38411110639572144\n",
      "\u001b[1m245/689\u001b[0m \u001b[32m━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━\u001b[0m \u001b[1m25s\u001b[0m 58ms/step - auc: 0.9260 - binary_accuracy: 0.8676 - false_negatives: 125.4980 - false_positives: 388.4204 - loss: 0.4061 - precision: 0.8263 - recall: 0.9371 - true_negatives: 1556.4326 - true_positives: 1865.6489\n",
      "For batch number 246 the model has a loss of 0.3837600350379944\n",
      "\u001b[1m246/689\u001b[0m \u001b[32m━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━\u001b[0m \u001b[1m25s\u001b[0m 58ms/step - auc: 0.9260 - binary_accuracy: 0.8677 - false_negatives: 125.9309 - false_positives: 389.8577 - loss: 0.4061 - precision: 0.8264 - recall: 0.9371 - true_negatives: 1563.0366 - true_positives: 1873.1748\n",
      "For batch number 247 the model has a loss of 0.3844999074935913\n",
      "\u001b[1m247/689\u001b[0m \u001b[32m━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━\u001b[0m \u001b[1m25s\u001b[0m 58ms/step - auc: 0.9260 - binary_accuracy: 0.8677 - false_negatives: 126.3684 - false_positives: 391.2915 - loss: 0.4060 - precision: 0.8264 - recall: 0.9371 - true_negatives: 1569.6356 - true_positives: 1880.7045\n",
      "For batch number 248 the model has a loss of 0.38412168622016907\n",
      "\u001b[1m248/689\u001b[0m \u001b[32m━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━\u001b[0m \u001b[1m25s\u001b[0m 58ms/step - auc: 0.9260 - binary_accuracy: 0.8677 - false_negatives: 126.8024 - false_positives: 392.7258 - loss: 0.4059 - precision: 0.8264 - recall: 0.9371 - true_negatives: 1576.2419 - true_positives: 1888.2299\n",
      "For batch number 249 the model has a loss of 0.38490790128707886\n",
      "\u001b[1m249/689\u001b[0m \u001b[32m━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━\u001b[0m \u001b[1m25s\u001b[0m 58ms/step - auc: 0.9260 - binary_accuracy: 0.8678 - false_negatives: 127.2369 - false_positives: 394.1687 - loss: 0.4058 - precision: 0.8264 - recall: 0.9372 - true_negatives: 1582.8434 - true_positives: 1895.7510\n",
      "For batch number 250 the model has a loss of 0.3843815326690674\n",
      "\u001b[1m250/689\u001b[0m \u001b[32m━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━\u001b[0m \u001b[1m25s\u001b[0m 58ms/step - auc: 0.9260 - binary_accuracy: 0.8678 - false_negatives: 127.6720 - false_positives: 395.6120 - loss: 0.4057 - precision: 0.8265 - recall: 0.9372 - true_negatives: 1589.4440 - true_positives: 1903.2720\n",
      "For batch number 251 the model has a loss of 0.38527461886405945\n",
      "\u001b[1m251/689\u001b[0m \u001b[32m━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━\u001b[0m \u001b[1m25s\u001b[0m 58ms/step - auc: 0.9260 - binary_accuracy: 0.8679 - false_negatives: 128.1116 - false_positives: 397.0478 - loss: 0.4056 - precision: 0.8265 - recall: 0.9372 - true_negatives: 1596.0278 - true_positives: 1910.8127\n",
      "For batch number 252 the model has a loss of 0.38502299785614014\n",
      "\u001b[1m252/689\u001b[0m \u001b[32m━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━\u001b[0m \u001b[1m25s\u001b[0m 58ms/step - auc: 0.9260 - binary_accuracy: 0.8679 - false_negatives: 128.5476 - false_positives: 398.4841 - loss: 0.4055 - precision: 0.8265 - recall: 0.9372 - true_negatives: 1602.6111 - true_positives: 1918.3572\n",
      "For batch number 253 the model has a loss of 0.38453471660614014\n",
      "\u001b[1m253/689\u001b[0m \u001b[32m━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━\u001b[0m \u001b[1m25s\u001b[0m 58ms/step - auc: 0.9261 - binary_accuracy: 0.8679 - false_negatives: 128.9802 - false_positives: 399.9249 - loss: 0.4055 - precision: 0.8266 - recall: 0.9372 - true_negatives: 1609.2095 - true_positives: 1925.8854\n",
      "For batch number 254 the model has a loss of 0.38563498854637146\n",
      "\u001b[1m254/689\u001b[0m \u001b[32m━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━\u001b[0m \u001b[1m25s\u001b[0m 58ms/step - auc: 0.9261 - binary_accuracy: 0.8679 - false_negatives: 129.4134 - false_positives: 401.3858 - loss: 0.4054 - precision: 0.8266 - recall: 0.9372 - true_negatives: 1615.8188 - true_positives: 1933.3818\n",
      "For batch number 255 the model has a loss of 0.38548582792282104\n",
      "\u001b[1m255/689\u001b[0m \u001b[32m━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━\u001b[0m \u001b[1m25s\u001b[0m 58ms/step - auc: 0.9261 - binary_accuracy: 0.8680 - false_negatives: 129.8510 - false_positives: 402.8392 - loss: 0.4053 - precision: 0.8266 - recall: 0.9372 - true_negatives: 1622.4156 - true_positives: 1940.8942\n",
      "For batch number 256 the model has a loss of 0.38499897718429565\n",
      "\u001b[1m256/689\u001b[0m \u001b[32m━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━\u001b[0m \u001b[1m25s\u001b[0m 58ms/step - auc: 0.9261 - binary_accuracy: 0.8680 - false_negatives: 130.2891 - false_positives: 404.2852 - loss: 0.4052 - precision: 0.8266 - recall: 0.9373 - true_negatives: 1629.0117 - true_positives: 1948.4141\n",
      "For batch number 257 the model has a loss of 0.3848087191581726\n",
      "\u001b[1m257/689\u001b[0m \u001b[32m━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━\u001b[0m \u001b[1m25s\u001b[0m 58ms/step - auc: 0.9261 - binary_accuracy: 0.8680 - false_negatives: 130.7315 - false_positives: 405.7354 - loss: 0.4051 - precision: 0.8267 - recall: 0.9373 - true_negatives: 1635.6071 - true_positives: 1955.9260\n",
      "For batch number 258 the model has a loss of 0.38435450196266174\n",
      "\u001b[1m258/689\u001b[0m \u001b[32m━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━\u001b[0m \u001b[1m25s\u001b[0m 58ms/step - auc: 0.9261 - binary_accuracy: 0.8681 - false_negatives: 131.1744 - false_positives: 407.1860 - loss: 0.4051 - precision: 0.8267 - recall: 0.9373 - true_negatives: 1642.2015 - true_positives: 1963.4380\n",
      "For batch number 259 the model has a loss of 0.3837520182132721\n",
      "\u001b[1m259/689\u001b[0m \u001b[32m━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━\u001b[0m \u001b[1m25s\u001b[0m 58ms/step - auc: 0.9261 - binary_accuracy: 0.8681 - false_negatives: 131.6216 - false_positives: 408.6332 - loss: 0.4050 - precision: 0.8267 - recall: 0.9373 - true_negatives: 1648.7876 - true_positives: 1970.9575\n",
      "For batch number 260 the model has a loss of 0.3831527829170227\n",
      "\u001b[1m260/689\u001b[0m \u001b[32m━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━\u001b[0m \u001b[1m24s\u001b[0m 58ms/step - auc: 0.9261 - binary_accuracy: 0.8681 - false_negatives: 132.0692 - false_positives: 410.0731 - loss: 0.4049 - precision: 0.8267 - recall: 0.9373 - true_negatives: 1655.3846 - true_positives: 1978.4730\n",
      "For batch number 261 the model has a loss of 0.3828223943710327\n",
      "\u001b[1m261/689\u001b[0m \u001b[32m━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━\u001b[0m \u001b[1m24s\u001b[0m 58ms/step - auc: 0.9261 - binary_accuracy: 0.8682 - false_negatives: 132.5172 - false_positives: 411.5057 - loss: 0.4048 - precision: 0.8268 - recall: 0.9373 - true_negatives: 1661.9771 - true_positives: 1986.0000\n",
      "For batch number 262 the model has a loss of 0.38292258977890015\n",
      "\u001b[1m262/689\u001b[0m \u001b[32m━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━\u001b[0m \u001b[1m24s\u001b[0m 58ms/step - auc: 0.9261 - binary_accuracy: 0.8682 - false_negatives: 132.9657 - false_positives: 412.9504 - loss: 0.4047 - precision: 0.8268 - recall: 0.9373 - true_negatives: 1668.5725 - true_positives: 1993.5115\n",
      "For batch number 263 the model has a loss of 0.3826207220554352\n",
      "\u001b[1m263/689\u001b[0m \u001b[32m━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━\u001b[0m \u001b[1m24s\u001b[0m 58ms/step - auc: 0.9261 - binary_accuracy: 0.8682 - false_negatives: 133.4144 - false_positives: 414.3840 - loss: 0.4046 - precision: 0.8268 - recall: 0.9374 - true_negatives: 1675.1863 - true_positives: 2001.0153\n",
      "For batch number 264 the model has a loss of 0.38242006301879883\n",
      "\u001b[1m264/689\u001b[0m \u001b[32m━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━\u001b[0m \u001b[1m24s\u001b[0m 58ms/step - auc: 0.9261 - binary_accuracy: 0.8683 - false_negatives: 133.8598 - false_positives: 415.8182 - loss: 0.4046 - precision: 0.8268 - recall: 0.9374 - true_negatives: 1681.8030 - true_positives: 2008.5189\n",
      "For batch number 265 the model has a loss of 0.3815590441226959\n",
      "\u001b[1m265/689\u001b[0m \u001b[32m━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━\u001b[0m \u001b[1m24s\u001b[0m 58ms/step - auc: 0.9261 - binary_accuracy: 0.8683 - false_negatives: 134.3057 - false_positives: 417.2415 - loss: 0.4045 - precision: 0.8269 - recall: 0.9374 - true_negatives: 1688.4302 - true_positives: 2016.0226\n",
      "For batch number 266 the model has a loss of 0.3822382688522339\n",
      "\u001b[1m266/689\u001b[0m \u001b[32m━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━\u001b[0m \u001b[1m24s\u001b[0m 58ms/step - auc: 0.9261 - binary_accuracy: 0.8683 - false_negatives: 134.7594 - false_positives: 418.6617 - loss: 0.4044 - precision: 0.8269 - recall: 0.9374 - true_negatives: 1695.0564 - true_positives: 2023.5226\n",
      "For batch number 267 the model has a loss of 0.38159334659576416\n",
      "\u001b[1m267/689\u001b[0m \u001b[32m━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━\u001b[0m \u001b[1m24s\u001b[0m 58ms/step - auc: 0.9261 - binary_accuracy: 0.8684 - false_negatives: 135.2097 - false_positives: 420.0749 - loss: 0.4043 - precision: 0.8269 - recall: 0.9374 - true_negatives: 1701.6816 - true_positives: 2031.0337\n",
      "For batch number 268 the model has a loss of 0.3811708688735962\n",
      "\u001b[1m268/689\u001b[0m \u001b[32m━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━\u001b[0m \u001b[1m24s\u001b[0m 58ms/step - auc: 0.9261 - binary_accuracy: 0.8684 - false_negatives: 135.6567 - false_positives: 421.4776 - loss: 0.4042 - precision: 0.8270 - recall: 0.9374 - true_negatives: 1708.3135 - true_positives: 2038.5522\n",
      "For batch number 269 the model has a loss of 0.3816696107387543\n",
      "\u001b[1m269/689\u001b[0m \u001b[32m━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━\u001b[0m \u001b[1m24s\u001b[0m 58ms/step - auc: 0.9261 - binary_accuracy: 0.8684 - false_negatives: 136.1078 - false_positives: 422.8810 - loss: 0.4041 - precision: 0.8270 - recall: 0.9374 - true_negatives: 1714.9517 - true_positives: 2046.0594\n",
      "For batch number 270 the model has a loss of 0.38156047463417053\n",
      "\u001b[1m270/689\u001b[0m \u001b[32m━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━\u001b[0m \u001b[1m24s\u001b[0m 58ms/step - auc: 0.9261 - binary_accuracy: 0.8685 - false_negatives: 136.5593 - false_positives: 424.2852 - loss: 0.4041 - precision: 0.8270 - recall: 0.9374 - true_negatives: 1721.5889 - true_positives: 2053.5667\n",
      "For batch number 271 the model has a loss of 0.38234013319015503\n",
      "\u001b[1m271/689\u001b[0m \u001b[32m━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━\u001b[0m \u001b[1m24s\u001b[0m 58ms/step - auc: 0.9261 - binary_accuracy: 0.8685 - false_negatives: 137.0184 - false_positives: 425.6937 - loss: 0.4040 - precision: 0.8271 - recall: 0.9374 - true_negatives: 1728.2177 - true_positives: 2061.0701\n",
      "For batch number 272 the model has a loss of 0.38184696435928345\n",
      "\u001b[1m272/689\u001b[0m \u001b[32m━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━\u001b[0m \u001b[1m24s\u001b[0m 58ms/step - auc: 0.9261 - binary_accuracy: 0.8685 - false_negatives: 137.4743 - false_positives: 427.0956 - loss: 0.4039 - precision: 0.8271 - recall: 0.9375 - true_negatives: 1734.8419 - true_positives: 2068.5881\n",
      "For batch number 273 the model has a loss of 0.381570965051651\n",
      "\u001b[1m273/689\u001b[0m \u001b[32m━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━\u001b[0m \u001b[1m24s\u001b[0m 58ms/step - auc: 0.9262 - binary_accuracy: 0.8685 - false_negatives: 137.9304 - false_positives: 428.4945 - loss: 0.4038 - precision: 0.8271 - recall: 0.9375 - true_negatives: 1741.4652 - true_positives: 2076.1099\n",
      "For batch number 274 the model has a loss of 0.381148099899292\n",
      "\u001b[1m274/689\u001b[0m \u001b[32m━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━\u001b[0m \u001b[1m24s\u001b[0m 58ms/step - auc: 0.9262 - binary_accuracy: 0.8686 - false_negatives: 138.3832 - false_positives: 429.8905 - loss: 0.4037 - precision: 0.8272 - recall: 0.9375 - true_negatives: 1748.0985 - true_positives: 2083.6277\n",
      "For batch number 275 the model has a loss of 0.38215863704681396\n",
      "\u001b[1m275/689\u001b[0m \u001b[32m━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━\u001b[0m \u001b[1m24s\u001b[0m 58ms/step - auc: 0.9262 - binary_accuracy: 0.8686 - false_negatives: 138.8364 - false_positives: 431.2873 - loss: 0.4037 - precision: 0.8272 - recall: 0.9375 - true_negatives: 1754.7200 - true_positives: 2091.1562\n",
      "For batch number 276 the model has a loss of 0.3824274241924286\n",
      "\u001b[1m276/689\u001b[0m \u001b[32m━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━\u001b[0m \u001b[1m24s\u001b[0m 58ms/step - auc: 0.9262 - binary_accuracy: 0.8686 - false_negatives: 139.2899 - false_positives: 432.6884 - loss: 0.4036 - precision: 0.8272 - recall: 0.9375 - true_negatives: 1761.3551 - true_positives: 2098.6667\n",
      "For batch number 277 the model has a loss of 0.38165411353111267\n",
      "\u001b[1m277/689\u001b[0m \u001b[32m━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━\u001b[0m \u001b[1m23s\u001b[0m 58ms/step - auc: 0.9262 - binary_accuracy: 0.8687 - false_negatives: 139.7401 - false_positives: 434.0830 - loss: 0.4035 - precision: 0.8273 - recall: 0.9375 - true_negatives: 1767.9963 - true_positives: 2106.1804\n",
      "For batch number 278 the model has a loss of 0.3823705017566681\n",
      "\u001b[1m278/689\u001b[0m \u001b[32m━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━\u001b[0m \u001b[1m23s\u001b[0m 58ms/step - auc: 0.9262 - binary_accuracy: 0.8687 - false_negatives: 140.1906 - false_positives: 435.4748 - loss: 0.4034 - precision: 0.8273 - recall: 0.9375 - true_negatives: 1774.6367 - true_positives: 2113.6978\n",
      "For batch number 279 the model has a loss of 0.38174521923065186\n",
      "\u001b[1m279/689\u001b[0m \u001b[32m━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━\u001b[0m \u001b[1m23s\u001b[0m 58ms/step - auc: 0.9262 - binary_accuracy: 0.8687 - false_negatives: 140.6380 - false_positives: 436.8602 - loss: 0.4033 - precision: 0.8273 - recall: 0.9375 - true_negatives: 1781.2832 - true_positives: 2121.2188\n",
      "For batch number 280 the model has a loss of 0.3832046687602997\n",
      "\u001b[1m280/689\u001b[0m \u001b[32m━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━\u001b[0m \u001b[1m23s\u001b[0m 58ms/step - auc: 0.9262 - binary_accuracy: 0.8688 - false_negatives: 141.0857 - false_positives: 438.2571 - loss: 0.4033 - precision: 0.8274 - recall: 0.9375 - true_negatives: 1787.9250 - true_positives: 2128.7322\n",
      "For batch number 281 the model has a loss of 0.3831128776073456\n",
      "\u001b[1m281/689\u001b[0m \u001b[32m━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━\u001b[0m \u001b[1m23s\u001b[0m 58ms/step - auc: 0.9262 - binary_accuracy: 0.8688 - false_negatives: 141.5338 - false_positives: 439.6619 - loss: 0.4032 - precision: 0.8274 - recall: 0.9376 - true_negatives: 1794.5693 - true_positives: 2136.2349\n",
      "For batch number 282 the model has a loss of 0.3837031126022339\n",
      "\u001b[1m282/689\u001b[0m \u001b[32m━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━\u001b[0m \u001b[1m23s\u001b[0m 58ms/step - auc: 0.9262 - binary_accuracy: 0.8688 - false_negatives: 142.0000 - false_positives: 441.0709 - loss: 0.4031 - precision: 0.8274 - recall: 0.9376 - true_negatives: 1801.2057 - true_positives: 2143.7234\n",
      "For batch number 283 the model has a loss of 0.3829394280910492\n",
      "\u001b[1m283/689\u001b[0m \u001b[32m━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━\u001b[0m \u001b[1m23s\u001b[0m 58ms/step - auc: 0.9262 - binary_accuracy: 0.8689 - false_negatives: 142.4629 - false_positives: 442.4806 - loss: 0.4031 - precision: 0.8274 - recall: 0.9376 - true_negatives: 1807.8374 - true_positives: 2151.2190\n",
      "For batch number 284 the model has a loss of 0.3833039700984955\n",
      "\u001b[1m284/689\u001b[0m \u001b[32m━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━\u001b[0m \u001b[1m23s\u001b[0m 58ms/step - auc: 0.9262 - binary_accuracy: 0.8689 - false_negatives: 142.9331 - false_positives: 443.8838 - loss: 0.4030 - precision: 0.8275 - recall: 0.9376 - true_negatives: 1814.4648 - true_positives: 2158.7183\n",
      "For batch number 285 the model has a loss of 0.38357874751091003\n",
      "\u001b[1m285/689\u001b[0m \u001b[32m━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━\u001b[0m \u001b[1m23s\u001b[0m 58ms/step - auc: 0.9262 - binary_accuracy: 0.8689 - false_negatives: 143.4105 - false_positives: 445.2982 - loss: 0.4029 - precision: 0.8275 - recall: 0.9376 - true_negatives: 1821.0912 - true_positives: 2166.2000\n",
      "For batch number 286 the model has a loss of 0.3833232522010803\n",
      "\u001b[1m286/689\u001b[0m \u001b[32m━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━\u001b[0m \u001b[1m23s\u001b[0m 58ms/step - auc: 0.9262 - binary_accuracy: 0.8690 - false_negatives: 143.8846 - false_positives: 446.7098 - loss: 0.4029 - precision: 0.8275 - recall: 0.9376 - true_negatives: 1827.7203 - true_positives: 2173.6853\n",
      "For batch number 287 the model has a loss of 0.38300734758377075\n",
      "\u001b[1m287/689\u001b[0m \u001b[32m━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━\u001b[0m \u001b[1m23s\u001b[0m 58ms/step - auc: 0.9262 - binary_accuracy: 0.8690 - false_negatives: 144.3554 - false_positives: 448.1289 - loss: 0.4028 - precision: 0.8276 - recall: 0.9376 - true_negatives: 1834.3414 - true_positives: 2181.1743\n",
      "For batch number 288 the model has a loss of 0.3825848698616028\n",
      "\u001b[1m288/689\u001b[0m \u001b[32m━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━\u001b[0m \u001b[1m23s\u001b[0m 58ms/step - auc: 0.9262 - binary_accuracy: 0.8690 - false_negatives: 144.8299 - false_positives: 449.5451 - loss: 0.4027 - precision: 0.8276 - recall: 0.9376 - true_negatives: 1840.9584 - true_positives: 2188.6667\n",
      "For batch number 289 the model has a loss of 0.3819665312767029\n",
      "\u001b[1m289/689\u001b[0m \u001b[32m━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━\u001b[0m \u001b[1m23s\u001b[0m 58ms/step - auc: 0.9262 - binary_accuracy: 0.8690 - false_negatives: 145.3010 - false_positives: 450.9619 - loss: 0.4026 - precision: 0.8276 - recall: 0.9376 - true_negatives: 1847.5813 - true_positives: 2196.1558\n",
      "For batch number 290 the model has a loss of 0.3818633258342743\n",
      "\u001b[1m290/689\u001b[0m \u001b[32m━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━\u001b[0m \u001b[1m23s\u001b[0m 58ms/step - auc: 0.9262 - binary_accuracy: 0.8691 - false_negatives: 145.7724 - false_positives: 452.3828 - loss: 0.4026 - precision: 0.8276 - recall: 0.9376 - true_negatives: 1854.2207 - true_positives: 2203.6240\n",
      "For batch number 291 the model has a loss of 0.38209274411201477\n",
      "\u001b[1m291/689\u001b[0m \u001b[32m━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━\u001b[0m \u001b[1m23s\u001b[0m 58ms/step - auc: 0.9262 - binary_accuracy: 0.8691 - false_negatives: 146.2474 - false_positives: 453.8076 - loss: 0.4025 - precision: 0.8277 - recall: 0.9376 - true_negatives: 1860.8625 - true_positives: 2211.0825\n",
      "For batch number 292 the model has a loss of 0.38181495666503906\n",
      "\u001b[1m292/689\u001b[0m \u001b[32m━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━\u001b[0m \u001b[1m23s\u001b[0m 58ms/step - auc: 0.9262 - binary_accuracy: 0.8691 - false_negatives: 146.7226 - false_positives: 455.2295 - loss: 0.4024 - precision: 0.8277 - recall: 0.9376 - true_negatives: 1867.5034 - true_positives: 2218.5444\n",
      "For batch number 293 the model has a loss of 0.3818264305591583\n",
      "\u001b[1m293/689\u001b[0m \u001b[32m━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━\u001b[0m \u001b[1m23s\u001b[0m 58ms/step - auc: 0.9262 - binary_accuracy: 0.8691 - false_negatives: 147.1980 - false_positives: 456.6451 - loss: 0.4024 - precision: 0.8277 - recall: 0.9376 - true_negatives: 1874.1365 - true_positives: 2226.0205\n",
      "For batch number 294 the model has a loss of 0.3821389973163605\n",
      "\u001b[1m294/689\u001b[0m \u001b[32m━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━\u001b[0m \u001b[1m22s\u001b[0m 58ms/step - auc: 0.9262 - binary_accuracy: 0.8692 - false_negatives: 147.6769 - false_positives: 458.0680 - loss: 0.4023 - precision: 0.8277 - recall: 0.9376 - true_negatives: 1880.7585 - true_positives: 2233.4966\n",
      "For batch number 295 the model has a loss of 0.38258543610572815\n",
      "\u001b[1m295/689\u001b[0m \u001b[32m━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━\u001b[0m \u001b[1m22s\u001b[0m 58ms/step - auc: 0.9262 - binary_accuracy: 0.8692 - false_negatives: 148.1593 - false_positives: 459.4983 - loss: 0.4022 - precision: 0.8277 - recall: 0.9376 - true_negatives: 1887.3865 - true_positives: 2240.9558\n",
      "For batch number 296 the model has a loss of 0.3828347325325012\n",
      "\u001b[1m296/689\u001b[0m \u001b[32m━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━\u001b[0m \u001b[1m22s\u001b[0m 58ms/step - auc: 0.9262 - binary_accuracy: 0.8692 - false_negatives: 148.6419 - false_positives: 460.9358 - loss: 0.4022 - precision: 0.8278 - recall: 0.9376 - true_negatives: 1894.0067 - true_positives: 2248.4155\n",
      "For batch number 297 the model has a loss of 0.3836217224597931\n",
      "\u001b[1m297/689\u001b[0m \u001b[32m━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━\u001b[0m \u001b[1m22s\u001b[0m 58ms/step - auc: 0.9262 - binary_accuracy: 0.8692 - false_negatives: 149.1279 - false_positives: 462.3737 - loss: 0.4021 - precision: 0.8278 - recall: 0.9376 - true_negatives: 1900.6296 - true_positives: 2255.8687\n",
      "For batch number 298 the model has a loss of 0.38311195373535156\n",
      "\u001b[1m298/689\u001b[0m \u001b[32m━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━\u001b[0m \u001b[1m22s\u001b[0m 58ms/step - auc: 0.9262 - binary_accuracy: 0.8693 - false_negatives: 149.6107 - false_positives: 463.8054 - loss: 0.4020 - precision: 0.8278 - recall: 0.9376 - true_negatives: 1907.2617 - true_positives: 2263.3223\n",
      "For batch number 299 the model has a loss of 0.3826393485069275\n",
      "\u001b[1m299/689\u001b[0m \u001b[32m━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━\u001b[0m \u001b[1m22s\u001b[0m 58ms/step - auc: 0.9262 - binary_accuracy: 0.8693 - false_negatives: 150.0936 - false_positives: 465.2274 - loss: 0.4020 - precision: 0.8278 - recall: 0.9376 - true_negatives: 1913.8896 - true_positives: 2270.7893\n",
      "For batch number 300 the model has a loss of 0.38226377964019775\n",
      "\u001b[1m300/689\u001b[0m \u001b[32m━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━\u001b[0m \u001b[1m22s\u001b[0m 58ms/step - auc: 0.9262 - binary_accuracy: 0.8693 - false_negatives: 150.5733 - false_positives: 466.6433 - loss: 0.4019 - precision: 0.8279 - recall: 0.9376 - true_negatives: 1920.5133 - true_positives: 2278.2700\n",
      "For batch number 301 the model has a loss of 0.3816894292831421\n",
      "\u001b[1m301/689\u001b[0m \u001b[32m━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━\u001b[0m \u001b[1m22s\u001b[0m 58ms/step - auc: 0.9262 - binary_accuracy: 0.8693 - false_negatives: 151.0498 - false_positives: 468.0565 - loss: 0.4018 - precision: 0.8279 - recall: 0.9377 - true_negatives: 1927.1395 - true_positives: 2285.7542\n",
      "For batch number 302 the model has a loss of 0.3810168504714966\n",
      "\u001b[1m302/689\u001b[0m \u001b[32m━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━\u001b[0m \u001b[1m22s\u001b[0m 58ms/step - auc: 0.9262 - binary_accuracy: 0.8694 - false_negatives: 151.5232 - false_positives: 469.4603 - loss: 0.4018 - precision: 0.8279 - recall: 0.9377 - true_negatives: 1933.7682 - true_positives: 2293.2483\n",
      "For batch number 303 the model has a loss of 0.3804819583892822\n",
      "\u001b[1m303/689\u001b[0m \u001b[32m━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━\u001b[0m \u001b[1m22s\u001b[0m 58ms/step - auc: 0.9262 - binary_accuracy: 0.8694 - false_negatives: 151.9934 - false_positives: 470.8581 - loss: 0.4017 - precision: 0.8279 - recall: 0.9377 - true_negatives: 1940.4126 - true_positives: 2300.7361\n",
      "For batch number 304 the model has a loss of 0.38010987639427185\n",
      "\u001b[1m304/689\u001b[0m \u001b[32m━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━\u001b[0m \u001b[1m22s\u001b[0m 58ms/step - auc: 0.9262 - binary_accuracy: 0.8694 - false_negatives: 152.4605 - false_positives: 472.2566 - loss: 0.4016 - precision: 0.8280 - recall: 0.9377 - true_negatives: 1947.0526 - true_positives: 2308.2302\n",
      "For batch number 305 the model has a loss of 0.3796828091144562\n",
      "\u001b[1m305/689\u001b[0m \u001b[32m━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━\u001b[0m \u001b[1m22s\u001b[0m 58ms/step - auc: 0.9263 - binary_accuracy: 0.8694 - false_negatives: 152.9246 - false_positives: 473.6525 - loss: 0.4015 - precision: 0.8280 - recall: 0.9377 - true_negatives: 1953.6885 - true_positives: 2315.7344\n",
      "For batch number 306 the model has a loss of 0.3800159990787506\n",
      "\u001b[1m306/689\u001b[0m \u001b[32m━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━\u001b[0m \u001b[1m22s\u001b[0m 58ms/step - auc: 0.9263 - binary_accuracy: 0.8695 - false_negatives: 153.3856 - false_positives: 475.0686 - loss: 0.4015 - precision: 0.8280 - recall: 0.9377 - true_negatives: 1960.3334 - true_positives: 2323.2124\n",
      "For batch number 307 the model has a loss of 0.37991863489151\n",
      "\u001b[1m307/689\u001b[0m \u001b[32m━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━\u001b[0m \u001b[1m22s\u001b[0m 58ms/step - auc: 0.9263 - binary_accuracy: 0.8695 - false_negatives: 153.8436 - false_positives: 476.4886 - loss: 0.4014 - precision: 0.8280 - recall: 0.9377 - true_negatives: 1966.9869 - true_positives: 2330.6807\n",
      "For batch number 308 the model has a loss of 0.3801276385784149\n",
      "\u001b[1m308/689\u001b[0m \u001b[32m━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━\u001b[0m \u001b[1m22s\u001b[0m 58ms/step - auc: 0.9263 - binary_accuracy: 0.8695 - false_negatives: 154.3019 - false_positives: 477.9156 - loss: 0.4013 - precision: 0.8281 - recall: 0.9377 - true_negatives: 1973.6331 - true_positives: 2338.1494\n",
      "For batch number 309 the model has a loss of 0.3800186216831207\n",
      "\u001b[1m309/689\u001b[0m \u001b[32m━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━\u001b[0m \u001b[1m22s\u001b[0m 58ms/step - auc: 0.9263 - binary_accuracy: 0.8695 - false_negatives: 154.7605 - false_positives: 479.3366 - loss: 0.4013 - precision: 0.8281 - recall: 0.9377 - true_negatives: 1980.2783 - true_positives: 2345.6245\n",
      "For batch number 310 the model has a loss of 0.3801012337207794\n",
      "\u001b[1m310/689\u001b[0m \u001b[32m━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━\u001b[0m \u001b[1m22s\u001b[0m 58ms/step - auc: 0.9263 - binary_accuracy: 0.8696 - false_negatives: 155.2194 - false_positives: 480.7516 - loss: 0.4012 - precision: 0.8281 - recall: 0.9377 - true_negatives: 1986.9258 - true_positives: 2353.1033\n",
      "For batch number 311 the model has a loss of 0.3795112669467926\n",
      "\u001b[1m311/689\u001b[0m \u001b[32m━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━\u001b[0m \u001b[1m21s\u001b[0m 58ms/step - auc: 0.9263 - binary_accuracy: 0.8696 - false_negatives: 155.6752 - false_positives: 482.1576 - loss: 0.4011 - precision: 0.8281 - recall: 0.9377 - true_negatives: 1993.5756 - true_positives: 2360.5916\n",
      "For batch number 312 the model has a loss of 0.37904903292655945\n",
      "\u001b[1m312/689\u001b[0m \u001b[32m━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━\u001b[0m \u001b[1m21s\u001b[0m 58ms/step - auc: 0.9263 - binary_accuracy: 0.8696 - false_negatives: 156.1282 - false_positives: 483.5609 - loss: 0.4011 - precision: 0.8282 - recall: 0.9377 - true_negatives: 2000.2244 - true_positives: 2368.0864\n",
      "For batch number 313 the model has a loss of 0.37904253602027893\n",
      "\u001b[1m313/689\u001b[0m \u001b[32m━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━\u001b[0m \u001b[1m21s\u001b[0m 58ms/step - auc: 0.9263 - binary_accuracy: 0.8696 - false_negatives: 156.5815 - false_positives: 484.9585 - loss: 0.4010 - precision: 0.8282 - recall: 0.9377 - true_negatives: 2006.8594 - true_positives: 2375.6006\n",
      "For batch number 314 the model has a loss of 0.3799072206020355\n",
      "\u001b[1m314/689\u001b[0m \u001b[32m━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━\u001b[0m \u001b[1m21s\u001b[0m 58ms/step - auc: 0.9263 - binary_accuracy: 0.8697 - false_negatives: 157.0382 - false_positives: 486.3535 - loss: 0.4009 - precision: 0.8282 - recall: 0.9378 - true_negatives: 2013.5032 - true_positives: 2383.1050\n",
      "For batch number 315 the model has a loss of 0.37993988394737244\n",
      "\u001b[1m315/689\u001b[0m \u001b[32m━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━\u001b[0m \u001b[1m21s\u001b[0m 58ms/step - auc: 0.9263 - binary_accuracy: 0.8697 - false_negatives: 157.4952 - false_positives: 487.7524 - loss: 0.4009 - precision: 0.8282 - recall: 0.9378 - true_negatives: 2020.1523 - true_positives: 2390.6001\n",
      "For batch number 316 the model has a loss of 0.3799985349178314\n",
      "\u001b[1m316/689\u001b[0m \u001b[32m━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━\u001b[0m \u001b[1m21s\u001b[0m 58ms/step - auc: 0.9263 - binary_accuracy: 0.8697 - false_negatives: 157.9494 - false_positives: 489.1582 - loss: 0.4008 - precision: 0.8283 - recall: 0.9378 - true_negatives: 2026.7975 - true_positives: 2398.0950\n",
      "For batch number 317 the model has a loss of 0.3806304931640625\n",
      "\u001b[1m317/689\u001b[0m \u001b[32m━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━\u001b[0m \u001b[1m21s\u001b[0m 58ms/step - auc: 0.9263 - binary_accuracy: 0.8697 - false_negatives: 158.4133 - false_positives: 490.5583 - loss: 0.4007 - precision: 0.8283 - recall: 0.9378 - true_negatives: 2033.4385 - true_positives: 2405.5898\n",
      "For batch number 318 the model has a loss of 0.38017457723617554\n",
      "\u001b[1m318/689\u001b[0m \u001b[32m━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━\u001b[0m \u001b[1m21s\u001b[0m 58ms/step - auc: 0.9263 - binary_accuracy: 0.8698 - false_negatives: 158.8742 - false_positives: 491.9591 - loss: 0.4007 - precision: 0.8283 - recall: 0.9378 - true_negatives: 2040.0850 - true_positives: 2413.0818\n",
      "For batch number 319 the model has a loss of 0.3796769380569458\n",
      "\u001b[1m319/689\u001b[0m \u001b[32m━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━\u001b[0m \u001b[1m21s\u001b[0m 58ms/step - auc: 0.9263 - binary_accuracy: 0.8698 - false_negatives: 159.3354 - false_positives: 493.3542 - loss: 0.4006 - precision: 0.8284 - recall: 0.9378 - true_negatives: 2046.7335 - true_positives: 2420.5769\n",
      "For batch number 320 the model has a loss of 0.3790585994720459\n",
      "\u001b[1m320/689\u001b[0m \u001b[32m━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━\u001b[0m \u001b[1m21s\u001b[0m 58ms/step - auc: 0.9263 - binary_accuracy: 0.8698 - false_negatives: 159.7937 - false_positives: 494.7406 - loss: 0.4005 - precision: 0.8284 - recall: 0.9378 - true_negatives: 2053.3813 - true_positives: 2428.0845\n",
      "For batch number 321 the model has a loss of 0.3787040412425995\n",
      "\u001b[1m321/689\u001b[0m \u001b[32m━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━\u001b[0m \u001b[1m21s\u001b[0m 58ms/step - auc: 0.9263 - binary_accuracy: 0.8699 - false_negatives: 160.2492 - false_positives: 496.1215 - loss: 0.4005 - precision: 0.8284 - recall: 0.9378 - true_negatives: 2060.0374 - true_positives: 2435.5918\n",
      "For batch number 322 the model has a loss of 0.37964704632759094\n",
      "\u001b[1m322/689\u001b[0m \u001b[32m━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━\u001b[0m \u001b[1m21s\u001b[0m 58ms/step - auc: 0.9263 - binary_accuracy: 0.8699 - false_negatives: 160.7050 - false_positives: 497.5062 - loss: 0.4004 - precision: 0.8284 - recall: 0.9378 - true_negatives: 2066.6987 - true_positives: 2443.0901\n",
      "For batch number 323 the model has a loss of 0.38002973794937134\n",
      "\u001b[1m323/689\u001b[0m \u001b[32m━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━\u001b[0m \u001b[1m21s\u001b[0m 58ms/step - auc: 0.9263 - binary_accuracy: 0.8699 - false_negatives: 161.1610 - false_positives: 498.8978 - loss: 0.4003 - precision: 0.8285 - recall: 0.9378 - true_negatives: 2073.3591 - true_positives: 2450.5820\n",
      "For batch number 324 the model has a loss of 0.3797590136528015\n",
      "\u001b[1m324/689\u001b[0m \u001b[32m━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━\u001b[0m \u001b[1m21s\u001b[0m 58ms/step - auc: 0.9263 - binary_accuracy: 0.8699 - false_negatives: 161.6173 - false_positives: 500.2870 - loss: 0.4003 - precision: 0.8285 - recall: 0.9378 - true_negatives: 2080.0247 - true_positives: 2458.0710\n",
      "For batch number 325 the model has a loss of 0.37952879071235657\n",
      "\u001b[1m325/689\u001b[0m \u001b[32m━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━\u001b[0m \u001b[1m21s\u001b[0m 58ms/step - auc: 0.9264 - binary_accuracy: 0.8700 - false_negatives: 162.0739 - false_positives: 501.6739 - loss: 0.4002 - precision: 0.8285 - recall: 0.9378 - true_negatives: 2086.6892 - true_positives: 2465.5630\n",
      "For batch number 326 the model has a loss of 0.3792586624622345\n",
      "\u001b[1m326/689\u001b[0m \u001b[32m━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━\u001b[0m \u001b[1m21s\u001b[0m 58ms/step - auc: 0.9264 - binary_accuracy: 0.8700 - false_negatives: 162.5276 - false_positives: 503.0552 - loss: 0.4001 - precision: 0.8285 - recall: 0.9378 - true_negatives: 2093.3528 - true_positives: 2473.0645\n",
      "For batch number 327 the model has a loss of 0.3795185089111328\n",
      "\u001b[1m327/689\u001b[0m \u001b[32m━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━\u001b[0m \u001b[1m21s\u001b[0m 58ms/step - auc: 0.9264 - binary_accuracy: 0.8700 - false_negatives: 162.9817 - false_positives: 504.4343 - loss: 0.4001 - precision: 0.8286 - recall: 0.9379 - true_negatives: 2100.0305 - true_positives: 2480.5535\n",
      "For batch number 328 the model has a loss of 0.3791749179363251\n",
      "\u001b[1m328/689\u001b[0m \u001b[32m━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━\u001b[0m \u001b[1m20s\u001b[0m 58ms/step - auc: 0.9264 - binary_accuracy: 0.8700 - false_negatives: 163.4390 - false_positives: 505.8140 - loss: 0.4000 - precision: 0.8286 - recall: 0.9379 - true_negatives: 2106.7134 - true_positives: 2488.0334\n",
      "For batch number 329 the model has a loss of 0.37901535630226135\n",
      "\u001b[1m329/689\u001b[0m \u001b[32m━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━\u001b[0m \u001b[1m20s\u001b[0m 58ms/step - auc: 0.9264 - binary_accuracy: 0.8701 - false_negatives: 163.8936 - false_positives: 507.1915 - loss: 0.4000 - precision: 0.8286 - recall: 0.9379 - true_negatives: 2113.3950 - true_positives: 2495.5198\n",
      "For batch number 330 the model has a loss of 0.3787153661251068\n",
      "\u001b[1m330/689\u001b[0m \u001b[32m━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━\u001b[0m \u001b[1m20s\u001b[0m 58ms/step - auc: 0.9264 - binary_accuracy: 0.8701 - false_negatives: 164.3455 - false_positives: 508.5667 - loss: 0.3999 - precision: 0.8287 - recall: 0.9379 - true_negatives: 2120.0969 - true_positives: 2502.9910\n",
      "For batch number 331 the model has a loss of 0.37843748927116394\n",
      "\u001b[1m331/689\u001b[0m \u001b[32m━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━\u001b[0m \u001b[1m20s\u001b[0m 58ms/step - auc: 0.9264 - binary_accuracy: 0.8701 - false_negatives: 164.7946 - false_positives: 509.9426 - loss: 0.3998 - precision: 0.8287 - recall: 0.9379 - true_negatives: 2126.8096 - true_positives: 2510.4531\n",
      "For batch number 332 the model has a loss of 0.3783823549747467\n",
      "\u001b[1m332/689\u001b[0m \u001b[32m━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━\u001b[0m \u001b[1m20s\u001b[0m 58ms/step - auc: 0.9264 - binary_accuracy: 0.8702 - false_negatives: 165.2470 - false_positives: 511.3133 - loss: 0.3998 - precision: 0.8287 - recall: 0.9379 - true_negatives: 2133.5061 - true_positives: 2517.9338\n",
      "For batch number 333 the model has a loss of 0.3783252537250519\n",
      "\u001b[1m333/689\u001b[0m \u001b[32m━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━\u001b[0m \u001b[1m20s\u001b[0m 58ms/step - auc: 0.9264 - binary_accuracy: 0.8702 - false_negatives: 165.6967 - false_positives: 512.6817 - loss: 0.3997 - precision: 0.8287 - recall: 0.9379 - true_negatives: 2140.1982 - true_positives: 2525.4233\n",
      "For batch number 334 the model has a loss of 0.37847504019737244\n",
      "\u001b[1m334/689\u001b[0m \u001b[32m━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━\u001b[0m \u001b[1m20s\u001b[0m 58ms/step - auc: 0.9264 - binary_accuracy: 0.8702 - false_negatives: 166.1497 - false_positives: 514.0419 - loss: 0.3996 - precision: 0.8288 - recall: 0.9379 - true_negatives: 2146.8982 - true_positives: 2532.9102\n",
      "For batch number 335 the model has a loss of 0.37821340560913086\n",
      "\u001b[1m335/689\u001b[0m \u001b[32m━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━\u001b[0m \u001b[1m20s\u001b[0m 58ms/step - auc: 0.9264 - binary_accuracy: 0.8702 - false_negatives: 166.6060 - false_positives: 515.3940 - loss: 0.3996 - precision: 0.8288 - recall: 0.9379 - true_negatives: 2153.5940 - true_positives: 2540.4060\n",
      "For batch number 336 the model has a loss of 0.37806230783462524\n",
      "\u001b[1m336/689\u001b[0m \u001b[32m━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━\u001b[0m \u001b[1m20s\u001b[0m 58ms/step - auc: 0.9264 - binary_accuracy: 0.8703 - false_negatives: 167.0625 - false_positives: 516.7470 - loss: 0.3995 - precision: 0.8288 - recall: 0.9379 - true_negatives: 2160.2947 - true_positives: 2547.8958\n",
      "For batch number 337 the model has a loss of 0.3793606162071228\n",
      "\u001b[1m337/689\u001b[0m \u001b[32m━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━\u001b[0m \u001b[1m20s\u001b[0m 58ms/step - auc: 0.9264 - binary_accuracy: 0.8703 - false_negatives: 167.5163 - false_positives: 518.1157 - loss: 0.3994 - precision: 0.8289 - recall: 0.9379 - true_negatives: 2166.9912 - true_positives: 2555.3770\n",
      "For batch number 338 the model has a loss of 0.3790726661682129\n",
      "\u001b[1m338/689\u001b[0m \u001b[32m━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━\u001b[0m \u001b[1m20s\u001b[0m 58ms/step - auc: 0.9264 - binary_accuracy: 0.8703 - false_negatives: 167.9675 - false_positives: 519.4822 - loss: 0.3994 - precision: 0.8289 - recall: 0.9379 - true_negatives: 2173.6924 - true_positives: 2562.8579\n",
      "For batch number 339 the model has a loss of 0.3790144622325897\n",
      "\u001b[1m339/689\u001b[0m \u001b[32m━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━\u001b[0m \u001b[1m20s\u001b[0m 58ms/step - auc: 0.9264 - binary_accuracy: 0.8704 - false_negatives: 168.4218 - false_positives: 520.8466 - loss: 0.3993 - precision: 0.8289 - recall: 0.9380 - true_negatives: 2180.3894 - true_positives: 2570.3423\n",
      "For batch number 340 the model has a loss of 0.37950924038887024\n",
      "\u001b[1m340/689\u001b[0m \u001b[32m━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━\u001b[0m \u001b[1m20s\u001b[0m 58ms/step - auc: 0.9264 - binary_accuracy: 0.8704 - false_negatives: 168.8824 - false_positives: 522.2088 - loss: 0.3993 - precision: 0.8289 - recall: 0.9380 - true_negatives: 2187.0764 - true_positives: 2577.8323\n",
      "For batch number 341 the model has a loss of 0.37966543436050415\n",
      "\u001b[1m341/689\u001b[0m \u001b[32m━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━\u001b[0m \u001b[1m20s\u001b[0m 58ms/step - auc: 0.9265 - binary_accuracy: 0.8704 - false_negatives: 169.3431 - false_positives: 523.5806 - loss: 0.3992 - precision: 0.8290 - recall: 0.9380 - true_negatives: 2193.7654 - true_positives: 2585.3108\n",
      "For batch number 342 the model has a loss of 0.37994077801704407\n",
      "\u001b[1m342/689\u001b[0m \u001b[32m━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━\u001b[0m \u001b[1m20s\u001b[0m 58ms/step - auc: 0.9265 - binary_accuracy: 0.8704 - false_negatives: 169.8070 - false_positives: 524.9532 - loss: 0.3992 - precision: 0.8290 - recall: 0.9380 - true_negatives: 2200.4473 - true_positives: 2592.7925\n",
      "For batch number 343 the model has a loss of 0.37933677434921265\n",
      "\u001b[1m343/689\u001b[0m \u001b[32m━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━\u001b[0m \u001b[1m20s\u001b[0m 58ms/step - auc: 0.9265 - binary_accuracy: 0.8705 - false_negatives: 170.2711 - false_positives: 526.3207 - loss: 0.3991 - precision: 0.8290 - recall: 0.9380 - true_negatives: 2207.1282 - true_positives: 2600.2798\n",
      "For batch number 344 the model has a loss of 0.379258394241333\n",
      "\u001b[1m344/689\u001b[0m \u001b[32m━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━\u001b[0m \u001b[1m20s\u001b[0m 58ms/step - auc: 0.9265 - binary_accuracy: 0.8705 - false_negatives: 170.7384 - false_positives: 527.6890 - loss: 0.3990 - precision: 0.8291 - recall: 0.9380 - true_negatives: 2213.8140 - true_positives: 2607.7588\n",
      "For batch number 345 the model has a loss of 0.38008812069892883\n",
      "\u001b[1m345/689\u001b[0m \u001b[32m━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━\u001b[0m \u001b[1m20s\u001b[0m 58ms/step - auc: 0.9265 - binary_accuracy: 0.8705 - false_negatives: 171.2058 - false_positives: 529.0638 - loss: 0.3990 - precision: 0.8291 - recall: 0.9380 - true_negatives: 2220.5073 - true_positives: 2615.2231\n",
      "For batch number 346 the model has a loss of 0.3801288902759552\n",
      "\u001b[1m346/689\u001b[0m \u001b[32m━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━\u001b[0m \u001b[1m19s\u001b[0m 58ms/step - auc: 0.9265 - binary_accuracy: 0.8705 - false_negatives: 171.6734 - false_positives: 530.4451 - loss: 0.3989 - precision: 0.8291 - recall: 0.9380 - true_negatives: 2227.1995 - true_positives: 2622.6821\n",
      "For batch number 347 the model has a loss of 0.38020890951156616\n",
      "\u001b[1m347/689\u001b[0m \u001b[32m━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━\u001b[0m \u001b[1m19s\u001b[0m 58ms/step - auc: 0.9265 - binary_accuracy: 0.8706 - false_negatives: 172.1383 - false_positives: 531.8386 - loss: 0.3989 - precision: 0.8291 - recall: 0.9380 - true_negatives: 2233.8962 - true_positives: 2630.1267\n",
      "For batch number 348 the model has a loss of 0.38072019815444946\n",
      "\u001b[1m348/689\u001b[0m \u001b[32m━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━\u001b[0m \u001b[1m19s\u001b[0m 58ms/step - auc: 0.9265 - binary_accuracy: 0.8706 - false_negatives: 172.6063 - false_positives: 533.2357 - loss: 0.3988 - precision: 0.8291 - recall: 0.9380 - true_negatives: 2240.6006 - true_positives: 2637.5574\n",
      "For batch number 349 the model has a loss of 0.3804304599761963\n",
      "\u001b[1m349/689\u001b[0m \u001b[32m━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━\u001b[0m \u001b[1m19s\u001b[0m 58ms/step - auc: 0.9265 - binary_accuracy: 0.8706 - false_negatives: 173.0716 - false_positives: 534.6332 - loss: 0.3988 - precision: 0.8292 - recall: 0.9380 - true_negatives: 2247.3123 - true_positives: 2644.9829\n",
      "For batch number 350 the model has a loss of 0.3800279498100281\n",
      "\u001b[1m350/689\u001b[0m \u001b[32m━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━\u001b[0m \u001b[1m19s\u001b[0m 58ms/step - auc: 0.9265 - binary_accuracy: 0.8706 - false_negatives: 173.5343 - false_positives: 536.0314 - loss: 0.3987 - precision: 0.8292 - recall: 0.9380 - true_negatives: 2254.0256 - true_positives: 2652.4087\n",
      "For batch number 351 the model has a loss of 0.38031357526779175\n",
      "\u001b[1m351/689\u001b[0m \u001b[32m━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━\u001b[0m \u001b[1m19s\u001b[0m 58ms/step - auc: 0.9265 - binary_accuracy: 0.8707 - false_negatives: 174.0000 - false_positives: 537.4302 - loss: 0.3987 - precision: 0.8292 - recall: 0.9380 - true_negatives: 2260.7236 - true_positives: 2659.8462\n",
      "For batch number 352 the model has a loss of 0.3800102472305298\n",
      "\u001b[1m352/689\u001b[0m \u001b[32m━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━\u001b[0m \u001b[1m19s\u001b[0m 58ms/step - auc: 0.9265 - binary_accuracy: 0.8707 - false_negatives: 174.4659 - false_positives: 538.8239 - loss: 0.3986 - precision: 0.8292 - recall: 0.9380 - true_negatives: 2267.4177 - true_positives: 2667.2927\n",
      "For batch number 353 the model has a loss of 0.3800169825553894\n",
      "\u001b[1m353/689\u001b[0m \u001b[32m━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━\u001b[0m \u001b[1m19s\u001b[0m 58ms/step - auc: 0.9265 - binary_accuracy: 0.8707 - false_negatives: 174.9292 - false_positives: 540.2209 - loss: 0.3986 - precision: 0.8293 - recall: 0.9380 - true_negatives: 2274.1218 - true_positives: 2674.7280\n",
      "For batch number 354 the model has a loss of 0.3801504075527191\n",
      "\u001b[1m354/689\u001b[0m \u001b[32m━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━\u001b[0m \u001b[1m19s\u001b[0m 58ms/step - auc: 0.9265 - binary_accuracy: 0.8707 - false_negatives: 175.4011 - false_positives: 541.6130 - loss: 0.3985 - precision: 0.8293 - recall: 0.9380 - true_negatives: 2280.8220 - true_positives: 2682.1638\n",
      "For batch number 355 the model has a loss of 0.3806757926940918\n",
      "\u001b[1m355/689\u001b[0m \u001b[32m━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━\u001b[0m \u001b[1m19s\u001b[0m 58ms/step - auc: 0.9265 - binary_accuracy: 0.8707 - false_negatives: 175.8817 - false_positives: 543.0028 - loss: 0.3985 - precision: 0.8293 - recall: 0.9380 - true_negatives: 2287.5098 - true_positives: 2689.6057\n",
      "For batch number 356 the model has a loss of 0.3803079128265381\n",
      "\u001b[1m356/689\u001b[0m \u001b[32m━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━\u001b[0m \u001b[1m19s\u001b[0m 58ms/step - auc: 0.9265 - binary_accuracy: 0.8708 - false_negatives: 176.3596 - false_positives: 544.3932 - loss: 0.3984 - precision: 0.8293 - recall: 0.9380 - true_negatives: 2294.2078 - true_positives: 2697.0393\n",
      "For batch number 357 the model has a loss of 0.3807307779788971\n",
      "\u001b[1m357/689\u001b[0m \u001b[32m━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━\u001b[0m \u001b[1m19s\u001b[0m 58ms/step - auc: 0.9265 - binary_accuracy: 0.8708 - false_negatives: 176.8543 - false_positives: 545.7843 - loss: 0.3984 - precision: 0.8293 - recall: 0.9380 - true_negatives: 2300.8909 - true_positives: 2704.4707\n",
      "For batch number 358 the model has a loss of 0.3802908658981323\n",
      "\u001b[1m358/689\u001b[0m \u001b[32m━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━\u001b[0m \u001b[1m19s\u001b[0m 58ms/step - auc: 0.9265 - binary_accuracy: 0.8708 - false_negatives: 177.3492 - false_positives: 547.1676 - loss: 0.3983 - precision: 0.8294 - recall: 0.9380 - true_negatives: 2307.5781 - true_positives: 2711.9050\n",
      "For batch number 359 the model has a loss of 0.38017764687538147\n",
      "\u001b[1m359/689\u001b[0m \u001b[32m━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━\u001b[0m \u001b[1m19s\u001b[0m 58ms/step - auc: 0.9265 - binary_accuracy: 0.8708 - false_negatives: 177.8412 - false_positives: 548.5571 - loss: 0.3983 - precision: 0.8294 - recall: 0.9380 - true_negatives: 2314.2563 - true_positives: 2719.3455\n",
      "For batch number 360 the model has a loss of 0.379917174577713\n",
      "\u001b[1m360/689\u001b[0m \u001b[32m━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━\u001b[0m \u001b[1m19s\u001b[0m 58ms/step - auc: 0.9265 - binary_accuracy: 0.8709 - false_negatives: 178.3333 - false_positives: 549.9389 - loss: 0.3982 - precision: 0.8294 - recall: 0.9380 - true_negatives: 2320.9221 - true_positives: 2726.8057\n",
      "For batch number 361 the model has a loss of 0.38030776381492615\n",
      "\u001b[1m361/689\u001b[0m \u001b[32m━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━\u001b[0m \u001b[1m19s\u001b[0m 58ms/step - auc: 0.9265 - binary_accuracy: 0.8709 - false_negatives: 178.8227 - false_positives: 551.3324 - loss: 0.3982 - precision: 0.8294 - recall: 0.9380 - true_negatives: 2327.5955 - true_positives: 2734.2493\n",
      "For batch number 362 the model has a loss of 0.3798382878303528\n",
      "\u001b[1m362/689\u001b[0m \u001b[32m━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━\u001b[0m \u001b[1m19s\u001b[0m 58ms/step - auc: 0.9265 - binary_accuracy: 0.8709 - false_negatives: 179.3094 - false_positives: 552.7210 - loss: 0.3981 - precision: 0.8295 - recall: 0.9380 - true_negatives: 2334.2625 - true_positives: 2741.7073\n",
      "For batch number 363 the model has a loss of 0.3798343539237976\n",
      "\u001b[1m363/689\u001b[0m \u001b[32m━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━\u001b[0m \u001b[1m18s\u001b[0m 58ms/step - auc: 0.9265 - binary_accuracy: 0.8709 - false_negatives: 179.7934 - false_positives: 554.1130 - loss: 0.3981 - precision: 0.8295 - recall: 0.9381 - true_negatives: 2340.9285 - true_positives: 2749.1653\n",
      "For batch number 364 the model has a loss of 0.38037580251693726\n",
      "\u001b[1m364/689\u001b[0m \u001b[32m━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━\u001b[0m \u001b[1m18s\u001b[0m 58ms/step - auc: 0.9265 - binary_accuracy: 0.8709 - false_negatives: 180.2775 - false_positives: 555.5055 - loss: 0.3980 - precision: 0.8295 - recall: 0.9381 - true_negatives: 2347.5906 - true_positives: 2756.6265\n",
      "For batch number 365 the model has a loss of 0.37974005937576294\n",
      "\u001b[1m365/689\u001b[0m \u001b[32m━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━\u001b[0m \u001b[1m18s\u001b[0m 58ms/step - auc: 0.9265 - binary_accuracy: 0.8710 - false_negatives: 180.7589 - false_positives: 556.8959 - loss: 0.3980 - precision: 0.8295 - recall: 0.9381 - true_negatives: 2354.2603 - true_positives: 2764.0850\n",
      "For batch number 366 the model has a loss of 0.3797461986541748\n",
      "\u001b[1m366/689\u001b[0m \u001b[32m━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━\u001b[0m \u001b[1m18s\u001b[0m 58ms/step - auc: 0.9265 - binary_accuracy: 0.8710 - false_negatives: 181.2377 - false_positives: 558.2951 - loss: 0.3979 - precision: 0.8295 - recall: 0.9381 - true_negatives: 2360.9290 - true_positives: 2771.5383\n",
      "For batch number 367 the model has a loss of 0.38001665472984314\n",
      "\u001b[1m367/689\u001b[0m \u001b[32m━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━\u001b[0m \u001b[1m18s\u001b[0m 58ms/step - auc: 0.9265 - binary_accuracy: 0.8710 - false_negatives: 181.7166 - false_positives: 559.6948 - loss: 0.3979 - precision: 0.8296 - recall: 0.9381 - true_negatives: 2367.5994 - true_positives: 2778.9890\n",
      "For batch number 368 the model has a loss of 0.3801206052303314\n",
      "\u001b[1m368/689\u001b[0m \u001b[32m━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━\u001b[0m \u001b[1m18s\u001b[0m 58ms/step - auc: 0.9265 - binary_accuracy: 0.8710 - false_negatives: 182.1956 - false_positives: 561.0978 - loss: 0.3978 - precision: 0.8296 - recall: 0.9381 - true_negatives: 2374.2717 - true_positives: 2786.4348\n",
      "For batch number 369 the model has a loss of 0.3804330825805664\n",
      "\u001b[1m369/689\u001b[0m \u001b[32m━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━\u001b[0m \u001b[1m18s\u001b[0m 58ms/step - auc: 0.9265 - binary_accuracy: 0.8710 - false_negatives: 182.6748 - false_positives: 562.4932 - loss: 0.3978 - precision: 0.8296 - recall: 0.9381 - true_negatives: 2380.9431 - true_positives: 2793.8889\n",
      "For batch number 370 the model has a loss of 0.3797215223312378\n",
      "\u001b[1m370/689\u001b[0m \u001b[32m━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━\u001b[0m \u001b[1m18s\u001b[0m 58ms/step - auc: 0.9265 - binary_accuracy: 0.8711 - false_negatives: 183.1514 - false_positives: 563.8811 - loss: 0.3977 - precision: 0.8296 - recall: 0.9381 - true_negatives: 2387.6189 - true_positives: 2801.3486\n",
      "For batch number 371 the model has a loss of 0.3794826567173004\n",
      "\u001b[1m371/689\u001b[0m \u001b[32m━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━\u001b[0m \u001b[1m18s\u001b[0m 58ms/step - auc: 0.9265 - binary_accuracy: 0.8711 - false_negatives: 183.6280 - false_positives: 565.2695 - loss: 0.3977 - precision: 0.8296 - recall: 0.9381 - true_negatives: 2394.2910 - true_positives: 2808.8113\n",
      "For batch number 372 the model has a loss of 0.3791765868663788\n",
      "\u001b[1m372/689\u001b[0m \u001b[32m━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━\u001b[0m \u001b[1m18s\u001b[0m 58ms/step - auc: 0.9265 - binary_accuracy: 0.8711 - false_negatives: 184.1048 - false_positives: 566.6586 - loss: 0.3976 - precision: 0.8297 - recall: 0.9381 - true_negatives: 2400.9705 - true_positives: 2816.2661\n",
      "For batch number 373 the model has a loss of 0.37891268730163574\n",
      "\u001b[1m373/689\u001b[0m \u001b[32m━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━\u001b[0m \u001b[1m18s\u001b[0m 58ms/step - auc: 0.9265 - binary_accuracy: 0.8711 - false_negatives: 184.5818 - false_positives: 568.0483 - loss: 0.3976 - precision: 0.8297 - recall: 0.9381 - true_negatives: 2407.6541 - true_positives: 2823.7158\n",
      "For batch number 374 the model has a loss of 0.37858879566192627\n",
      "\u001b[1m374/689\u001b[0m \u001b[32m━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━\u001b[0m \u001b[1m18s\u001b[0m 58ms/step - auc: 0.9265 - binary_accuracy: 0.8711 - false_negatives: 185.0562 - false_positives: 569.4332 - loss: 0.3975 - precision: 0.8297 - recall: 0.9381 - true_negatives: 2414.3423 - true_positives: 2831.1685\n",
      "For batch number 375 the model has a loss of 0.3784523606300354\n",
      "\u001b[1m375/689\u001b[0m \u001b[32m━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━\u001b[0m \u001b[1m18s\u001b[0m 58ms/step - auc: 0.9265 - binary_accuracy: 0.8712 - false_negatives: 185.5280 - false_positives: 570.8134 - loss: 0.3975 - precision: 0.8297 - recall: 0.9381 - true_negatives: 2421.0320 - true_positives: 2838.6267\n",
      "For batch number 376 the model has a loss of 0.3792977035045624\n",
      "\u001b[1m376/689\u001b[0m \u001b[32m━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━\u001b[0m \u001b[1m18s\u001b[0m 58ms/step - auc: 0.9265 - binary_accuracy: 0.8712 - false_negatives: 186.0000 - false_positives: 572.1888 - loss: 0.3974 - precision: 0.8298 - recall: 0.9381 - true_negatives: 2427.7126 - true_positives: 2846.0984\n",
      "For batch number 377 the model has a loss of 0.3791567385196686\n",
      "\u001b[1m377/689\u001b[0m \u001b[32m━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━\u001b[0m \u001b[1m18s\u001b[0m 58ms/step - auc: 0.9265 - binary_accuracy: 0.8712 - false_negatives: 186.4722 - false_positives: 573.5650 - loss: 0.3974 - precision: 0.8298 - recall: 0.9381 - true_negatives: 2434.3926 - true_positives: 2853.5703\n",
      "For batch number 378 the model has a loss of 0.3795986771583557\n",
      "\u001b[1m378/689\u001b[0m \u001b[32m━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━\u001b[0m \u001b[1m18s\u001b[0m 58ms/step - auc: 0.9265 - binary_accuracy: 0.8712 - false_negatives: 186.9444 - false_positives: 574.9365 - loss: 0.3973 - precision: 0.8298 - recall: 0.9381 - true_negatives: 2441.0740 - true_positives: 2861.0449\n",
      "For batch number 379 the model has a loss of 0.379172682762146\n",
      "\u001b[1m379/689\u001b[0m \u001b[32m━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━\u001b[0m \u001b[1m18s\u001b[0m 58ms/step - auc: 0.9265 - binary_accuracy: 0.8712 - false_negatives: 187.4142 - false_positives: 576.3087 - loss: 0.3973 - precision: 0.8298 - recall: 0.9381 - true_negatives: 2447.7625 - true_positives: 2868.5144\n",
      "For batch number 380 the model has a loss of 0.3788287043571472\n",
      "\u001b[1m380/689\u001b[0m \u001b[32m━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━\u001b[0m \u001b[1m18s\u001b[0m 58ms/step - auc: 0.9265 - binary_accuracy: 0.8713 - false_negatives: 187.8816 - false_positives: 577.6790 - loss: 0.3972 - precision: 0.8299 - recall: 0.9381 - true_negatives: 2454.4473 - true_positives: 2875.9922\n",
      "For batch number 381 the model has a loss of 0.37860071659088135\n",
      "\u001b[1m381/689\u001b[0m \u001b[32m━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━\u001b[0m \u001b[1m17s\u001b[0m 58ms/step - auc: 0.9265 - binary_accuracy: 0.8713 - false_negatives: 188.3465 - false_positives: 579.0525 - loss: 0.3972 - precision: 0.8299 - recall: 0.9381 - true_negatives: 2461.1392 - true_positives: 2883.4619\n",
      "For batch number 382 the model has a loss of 0.3785080909729004\n",
      "\u001b[1m382/689\u001b[0m \u001b[32m━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━\u001b[0m \u001b[1m17s\u001b[0m 58ms/step - auc: 0.9266 - binary_accuracy: 0.8713 - false_negatives: 188.8115 - false_positives: 580.4241 - loss: 0.3971 - precision: 0.8299 - recall: 0.9381 - true_negatives: 2467.8325 - true_positives: 2890.9319\n",
      "For batch number 383 the model has a loss of 0.3780066668987274\n",
      "\u001b[1m383/689\u001b[0m \u001b[32m━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━\u001b[0m \u001b[1m17s\u001b[0m 58ms/step - auc: 0.9266 - binary_accuracy: 0.8713 - false_negatives: 189.2742 - false_positives: 581.7911 - loss: 0.3971 - precision: 0.8299 - recall: 0.9381 - true_negatives: 2474.5352 - true_positives: 2898.3994\n",
      "For batch number 384 the model has a loss of 0.3775169551372528\n",
      "\u001b[1m384/689\u001b[0m \u001b[32m━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━\u001b[0m \u001b[1m17s\u001b[0m 58ms/step - auc: 0.9266 - binary_accuracy: 0.8714 - false_negatives: 189.7344 - false_positives: 583.1511 - loss: 0.3970 - precision: 0.8299 - recall: 0.9381 - true_negatives: 2481.2422 - true_positives: 2905.8723\n",
      "For batch number 385 the model has a loss of 0.3772072494029999\n",
      "\u001b[1m385/689\u001b[0m \u001b[32m━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━\u001b[0m \u001b[1m17s\u001b[0m 58ms/step - auc: 0.9266 - binary_accuracy: 0.8714 - false_negatives: 190.1922 - false_positives: 584.5091 - loss: 0.3970 - precision: 0.8300 - recall: 0.9381 - true_negatives: 2487.9558 - true_positives: 2913.3428\n",
      "For batch number 386 the model has a loss of 0.37747588753700256\n",
      "\u001b[1m386/689\u001b[0m \u001b[32m━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━\u001b[0m \u001b[1m17s\u001b[0m 58ms/step - auc: 0.9266 - binary_accuracy: 0.8714 - false_negatives: 190.6528 - false_positives: 585.8627 - loss: 0.3969 - precision: 0.8300 - recall: 0.9381 - true_negatives: 2494.6633 - true_positives: 2920.8213\n",
      "For batch number 387 the model has a loss of 0.3776700496673584\n",
      "\u001b[1m387/689\u001b[0m \u001b[32m━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━\u001b[0m \u001b[1m17s\u001b[0m 58ms/step - auc: 0.9266 - binary_accuracy: 0.8714 - false_negatives: 191.1111 - false_positives: 587.2248 - loss: 0.3969 - precision: 0.8300 - recall: 0.9381 - true_negatives: 2501.3696 - true_positives: 2928.2947\n",
      "For batch number 388 the model has a loss of 0.37701234221458435\n",
      "\u001b[1m388/689\u001b[0m \u001b[32m━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━\u001b[0m \u001b[1m17s\u001b[0m 58ms/step - auc: 0.9266 - binary_accuracy: 0.8715 - false_negatives: 191.5696 - false_positives: 588.5799 - loss: 0.3968 - precision: 0.8300 - recall: 0.9382 - true_negatives: 2508.0774 - true_positives: 2935.7732\n",
      "For batch number 389 the model has a loss of 0.37675240635871887\n",
      "\u001b[1m389/689\u001b[0m \u001b[32m━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━\u001b[0m \u001b[1m17s\u001b[0m 58ms/step - auc: 0.9266 - binary_accuracy: 0.8715 - false_negatives: 192.0257 - false_positives: 589.9332 - loss: 0.3968 - precision: 0.8301 - recall: 0.9382 - true_negatives: 2514.7996 - true_positives: 2943.2417\n",
      "For batch number 390 the model has a loss of 0.3767273724079132\n",
      "\u001b[1m390/689\u001b[0m \u001b[32m━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━\u001b[0m \u001b[1m17s\u001b[0m 58ms/step - auc: 0.9266 - binary_accuracy: 0.8715 - false_negatives: 192.4795 - false_positives: 591.2949 - loss: 0.3967 - precision: 0.8301 - recall: 0.9382 - true_negatives: 2521.5205 - true_positives: 2950.7051\n",
      "For batch number 391 the model has a loss of 0.3781282603740692\n",
      "\u001b[1m391/689\u001b[0m \u001b[32m━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━\u001b[0m \u001b[1m17s\u001b[0m 58ms/step - auc: 0.9266 - binary_accuracy: 0.8715 - false_negatives: 192.9361 - false_positives: 592.6624 - loss: 0.3967 - precision: 0.8301 - recall: 0.9382 - true_negatives: 2528.2405 - true_positives: 2958.1611\n",
      "For batch number 392 the model has a loss of 0.37833431363105774\n",
      "\u001b[1m392/689\u001b[0m \u001b[32m━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━\u001b[0m \u001b[1m17s\u001b[0m 58ms/step - auc: 0.9266 - binary_accuracy: 0.8715 - false_negatives: 193.3980 - false_positives: 594.0255 - loss: 0.3966 - precision: 0.8301 - recall: 0.9382 - true_negatives: 2534.9541 - true_positives: 2965.6226\n",
      "For batch number 393 the model has a loss of 0.3781186640262604\n",
      "\u001b[1m393/689\u001b[0m \u001b[32m━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━\u001b[0m \u001b[1m17s\u001b[0m 58ms/step - auc: 0.9266 - binary_accuracy: 0.8716 - false_negatives: 193.8600 - false_positives: 595.3842 - loss: 0.3966 - precision: 0.8302 - recall: 0.9382 - true_negatives: 2541.6667 - true_positives: 2973.0891\n",
      "For batch number 394 the model has a loss of 0.37883681058883667\n",
      "\u001b[1m394/689\u001b[0m \u001b[32m━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━\u001b[0m \u001b[1m17s\u001b[0m 58ms/step - auc: 0.9266 - binary_accuracy: 0.8716 - false_negatives: 194.3274 - false_positives: 596.7411 - loss: 0.3965 - precision: 0.8302 - recall: 0.9382 - true_negatives: 2548.3604 - true_positives: 2980.5710\n",
      "For batch number 395 the model has a loss of 0.37864500284194946\n",
      "\u001b[1m395/689\u001b[0m \u001b[32m━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━\u001b[0m \u001b[1m17s\u001b[0m 58ms/step - auc: 0.9266 - binary_accuracy: 0.8716 - false_negatives: 194.7949 - false_positives: 598.0962 - loss: 0.3965 - precision: 0.8302 - recall: 0.9382 - true_negatives: 2555.0632 - true_positives: 2988.0457\n",
      "For batch number 396 the model has a loss of 0.37925657629966736\n",
      "\u001b[1m396/689\u001b[0m \u001b[32m━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━\u001b[0m \u001b[1m17s\u001b[0m 58ms/step - auc: 0.9266 - binary_accuracy: 0.8716 - false_negatives: 195.2677 - false_positives: 599.4495 - loss: 0.3964 - precision: 0.8302 - recall: 0.9382 - true_negatives: 2561.7576 - true_positives: 2995.5251\n",
      "For batch number 397 the model has a loss of 0.37895193696022034\n",
      "\u001b[1m397/689\u001b[0m \u001b[32m━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━\u001b[0m \u001b[1m17s\u001b[0m 58ms/step - auc: 0.9266 - binary_accuracy: 0.8717 - false_negatives: 195.7406 - false_positives: 600.8010 - loss: 0.3964 - precision: 0.8303 - recall: 0.9382 - true_negatives: 2568.4585 - true_positives: 3003.0000\n",
      "For batch number 398 the model has a loss of 0.3791200518608093\n",
      "\u001b[1m398/689\u001b[0m \u001b[32m━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━\u001b[0m \u001b[1m16s\u001b[0m 58ms/step - auc: 0.9266 - binary_accuracy: 0.8717 - false_negatives: 196.2186 - false_positives: 602.1483 - loss: 0.3964 - precision: 0.8303 - recall: 0.9382 - true_negatives: 2575.1433 - true_positives: 3010.4900\n",
      "For batch number 399 the model has a loss of 0.37890946865081787\n",
      "\u001b[1m399/689\u001b[0m \u001b[32m━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━\u001b[0m \u001b[1m16s\u001b[0m 58ms/step - auc: 0.9266 - binary_accuracy: 0.8717 - false_negatives: 196.6942 - false_positives: 603.4987 - loss: 0.3963 - precision: 0.8303 - recall: 0.9382 - true_negatives: 2581.8296 - true_positives: 3017.9775\n",
      "For batch number 400 the model has a loss of 0.3790753185749054\n",
      "\u001b[1m400/689\u001b[0m \u001b[32m━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━\u001b[0m \u001b[1m16s\u001b[0m 58ms/step - auc: 0.9266 - binary_accuracy: 0.8717 - false_negatives: 197.1700 - false_positives: 604.8500 - loss: 0.3963 - precision: 0.8303 - recall: 0.9382 - true_negatives: 2588.5149 - true_positives: 3025.4651\n",
      "For batch number 401 the model has a loss of 0.37911999225616455\n",
      "\u001b[1m401/689\u001b[0m \u001b[32m━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━\u001b[0m \u001b[1m16s\u001b[0m 58ms/step - auc: 0.9266 - binary_accuracy: 0.8717 - false_negatives: 197.6534 - false_positives: 606.1970 - loss: 0.3962 - precision: 0.8304 - recall: 0.9382 - true_negatives: 2595.1995 - true_positives: 3032.9502\n",
      "For batch number 402 the model has a loss of 0.3789084553718567\n",
      "\u001b[1m402/689\u001b[0m \u001b[32m━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━\u001b[0m \u001b[1m16s\u001b[0m 58ms/step - auc: 0.9266 - binary_accuracy: 0.8718 - false_negatives: 198.1368 - false_positives: 607.5497 - loss: 0.3962 - precision: 0.8304 - recall: 0.9382 - true_negatives: 2601.8855 - true_positives: 3040.4280\n",
      "For batch number 403 the model has a loss of 0.379581093788147\n",
      "\u001b[1m403/689\u001b[0m \u001b[32m━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━\u001b[0m \u001b[1m16s\u001b[0m 58ms/step - auc: 0.9266 - binary_accuracy: 0.8718 - false_negatives: 198.6253 - false_positives: 608.9057 - loss: 0.3961 - precision: 0.8304 - recall: 0.9382 - true_negatives: 2608.5632 - true_positives: 3047.9058\n",
      "For batch number 404 the model has a loss of 0.37995195388793945\n",
      "\u001b[1m404/689\u001b[0m \u001b[32m━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━\u001b[0m \u001b[1m16s\u001b[0m 58ms/step - auc: 0.9266 - binary_accuracy: 0.8718 - false_negatives: 199.1139 - false_positives: 610.2624 - loss: 0.3961 - precision: 0.8304 - recall: 0.9382 - true_negatives: 2615.2451 - true_positives: 3055.3787\n",
      "For batch number 405 the model has a loss of 0.3800087571144104\n",
      "\u001b[1m405/689\u001b[0m \u001b[32m━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━\u001b[0m \u001b[1m16s\u001b[0m 58ms/step - auc: 0.9266 - binary_accuracy: 0.8718 - false_negatives: 199.6025 - false_positives: 611.6222 - loss: 0.3961 - precision: 0.8305 - recall: 0.9382 - true_negatives: 2621.9233 - true_positives: 3062.8518\n",
      "For batch number 406 the model has a loss of 0.38064777851104736\n",
      "\u001b[1m406/689\u001b[0m \u001b[32m━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━\u001b[0m \u001b[1m16s\u001b[0m 58ms/step - auc: 0.9266 - binary_accuracy: 0.8718 - false_negatives: 200.0911 - false_positives: 612.9877 - loss: 0.3960 - precision: 0.8305 - recall: 0.9382 - true_negatives: 2628.6035 - true_positives: 3070.3176\n",
      "For batch number 407 the model has a loss of 0.3811356723308563\n",
      "\u001b[1m407/689\u001b[0m \u001b[32m━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━\u001b[0m \u001b[1m16s\u001b[0m 58ms/step - auc: 0.9266 - binary_accuracy: 0.8719 - false_negatives: 200.5798 - false_positives: 614.3538 - loss: 0.3960 - precision: 0.8305 - recall: 0.9382 - true_negatives: 2635.2800 - true_positives: 3077.7861\n",
      "For batch number 408 the model has a loss of 0.38128724694252014\n",
      "\u001b[1m408/689\u001b[0m \u001b[32m━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━\u001b[0m \u001b[1m16s\u001b[0m 58ms/step - auc: 0.9266 - binary_accuracy: 0.8719 - false_negatives: 201.0662 - false_positives: 615.7328 - loss: 0.3959 - precision: 0.8305 - recall: 0.9382 - true_negatives: 2641.9534 - true_positives: 3085.2476\n",
      "For batch number 409 the model has a loss of 0.380828320980072\n",
      "\u001b[1m409/689\u001b[0m \u001b[32m━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━\u001b[0m \u001b[1m16s\u001b[0m 58ms/step - auc: 0.9266 - binary_accuracy: 0.8719 - false_negatives: 201.5501 - false_positives: 617.1100 - loss: 0.3959 - precision: 0.8305 - recall: 0.9382 - true_negatives: 2648.6284 - true_positives: 3092.7114\n",
      "For batch number 410 the model has a loss of 0.38053035736083984\n",
      "\u001b[1m410/689\u001b[0m \u001b[32m━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━\u001b[0m \u001b[1m16s\u001b[0m 58ms/step - auc: 0.9266 - binary_accuracy: 0.8719 - false_negatives: 202.0317 - false_positives: 618.4854 - loss: 0.3959 - precision: 0.8306 - recall: 0.9382 - true_negatives: 2655.3049 - true_positives: 3100.1780\n",
      "For batch number 411 the model has a loss of 0.380739688873291\n",
      "\u001b[1m411/689\u001b[0m \u001b[32m━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━\u001b[0m \u001b[1m16s\u001b[0m 58ms/step - auc: 0.9266 - binary_accuracy: 0.8719 - false_negatives: 202.5182 - false_positives: 619.8613 - loss: 0.3958 - precision: 0.8306 - recall: 0.9382 - true_negatives: 2661.9707 - true_positives: 3107.6497\n",
      "For batch number 412 the model has a loss of 0.3806084096431732\n",
      "\u001b[1m412/689\u001b[0m \u001b[32m━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━\u001b[0m \u001b[1m16s\u001b[0m 58ms/step - auc: 0.9267 - binary_accuracy: 0.8720 - false_negatives: 203.0024 - false_positives: 621.2403 - loss: 0.3958 - precision: 0.8306 - recall: 0.9382 - true_negatives: 2668.6360 - true_positives: 3115.1213\n",
      "For batch number 413 the model has a loss of 0.3801950216293335\n",
      "\u001b[1m413/689\u001b[0m \u001b[32m━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━\u001b[0m \u001b[1m16s\u001b[0m 58ms/step - auc: 0.9267 - binary_accuracy: 0.8720 - false_negatives: 203.4843 - false_positives: 622.6150 - loss: 0.3958 - precision: 0.8306 - recall: 0.9382 - true_negatives: 2675.3147 - true_positives: 3122.5859\n",
      "For batch number 414 the model has a loss of 0.37999507784843445\n",
      "\u001b[1m414/689\u001b[0m \u001b[32m━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━\u001b[0m \u001b[1m16s\u001b[0m 58ms/step - auc: 0.9267 - binary_accuracy: 0.8720 - false_negatives: 203.9686 - false_positives: 623.9855 - loss: 0.3957 - precision: 0.8306 - recall: 0.9382 - true_negatives: 2681.9832 - true_positives: 3130.0627\n",
      "For batch number 415 the model has a loss of 0.3797752559185028\n",
      "\u001b[1m415/689\u001b[0m \u001b[32m━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━\u001b[0m \u001b[1m16s\u001b[0m 58ms/step - auc: 0.9267 - binary_accuracy: 0.8720 - false_negatives: 204.4506 - false_positives: 625.3590 - loss: 0.3957 - precision: 0.8307 - recall: 0.9382 - true_negatives: 2688.6482 - true_positives: 3137.5422\n",
      "For batch number 416 the model has a loss of 0.3794328570365906\n",
      "\u001b[1m416/689\u001b[0m \u001b[32m━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━\u001b[0m \u001b[1m15s\u001b[0m 59ms/step - auc: 0.9267 - binary_accuracy: 0.8720 - false_negatives: 204.9327 - false_positives: 626.7356 - loss: 0.3956 - precision: 0.8307 - recall: 0.9382 - true_negatives: 2695.3101 - true_positives: 3145.0217\n",
      "For batch number 417 the model has a loss of 0.37973129749298096\n",
      "\u001b[1m417/689\u001b[0m \u001b[32m━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━\u001b[0m \u001b[1m15s\u001b[0m 59ms/step - auc: 0.9267 - binary_accuracy: 0.8720 - false_negatives: 205.4125 - false_positives: 628.1271 - loss: 0.3956 - precision: 0.8307 - recall: 0.9382 - true_negatives: 2701.9736 - true_positives: 3152.4868\n",
      "For batch number 418 the model has a loss of 0.3794820308685303\n",
      "\u001b[1m418/689\u001b[0m \u001b[32m━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━\u001b[0m \u001b[1m15s\u001b[0m 59ms/step - auc: 0.9267 - binary_accuracy: 0.8721 - false_negatives: 205.8900 - false_positives: 629.5192 - loss: 0.3956 - precision: 0.8307 - recall: 0.9382 - true_negatives: 2708.6387 - true_positives: 3159.9521\n",
      "For batch number 419 the model has a loss of 0.37931233644485474\n",
      "\u001b[1m419/689\u001b[0m \u001b[32m━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━\u001b[0m \u001b[1m15s\u001b[0m 59ms/step - auc: 0.9267 - binary_accuracy: 0.8721 - false_negatives: 206.3675 - false_positives: 630.9093 - loss: 0.3955 - precision: 0.8307 - recall: 0.9382 - true_negatives: 2715.3054 - true_positives: 3167.4177\n",
      "For batch number 420 the model has a loss of 0.3791380822658539\n",
      "\u001b[1m420/689\u001b[0m \u001b[32m━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━\u001b[0m \u001b[1m15s\u001b[0m 59ms/step - auc: 0.9267 - binary_accuracy: 0.8721 - false_negatives: 206.8476 - false_positives: 632.2928 - loss: 0.3955 - precision: 0.8308 - recall: 0.9382 - true_negatives: 2721.9644 - true_positives: 3174.8953\n",
      "For batch number 421 the model has a loss of 0.37878602743148804\n",
      "\u001b[1m421/689\u001b[0m \u001b[32m━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━\u001b[0m \u001b[1m15s\u001b[0m 59ms/step - auc: 0.9267 - binary_accuracy: 0.8721 - false_negatives: 207.3278 - false_positives: 633.6722 - loss: 0.3955 - precision: 0.8308 - recall: 0.9382 - true_negatives: 2728.6248 - true_positives: 3182.3752\n",
      "For batch number 422 the model has a loss of 0.3786471486091614\n",
      "\u001b[1m422/689\u001b[0m \u001b[32m━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━\u001b[0m \u001b[1m15s\u001b[0m 59ms/step - auc: 0.9267 - binary_accuracy: 0.8721 - false_negatives: 207.8057 - false_positives: 635.0569 - loss: 0.3954 - precision: 0.8308 - recall: 0.9383 - true_negatives: 2735.2866 - true_positives: 3189.8508\n",
      "For batch number 423 the model has a loss of 0.37837937474250793\n",
      "\u001b[1m423/689\u001b[0m \u001b[32m━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━\u001b[0m \u001b[1m15s\u001b[0m 59ms/step - auc: 0.9267 - binary_accuracy: 0.8722 - false_negatives: 208.2813 - false_positives: 636.4421 - loss: 0.3954 - precision: 0.8308 - recall: 0.9383 - true_negatives: 2741.9526 - true_positives: 3197.3240\n",
      "For batch number 424 the model has a loss of 0.37805014848709106\n",
      "\u001b[1m424/689\u001b[0m \u001b[32m━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━\u001b[0m \u001b[1m15s\u001b[0m 59ms/step - auc: 0.9267 - binary_accuracy: 0.8722 - false_negatives: 208.7547 - false_positives: 637.8207 - loss: 0.3953 - precision: 0.8308 - recall: 0.9383 - true_negatives: 2748.6179 - true_positives: 3204.8066\n",
      "For batch number 425 the model has a loss of 0.3788697421550751\n",
      "\u001b[1m425/689\u001b[0m \u001b[32m━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━\u001b[0m \u001b[1m15s\u001b[0m 59ms/step - auc: 0.9267 - binary_accuracy: 0.8722 - false_negatives: 209.2376 - false_positives: 639.1953 - loss: 0.3953 - precision: 0.8309 - recall: 0.9383 - true_negatives: 2755.2705 - true_positives: 3212.2964\n",
      "For batch number 426 the model has a loss of 0.37875404953956604\n",
      "\u001b[1m426/689\u001b[0m \u001b[32m━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━\u001b[0m \u001b[1m15s\u001b[0m 59ms/step - auc: 0.9267 - binary_accuracy: 0.8722 - false_negatives: 209.7207 - false_positives: 640.5728 - loss: 0.3953 - precision: 0.8309 - recall: 0.9383 - true_negatives: 2761.9272 - true_positives: 3219.7793\n",
      "For batch number 427 the model has a loss of 0.37857723236083984\n",
      "\u001b[1m427/689\u001b[0m \u001b[32m━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━\u001b[0m \u001b[1m15s\u001b[0m 59ms/step - auc: 0.9267 - binary_accuracy: 0.8722 - false_negatives: 210.2061 - false_positives: 641.9508 - loss: 0.3952 - precision: 0.8309 - recall: 0.9383 - true_negatives: 2768.5808 - true_positives: 3227.2622\n",
      "For batch number 428 the model has a loss of 0.37940695881843567\n",
      "\u001b[1m428/689\u001b[0m \u001b[32m━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━\u001b[0m \u001b[1m15s\u001b[0m 59ms/step - auc: 0.9267 - binary_accuracy: 0.8722 - false_negatives: 210.6963 - false_positives: 643.3341 - loss: 0.3952 - precision: 0.8309 - recall: 0.9383 - true_negatives: 2775.2266 - true_positives: 3234.7429\n",
      "For batch number 429 the model has a loss of 0.3793099522590637\n",
      "\u001b[1m429/689\u001b[0m \u001b[32m━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━\u001b[0m \u001b[1m15s\u001b[0m 59ms/step - auc: 0.9267 - binary_accuracy: 0.8723 - false_negatives: 211.1865 - false_positives: 644.7203 - loss: 0.3951 - precision: 0.8309 - recall: 0.9383 - true_negatives: 2781.8718 - true_positives: 3242.2214\n",
      "For batch number 430 the model has a loss of 0.37928053736686707\n",
      "\u001b[1m430/689\u001b[0m \u001b[32m━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━\u001b[0m \u001b[1m15s\u001b[0m 59ms/step - auc: 0.9267 - binary_accuracy: 0.8723 - false_negatives: 211.6837 - false_positives: 646.1024 - loss: 0.3951 - precision: 0.8310 - recall: 0.9383 - true_negatives: 2788.5071 - true_positives: 3249.7070\n",
      "For batch number 431 the model has a loss of 0.37913256883621216\n",
      "\u001b[1m431/689\u001b[0m \u001b[32m━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━\u001b[0m \u001b[1m15s\u001b[0m 59ms/step - auc: 0.9267 - binary_accuracy: 0.8723 - false_negatives: 212.1810 - false_positives: 647.4872 - loss: 0.3951 - precision: 0.8310 - recall: 0.9383 - true_negatives: 2795.1462 - true_positives: 3257.1855\n",
      "For batch number 432 the model has a loss of 0.37930166721343994\n",
      "\u001b[1m432/689\u001b[0m \u001b[32m━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━\u001b[0m \u001b[1m15s\u001b[0m 59ms/step - auc: 0.9267 - binary_accuracy: 0.8723 - false_negatives: 212.6782 - false_positives: 648.8727 - loss: 0.3950 - precision: 0.8310 - recall: 0.9383 - true_negatives: 2801.7800 - true_positives: 3264.6689\n",
      "For batch number 433 the model has a loss of 0.37956738471984863\n",
      "\u001b[1m433/689\u001b[0m \u001b[32m━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━\u001b[0m \u001b[1m14s\u001b[0m 59ms/step - auc: 0.9267 - binary_accuracy: 0.8723 - false_negatives: 213.1801 - false_positives: 650.2540 - loss: 0.3950 - precision: 0.8310 - recall: 0.9383 - true_negatives: 2808.4133 - true_positives: 3272.1523\n",
      "For batch number 434 the model has a loss of 0.379234254360199\n",
      "\u001b[1m434/689\u001b[0m \u001b[32m━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━\u001b[0m \u001b[1m14s\u001b[0m 59ms/step - auc: 0.9267 - binary_accuracy: 0.8723 - false_negatives: 213.6820 - false_positives: 651.6337 - loss: 0.3950 - precision: 0.8310 - recall: 0.9383 - true_negatives: 2815.0437 - true_positives: 3279.6406\n",
      "For batch number 435 the model has a loss of 0.37880268692970276\n",
      "\u001b[1m435/689\u001b[0m \u001b[32m━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━\u001b[0m \u001b[1m14s\u001b[0m 59ms/step - auc: 0.9267 - binary_accuracy: 0.8724 - false_negatives: 214.1816 - false_positives: 653.0092 - loss: 0.3949 - precision: 0.8310 - recall: 0.9383 - true_negatives: 2821.6782 - true_positives: 3287.1311\n",
      "For batch number 436 the model has a loss of 0.378432035446167\n",
      "\u001b[1m436/689\u001b[0m \u001b[32m━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━\u001b[0m \u001b[1m14s\u001b[0m 59ms/step - auc: 0.9267 - binary_accuracy: 0.8724 - false_negatives: 214.6789 - false_positives: 654.3831 - loss: 0.3949 - precision: 0.8311 - recall: 0.9383 - true_negatives: 2828.3188 - true_positives: 3294.6194\n",
      "For batch number 437 the model has a loss of 0.3780274987220764\n",
      "\u001b[1m437/689\u001b[0m \u001b[32m━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━\u001b[0m \u001b[1m14s\u001b[0m 59ms/step - auc: 0.9267 - binary_accuracy: 0.8724 - false_negatives: 215.1762 - false_positives: 655.7505 - loss: 0.3948 - precision: 0.8311 - recall: 0.9383 - true_negatives: 2834.9656 - true_positives: 3302.1077\n",
      "For batch number 438 the model has a loss of 0.3777558505535126\n",
      "\u001b[1m438/689\u001b[0m \u001b[32m━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━\u001b[0m \u001b[1m14s\u001b[0m 59ms/step - auc: 0.9267 - binary_accuracy: 0.8724 - false_negatives: 215.6712 - false_positives: 657.1187 - loss: 0.3948 - precision: 0.8311 - recall: 0.9383 - true_negatives: 2841.6165 - true_positives: 3309.5935\n",
      "For batch number 439 the model has a loss of 0.3774762451648712\n",
      "\u001b[1m439/689\u001b[0m \u001b[32m━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━\u001b[0m \u001b[1m14s\u001b[0m 59ms/step - auc: 0.9267 - binary_accuracy: 0.8724 - false_negatives: 216.1640 - false_positives: 658.4852 - loss: 0.3948 - precision: 0.8311 - recall: 0.9383 - true_negatives: 2848.2642 - true_positives: 3317.0867\n",
      "For batch number 440 the model has a loss of 0.3772045075893402\n",
      "\u001b[1m440/689\u001b[0m \u001b[32m━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━\u001b[0m \u001b[1m14s\u001b[0m 59ms/step - auc: 0.9267 - binary_accuracy: 0.8724 - false_negatives: 216.6568 - false_positives: 659.8523 - loss: 0.3947 - precision: 0.8311 - recall: 0.9383 - true_negatives: 2854.9067 - true_positives: 3324.5840\n",
      "For batch number 441 the model has a loss of 0.37684038281440735\n",
      "\u001b[1m441/689\u001b[0m \u001b[32m━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━\u001b[0m \u001b[1m14s\u001b[0m 59ms/step - auc: 0.9267 - binary_accuracy: 0.8724 - false_negatives: 217.1474 - false_positives: 661.2200 - loss: 0.3947 - precision: 0.8312 - recall: 0.9383 - true_negatives: 2861.5557 - true_positives: 3332.0771\n",
      "For batch number 442 the model has a loss of 0.37683993577957153\n",
      "\u001b[1m442/689\u001b[0m \u001b[32m━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━\u001b[0m \u001b[1m14s\u001b[0m 59ms/step - auc: 0.9267 - binary_accuracy: 0.8725 - false_negatives: 217.6380 - false_positives: 662.5883 - loss: 0.3946 - precision: 0.8312 - recall: 0.9383 - true_negatives: 2868.2083 - true_positives: 3339.5657\n",
      "For batch number 443 the model has a loss of 0.3765903115272522\n",
      "\u001b[1m443/689\u001b[0m \u001b[32m━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━\u001b[0m \u001b[1m14s\u001b[0m 59ms/step - auc: 0.9267 - binary_accuracy: 0.8725 - false_negatives: 218.1264 - false_positives: 663.9571 - loss: 0.3946 - precision: 0.8312 - recall: 0.9383 - true_negatives: 2874.8645 - true_positives: 3347.0520\n",
      "For batch number 444 the model has a loss of 0.3764798939228058\n",
      "\u001b[1m444/689\u001b[0m \u001b[32m━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━\u001b[0m \u001b[1m14s\u001b[0m 59ms/step - auc: 0.9267 - binary_accuracy: 0.8725 - false_negatives: 218.6126 - false_positives: 665.3289 - loss: 0.3946 - precision: 0.8312 - recall: 0.9383 - true_negatives: 2881.5159 - true_positives: 3354.5427\n",
      "For batch number 445 the model has a loss of 0.37635526061058044\n",
      "\u001b[1m445/689\u001b[0m \u001b[32m━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━\u001b[0m \u001b[1m14s\u001b[0m 59ms/step - auc: 0.9268 - binary_accuracy: 0.8725 - false_negatives: 219.0966 - false_positives: 666.7034 - loss: 0.3945 - precision: 0.8312 - recall: 0.9383 - true_negatives: 2888.1663 - true_positives: 3362.0337\n",
      "For batch number 446 the model has a loss of 0.376149445772171\n",
      "\u001b[1m446/689\u001b[0m \u001b[32m━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━\u001b[0m \u001b[1m14s\u001b[0m 59ms/step - auc: 0.9268 - binary_accuracy: 0.8725 - false_negatives: 219.5830 - false_positives: 668.0740 - loss: 0.3945 - precision: 0.8313 - recall: 0.9383 - true_negatives: 2894.8184 - true_positives: 3369.5247\n",
      "For batch number 447 the model has a loss of 0.3761598765850067\n",
      "\u001b[1m447/689\u001b[0m \u001b[32m━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━\u001b[0m \u001b[1m14s\u001b[0m 59ms/step - auc: 0.9268 - binary_accuracy: 0.8725 - false_negatives: 220.0694 - false_positives: 669.4496 - loss: 0.3944 - precision: 0.8313 - recall: 0.9383 - true_negatives: 2901.4719 - true_positives: 3377.0090\n",
      "For batch number 448 the model has a loss of 0.37578997015953064\n",
      "\u001b[1m448/689\u001b[0m \u001b[32m━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━\u001b[0m \u001b[1m14s\u001b[0m 59ms/step - auc: 0.9268 - binary_accuracy: 0.8726 - false_negatives: 220.5558 - false_positives: 670.8192 - loss: 0.3944 - precision: 0.8313 - recall: 0.9383 - true_negatives: 2908.1250 - true_positives: 3384.5000\n",
      "For batch number 449 the model has a loss of 0.3755823075771332\n",
      "\u001b[1m449/689\u001b[0m \u001b[32m━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━\u001b[0m \u001b[1m14s\u001b[0m 59ms/step - auc: 0.9268 - binary_accuracy: 0.8726 - false_negatives: 221.0401 - false_positives: 672.1938 - loss: 0.3944 - precision: 0.8313 - recall: 0.9383 - true_negatives: 2914.7817 - true_positives: 3391.9844\n",
      "For batch number 450 the model has a loss of 0.37498870491981506\n",
      "\u001b[1m450/689\u001b[0m \u001b[32m━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━\u001b[0m \u001b[1m13s\u001b[0m 59ms/step - auc: 0.9268 - binary_accuracy: 0.8726 - false_negatives: 221.5222 - false_positives: 673.5645 - loss: 0.3943 - precision: 0.8313 - recall: 0.9383 - true_negatives: 2921.4399 - true_positives: 3399.4734\n",
      "For batch number 451 the model has a loss of 0.3747086524963379\n",
      "\u001b[1m451/689\u001b[0m \u001b[32m━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━\u001b[0m \u001b[1m13s\u001b[0m 59ms/step - auc: 0.9268 - binary_accuracy: 0.8726 - false_negatives: 222.0044 - false_positives: 674.9335 - loss: 0.3943 - precision: 0.8314 - recall: 0.9383 - true_negatives: 2928.1021 - true_positives: 3406.9602\n",
      "For batch number 452 the model has a loss of 0.37459105253219604\n",
      "\u001b[1m452/689\u001b[0m \u001b[32m━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━\u001b[0m \u001b[1m13s\u001b[0m 59ms/step - auc: 0.9268 - binary_accuracy: 0.8726 - false_negatives: 222.4889 - false_positives: 676.3031 - loss: 0.3942 - precision: 0.8314 - recall: 0.9383 - true_negatives: 2934.7654 - true_positives: 3414.4424\n",
      "For batch number 453 the model has a loss of 0.37431448698043823\n",
      "\u001b[1m453/689\u001b[0m \u001b[32m━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━\u001b[0m \u001b[1m13s\u001b[0m 59ms/step - auc: 0.9268 - binary_accuracy: 0.8726 - false_negatives: 222.9735 - false_positives: 677.6667 - loss: 0.3942 - precision: 0.8314 - recall: 0.9383 - true_negatives: 2941.4260 - true_positives: 3421.9338\n",
      "For batch number 454 the model has a loss of 0.3744146525859833\n",
      "\u001b[1m454/689\u001b[0m \u001b[32m━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━\u001b[0m \u001b[1m13s\u001b[0m 59ms/step - auc: 0.9268 - binary_accuracy: 0.8727 - false_negatives: 223.4604 - false_positives: 679.0264 - loss: 0.3941 - precision: 0.8314 - recall: 0.9383 - true_negatives: 2948.0837 - true_positives: 3429.4294\n",
      "For batch number 455 the model has a loss of 0.3751726448535919\n",
      "\u001b[1m455/689\u001b[0m \u001b[32m━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━\u001b[0m \u001b[1m13s\u001b[0m 59ms/step - auc: 0.9268 - binary_accuracy: 0.8727 - false_negatives: 223.9473 - false_positives: 680.3824 - loss: 0.3941 - precision: 0.8314 - recall: 0.9383 - true_negatives: 2954.7407 - true_positives: 3436.9297\n",
      "For batch number 456 the model has a loss of 0.3747410774230957\n",
      "\u001b[1m456/689\u001b[0m \u001b[32m━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━\u001b[0m \u001b[1m13s\u001b[0m 59ms/step - auc: 0.9268 - binary_accuracy: 0.8727 - false_negatives: 224.4320 - false_positives: 681.7325 - loss: 0.3941 - precision: 0.8315 - recall: 0.9383 - true_negatives: 2961.3926 - true_positives: 3444.4429\n",
      "For batch number 457 the model has a loss of 0.3743818700313568\n",
      "\u001b[1m457/689\u001b[0m \u001b[32m━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━\u001b[0m \u001b[1m13s\u001b[0m 59ms/step - auc: 0.9268 - binary_accuracy: 0.8727 - false_negatives: 224.9147 - false_positives: 683.0809 - loss: 0.3940 - precision: 0.8315 - recall: 0.9383 - true_negatives: 2968.0503 - true_positives: 3451.9541\n",
      "For batch number 458 the model has a loss of 0.37422671914100647\n",
      "\u001b[1m458/689\u001b[0m \u001b[32m━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━\u001b[0m \u001b[1m13s\u001b[0m 59ms/step - auc: 0.9268 - binary_accuracy: 0.8727 - false_negatives: 225.3974 - false_positives: 684.4279 - loss: 0.3940 - precision: 0.8315 - recall: 0.9383 - true_negatives: 2974.7117 - true_positives: 3459.4629\n",
      "For batch number 459 the model has a loss of 0.3738032579421997\n",
      "\u001b[1m459/689\u001b[0m \u001b[32m━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━\u001b[0m \u001b[1m13s\u001b[0m 59ms/step - auc: 0.9268 - binary_accuracy: 0.8728 - false_negatives: 225.8780 - false_positives: 685.7734 - loss: 0.3939 - precision: 0.8315 - recall: 0.9383 - true_negatives: 2981.3813 - true_positives: 3466.9673\n",
      "For batch number 460 the model has a loss of 0.3736112117767334\n",
      "\u001b[1m460/689\u001b[0m \u001b[32m━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━\u001b[0m \u001b[1m13s\u001b[0m 59ms/step - auc: 0.9268 - binary_accuracy: 0.8728 - false_negatives: 226.3587 - false_positives: 687.1174 - loss: 0.3939 - precision: 0.8315 - recall: 0.9383 - true_negatives: 2988.0566 - true_positives: 3474.4673\n",
      "For batch number 461 the model has a loss of 0.3737919330596924\n",
      "\u001b[1m461/689\u001b[0m \u001b[32m━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━\u001b[0m \u001b[1m13s\u001b[0m 58ms/step - auc: 0.9269 - binary_accuracy: 0.8728 - false_negatives: 226.8438 - false_positives: 688.4620 - loss: 0.3938 - precision: 0.8316 - recall: 0.9383 - true_negatives: 2994.7332 - true_positives: 3481.9609\n",
      "For batch number 462 the model has a loss of 0.3734125792980194\n",
      "\u001b[1m462/689\u001b[0m \u001b[32m━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━\u001b[0m \u001b[1m13s\u001b[0m 58ms/step - auc: 0.9269 - binary_accuracy: 0.8728 - false_negatives: 227.3268 - false_positives: 689.8008 - loss: 0.3938 - precision: 0.8316 - recall: 0.9383 - true_negatives: 3001.4114 - true_positives: 3489.4609\n",
      "For batch number 463 the model has a loss of 0.37312811613082886\n",
      "\u001b[1m463/689\u001b[0m \u001b[32m━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━\u001b[0m \u001b[1m13s\u001b[0m 58ms/step - auc: 0.9269 - binary_accuracy: 0.8728 - false_negatives: 227.8099 - false_positives: 691.1382 - loss: 0.3937 - precision: 0.8316 - recall: 0.9383 - true_negatives: 3008.0928 - true_positives: 3496.9590\n",
      "For batch number 464 the model has a loss of 0.37302616238594055\n",
      "\u001b[1m464/689\u001b[0m \u001b[32m━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━\u001b[0m \u001b[1m13s\u001b[0m 58ms/step - auc: 0.9269 - binary_accuracy: 0.8728 - false_negatives: 228.2910 - false_positives: 692.4763 - loss: 0.3937 - precision: 0.8316 - recall: 0.9383 - true_negatives: 3014.7759 - true_positives: 3504.4568\n",
      "For batch number 465 the model has a loss of 0.3730478882789612\n",
      "\u001b[1m465/689\u001b[0m \u001b[32m━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━\u001b[0m \u001b[1m13s\u001b[0m 58ms/step - auc: 0.9269 - binary_accuracy: 0.8729 - false_negatives: 228.7720 - false_positives: 693.8193 - loss: 0.3937 - precision: 0.8317 - recall: 0.9383 - true_negatives: 3021.4558 - true_positives: 3511.9526\n",
      "For batch number 466 the model has a loss of 0.3728939890861511\n",
      "\u001b[1m466/689\u001b[0m \u001b[32m━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━\u001b[0m \u001b[1m13s\u001b[0m 58ms/step - auc: 0.9269 - binary_accuracy: 0.8729 - false_negatives: 229.2511 - false_positives: 695.1652 - loss: 0.3936 - precision: 0.8317 - recall: 0.9383 - true_negatives: 3028.1353 - true_positives: 3519.4485\n",
      "For batch number 467 the model has a loss of 0.37250250577926636\n",
      "\u001b[1m467/689\u001b[0m \u001b[32m━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━\u001b[0m \u001b[1m12s\u001b[0m 58ms/step - auc: 0.9269 - binary_accuracy: 0.8729 - false_negatives: 229.7281 - false_positives: 696.5054 - loss: 0.3936 - precision: 0.8317 - recall: 0.9383 - true_negatives: 3034.8179 - true_positives: 3526.9485\n",
      "For batch number 468 the model has a loss of 0.3722475469112396\n",
      "\u001b[1m468/689\u001b[0m \u001b[32m━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━\u001b[0m \u001b[1m12s\u001b[0m 58ms/step - auc: 0.9269 - binary_accuracy: 0.8729 - false_negatives: 230.2030 - false_positives: 697.8440 - loss: 0.3935 - precision: 0.8317 - recall: 0.9383 - true_negatives: 3041.5022 - true_positives: 3534.4509\n",
      "For batch number 469 the model has a loss of 0.37193530797958374\n",
      "\u001b[1m469/689\u001b[0m \u001b[32m━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━\u001b[0m \u001b[1m12s\u001b[0m 58ms/step - auc: 0.9269 - binary_accuracy: 0.8729 - false_negatives: 230.6759 - false_positives: 699.1812 - loss: 0.3935 - precision: 0.8317 - recall: 0.9383 - true_negatives: 3048.1855 - true_positives: 3541.9573\n",
      "For batch number 470 the model has a loss of 0.3725774884223938\n",
      "\u001b[1m470/689\u001b[0m \u001b[32m━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━\u001b[0m \u001b[1m12s\u001b[0m 58ms/step - auc: 0.9269 - binary_accuracy: 0.8730 - false_negatives: 231.1489 - false_positives: 700.5170 - loss: 0.3934 - precision: 0.8318 - recall: 0.9383 - true_negatives: 3054.8723 - true_positives: 3549.4617\n",
      "For batch number 471 the model has a loss of 0.37263885140419006\n",
      "\u001b[1m471/689\u001b[0m \u001b[32m━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━\u001b[0m \u001b[1m12s\u001b[0m 59ms/step - auc: 0.9269 - binary_accuracy: 0.8730 - false_negatives: 231.6221 - false_positives: 701.8492 - loss: 0.3934 - precision: 0.8318 - recall: 0.9383 - true_negatives: 3061.5583 - true_positives: 3556.9702\n",
      "For batch number 472 the model has a loss of 0.37241366505622864\n",
      "\u001b[1m472/689\u001b[0m \u001b[32m━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━\u001b[0m \u001b[1m12s\u001b[0m 59ms/step - auc: 0.9269 - binary_accuracy: 0.8730 - false_negatives: 232.0932 - false_positives: 703.1822 - loss: 0.3933 - precision: 0.8318 - recall: 0.9383 - true_negatives: 3068.2500 - true_positives: 3564.4746\n",
      "For batch number 473 the model has a loss of 0.371913343667984\n",
      "\u001b[1m473/689\u001b[0m \u001b[32m━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━\u001b[0m \u001b[1m12s\u001b[0m 58ms/step - auc: 0.9270 - binary_accuracy: 0.8730 - false_negatives: 232.5624 - false_positives: 704.5095 - loss: 0.3933 - precision: 0.8318 - recall: 0.9383 - true_negatives: 3074.9534 - true_positives: 3571.9746\n",
      "For batch number 474 the model has a loss of 0.37165918946266174\n",
      "\u001b[1m474/689\u001b[0m \u001b[32m━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━\u001b[0m \u001b[1m12s\u001b[0m 58ms/step - auc: 0.9270 - binary_accuracy: 0.8730 - false_negatives: 233.0295 - false_positives: 705.8397 - loss: 0.3933 - precision: 0.8318 - recall: 0.9383 - true_negatives: 3081.6604 - true_positives: 3579.4705\n",
      "For batch number 475 the model has a loss of 0.37134185433387756\n",
      "\u001b[1m475/689\u001b[0m \u001b[32m━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━\u001b[0m \u001b[1m12s\u001b[0m 58ms/step - auc: 0.9270 - binary_accuracy: 0.8730 - false_negatives: 233.4947 - false_positives: 707.1684 - loss: 0.3932 - precision: 0.8319 - recall: 0.9383 - true_negatives: 3088.3643 - true_positives: 3586.9727\n",
      "For batch number 476 the model has a loss of 0.37090376019477844\n",
      "\u001b[1m476/689\u001b[0m \u001b[32m━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━\u001b[0m \u001b[1m12s\u001b[0m 58ms/step - auc: 0.9270 - binary_accuracy: 0.8731 - false_negatives: 233.9580 - false_positives: 708.4916 - loss: 0.3932 - precision: 0.8319 - recall: 0.9383 - true_negatives: 3095.0735 - true_positives: 3594.4768\n",
      "For batch number 477 the model has a loss of 0.37059345841407776\n",
      "\u001b[1m477/689\u001b[0m \u001b[32m━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━\u001b[0m \u001b[1m12s\u001b[0m 58ms/step - auc: 0.9270 - binary_accuracy: 0.8731 - false_negatives: 234.4214 - false_positives: 709.8092 - loss: 0.3931 - precision: 0.8319 - recall: 0.9383 - true_negatives: 3101.7820 - true_positives: 3601.9873\n",
      "For batch number 478 the model has a loss of 0.3704903721809387\n",
      "\u001b[1m478/689\u001b[0m \u001b[32m━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━\u001b[0m \u001b[1m12s\u001b[0m 58ms/step - auc: 0.9270 - binary_accuracy: 0.8731 - false_negatives: 234.8849 - false_positives: 711.1255 - loss: 0.3931 - precision: 0.8319 - recall: 0.9384 - true_negatives: 3108.4854 - true_positives: 3609.5042\n",
      "For batch number 479 the model has a loss of 0.370074063539505\n",
      "\u001b[1m479/689\u001b[0m \u001b[32m━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━\u001b[0m \u001b[1m12s\u001b[0m 58ms/step - auc: 0.9270 - binary_accuracy: 0.8731 - false_negatives: 235.3466 - false_positives: 712.4384 - loss: 0.3930 - precision: 0.8320 - recall: 0.9384 - true_negatives: 3115.1921 - true_positives: 3617.0229\n",
      "For batch number 480 the model has a loss of 0.3699437379837036\n",
      "\u001b[1m480/689\u001b[0m \u001b[32m━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━\u001b[0m \u001b[1m12s\u001b[0m 58ms/step - auc: 0.9270 - binary_accuracy: 0.8731 - false_negatives: 235.8083 - false_positives: 713.7521 - loss: 0.3930 - precision: 0.8320 - recall: 0.9384 - true_negatives: 3121.9126 - true_positives: 3624.5271\n",
      "For batch number 481 the model has a loss of 0.3698160648345947\n",
      "\u001b[1m481/689\u001b[0m \u001b[32m━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━\u001b[0m \u001b[1m12s\u001b[0m 58ms/step - auc: 0.9270 - binary_accuracy: 0.8732 - false_negatives: 236.2703 - false_positives: 715.0707 - loss: 0.3929 - precision: 0.8320 - recall: 0.9384 - true_negatives: 3128.6362 - true_positives: 3632.0229\n",
      "For batch number 482 the model has a loss of 0.3694595396518707\n",
      "\u001b[1m482/689\u001b[0m \u001b[32m━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━\u001b[0m \u001b[1m12s\u001b[0m 58ms/step - auc: 0.9270 - binary_accuracy: 0.8732 - false_negatives: 236.7324 - false_positives: 716.3838 - loss: 0.3929 - precision: 0.8320 - recall: 0.9384 - true_negatives: 3135.3589 - true_positives: 3639.5249\n",
      "For batch number 483 the model has a loss of 0.3693968951702118\n",
      "\u001b[1m483/689\u001b[0m \u001b[32m━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━\u001b[0m \u001b[1m12s\u001b[0m 58ms/step - auc: 0.9270 - binary_accuracy: 0.8732 - false_negatives: 237.1946 - false_positives: 717.6936 - loss: 0.3928 - precision: 0.8321 - recall: 0.9384 - true_negatives: 3142.0891 - true_positives: 3647.0227\n",
      "For batch number 484 the model has a loss of 0.36913734674453735\n",
      "\u001b[1m484/689\u001b[0m \u001b[32m━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━\u001b[0m \u001b[1m11s\u001b[0m 58ms/step - auc: 0.9271 - binary_accuracy: 0.8732 - false_negatives: 237.6550 - false_positives: 719.0000 - loss: 0.3928 - precision: 0.8321 - recall: 0.9384 - true_negatives: 3148.8162 - true_positives: 3654.5288\n",
      "For batch number 485 the model has a loss of 0.36883729696273804\n",
      "\u001b[1m485/689\u001b[0m \u001b[32m━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━\u001b[0m \u001b[1m11s\u001b[0m 58ms/step - auc: 0.9271 - binary_accuracy: 0.8732 - false_negatives: 238.1134 - false_positives: 720.3031 - loss: 0.3927 - precision: 0.8321 - recall: 0.9384 - true_negatives: 3155.5403 - true_positives: 3662.0432\n",
      "For batch number 486 the model has a loss of 0.368854284286499\n",
      "\u001b[1m486/689\u001b[0m \u001b[32m━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━\u001b[0m \u001b[1m11s\u001b[0m 58ms/step - auc: 0.9271 - binary_accuracy: 0.8733 - false_negatives: 238.5720 - false_positives: 721.6029 - loss: 0.3927 - precision: 0.8321 - recall: 0.9384 - true_negatives: 3162.2695 - true_positives: 3669.5557\n",
      "For batch number 487 the model has a loss of 0.3686732351779938\n",
      "\u001b[1m487/689\u001b[0m \u001b[32m━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━\u001b[0m \u001b[1m11s\u001b[0m 58ms/step - auc: 0.9271 - binary_accuracy: 0.8733 - false_negatives: 239.0287 - false_positives: 722.9035 - loss: 0.3926 - precision: 0.8321 - recall: 0.9384 - true_negatives: 3169.0061 - true_positives: 3677.0615\n",
      "For batch number 488 the model has a loss of 0.36897045373916626\n",
      "\u001b[1m488/689\u001b[0m \u001b[32m━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━\u001b[0m \u001b[1m11s\u001b[0m 58ms/step - auc: 0.9271 - binary_accuracy: 0.8733 - false_negatives: 239.4877 - false_positives: 724.2029 - loss: 0.3926 - precision: 0.8322 - recall: 0.9384 - true_negatives: 3175.7458 - true_positives: 3684.5635\n",
      "For batch number 489 the model has a loss of 0.368722140789032\n",
      "\u001b[1m489/689\u001b[0m \u001b[32m━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━\u001b[0m \u001b[1m11s\u001b[0m 58ms/step - auc: 0.9271 - binary_accuracy: 0.8733 - false_negatives: 239.9448 - false_positives: 725.5031 - loss: 0.3925 - precision: 0.8322 - recall: 0.9384 - true_negatives: 3182.4827 - true_positives: 3692.0696\n",
      "For batch number 490 the model has a loss of 0.3686615526676178\n",
      "\u001b[1m490/689\u001b[0m \u001b[32m━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━\u001b[0m \u001b[1m11s\u001b[0m 58ms/step - auc: 0.9271 - binary_accuracy: 0.8734 - false_negatives: 240.4061 - false_positives: 726.7980 - loss: 0.3925 - precision: 0.8322 - recall: 0.9384 - true_negatives: 3189.2102 - true_positives: 3699.5857\n",
      "For batch number 491 the model has a loss of 0.36831003427505493\n",
      "\u001b[1m491/689\u001b[0m \u001b[32m━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━\u001b[0m \u001b[1m11s\u001b[0m 58ms/step - auc: 0.9271 - binary_accuracy: 0.8734 - false_negatives: 240.8676 - false_positives: 728.0896 - loss: 0.3924 - precision: 0.8322 - recall: 0.9384 - true_negatives: 3195.9348 - true_positives: 3707.1079\n",
      "For batch number 492 the model has a loss of 0.3682875633239746\n",
      "\u001b[1m492/689\u001b[0m \u001b[32m━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━\u001b[0m \u001b[1m11s\u001b[0m 58ms/step - auc: 0.9271 - binary_accuracy: 0.8734 - false_negatives: 241.3293 - false_positives: 729.3821 - loss: 0.3924 - precision: 0.8323 - recall: 0.9384 - true_negatives: 3202.6606 - true_positives: 3714.6279\n",
      "For batch number 493 the model has a loss of 0.36791568994522095\n",
      "\u001b[1m493/689\u001b[0m \u001b[32m━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━\u001b[0m \u001b[1m11s\u001b[0m 58ms/step - auc: 0.9272 - binary_accuracy: 0.8734 - false_negatives: 241.7890 - false_positives: 730.6734 - loss: 0.3923 - precision: 0.8323 - recall: 0.9384 - true_negatives: 3209.3894 - true_positives: 3722.1482\n",
      "For batch number 494 the model has a loss of 0.36818474531173706\n",
      "\u001b[1m494/689\u001b[0m \u001b[32m━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━\u001b[0m \u001b[1m11s\u001b[0m 58ms/step - auc: 0.9272 - binary_accuracy: 0.8734 - false_negatives: 242.2490 - false_positives: 731.9636 - loss: 0.3923 - precision: 0.8323 - recall: 0.9384 - true_negatives: 3216.1174 - true_positives: 3729.6699\n",
      "For batch number 495 the model has a loss of 0.36793187260627747\n",
      "\u001b[1m495/689\u001b[0m \u001b[32m━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━\u001b[0m \u001b[1m11s\u001b[0m 58ms/step - auc: 0.9272 - binary_accuracy: 0.8735 - false_negatives: 242.7091 - false_positives: 733.2505 - loss: 0.3922 - precision: 0.8323 - recall: 0.9384 - true_negatives: 3222.8506 - true_positives: 3737.1899\n",
      "For batch number 496 the model has a loss of 0.36815279722213745\n",
      "\u001b[1m496/689\u001b[0m \u001b[32m━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━\u001b[0m \u001b[1m11s\u001b[0m 58ms/step - auc: 0.9272 - binary_accuracy: 0.8735 - false_negatives: 243.1714 - false_positives: 734.5424 - loss: 0.3922 - precision: 0.8324 - recall: 0.9384 - true_negatives: 3229.5847 - true_positives: 3744.7017\n",
      "For batch number 497 the model has a loss of 0.3679053783416748\n",
      "\u001b[1m497/689\u001b[0m \u001b[32m━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━\u001b[0m \u001b[1m11s\u001b[0m 58ms/step - auc: 0.9272 - binary_accuracy: 0.8735 - false_negatives: 243.6338 - false_positives: 735.8330 - loss: 0.3921 - precision: 0.8324 - recall: 0.9384 - true_negatives: 3236.3179 - true_positives: 3752.2153\n",
      "For batch number 498 the model has a loss of 0.3681873679161072\n",
      "\u001b[1m498/689\u001b[0m \u001b[32m━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━\u001b[0m \u001b[1m11s\u001b[0m 58ms/step - auc: 0.9272 - binary_accuracy: 0.8735 - false_negatives: 244.0964 - false_positives: 737.1265 - loss: 0.3921 - precision: 0.8324 - recall: 0.9384 - true_negatives: 3243.0583 - true_positives: 3759.7190\n",
      "For batch number 499 the model has a loss of 0.3680001199245453\n",
      "\u001b[1m499/689\u001b[0m \u001b[32m━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━\u001b[0m \u001b[1m11s\u001b[0m 58ms/step - auc: 0.9272 - binary_accuracy: 0.8735 - false_negatives: 244.5631 - false_positives: 738.4168 - loss: 0.3920 - precision: 0.8324 - recall: 0.9384 - true_negatives: 3249.7957 - true_positives: 3767.2244\n",
      "For batch number 500 the model has a loss of 0.36744681000709534\n",
      "\u001b[1m500/689\u001b[0m \u001b[32m━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━\u001b[0m \u001b[1m11s\u001b[0m 58ms/step - auc: 0.9272 - binary_accuracy: 0.8736 - false_negatives: 245.0280 - false_positives: 739.7020 - loss: 0.3920 - precision: 0.8325 - recall: 0.9384 - true_negatives: 3256.5320 - true_positives: 3774.7380\n",
      "For batch number 501 the model has a loss of 0.36702120304107666\n",
      "\u001b[1m501/689\u001b[0m \u001b[32m━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━\u001b[0m \u001b[1m10s\u001b[0m 58ms/step - auc: 0.9272 - binary_accuracy: 0.8736 - false_negatives: 245.4910 - false_positives: 740.9840 - loss: 0.3919 - precision: 0.8325 - recall: 0.9385 - true_negatives: 3263.2654 - true_positives: 3782.2595\n",
      "For batch number 502 the model has a loss of 0.366486519575119\n",
      "\u001b[1m502/689\u001b[0m \u001b[32m━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━\u001b[0m \u001b[1m10s\u001b[0m 58ms/step - auc: 0.9273 - binary_accuracy: 0.8736 - false_negatives: 245.9522 - false_positives: 742.2629 - loss: 0.3919 - precision: 0.8325 - recall: 0.9385 - true_negatives: 3270.0059 - true_positives: 3789.7788\n",
      "For batch number 503 the model has a loss of 0.3661627173423767\n",
      "\u001b[1m503/689\u001b[0m \u001b[32m━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━\u001b[0m \u001b[1m10s\u001b[0m 58ms/step - auc: 0.9273 - binary_accuracy: 0.8736 - false_negatives: 246.4115 - false_positives: 743.5388 - loss: 0.3918 - precision: 0.8325 - recall: 0.9385 - true_negatives: 3276.7476 - true_positives: 3797.3022\n",
      "For batch number 504 the model has a loss of 0.36588627099990845\n",
      "\u001b[1m504/689\u001b[0m \u001b[32m━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━\u001b[0m \u001b[1m10s\u001b[0m 58ms/step - auc: 0.9273 - binary_accuracy: 0.8736 - false_negatives: 246.8690 - false_positives: 744.8135 - loss: 0.3918 - precision: 0.8326 - recall: 0.9385 - true_negatives: 3283.4919 - true_positives: 3804.8254\n",
      "For batch number 505 the model has a loss of 0.3656851351261139\n",
      "\u001b[1m505/689\u001b[0m \u001b[32m━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━\u001b[0m \u001b[1m10s\u001b[0m 58ms/step - auc: 0.9273 - binary_accuracy: 0.8737 - false_negatives: 247.3287 - false_positives: 746.0832 - loss: 0.3917 - precision: 0.8326 - recall: 0.9385 - true_negatives: 3290.2395 - true_positives: 3812.3486\n",
      "For batch number 506 the model has a loss of 0.3654017746448517\n",
      "\u001b[1m506/689\u001b[0m \u001b[32m━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━\u001b[0m \u001b[1m10s\u001b[0m 58ms/step - auc: 0.9273 - binary_accuracy: 0.8737 - false_negatives: 247.7885 - false_positives: 747.3498 - loss: 0.3917 - precision: 0.8326 - recall: 0.9385 - true_negatives: 3296.9922 - true_positives: 3819.8696\n",
      "For batch number 507 the model has a loss of 0.365590900182724\n",
      "\u001b[1m507/689\u001b[0m \u001b[32m━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━\u001b[0m \u001b[1m10s\u001b[0m 58ms/step - auc: 0.9273 - binary_accuracy: 0.8737 - false_negatives: 248.2466 - false_positives: 748.6154 - loss: 0.3916 - precision: 0.8327 - recall: 0.9385 - true_negatives: 3303.7397 - true_positives: 3827.3984\n",
      "For batch number 508 the model has a loss of 0.3658984303474426\n",
      "\u001b[1m508/689\u001b[0m \u001b[32m━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━\u001b[0m \u001b[1m10s\u001b[0m 58ms/step - auc: 0.9273 - binary_accuracy: 0.8737 - false_negatives: 248.7087 - false_positives: 749.8799 - loss: 0.3916 - precision: 0.8327 - recall: 0.9385 - true_negatives: 3310.4783 - true_positives: 3834.9331\n",
      "For batch number 509 the model has a loss of 0.3655751645565033\n",
      "\u001b[1m509/689\u001b[0m \u001b[32m━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━\u001b[0m \u001b[1m10s\u001b[0m 58ms/step - auc: 0.9273 - binary_accuracy: 0.8738 - false_negatives: 249.1690 - false_positives: 751.1434 - loss: 0.3915 - precision: 0.8327 - recall: 0.9385 - true_negatives: 3317.2180 - true_positives: 3842.4695\n",
      "For batch number 510 the model has a loss of 0.3656153976917267\n",
      "\u001b[1m510/689\u001b[0m \u001b[32m━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━\u001b[0m \u001b[1m10s\u001b[0m 58ms/step - auc: 0.9273 - binary_accuracy: 0.8738 - false_negatives: 249.6314 - false_positives: 752.4078 - loss: 0.3915 - precision: 0.8327 - recall: 0.9385 - true_negatives: 3323.9587 - true_positives: 3850.0020\n",
      "For batch number 511 the model has a loss of 0.365770548582077\n",
      "\u001b[1m511/689\u001b[0m \u001b[32m━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━\u001b[0m \u001b[1m10s\u001b[0m 58ms/step - auc: 0.9274 - binary_accuracy: 0.8738 - false_negatives: 250.0939 - false_positives: 753.6752 - loss: 0.3914 - precision: 0.8328 - recall: 0.9385 - true_negatives: 3330.7046 - true_positives: 3857.5264\n",
      "For batch number 512 the model has a loss of 0.3656112253665924\n",
      "\u001b[1m512/689\u001b[0m \u001b[32m━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━\u001b[0m \u001b[1m10s\u001b[0m 58ms/step - auc: 0.9274 - binary_accuracy: 0.8738 - false_negatives: 250.5566 - false_positives: 754.9453 - loss: 0.3914 - precision: 0.8328 - recall: 0.9385 - true_negatives: 3337.4414 - true_positives: 3865.0566\n",
      "For batch number 513 the model has a loss of 0.3651847839355469\n",
      "\u001b[1m513/689\u001b[0m \u001b[32m━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━\u001b[0m \u001b[1m10s\u001b[0m 58ms/step - auc: 0.9274 - binary_accuracy: 0.8738 - false_negatives: 251.0175 - false_positives: 756.2164 - loss: 0.3913 - precision: 0.8328 - recall: 0.9385 - true_negatives: 3344.1853 - true_positives: 3872.5808\n",
      "For batch number 514 the model has a loss of 0.3649148643016815\n",
      "\u001b[1m514/689\u001b[0m \u001b[32m━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━\u001b[0m \u001b[1m10s\u001b[0m 58ms/step - auc: 0.9274 - binary_accuracy: 0.8739 - false_negatives: 251.4767 - false_positives: 757.4883 - loss: 0.3913 - precision: 0.8328 - recall: 0.9385 - true_negatives: 3350.9280 - true_positives: 3880.1069\n",
      "For batch number 515 the model has a loss of 0.364860862493515\n",
      "\u001b[1m515/689\u001b[0m \u001b[32m━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━\u001b[0m \u001b[1m10s\u001b[0m 58ms/step - auc: 0.9274 - binary_accuracy: 0.8739 - false_negatives: 251.9340 - false_positives: 758.7631 - loss: 0.3912 - precision: 0.8329 - recall: 0.9385 - true_negatives: 3357.6699 - true_positives: 3887.6331\n",
      "For batch number 516 the model has a loss of 0.3650886118412018\n",
      "\u001b[1m516/689\u001b[0m \u001b[32m━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━\u001b[0m \u001b[1m10s\u001b[0m 58ms/step - auc: 0.9274 - binary_accuracy: 0.8739 - false_negatives: 252.3915 - false_positives: 760.0427 - loss: 0.3912 - precision: 0.8329 - recall: 0.9385 - true_negatives: 3364.4167 - true_positives: 3895.1492\n",
      "For batch number 517 the model has a loss of 0.36491307616233826\n",
      "\u001b[1m517/689\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m10s\u001b[0m 58ms/step - auc: 0.9274 - binary_accuracy: 0.8739 - false_negatives: 252.8491 - false_positives: 761.3211 - loss: 0.3911 - precision: 0.8329 - recall: 0.9385 - true_negatives: 3371.1702 - true_positives: 3902.6597\n",
      "For batch number 518 the model has a loss of 0.3647414445877075\n",
      "\u001b[1m518/689\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m9s\u001b[0m 58ms/step - auc: 0.9274 - binary_accuracy: 0.8739 - false_negatives: 253.3050 - false_positives: 762.5984 - loss: 0.3911 - precision: 0.8329 - recall: 0.9385 - true_negatives: 3377.9343 - true_positives: 3910.1621 \n",
      "For batch number 519 the model has a loss of 0.3643895387649536\n",
      "\u001b[1m519/689\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m9s\u001b[0m 58ms/step - auc: 0.9275 - binary_accuracy: 0.8740 - false_negatives: 253.7592 - false_positives: 763.8767 - loss: 0.3910 - precision: 0.8330 - recall: 0.9385 - true_negatives: 3384.6975 - true_positives: 3917.6667\n",
      "For batch number 520 the model has a loss of 0.3644179403781891\n",
      "\u001b[1m520/689\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m9s\u001b[0m 58ms/step - auc: 0.9275 - binary_accuracy: 0.8740 - false_negatives: 254.2115 - false_positives: 765.1635 - loss: 0.3910 - precision: 0.8330 - recall: 0.9385 - true_negatives: 3391.4614 - true_positives: 3925.1636\n",
      "For batch number 521 the model has a loss of 0.36400511860847473\n",
      "\u001b[1m521/689\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m9s\u001b[0m 58ms/step - auc: 0.9275 - binary_accuracy: 0.8740 - false_negatives: 254.6660 - false_positives: 766.4453 - loss: 0.3909 - precision: 0.8330 - recall: 0.9385 - true_negatives: 3398.2207 - true_positives: 3932.6680\n",
      "For batch number 522 the model has a loss of 0.3637036681175232\n",
      "\u001b[1m522/689\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m9s\u001b[0m 58ms/step - auc: 0.9275 - binary_accuracy: 0.8740 - false_negatives: 255.1188 - false_positives: 767.7261 - loss: 0.3909 - precision: 0.8330 - recall: 0.9385 - true_negatives: 3404.9790 - true_positives: 3940.1763\n",
      "For batch number 523 the model has a loss of 0.363338440656662\n",
      "\u001b[1m523/689\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m9s\u001b[0m 58ms/step - auc: 0.9275 - binary_accuracy: 0.8740 - false_negatives: 255.5698 - false_positives: 769.0057 - loss: 0.3908 - precision: 0.8331 - recall: 0.9386 - true_negatives: 3411.7400 - true_positives: 3947.6846\n",
      "For batch number 524 the model has a loss of 0.3635244369506836\n",
      "\u001b[1m524/689\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m9s\u001b[0m 58ms/step - auc: 0.9275 - binary_accuracy: 0.8741 - false_negatives: 256.0191 - false_positives: 770.2958 - loss: 0.3908 - precision: 0.8331 - recall: 0.9386 - true_negatives: 3418.5056 - true_positives: 3955.1794\n",
      "For batch number 525 the model has a loss of 0.36349350214004517\n",
      "\u001b[1m525/689\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m9s\u001b[0m 58ms/step - auc: 0.9275 - binary_accuracy: 0.8741 - false_negatives: 256.4686 - false_positives: 771.5886 - loss: 0.3907 - precision: 0.8331 - recall: 0.9386 - true_negatives: 3425.2705 - true_positives: 3962.6724\n",
      "For batch number 526 the model has a loss of 0.36301738023757935\n",
      "\u001b[1m526/689\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m9s\u001b[0m 58ms/step - auc: 0.9275 - binary_accuracy: 0.8741 - false_negatives: 256.9164 - false_positives: 772.8784 - loss: 0.3907 - precision: 0.8331 - recall: 0.9386 - true_negatives: 3432.0417 - true_positives: 3970.1636\n",
      "For batch number 527 the model has a loss of 0.3628716468811035\n",
      "\u001b[1m527/689\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m9s\u001b[0m 58ms/step - auc: 0.9275 - binary_accuracy: 0.8741 - false_negatives: 257.3643 - false_positives: 774.1651 - loss: 0.3906 - precision: 0.8331 - recall: 0.9386 - true_negatives: 3438.8103 - true_positives: 3977.6604\n",
      "For batch number 528 the model has a loss of 0.3625045120716095\n",
      "\u001b[1m528/689\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m9s\u001b[0m 58ms/step - auc: 0.9276 - binary_accuracy: 0.8741 - false_negatives: 257.8106 - false_positives: 775.4489 - loss: 0.3906 - precision: 0.8332 - recall: 0.9386 - true_negatives: 3445.5852 - true_positives: 3985.1553\n",
      "For batch number 529 the model has a loss of 0.36210036277770996\n",
      "\u001b[1m529/689\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m9s\u001b[0m 58ms/step - auc: 0.9276 - binary_accuracy: 0.8742 - false_negatives: 258.2571 - false_positives: 776.7278 - loss: 0.3905 - precision: 0.8332 - recall: 0.9386 - true_negatives: 3452.3630 - true_positives: 3992.6521\n",
      "For batch number 530 the model has a loss of 0.36251798272132874\n",
      "\u001b[1m530/689\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m9s\u001b[0m 58ms/step - auc: 0.9276 - binary_accuracy: 0.8742 - false_negatives: 258.7094 - false_positives: 778.0038 - loss: 0.3904 - precision: 0.8332 - recall: 0.9386 - true_negatives: 3459.1377 - true_positives: 4000.1492\n",
      "For batch number 531 the model has a loss of 0.36246180534362793\n",
      "\u001b[1m531/689\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m9s\u001b[0m 58ms/step - auc: 0.9276 - binary_accuracy: 0.8742 - false_negatives: 259.1601 - false_positives: 779.2844 - loss: 0.3904 - precision: 0.8332 - recall: 0.9386 - true_negatives: 3465.9172 - true_positives: 4007.6384\n",
      "For batch number 532 the model has a loss of 0.3622841238975525\n",
      "\u001b[1m532/689\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m9s\u001b[0m 58ms/step - auc: 0.9276 - binary_accuracy: 0.8742 - false_negatives: 259.6128 - false_positives: 780.5602 - loss: 0.3903 - precision: 0.8333 - recall: 0.9386 - true_negatives: 3472.6936 - true_positives: 4015.1335\n",
      "For batch number 533 the model has a loss of 0.36235132813453674\n",
      "\u001b[1m533/689\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m9s\u001b[0m 58ms/step - auc: 0.9276 - binary_accuracy: 0.8742 - false_negatives: 260.0657 - false_positives: 781.8311 - loss: 0.3903 - precision: 0.8333 - recall: 0.9386 - true_negatives: 3479.4709 - true_positives: 4022.6323\n",
      "For batch number 534 the model has a loss of 0.36240851879119873\n",
      "\u001b[1m534/689\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m9s\u001b[0m 58ms/step - auc: 0.9276 - binary_accuracy: 0.8743 - false_negatives: 260.5225 - false_positives: 783.1011 - loss: 0.3902 - precision: 0.8333 - recall: 0.9386 - true_negatives: 3486.2529 - true_positives: 4030.1235\n",
      "For batch number 535 the model has a loss of 0.36220189929008484\n",
      "\u001b[1m535/689\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m8s\u001b[0m 58ms/step - auc: 0.9276 - binary_accuracy: 0.8743 - false_negatives: 260.9776 - false_positives: 784.3682 - loss: 0.3902 - precision: 0.8333 - recall: 0.9386 - true_negatives: 3493.0298 - true_positives: 4037.6243\n",
      "For batch number 536 the model has a loss of 0.36179956793785095\n",
      "\u001b[1m536/689\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m8s\u001b[0m 58ms/step - auc: 0.9277 - binary_accuracy: 0.8743 - false_negatives: 261.4310 - false_positives: 785.6343 - loss: 0.3901 - precision: 0.8334 - recall: 0.9386 - true_negatives: 3499.8152 - true_positives: 4045.1194\n",
      "For batch number 537 the model has a loss of 0.36185628175735474\n",
      "\u001b[1m537/689\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m8s\u001b[0m 58ms/step - auc: 0.9277 - binary_accuracy: 0.8743 - false_negatives: 261.8864 - false_positives: 786.8994 - loss: 0.3901 - precision: 0.8334 - recall: 0.9386 - true_negatives: 3506.6016 - true_positives: 4052.6125\n",
      "For batch number 538 the model has a loss of 0.36173030734062195\n",
      "\u001b[1m538/689\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m8s\u001b[0m 58ms/step - auc: 0.9277 - binary_accuracy: 0.8744 - false_negatives: 262.3439 - false_positives: 788.1599 - loss: 0.3900 - precision: 0.8334 - recall: 0.9386 - true_negatives: 3513.3884 - true_positives: 4060.1079\n",
      "For batch number 539 the model has a loss of 0.3615478575229645\n",
      "\u001b[1m539/689\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m8s\u001b[0m 58ms/step - auc: 0.9277 - binary_accuracy: 0.8744 - false_negatives: 262.8033 - false_positives: 789.4156 - loss: 0.3900 - precision: 0.8334 - recall: 0.9386 - true_negatives: 3520.1782 - true_positives: 4067.6030\n",
      "For batch number 540 the model has a loss of 0.3612441122531891\n",
      "\u001b[1m540/689\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m8s\u001b[0m 58ms/step - auc: 0.9277 - binary_accuracy: 0.8744 - false_negatives: 263.2648 - false_positives: 790.6667 - loss: 0.3899 - precision: 0.8335 - recall: 0.9386 - true_negatives: 3526.9741 - true_positives: 4075.0945\n",
      "For batch number 541 the model has a loss of 0.36119264364242554\n",
      "\u001b[1m541/689\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m8s\u001b[0m 58ms/step - auc: 0.9277 - binary_accuracy: 0.8744 - false_negatives: 263.7264 - false_positives: 791.9150 - loss: 0.3899 - precision: 0.8335 - recall: 0.9386 - true_negatives: 3533.7690 - true_positives: 4082.5896\n",
      "For batch number 542 the model has a loss of 0.3607628643512726\n",
      "\u001b[1m542/689\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m8s\u001b[0m 58ms/step - auc: 0.9277 - binary_accuracy: 0.8744 - false_negatives: 264.1863 - false_positives: 793.1605 - loss: 0.3898 - precision: 0.8335 - recall: 0.9386 - true_negatives: 3540.5684 - true_positives: 4090.0850\n",
      "For batch number 543 the model has a loss of 0.3606787919998169\n",
      "\u001b[1m543/689\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m8s\u001b[0m 58ms/step - auc: 0.9277 - binary_accuracy: 0.8745 - false_negatives: 264.6464 - false_positives: 794.4052 - loss: 0.3898 - precision: 0.8335 - recall: 0.9386 - true_negatives: 3547.3684 - true_positives: 4097.5801\n",
      "For batch number 544 the model has a loss of 0.3608110249042511\n",
      "\u001b[1m544/689\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m8s\u001b[0m 58ms/step - auc: 0.9278 - binary_accuracy: 0.8745 - false_negatives: 265.1066 - false_positives: 795.6489 - loss: 0.3897 - precision: 0.8336 - recall: 0.9387 - true_negatives: 3554.1619 - true_positives: 4105.0825\n",
      "For batch number 545 the model has a loss of 0.3610977828502655\n",
      "\u001b[1m545/689\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m8s\u001b[0m 58ms/step - auc: 0.9278 - binary_accuracy: 0.8745 - false_negatives: 265.5670 - false_positives: 796.8936 - loss: 0.3897 - precision: 0.8336 - recall: 0.9387 - true_negatives: 3560.9504 - true_positives: 4112.5889\n",
      "For batch number 546 the model has a loss of 0.3612475097179413\n",
      "\u001b[1m546/689\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m8s\u001b[0m 58ms/step - auc: 0.9278 - binary_accuracy: 0.8745 - false_negatives: 266.0275 - false_positives: 798.1392 - loss: 0.3896 - precision: 0.8336 - recall: 0.9387 - true_negatives: 3567.7417 - true_positives: 4120.0918\n",
      "For batch number 547 the model has a loss of 0.36137181520462036\n",
      "\u001b[1m547/689\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m8s\u001b[0m 58ms/step - auc: 0.9278 - binary_accuracy: 0.8745 - false_negatives: 266.4918 - false_positives: 799.3876 - loss: 0.3895 - precision: 0.8336 - recall: 0.9387 - true_negatives: 3574.5356 - true_positives: 4127.5850\n",
      "For batch number 548 the model has a loss of 0.36132723093032837\n",
      "\u001b[1m548/689\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m8s\u001b[0m 58ms/step - auc: 0.9278 - binary_accuracy: 0.8746 - false_negatives: 266.9598 - false_positives: 800.6332 - loss: 0.3895 - precision: 0.8337 - recall: 0.9387 - true_negatives: 3581.3303 - true_positives: 4135.0767\n",
      "For batch number 549 the model has a loss of 0.36112865805625916\n",
      "\u001b[1m549/689\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m8s\u001b[0m 58ms/step - auc: 0.9278 - binary_accuracy: 0.8746 - false_negatives: 267.4262 - false_positives: 801.8798 - loss: 0.3894 - precision: 0.8337 - recall: 0.9387 - true_negatives: 3588.1238 - true_positives: 4142.5703\n",
      "For batch number 550 the model has a loss of 0.36123770475387573\n",
      "\u001b[1m550/689\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m8s\u001b[0m 58ms/step - auc: 0.9278 - binary_accuracy: 0.8746 - false_negatives: 267.8945 - false_positives: 803.1254 - loss: 0.3894 - precision: 0.8337 - recall: 0.9387 - true_negatives: 3594.9072 - true_positives: 4150.0728\n",
      "For batch number 551 the model has a loss of 0.3610095977783203\n",
      "\u001b[1m551/689\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m8s\u001b[0m 58ms/step - auc: 0.9278 - binary_accuracy: 0.8746 - false_negatives: 268.3630 - false_positives: 804.3684 - loss: 0.3893 - precision: 0.8337 - recall: 0.9387 - true_negatives: 3601.6897 - true_positives: 4157.5791\n",
      "For batch number 552 the model has a loss of 0.3613147437572479\n",
      "\u001b[1m552/689\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m7s\u001b[0m 58ms/step - auc: 0.9279 - binary_accuracy: 0.8746 - false_negatives: 268.8351 - false_positives: 805.6069 - loss: 0.3893 - precision: 0.8338 - recall: 0.9387 - true_negatives: 3608.4656 - true_positives: 4165.0923\n",
      "For batch number 553 the model has a loss of 0.36121666431427\n",
      "\u001b[1m553/689\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m7s\u001b[0m 58ms/step - auc: 0.9279 - binary_accuracy: 0.8747 - false_negatives: 269.3074 - false_positives: 806.8445 - loss: 0.3892 - precision: 0.8338 - recall: 0.9387 - true_negatives: 3615.2441 - true_positives: 4172.6040\n",
      "For batch number 554 the model has a loss of 0.36126530170440674\n",
      "\u001b[1m554/689\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m7s\u001b[0m 58ms/step - auc: 0.9279 - binary_accuracy: 0.8747 - false_negatives: 269.7798 - false_positives: 808.0848 - loss: 0.3892 - precision: 0.8338 - recall: 0.9387 - true_negatives: 3622.0254 - true_positives: 4180.1104\n",
      "For batch number 555 the model has a loss of 0.3611317574977875\n",
      "\u001b[1m555/689\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m7s\u001b[0m 58ms/step - auc: 0.9279 - binary_accuracy: 0.8747 - false_negatives: 270.2523 - false_positives: 809.3261 - loss: 0.3891 - precision: 0.8338 - recall: 0.9387 - true_negatives: 3628.8071 - true_positives: 4187.6143\n",
      "For batch number 556 the model has a loss of 0.3610917329788208\n",
      "\u001b[1m556/689\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m7s\u001b[0m 58ms/step - auc: 0.9279 - binary_accuracy: 0.8747 - false_negatives: 270.7230 - false_positives: 810.5701 - loss: 0.3891 - precision: 0.8339 - recall: 0.9387 - true_negatives: 3635.5845 - true_positives: 4195.1221\n",
      "For batch number 557 the model has a loss of 0.3608418405056\n",
      "\u001b[1m557/689\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m7s\u001b[0m 58ms/step - auc: 0.9279 - binary_accuracy: 0.8747 - false_negatives: 271.1939 - false_positives: 811.8097 - loss: 0.3890 - precision: 0.8339 - recall: 0.9387 - true_negatives: 3642.3645 - true_positives: 4202.6318\n",
      "For batch number 558 the model has a loss of 0.3606177270412445\n",
      "\u001b[1m558/689\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m7s\u001b[0m 58ms/step - auc: 0.9279 - binary_accuracy: 0.8748 - false_negatives: 271.6649 - false_positives: 813.0484 - loss: 0.3890 - precision: 0.8339 - recall: 0.9387 - true_negatives: 3649.1523 - true_positives: 4210.1343\n",
      "For batch number 559 the model has a loss of 0.36041849851608276\n",
      "\u001b[1m559/689\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m7s\u001b[0m 58ms/step - auc: 0.9279 - binary_accuracy: 0.8748 - false_negatives: 272.1342 - false_positives: 814.2880 - loss: 0.3889 - precision: 0.8339 - recall: 0.9387 - true_negatives: 3655.9409 - true_positives: 4217.6367\n",
      "For batch number 560 the model has a loss of 0.3604298532009125\n",
      "\u001b[1m560/689\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m7s\u001b[0m 58ms/step - auc: 0.9280 - binary_accuracy: 0.8748 - false_negatives: 272.6018 - false_positives: 815.5357 - loss: 0.3889 - precision: 0.8340 - recall: 0.9387 - true_negatives: 3662.7285 - true_positives: 4225.1338\n",
      "For batch number 561 the model has a loss of 0.36053523421287537\n",
      "\u001b[1m561/689\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m7s\u001b[0m 58ms/step - auc: 0.9280 - binary_accuracy: 0.8748 - false_negatives: 273.0695 - false_positives: 816.7861 - loss: 0.3888 - precision: 0.8340 - recall: 0.9387 - true_negatives: 3669.5168 - true_positives: 4232.6274\n",
      "For batch number 562 the model has a loss of 0.36063119769096375\n",
      "\u001b[1m562/689\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m7s\u001b[0m 58ms/step - auc: 0.9280 - binary_accuracy: 0.8748 - false_negatives: 273.5356 - false_positives: 818.0374 - loss: 0.3888 - precision: 0.8340 - recall: 0.9387 - true_negatives: 3676.3079 - true_positives: 4240.1191\n",
      "For batch number 563 the model has a loss of 0.3608570694923401\n",
      "\u001b[1m563/689\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m7s\u001b[0m 58ms/step - auc: 0.9280 - binary_accuracy: 0.8749 - false_negatives: 274.0018 - false_positives: 819.2966 - loss: 0.3887 - precision: 0.8340 - recall: 0.9387 - true_negatives: 3683.0942 - true_positives: 4247.6074\n",
      "For batch number 564 the model has a loss of 0.36083540320396423\n",
      "\u001b[1m564/689\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m7s\u001b[0m 58ms/step - auc: 0.9280 - binary_accuracy: 0.8749 - false_negatives: 274.4698 - false_positives: 820.5585 - loss: 0.3887 - precision: 0.8341 - recall: 0.9387 - true_negatives: 3689.8811 - true_positives: 4255.0903\n",
      "For batch number 565 the model has a loss of 0.361083984375\n",
      "\u001b[1m565/689\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m7s\u001b[0m 58ms/step - auc: 0.9280 - binary_accuracy: 0.8749 - false_negatives: 274.9398 - false_positives: 821.8177 - loss: 0.3886 - precision: 0.8341 - recall: 0.9387 - true_negatives: 3696.6689 - true_positives: 4262.5732\n",
      "For batch number 566 the model has a loss of 0.3617933392524719\n",
      "\u001b[1m566/689\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m7s\u001b[0m 58ms/step - auc: 0.9280 - binary_accuracy: 0.8749 - false_negatives: 275.4152 - false_positives: 823.0778 - loss: 0.3886 - precision: 0.8341 - recall: 0.9387 - true_negatives: 3703.4524 - true_positives: 4270.0547\n",
      "For batch number 567 the model has a loss of 0.3616834878921509\n",
      "\u001b[1m567/689\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m7s\u001b[0m 58ms/step - auc: 0.9280 - binary_accuracy: 0.8749 - false_negatives: 275.8889 - false_positives: 824.3422 - loss: 0.3885 - precision: 0.8341 - recall: 0.9387 - true_negatives: 3710.2397 - true_positives: 4277.5293\n",
      "For batch number 568 the model has a loss of 0.36146342754364014\n",
      "\u001b[1m568/689\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m7s\u001b[0m 58ms/step - auc: 0.9280 - binary_accuracy: 0.8749 - false_negatives: 276.3627 - false_positives: 825.6039 - loss: 0.3885 - precision: 0.8341 - recall: 0.9387 - true_negatives: 3717.0229 - true_positives: 4285.0107\n",
      "For batch number 569 the model has a loss of 0.3611172139644623\n",
      "\u001b[1m569/689\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m7s\u001b[0m 58ms/step - auc: 0.9281 - binary_accuracy: 0.8750 - false_negatives: 276.8348 - false_positives: 826.8629 - loss: 0.3884 - precision: 0.8342 - recall: 0.9387 - true_negatives: 3723.8083 - true_positives: 4292.4937\n",
      "For batch number 570 the model has a loss of 0.36079394817352295\n",
      "\u001b[1m570/689\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m6s\u001b[0m 58ms/step - auc: 0.9281 - binary_accuracy: 0.8750 - false_negatives: 277.3070 - false_positives: 828.1176 - loss: 0.3884 - precision: 0.8342 - recall: 0.9387 - true_negatives: 3730.6001 - true_positives: 4299.9756\n",
      "For batch number 571 the model has a loss of 0.360932320356369\n",
      "\u001b[1m571/689\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m6s\u001b[0m 58ms/step - auc: 0.9281 - binary_accuracy: 0.8750 - false_negatives: 277.7811 - false_positives: 829.3713 - loss: 0.3884 - precision: 0.8342 - recall: 0.9387 - true_negatives: 3737.3940 - true_positives: 4307.4536\n",
      "For batch number 572 the model has a loss of 0.36099129915237427\n",
      "\u001b[1m572/689\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m6s\u001b[0m 58ms/step - auc: 0.9281 - binary_accuracy: 0.8750 - false_negatives: 278.2552 - false_positives: 830.6206 - loss: 0.3883 - precision: 0.8342 - recall: 0.9387 - true_negatives: 3744.1887 - true_positives: 4314.9355\n",
      "For batch number 573 the model has a loss of 0.3607304096221924\n",
      "\u001b[1m573/689\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m6s\u001b[0m 58ms/step - auc: 0.9281 - binary_accuracy: 0.8750 - false_negatives: 278.7295 - false_positives: 831.8674 - loss: 0.3883 - precision: 0.8343 - recall: 0.9387 - true_negatives: 3750.9807 - true_positives: 4322.4224\n",
      "For batch number 574 the model has a loss of 0.36030855774879456\n",
      "\u001b[1m574/689\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m6s\u001b[0m 58ms/step - auc: 0.9281 - binary_accuracy: 0.8751 - false_negatives: 279.2038 - false_positives: 833.1115 - loss: 0.3882 - precision: 0.8343 - recall: 0.9387 - true_negatives: 3757.7771 - true_positives: 4329.9077\n",
      "For batch number 575 the model has a loss of 0.3601706326007843\n",
      "\u001b[1m575/689\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m6s\u001b[0m 58ms/step - auc: 0.9281 - binary_accuracy: 0.8751 - false_negatives: 279.6765 - false_positives: 834.3583 - loss: 0.3882 - precision: 0.8343 - recall: 0.9387 - true_negatives: 3764.5808 - true_positives: 4337.3843\n",
      "For batch number 576 the model has a loss of 0.35998475551605225\n",
      "\u001b[1m576/689\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m6s\u001b[0m 58ms/step - auc: 0.9281 - binary_accuracy: 0.8751 - false_negatives: 280.1493 - false_positives: 835.6042 - loss: 0.3881 - precision: 0.8343 - recall: 0.9387 - true_negatives: 3771.3855 - true_positives: 4344.8613\n",
      "For batch number 577 the model has a loss of 0.35977068543434143\n",
      "\u001b[1m577/689\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m6s\u001b[0m 58ms/step - auc: 0.9281 - binary_accuracy: 0.8751 - false_negatives: 280.6205 - false_positives: 836.8510 - loss: 0.3881 - precision: 0.8344 - recall: 0.9387 - true_negatives: 3778.1941 - true_positives: 4352.3345\n",
      "For batch number 578 the model has a loss of 0.36024391651153564\n",
      "\u001b[1m578/689\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m6s\u001b[0m 58ms/step - auc: 0.9282 - binary_accuracy: 0.8751 - false_negatives: 281.0986 - false_positives: 838.0969 - loss: 0.3880 - precision: 0.8344 - recall: 0.9387 - true_negatives: 3784.9915 - true_positives: 4359.8130\n",
      "For batch number 579 the model has a loss of 0.3602339029312134\n",
      "\u001b[1m579/689\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m6s\u001b[0m 58ms/step - auc: 0.9282 - binary_accuracy: 0.8752 - false_negatives: 281.5786 - false_positives: 839.3385 - loss: 0.3880 - precision: 0.8344 - recall: 0.9387 - true_negatives: 3791.7859 - true_positives: 4367.2969\n",
      "For batch number 580 the model has a loss of 0.3605572283267975\n",
      "\u001b[1m580/689\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m6s\u001b[0m 58ms/step - auc: 0.9282 - binary_accuracy: 0.8752 - false_negatives: 282.0603 - false_positives: 840.5793 - loss: 0.3879 - precision: 0.8344 - recall: 0.9387 - true_negatives: 3798.5811 - true_positives: 4374.7793\n",
      "For batch number 581 the model has a loss of 0.36025843024253845\n",
      "\u001b[1m581/689\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m6s\u001b[0m 58ms/step - auc: 0.9282 - binary_accuracy: 0.8752 - false_negatives: 282.5404 - false_positives: 841.8193 - loss: 0.3879 - precision: 0.8345 - recall: 0.9387 - true_negatives: 3805.3787 - true_positives: 4382.2617\n",
      "For batch number 582 the model has a loss of 0.3604879677295685\n",
      "\u001b[1m582/689\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m6s\u001b[0m 58ms/step - auc: 0.9282 - binary_accuracy: 0.8752 - false_negatives: 283.0240 - false_positives: 843.0567 - loss: 0.3878 - precision: 0.8345 - recall: 0.9387 - true_negatives: 3812.1719 - true_positives: 4389.7476\n",
      "For batch number 583 the model has a loss of 0.36016497015953064\n",
      "\u001b[1m583/689\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m6s\u001b[0m 58ms/step - auc: 0.9282 - binary_accuracy: 0.8752 - false_negatives: 283.5077 - false_positives: 844.2933 - loss: 0.3878 - precision: 0.8345 - recall: 0.9387 - true_negatives: 3818.9622 - true_positives: 4397.2368\n",
      "For batch number 584 the model has a loss of 0.35974055528640747\n",
      "\u001b[1m584/689\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m6s\u001b[0m 58ms/step - auc: 0.9282 - binary_accuracy: 0.8752 - false_negatives: 283.9897 - false_positives: 845.5257 - loss: 0.3877 - precision: 0.8345 - recall: 0.9387 - true_negatives: 3825.7585 - true_positives: 4404.7261\n",
      "For batch number 585 the model has a loss of 0.3598187267780304\n",
      "\u001b[1m585/689\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m6s\u001b[0m 58ms/step - auc: 0.9282 - binary_accuracy: 0.8753 - false_negatives: 284.4718 - false_positives: 846.7556 - loss: 0.3877 - precision: 0.8345 - recall: 0.9387 - true_negatives: 3832.5574 - true_positives: 4412.2153\n",
      "For batch number 586 the model has a loss of 0.35960838198661804\n",
      "\u001b[1m586/689\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m6s\u001b[0m 58ms/step - auc: 0.9282 - binary_accuracy: 0.8753 - false_negatives: 284.9556 - false_positives: 847.9829 - loss: 0.3876 - precision: 0.8346 - recall: 0.9387 - true_negatives: 3839.3533 - true_positives: 4419.7080\n",
      "For batch number 587 the model has a loss of 0.3591792583465576\n",
      "\u001b[1m587/689\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m5s\u001b[0m 58ms/step - auc: 0.9283 - binary_accuracy: 0.8753 - false_negatives: 285.4395 - false_positives: 849.2061 - loss: 0.3876 - precision: 0.8346 - recall: 0.9388 - true_negatives: 3846.1516 - true_positives: 4427.2026\n",
      "For batch number 588 the model has a loss of 0.3590352535247803\n",
      "\u001b[1m588/689\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m5s\u001b[0m 58ms/step - auc: 0.9283 - binary_accuracy: 0.8753 - false_negatives: 285.9235 - false_positives: 850.4303 - loss: 0.3875 - precision: 0.8346 - recall: 0.9388 - true_negatives: 3852.9456 - true_positives: 4434.7007\n",
      "For batch number 589 the model has a loss of 0.3590942323207855\n",
      "\u001b[1m589/689\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m5s\u001b[0m 58ms/step - auc: 0.9283 - binary_accuracy: 0.8753 - false_negatives: 286.4075 - false_positives: 851.6520 - loss: 0.3875 - precision: 0.8346 - recall: 0.9388 - true_negatives: 3859.7402 - true_positives: 4442.2002\n",
      "For batch number 590 the model has a loss of 0.3594350814819336\n",
      "\u001b[1m590/689\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m5s\u001b[0m 58ms/step - auc: 0.9283 - binary_accuracy: 0.8754 - false_negatives: 286.8932 - false_positives: 852.8712 - loss: 0.3874 - precision: 0.8347 - recall: 0.9388 - true_negatives: 3866.5356 - true_positives: 4449.7002\n",
      "For batch number 591 the model has a loss of 0.35924920439720154\n",
      "\u001b[1m591/689\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m5s\u001b[0m 58ms/step - auc: 0.9283 - binary_accuracy: 0.8754 - false_negatives: 287.3773 - false_positives: 854.0897 - loss: 0.3874 - precision: 0.8347 - recall: 0.9388 - true_negatives: 3873.3298 - true_positives: 4457.2031\n",
      "For batch number 592 the model has a loss of 0.3591242730617523\n",
      "\u001b[1m592/689\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m5s\u001b[0m 58ms/step - auc: 0.9283 - binary_accuracy: 0.8754 - false_negatives: 287.8615 - false_positives: 855.3108 - loss: 0.3873 - precision: 0.8347 - recall: 0.9388 - true_negatives: 3880.1250 - true_positives: 4464.7026\n",
      "For batch number 593 the model has a loss of 0.35880813002586365\n",
      "\u001b[1m593/689\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m5s\u001b[0m 58ms/step - auc: 0.9283 - binary_accuracy: 0.8754 - false_negatives: 288.3440 - false_positives: 856.5295 - loss: 0.3873 - precision: 0.8347 - recall: 0.9388 - true_negatives: 3886.9207 - true_positives: 4472.2056\n",
      "For batch number 594 the model has a loss of 0.3584592640399933\n",
      "\u001b[1m594/689\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m5s\u001b[0m 58ms/step - auc: 0.9283 - binary_accuracy: 0.8754 - false_negatives: 288.8249 - false_positives: 857.7441 - loss: 0.3872 - precision: 0.8348 - recall: 0.9388 - true_negatives: 3893.7139 - true_positives: 4479.7173\n",
      "For batch number 595 the model has a loss of 0.3582187294960022\n",
      "\u001b[1m595/689\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m5s\u001b[0m 58ms/step - auc: 0.9283 - binary_accuracy: 0.8755 - false_negatives: 289.3042 - false_positives: 858.9597 - loss: 0.3872 - precision: 0.8348 - recall: 0.9388 - true_negatives: 3900.5127 - true_positives: 4487.2236\n",
      "For batch number 596 the model has a loss of 0.35810431838035583\n",
      "\u001b[1m596/689\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m5s\u001b[0m 58ms/step - auc: 0.9284 - binary_accuracy: 0.8755 - false_negatives: 289.7819 - false_positives: 860.1745 - loss: 0.3871 - precision: 0.8348 - recall: 0.9388 - true_negatives: 3907.3171 - true_positives: 4494.7266\n",
      "For batch number 597 the model has a loss of 0.3580932021141052\n",
      "\u001b[1m597/689\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m5s\u001b[0m 58ms/step - auc: 0.9284 - binary_accuracy: 0.8755 - false_negatives: 290.2580 - false_positives: 861.3936 - loss: 0.3871 - precision: 0.8348 - recall: 0.9388 - true_negatives: 3914.1240 - true_positives: 4502.2246\n",
      "For batch number 598 the model has a loss of 0.35804733633995056\n",
      "\u001b[1m598/689\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m5s\u001b[0m 58ms/step - auc: 0.9284 - binary_accuracy: 0.8755 - false_negatives: 290.7341 - false_positives: 862.6121 - loss: 0.3871 - precision: 0.8349 - recall: 0.9388 - true_negatives: 3920.9331 - true_positives: 4509.7207\n",
      "For batch number 599 the model has a loss of 0.35769885778427124\n",
      "\u001b[1m599/689\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m5s\u001b[0m 58ms/step - auc: 0.9284 - binary_accuracy: 0.8755 - false_negatives: 291.2087 - false_positives: 863.8281 - loss: 0.3870 - precision: 0.8349 - recall: 0.9388 - true_negatives: 3927.7446 - true_positives: 4517.2188\n",
      "For batch number 600 the model has a loss of 0.3574933707714081\n",
      "\u001b[1m600/689\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m5s\u001b[0m 58ms/step - auc: 0.9284 - binary_accuracy: 0.8755 - false_negatives: 291.6817 - false_positives: 865.0467 - loss: 0.3870 - precision: 0.8349 - recall: 0.9388 - true_negatives: 3934.5583 - true_positives: 4524.7134\n",
      "For batch number 601 the model has a loss of 0.35738855600357056\n",
      "\u001b[1m601/689\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m5s\u001b[0m 58ms/step - auc: 0.9284 - binary_accuracy: 0.8756 - false_negatives: 292.1548 - false_positives: 866.2646 - loss: 0.3869 - precision: 0.8349 - recall: 0.9388 - true_negatives: 3941.3694 - true_positives: 4532.2114\n",
      "For batch number 602 the model has a loss of 0.3569708466529846\n",
      "\u001b[1m602/689\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m5s\u001b[0m 58ms/step - auc: 0.9284 - binary_accuracy: 0.8756 - false_negatives: 292.6263 - false_positives: 867.4800 - loss: 0.3869 - precision: 0.8350 - recall: 0.9388 - true_negatives: 3948.1794 - true_positives: 4539.7144\n",
      "For batch number 603 the model has a loss of 0.35691434144973755\n",
      "\u001b[1m603/689\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m5s\u001b[0m 58ms/step - auc: 0.9284 - binary_accuracy: 0.8756 - false_negatives: 293.0962 - false_positives: 868.6982 - loss: 0.3868 - precision: 0.8350 - recall: 0.9388 - true_negatives: 3954.9900 - true_positives: 4547.2158\n",
      "For batch number 604 the model has a loss of 0.3566127419471741\n",
      "\u001b[1m604/689\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m4s\u001b[0m 58ms/step - auc: 0.9285 - binary_accuracy: 0.8756 - false_negatives: 293.5662 - false_positives: 869.9139 - loss: 0.3868 - precision: 0.8350 - recall: 0.9388 - true_negatives: 3961.7964 - true_positives: 4554.7236\n",
      "For batch number 605 the model has a loss of 0.35628050565719604\n",
      "\u001b[1m605/689\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m4s\u001b[0m 58ms/step - auc: 0.9285 - binary_accuracy: 0.8756 - false_negatives: 294.0347 - false_positives: 871.1273 - loss: 0.3867 - precision: 0.8350 - recall: 0.9388 - true_negatives: 3968.6050 - true_positives: 4562.2329\n",
      "For batch number 606 the model has a loss of 0.3558943569660187\n",
      "\u001b[1m606/689\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m4s\u001b[0m 58ms/step - auc: 0.9285 - binary_accuracy: 0.8757 - false_negatives: 294.5016 - false_positives: 872.3366 - loss: 0.3867 - precision: 0.8351 - recall: 0.9388 - true_negatives: 3975.4126 - true_positives: 4569.7490\n",
      "For batch number 607 the model has a loss of 0.35586780309677124\n",
      "\u001b[1m607/689\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m4s\u001b[0m 58ms/step - auc: 0.9285 - binary_accuracy: 0.8757 - false_negatives: 294.9687 - false_positives: 873.5486 - loss: 0.3866 - precision: 0.8351 - recall: 0.9388 - true_negatives: 3982.2224 - true_positives: 4577.2603\n",
      "For batch number 608 the model has a loss of 0.3562365472316742\n",
      "\u001b[1m608/689\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m4s\u001b[0m 58ms/step - auc: 0.9285 - binary_accuracy: 0.8757 - false_negatives: 295.4375 - false_positives: 874.7615 - loss: 0.3866 - precision: 0.8351 - recall: 0.9388 - true_negatives: 3989.0330 - true_positives: 4584.7681\n",
      "For batch number 609 the model has a loss of 0.35598137974739075\n",
      "\u001b[1m609/689\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m4s\u001b[0m 58ms/step - auc: 0.9285 - binary_accuracy: 0.8757 - false_negatives: 295.9064 - false_positives: 875.9705 - loss: 0.3865 - precision: 0.8351 - recall: 0.9388 - true_negatives: 3995.8408 - true_positives: 4592.2822\n",
      "For batch number 610 the model has a loss of 0.35582658648490906\n",
      "\u001b[1m610/689\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m4s\u001b[0m 58ms/step - auc: 0.9285 - binary_accuracy: 0.8757 - false_negatives: 296.3754 - false_positives: 877.1771 - loss: 0.3865 - precision: 0.8352 - recall: 0.9388 - true_negatives: 4002.6443 - true_positives: 4599.8032\n",
      "For batch number 611 the model has a loss of 0.35579562187194824\n",
      "\u001b[1m611/689\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m4s\u001b[0m 58ms/step - auc: 0.9285 - binary_accuracy: 0.8758 - false_negatives: 296.8429 - false_positives: 878.3830 - loss: 0.3864 - precision: 0.8352 - recall: 0.9388 - true_negatives: 4009.4485 - true_positives: 4607.3257\n",
      "For batch number 612 the model has a loss of 0.3560517132282257\n",
      "\u001b[1m612/689\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m4s\u001b[0m 58ms/step - auc: 0.9286 - binary_accuracy: 0.8758 - false_negatives: 297.3137 - false_positives: 879.5915 - loss: 0.3864 - precision: 0.8352 - recall: 0.9388 - true_negatives: 4016.2549 - true_positives: 4614.8398\n",
      "For batch number 613 the model has a loss of 0.35602840781211853\n",
      "\u001b[1m613/689\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m4s\u001b[0m 58ms/step - auc: 0.9286 - binary_accuracy: 0.8758 - false_negatives: 297.7830 - false_positives: 880.8026 - loss: 0.3863 - precision: 0.8352 - recall: 0.9388 - true_negatives: 4023.0588 - true_positives: 4622.3555\n",
      "For batch number 614 the model has a loss of 0.3565896451473236\n",
      "\u001b[1m614/689\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m4s\u001b[0m 58ms/step - auc: 0.9286 - binary_accuracy: 0.8758 - false_negatives: 298.2541 - false_positives: 882.0146 - loss: 0.3863 - precision: 0.8353 - recall: 0.9388 - true_negatives: 4029.8567 - true_positives: 4629.8745\n",
      "For batch number 615 the model has a loss of 0.35663965344429016\n",
      "\u001b[1m615/689\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m4s\u001b[0m 58ms/step - auc: 0.9286 - binary_accuracy: 0.8758 - false_negatives: 298.7236 - false_positives: 883.2277 - loss: 0.3862 - precision: 0.8353 - recall: 0.9388 - true_negatives: 4036.6487 - true_positives: 4637.3999\n",
      "For batch number 616 the model has a loss of 0.35642731189727783\n",
      "\u001b[1m616/689\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m4s\u001b[0m 58ms/step - auc: 0.9286 - binary_accuracy: 0.8759 - false_negatives: 299.1948 - false_positives: 884.4383 - loss: 0.3862 - precision: 0.8353 - recall: 0.9388 - true_negatives: 4043.4417 - true_positives: 4644.9253\n",
      "For batch number 617 the model has a loss of 0.35622361302375793\n",
      "\u001b[1m617/689\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m4s\u001b[0m 58ms/step - auc: 0.9286 - binary_accuracy: 0.8759 - false_negatives: 299.6678 - false_positives: 885.6483 - loss: 0.3861 - precision: 0.8353 - recall: 0.9388 - true_negatives: 4050.2285 - true_positives: 4652.4556\n",
      "For batch number 618 the model has a loss of 0.3571361005306244\n",
      "\u001b[1m618/689\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m4s\u001b[0m 58ms/step - auc: 0.9286 - binary_accuracy: 0.8759 - false_negatives: 300.1424 - false_positives: 886.8593 - loss: 0.3861 - precision: 0.8354 - recall: 0.9388 - true_negatives: 4057.0129 - true_positives: 4659.9854\n",
      "For batch number 619 the model has a loss of 0.35699039697647095\n",
      "\u001b[1m619/689\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m4s\u001b[0m 58ms/step - auc: 0.9286 - binary_accuracy: 0.8759 - false_negatives: 300.6171 - false_positives: 888.0695 - loss: 0.3860 - precision: 0.8354 - recall: 0.9388 - true_negatives: 4063.8030 - true_positives: 4667.5107\n",
      "For batch number 620 the model has a loss of 0.3570536971092224\n",
      "\u001b[1m620/689\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m4s\u001b[0m 58ms/step - auc: 0.9286 - binary_accuracy: 0.8759 - false_negatives: 301.0919 - false_positives: 889.2806 - loss: 0.3860 - precision: 0.8354 - recall: 0.9388 - true_negatives: 4070.5935 - true_positives: 4675.0337\n",
      "For batch number 621 the model has a loss of 0.3569522500038147\n",
      "\u001b[1m621/689\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m3s\u001b[0m 58ms/step - auc: 0.9287 - binary_accuracy: 0.8759 - false_negatives: 301.5685 - false_positives: 890.4911 - loss: 0.3859 - precision: 0.8354 - recall: 0.9388 - true_negatives: 4077.3833 - true_positives: 4682.5571\n",
      "For batch number 622 the model has a loss of 0.3569045662879944\n",
      "\u001b[1m622/689\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m3s\u001b[0m 58ms/step - auc: 0.9287 - binary_accuracy: 0.8760 - false_negatives: 302.0450 - false_positives: 891.7058 - loss: 0.3859 - precision: 0.8355 - recall: 0.9388 - true_negatives: 4084.1655 - true_positives: 4690.0835\n",
      "For batch number 623 the model has a loss of 0.3567740023136139\n",
      "\u001b[1m623/689\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m3s\u001b[0m 58ms/step - auc: 0.9287 - binary_accuracy: 0.8760 - false_negatives: 302.5233 - false_positives: 892.9197 - loss: 0.3858 - precision: 0.8355 - recall: 0.9388 - true_negatives: 4090.9470 - true_positives: 4697.6099\n",
      "For batch number 624 the model has a loss of 0.3564978837966919\n",
      "\u001b[1m624/689\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m3s\u001b[0m 58ms/step - auc: 0.9287 - binary_accuracy: 0.8760 - false_negatives: 303.0000 - false_positives: 894.1314 - loss: 0.3858 - precision: 0.8355 - recall: 0.9388 - true_negatives: 4097.7339 - true_positives: 4705.1348\n",
      "For batch number 625 the model has a loss of 0.3568434417247772\n",
      "\u001b[1m625/689\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m3s\u001b[0m 58ms/step - auc: 0.9287 - binary_accuracy: 0.8760 - false_negatives: 303.4784 - false_positives: 895.3424 - loss: 0.3857 - precision: 0.8355 - recall: 0.9388 - true_negatives: 4104.5200 - true_positives: 4712.6592\n",
      "For batch number 626 the model has a loss of 0.357092022895813\n",
      "\u001b[1m626/689\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m3s\u001b[0m 58ms/step - auc: 0.9287 - binary_accuracy: 0.8760 - false_negatives: 303.9569 - false_positives: 896.5511 - loss: 0.3857 - precision: 0.8356 - recall: 0.9388 - true_negatives: 4111.3018 - true_positives: 4720.1899\n",
      "For batch number 627 the model has a loss of 0.3569132089614868\n",
      "\u001b[1m627/689\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m3s\u001b[0m 58ms/step - auc: 0.9287 - binary_accuracy: 0.8761 - false_negatives: 304.4386 - false_positives: 897.7560 - loss: 0.3856 - precision: 0.8356 - recall: 0.9388 - true_negatives: 4118.0845 - true_positives: 4727.7207\n",
      "For batch number 628 the model has a loss of 0.3570743799209595\n",
      "\u001b[1m628/689\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m3s\u001b[0m 58ms/step - auc: 0.9287 - binary_accuracy: 0.8761 - false_negatives: 304.9204 - false_positives: 898.9618 - loss: 0.3856 - precision: 0.8356 - recall: 0.9388 - true_negatives: 4124.8613 - true_positives: 4735.2563\n",
      "For batch number 629 the model has a loss of 0.35688626766204834\n",
      "\u001b[1m629/689\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m3s\u001b[0m 58ms/step - auc: 0.9287 - binary_accuracy: 0.8761 - false_negatives: 305.4022 - false_positives: 900.1653 - loss: 0.3856 - precision: 0.8356 - recall: 0.9388 - true_negatives: 4131.6392 - true_positives: 4742.7935\n",
      "For batch number 630 the model has a loss of 0.35713431239128113\n",
      "\u001b[1m630/689\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m3s\u001b[0m 58ms/step - auc: 0.9288 - binary_accuracy: 0.8761 - false_negatives: 305.8873 - false_positives: 901.3666 - loss: 0.3855 - precision: 0.8357 - recall: 0.9388 - true_negatives: 4138.4160 - true_positives: 4750.3301\n",
      "For batch number 631 the model has a loss of 0.3570391535758972\n",
      "\u001b[1m631/689\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m3s\u001b[0m 58ms/step - auc: 0.9288 - binary_accuracy: 0.8761 - false_negatives: 306.3724 - false_positives: 902.5658 - loss: 0.3855 - precision: 0.8357 - recall: 0.9388 - true_negatives: 4145.1899 - true_positives: 4757.8716\n",
      "For batch number 632 the model has a loss of 0.35678553581237793\n",
      "\u001b[1m632/689\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m3s\u001b[0m 58ms/step - auc: 0.9288 - binary_accuracy: 0.8761 - false_negatives: 306.8560 - false_positives: 903.7626 - loss: 0.3854 - precision: 0.8357 - recall: 0.9388 - true_negatives: 4151.9683 - true_positives: 4765.4131\n",
      "For batch number 633 the model has a loss of 0.3571499288082123\n",
      "\u001b[1m633/689\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m3s\u001b[0m 58ms/step - auc: 0.9288 - binary_accuracy: 0.8762 - false_negatives: 307.3397 - false_positives: 904.9605 - loss: 0.3854 - precision: 0.8357 - recall: 0.9388 - true_negatives: 4158.7407 - true_positives: 4772.9590\n",
      "For batch number 634 the model has a loss of 0.35698917508125305\n",
      "\u001b[1m634/689\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m3s\u001b[0m 58ms/step - auc: 0.9288 - binary_accuracy: 0.8762 - false_negatives: 307.8218 - false_positives: 906.1577 - loss: 0.3853 - precision: 0.8358 - recall: 0.9388 - true_negatives: 4165.5078 - true_positives: 4780.5127\n",
      "For batch number 635 the model has a loss of 0.35694387555122375\n",
      "\u001b[1m635/689\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m3s\u001b[0m 58ms/step - auc: 0.9288 - binary_accuracy: 0.8762 - false_negatives: 308.3055 - false_positives: 907.3559 - loss: 0.3853 - precision: 0.8358 - recall: 0.9388 - true_negatives: 4172.2695 - true_positives: 4788.0693\n",
      "For batch number 636 the model has a loss of 0.3568647503852844\n",
      "\u001b[1m636/689\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m3s\u001b[0m 58ms/step - auc: 0.9288 - binary_accuracy: 0.8762 - false_negatives: 308.7878 - false_positives: 908.5582 - loss: 0.3852 - precision: 0.8358 - recall: 0.9388 - true_negatives: 4179.0312 - true_positives: 4795.6226\n",
      "For batch number 637 the model has a loss of 0.35683348774909973\n",
      "\u001b[1m637/689\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m3s\u001b[0m 58ms/step - auc: 0.9288 - binary_accuracy: 0.8762 - false_negatives: 309.2700 - false_positives: 909.7629 - loss: 0.3852 - precision: 0.8358 - recall: 0.9388 - true_negatives: 4185.7930 - true_positives: 4803.1743\n",
      "For batch number 638 the model has a loss of 0.3566076457500458\n",
      "\u001b[1m638/689\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m2s\u001b[0m 58ms/step - auc: 0.9288 - binary_accuracy: 0.8763 - false_negatives: 309.7523 - false_positives: 910.9655 - loss: 0.3851 - precision: 0.8359 - recall: 0.9388 - true_negatives: 4192.5469 - true_positives: 4810.7354\n",
      "For batch number 639 the model has a loss of 0.35635456442832947\n",
      "\u001b[1m639/689\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m2s\u001b[0m 58ms/step - auc: 0.9289 - binary_accuracy: 0.8763 - false_negatives: 310.2332 - false_positives: 912.1674 - loss: 0.3851 - precision: 0.8359 - recall: 0.9388 - true_negatives: 4199.3052 - true_positives: 4818.2944\n",
      "For batch number 640 the model has a loss of 0.35632315278053284\n",
      "\u001b[1m640/689\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m2s\u001b[0m 58ms/step - auc: 0.9289 - binary_accuracy: 0.8763 - false_negatives: 310.7125 - false_positives: 913.3750 - loss: 0.3851 - precision: 0.8359 - recall: 0.9388 - true_negatives: 4206.0576 - true_positives: 4825.8545\n",
      "For batch number 641 the model has a loss of 0.3560926020145416\n",
      "\u001b[1m641/689\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m2s\u001b[0m 58ms/step - auc: 0.9289 - binary_accuracy: 0.8763 - false_negatives: 311.1903 - false_positives: 914.5850 - loss: 0.3850 - precision: 0.8359 - recall: 0.9388 - true_negatives: 4212.8110 - true_positives: 4833.4136\n",
      "For batch number 642 the model has a loss of 0.3560583293437958\n",
      "\u001b[1m642/689\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m2s\u001b[0m 58ms/step - auc: 0.9289 - binary_accuracy: 0.8763 - false_negatives: 311.6667 - false_positives: 915.8006 - loss: 0.3850 - precision: 0.8360 - recall: 0.9389 - true_negatives: 4219.5654 - true_positives: 4840.9673\n",
      "For batch number 643 the model has a loss of 0.3557547628879547\n",
      "\u001b[1m643/689\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m2s\u001b[0m 58ms/step - auc: 0.9289 - binary_accuracy: 0.8763 - false_negatives: 312.1415 - false_positives: 917.0171 - loss: 0.3849 - precision: 0.8360 - recall: 0.9389 - true_negatives: 4226.3203 - true_positives: 4848.5210\n",
      "For batch number 644 the model has a loss of 0.35545116662979126\n",
      "\u001b[1m644/689\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m2s\u001b[0m 58ms/step - auc: 0.9289 - binary_accuracy: 0.8764 - false_negatives: 312.6149 - false_positives: 918.2329 - loss: 0.3849 - precision: 0.8360 - recall: 0.9389 - true_negatives: 4233.0747 - true_positives: 4856.0776\n",
      "For batch number 645 the model has a loss of 0.35518622398376465\n",
      "\u001b[1m645/689\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m2s\u001b[0m 58ms/step - auc: 0.9289 - binary_accuracy: 0.8764 - false_negatives: 313.0868 - false_positives: 919.4465 - loss: 0.3848 - precision: 0.8360 - recall: 0.9389 - true_negatives: 4239.8296 - true_positives: 4863.6372\n",
      "For batch number 646 the model has a loss of 0.35488492250442505\n",
      "\u001b[1m646/689\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m2s\u001b[0m 58ms/step - auc: 0.9289 - binary_accuracy: 0.8764 - false_negatives: 313.5588 - false_positives: 920.6564 - loss: 0.3848 - precision: 0.8361 - recall: 0.9389 - true_negatives: 4246.5806 - true_positives: 4871.2041\n",
      "For batch number 647 the model has a loss of 0.35470831394195557\n",
      "\u001b[1m647/689\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m2s\u001b[0m 58ms/step - auc: 0.9289 - binary_accuracy: 0.8764 - false_negatives: 314.0294 - false_positives: 921.8655 - loss: 0.3847 - precision: 0.8361 - recall: 0.9389 - true_negatives: 4253.3291 - true_positives: 4878.7759\n",
      "For batch number 648 the model has a loss of 0.3546566367149353\n",
      "\u001b[1m648/689\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m2s\u001b[0m 58ms/step - auc: 0.9290 - binary_accuracy: 0.8764 - false_negatives: 314.4984 - false_positives: 923.0756 - loss: 0.3847 - precision: 0.8361 - recall: 0.9389 - true_negatives: 4260.0850 - true_positives: 4886.3408\n",
      "For batch number 649 the model has a loss of 0.354472815990448\n",
      "\u001b[1m649/689\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m2s\u001b[0m 58ms/step - auc: 0.9290 - binary_accuracy: 0.8765 - false_negatives: 314.9661 - false_positives: 924.2850 - loss: 0.3846 - precision: 0.8361 - recall: 0.9389 - true_negatives: 4266.8413 - true_positives: 4893.9077\n",
      "For batch number 650 the model has a loss of 0.35417672991752625\n",
      "\u001b[1m650/689\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m2s\u001b[0m 58ms/step - auc: 0.9290 - binary_accuracy: 0.8765 - false_negatives: 315.4323 - false_positives: 925.4923 - loss: 0.3846 - precision: 0.8362 - recall: 0.9389 - true_negatives: 4273.5967 - true_positives: 4901.4785\n",
      "For batch number 651 the model has a loss of 0.3541845381259918\n",
      "\u001b[1m651/689\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m2s\u001b[0m 58ms/step - auc: 0.9290 - binary_accuracy: 0.8765 - false_negatives: 315.8986 - false_positives: 926.7004 - loss: 0.3846 - precision: 0.8362 - recall: 0.9389 - true_negatives: 4280.3535 - true_positives: 4909.0479\n",
      "For batch number 652 the model has a loss of 0.3538679778575897\n",
      "\u001b[1m652/689\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m2s\u001b[0m 58ms/step - auc: 0.9290 - binary_accuracy: 0.8765 - false_negatives: 316.3635 - false_positives: 927.9064 - loss: 0.3845 - precision: 0.8362 - recall: 0.9389 - true_negatives: 4287.1133 - true_positives: 4916.6167\n",
      "For batch number 653 the model has a loss of 0.3539988696575165\n",
      "\u001b[1m653/689\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m2s\u001b[0m 58ms/step - auc: 0.9290 - binary_accuracy: 0.8765 - false_negatives: 316.8285 - false_positives: 929.1133 - loss: 0.3845 - precision: 0.8362 - recall: 0.9389 - true_negatives: 4293.8774 - true_positives: 4924.1807\n",
      "For batch number 654 the model has a loss of 0.3540528416633606\n",
      "\u001b[1m654/689\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m2s\u001b[0m 58ms/step - auc: 0.9290 - binary_accuracy: 0.8765 - false_negatives: 317.2921 - false_positives: 930.3257 - loss: 0.3844 - precision: 0.8362 - recall: 0.9389 - true_negatives: 4300.6436 - true_positives: 4931.7388\n",
      "For batch number 655 the model has a loss of 0.3540765345096588\n",
      "\u001b[1m655/689\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m1s\u001b[0m 58ms/step - auc: 0.9290 - binary_accuracy: 0.8766 - false_negatives: 317.7557 - false_positives: 931.5374 - loss: 0.3844 - precision: 0.8363 - recall: 0.9389 - true_negatives: 4307.4092 - true_positives: 4939.2979\n",
      "For batch number 656 the model has a loss of 0.35381022095680237\n",
      "\u001b[1m656/689\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m1s\u001b[0m 58ms/step - auc: 0.9290 - binary_accuracy: 0.8766 - false_negatives: 318.2180 - false_positives: 932.7485 - loss: 0.3843 - precision: 0.8363 - recall: 0.9389 - true_negatives: 4314.1753 - true_positives: 4946.8584\n",
      "For batch number 657 the model has a loss of 0.35373955965042114\n",
      "\u001b[1m657/689\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m1s\u001b[0m 58ms/step - auc: 0.9291 - binary_accuracy: 0.8766 - false_negatives: 318.6804 - false_positives: 933.9574 - loss: 0.3843 - precision: 0.8363 - recall: 0.9389 - true_negatives: 4320.9375 - true_positives: 4954.4248\n",
      "For batch number 658 the model has a loss of 0.35383108258247375\n",
      "\u001b[1m658/689\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m1s\u001b[0m 58ms/step - auc: 0.9291 - binary_accuracy: 0.8766 - false_negatives: 319.1429 - false_positives: 935.1702 - loss: 0.3842 - precision: 0.8363 - recall: 0.9389 - true_negatives: 4327.7021 - true_positives: 4961.9849\n",
      "For batch number 659 the model has a loss of 0.3536001741886139\n",
      "\u001b[1m659/689\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m1s\u001b[0m 58ms/step - auc: 0.9291 - binary_accuracy: 0.8766 - false_negatives: 319.6039 - false_positives: 936.3824 - loss: 0.3842 - precision: 0.8364 - recall: 0.9389 - true_negatives: 4334.4702 - true_positives: 4969.5435\n",
      "For batch number 660 the model has a loss of 0.35335102677345276\n",
      "\u001b[1m660/689\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m1s\u001b[0m 58ms/step - auc: 0.9291 - binary_accuracy: 0.8766 - false_negatives: 320.0636 - false_positives: 937.5939 - loss: 0.3841 - precision: 0.8364 - recall: 0.9389 - true_negatives: 4341.2393 - true_positives: 4977.1030\n",
      "For batch number 661 the model has a loss of 0.353071928024292\n",
      "\u001b[1m661/689\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m1s\u001b[0m 58ms/step - auc: 0.9291 - binary_accuracy: 0.8767 - false_negatives: 320.5219 - false_positives: 938.8049 - loss: 0.3841 - precision: 0.8364 - recall: 0.9389 - true_negatives: 4348.0122 - true_positives: 4984.6611\n",
      "For batch number 662 the model has a loss of 0.3530123829841614\n",
      "\u001b[1m662/689\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m1s\u001b[0m 58ms/step - auc: 0.9291 - binary_accuracy: 0.8767 - false_negatives: 320.9789 - false_positives: 940.0181 - loss: 0.3840 - precision: 0.8364 - recall: 0.9389 - true_negatives: 4354.7944 - true_positives: 4992.2085\n",
      "For batch number 663 the model has a loss of 0.3529110252857208\n",
      "\u001b[1m663/689\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m1s\u001b[0m 58ms/step - auc: 0.9291 - binary_accuracy: 0.8767 - false_negatives: 321.4344 - false_positives: 941.2292 - loss: 0.3840 - precision: 0.8365 - recall: 0.9389 - true_negatives: 4361.5732 - true_positives: 4999.7632\n",
      "For batch number 664 the model has a loss of 0.35270780324935913\n",
      "\u001b[1m664/689\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m1s\u001b[0m 58ms/step - auc: 0.9291 - binary_accuracy: 0.8767 - false_negatives: 321.8885 - false_positives: 942.4398 - loss: 0.3839 - precision: 0.8365 - recall: 0.9389 - true_negatives: 4368.3613 - true_positives: 5007.3101\n",
      "For batch number 665 the model has a loss of 0.35276180505752563\n",
      "\u001b[1m665/689\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m1s\u001b[0m 58ms/step - auc: 0.9291 - binary_accuracy: 0.8767 - false_negatives: 322.3429 - false_positives: 943.6466 - loss: 0.3839 - precision: 0.8365 - recall: 0.9389 - true_negatives: 4375.1504 - true_positives: 5014.8604\n",
      "For batch number 666 the model has a loss of 0.352550208568573\n",
      "\u001b[1m666/689\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m1s\u001b[0m 58ms/step - auc: 0.9292 - binary_accuracy: 0.8768 - false_negatives: 322.7958 - false_positives: 944.8544 - loss: 0.3839 - precision: 0.8365 - recall: 0.9389 - true_negatives: 4381.9414 - true_positives: 5022.4082\n",
      "For batch number 667 the model has a loss of 0.35230568051338196\n",
      "\u001b[1m667/689\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m1s\u001b[0m 58ms/step - auc: 0.9292 - binary_accuracy: 0.8768 - false_negatives: 323.2474 - false_positives: 946.0600 - loss: 0.3838 - precision: 0.8365 - recall: 0.9389 - true_negatives: 4388.7363 - true_positives: 5029.9565\n",
      "For batch number 668 the model has a loss of 0.35233932733535767\n",
      "\u001b[1m668/689\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m1s\u001b[0m 58ms/step - auc: 0.9292 - binary_accuracy: 0.8768 - false_negatives: 323.6991 - false_positives: 947.2650 - loss: 0.3838 - precision: 0.8366 - recall: 0.9389 - true_negatives: 4395.5312 - true_positives: 5037.5044\n",
      "For batch number 669 the model has a loss of 0.3521823287010193\n",
      "\u001b[1m669/689\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m1s\u001b[0m 58ms/step - auc: 0.9292 - binary_accuracy: 0.8768 - false_negatives: 324.1495 - false_positives: 948.4694 - loss: 0.3837 - precision: 0.8366 - recall: 0.9389 - true_negatives: 4402.3335 - true_positives: 5045.0479\n",
      "For batch number 670 the model has a loss of 0.35187363624572754\n",
      "\u001b[1m670/689\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m1s\u001b[0m 58ms/step - auc: 0.9292 - binary_accuracy: 0.8768 - false_negatives: 324.5985 - false_positives: 949.6716 - loss: 0.3837 - precision: 0.8366 - recall: 0.9389 - true_negatives: 4409.1372 - true_positives: 5052.5928\n",
      "For batch number 671 the model has a loss of 0.35158273577690125\n",
      "\u001b[1m671/689\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m1s\u001b[0m 58ms/step - auc: 0.9292 - binary_accuracy: 0.8768 - false_negatives: 325.0462 - false_positives: 950.8704 - loss: 0.3836 - precision: 0.8366 - recall: 0.9389 - true_negatives: 4415.9448 - true_positives: 5060.1387\n",
      "For batch number 672 the model has a loss of 0.35125893354415894\n",
      "\u001b[1m672/689\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 58ms/step - auc: 0.9292 - binary_accuracy: 0.8769 - false_negatives: 325.4926 - false_positives: 952.0655 - loss: 0.3836 - precision: 0.8367 - recall: 0.9389 - true_negatives: 4422.7559 - true_positives: 5067.6860\n",
      "For batch number 673 the model has a loss of 0.35106244683265686\n",
      "\u001b[1m673/689\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 58ms/step - auc: 0.9292 - binary_accuracy: 0.8769 - false_negatives: 325.9376 - false_positives: 953.2615 - loss: 0.3835 - precision: 0.8367 - recall: 0.9390 - true_negatives: 4429.5693 - true_positives: 5075.2319\n",
      "For batch number 674 the model has a loss of 0.3509054481983185\n",
      "\u001b[1m674/689\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 58ms/step - auc: 0.9292 - binary_accuracy: 0.8769 - false_negatives: 326.3828 - false_positives: 954.4555 - loss: 0.3835 - precision: 0.8367 - recall: 0.9390 - true_negatives: 4436.3843 - true_positives: 5082.7773\n",
      "For batch number 675 the model has a loss of 0.35064762830734253\n",
      "\u001b[1m675/689\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 58ms/step - auc: 0.9293 - binary_accuracy: 0.8769 - false_negatives: 326.8282 - false_positives: 955.6459 - loss: 0.3834 - precision: 0.8367 - recall: 0.9390 - true_negatives: 4443.2017 - true_positives: 5090.3242\n",
      "For batch number 676 the model has a loss of 0.35039961338043213\n",
      "\u001b[1m676/689\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 58ms/step - auc: 0.9293 - binary_accuracy: 0.8769 - false_negatives: 327.2722 - false_positives: 956.8358 - loss: 0.3834 - precision: 0.8368 - recall: 0.9390 - true_negatives: 4450.0205 - true_positives: 5097.8711\n",
      "For batch number 677 the model has a loss of 0.35078924894332886\n",
      "\u001b[1m677/689\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 58ms/step - auc: 0.9293 - binary_accuracy: 0.8770 - false_negatives: 327.7149 - false_positives: 958.0281 - loss: 0.3833 - precision: 0.8368 - recall: 0.9390 - true_negatives: 4456.8374 - true_positives: 5105.4194\n",
      "For batch number 678 the model has a loss of 0.3509276509284973\n",
      "\u001b[1m678/689\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 58ms/step - auc: 0.9293 - binary_accuracy: 0.8770 - false_negatives: 328.1593 - false_positives: 959.2183 - loss: 0.3833 - precision: 0.8368 - recall: 0.9390 - true_negatives: 4463.6475 - true_positives: 5112.9751\n",
      "For batch number 679 the model has a loss of 0.3507503271102905\n",
      "\u001b[1m679/689\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 58ms/step - auc: 0.9293 - binary_accuracy: 0.8770 - false_negatives: 328.6024 - false_positives: 960.4080 - loss: 0.3832 - precision: 0.8368 - recall: 0.9390 - true_negatives: 4470.4595 - true_positives: 5120.5303\n",
      "For batch number 680 the model has a loss of 0.3504447937011719\n",
      "\u001b[1m680/689\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 58ms/step - auc: 0.9293 - binary_accuracy: 0.8770 - false_negatives: 329.0441 - false_positives: 961.5956 - loss: 0.3832 - precision: 0.8369 - recall: 0.9390 - true_negatives: 4477.2749 - true_positives: 5128.0854\n",
      "For batch number 681 the model has a loss of 0.3503914773464203\n",
      "\u001b[1m681/689\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 58ms/step - auc: 0.9293 - binary_accuracy: 0.8770 - false_negatives: 329.4861 - false_positives: 962.7812 - loss: 0.3831 - precision: 0.8369 - recall: 0.9390 - true_negatives: 4484.0952 - true_positives: 5135.6372\n",
      "For batch number 682 the model has a loss of 0.3503742516040802\n",
      "\u001b[1m682/689\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 58ms/step - auc: 0.9293 - binary_accuracy: 0.8771 - false_negatives: 329.9282 - false_positives: 963.9648 - loss: 0.3831 - precision: 0.8369 - recall: 0.9390 - true_negatives: 4490.9224 - true_positives: 5143.1846\n",
      "For batch number 683 the model has a loss of 0.3506181836128235\n",
      "\u001b[1m683/689\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 58ms/step - auc: 0.9294 - binary_accuracy: 0.8771 - false_negatives: 330.3719 - false_positives: 965.1450 - loss: 0.3830 - precision: 0.8369 - recall: 0.9390 - true_negatives: 4497.7466 - true_positives: 5150.7363\n",
      "For batch number 684 the model has a loss of 0.3505125045776367\n",
      "\u001b[1m684/689\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 58ms/step - auc: 0.9294 - binary_accuracy: 0.8771 - false_negatives: 330.8143 - false_positives: 966.3275 - loss: 0.3830 - precision: 0.8369 - recall: 0.9390 - true_negatives: 4504.5718 - true_positives: 5158.2866\n",
      "For batch number 685 the model has a loss of 0.35090312361717224\n",
      "\u001b[1m685/689\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 58ms/step - auc: 0.9294 - binary_accuracy: 0.8771 - false_negatives: 331.2584 - false_positives: 967.5124 - loss: 0.3829 - precision: 0.8370 - recall: 0.9390 - true_negatives: 4511.3955 - true_positives: 5165.8335\n",
      "For batch number 686 the model has a loss of 0.3508121967315674\n",
      "\u001b[1m686/689\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 58ms/step - auc: 0.9294 - binary_accuracy: 0.8771 - false_negatives: 331.7012 - false_positives: 968.6968 - loss: 0.3829 - precision: 0.8370 - recall: 0.9390 - true_negatives: 4518.2217 - true_positives: 5173.3804\n",
      "For batch number 687 the model has a loss of 0.3504488170146942\n",
      "\u001b[1m687/689\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 58ms/step - auc: 0.9294 - binary_accuracy: 0.8771 - false_negatives: 332.1426 - false_positives: 969.8777 - loss: 0.3828 - precision: 0.8370 - recall: 0.9390 - true_negatives: 4525.0479 - true_positives: 5180.9316\n",
      "For batch number 688 the model has a loss of 0.3502592146396637\n",
      "\u001b[1m688/689\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 58ms/step - auc: 0.9294 - binary_accuracy: 0.8772 - false_negatives: 332.5829 - false_positives: 971.0610 - loss: 0.3828 - precision: 0.8370 - recall: 0.9390 - true_negatives: 4531.8779 - true_positives: 5188.4780\n",
      "For batch number 689 the model has a loss of 0.35049915313720703\n",
      "\u001b[1m689/689\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 58ms/step - auc: 0.9294 - binary_accuracy: 0.8772 - false_negatives: 333.0232 - false_positives: 972.2438 - loss: 0.3828 - precision: 0.8371 - recall: 0.9390 - true_negatives: 4538.7085 - true_positives: 5196.0220\n",
      "For epoch number 2 the model has a loss of 0.35049915313720703\n",
      "\u001b[1m689/689\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m42s\u001b[0m 61ms/step - auc: 0.9294 - binary_accuracy: 0.8772 - false_negatives: 333.4623 - false_positives: 973.4232 - loss: 0.3827 - precision: 0.8371 - recall: 0.9390 - true_negatives: 4545.5190 - true_positives: 5203.5435 - val_auc: 0.9528 - val_binary_accuracy: 0.9263 - val_false_negatives: 125.0000 - val_false_positives: 78.0000 - val_loss: 0.3963 - val_precision: 0.9418 - val_recall: 0.9099 - val_true_negatives: 1290.0000 - val_true_positives: 1262.0000\n",
      "Epoch 3/5\n",
      "\n",
      "For batch number 1 the model has a loss of 0.14414480328559875\n",
      "\u001b[1m  1/689\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m52s\u001b[0m 76ms/step - auc: 1.0000 - binary_accuracy: 0.9688 - false_negatives: 0.0000e+00 - false_positives: 1.0000 - loss: 0.1441 - precision: 0.9333 - recall: 1.0000 - true_negatives: 17.0000 - true_positives: 14.0000\n",
      "For batch number 2 the model has a loss of 0.18382631242275238\n",
      "\u001b[1m  2/689\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m39s\u001b[0m 57ms/step - auc: 0.9936 - binary_accuracy: 0.9609 - false_negatives: 0.5000 - false_positives: 1.5000 - loss: 0.1640 - precision: 0.9373 - recall: 0.9848 - true_negatives: 23.0000 - true_positives: 23.0000    \n",
      "For batch number 3 the model has a loss of 0.2147083729505539\n",
      "\u001b[1m  3/689\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m39s\u001b[0m 57ms/step - auc: 0.9896 - binary_accuracy: 0.9566 - false_negatives: 1.0000 - false_positives: 2.0000 - loss: 0.1809 - precision: 0.9393 - recall: 0.9771 - true_negatives: 29.0000 - true_positives: 32.0000\n",
      "For batch number 4 the model has a loss of 0.22894100844860077\n",
      "\u001b[1m  4/689\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m38s\u001b[0m 56ms/step - auc: 0.9849 - binary_accuracy: 0.9518 - false_negatives: 1.2500 - false_positives: 3.0000 - loss: 0.1929 - precision: 0.9333 - recall: 0.9753 - true_negatives: 35.5000 - true_positives: 40.2500\n",
      "For batch number 5 the model has a loss of 0.215963676571846\n",
      "\u001b[1m  5/689\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m39s\u001b[0m 58ms/step - auc: 0.9828 - binary_accuracy: 0.9477 - false_negatives: 2.0000 - false_positives: 3.6000 - loss: 0.1975 - precision: 0.9326 - recall: 0.9684 - true_negatives: 42.4000 - true_positives: 48.0000\n",
      "For batch number 6 the model has a loss of 0.20661582052707672\n",
      "\u001b[1m  6/689\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m39s\u001b[0m 58ms/step - auc: 0.9817 - binary_accuracy: 0.9460 - false_negatives: 2.5000 - false_positives: 4.1667 - loss: 0.1990 - precision: 0.9327 - recall: 0.9656 - true_negatives: 49.0000 - true_positives: 56.3333\n",
      "For batch number 7 the model has a loss of 0.21637718379497528\n",
      "\u001b[1m  7/689\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m39s\u001b[0m 57ms/step - auc: 0.9808 - binary_accuracy: 0.9442 - false_negatives: 3.0000 - false_positives: 4.8571 - loss: 0.2015 - precision: 0.9319 - recall: 0.9634 - true_negatives: 55.4286 - true_positives: 64.7143\n",
      "For batch number 8 the model has a loss of 0.2398090660572052\n",
      "\u001b[1m  8/689\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m38s\u001b[0m 57ms/step - auc: 0.9795 - binary_accuracy: 0.9414 - false_negatives: 3.6250 - false_positives: 5.7500 - loss: 0.2063 - precision: 0.9297 - recall: 0.9606 - true_negatives: 62.1250 - true_positives: 72.5000\n",
      "For batch number 9 the model has a loss of 0.24072043597698212\n",
      "\u001b[1m  9/689\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m38s\u001b[0m 57ms/step - auc: 0.9783 - binary_accuracy: 0.9394 - false_negatives: 4.1111 - false_positives: 6.6667 - loss: 0.2101 - precision: 0.9274 - recall: 0.9590 - true_negatives: 69.1111 - true_positives: 80.1111\n",
      "For batch number 10 the model has a loss of 0.2487955391407013\n",
      "\u001b[1m 10/689\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m39s\u001b[0m 58ms/step - auc: 0.9773 - binary_accuracy: 0.9373 - false_negatives: 4.7000 - false_positives: 7.6000 - loss: 0.2140 - precision: 0.9253 - recall: 0.9570 - true_negatives: 76.2000 - true_positives: 87.5000\n",
      "For batch number 11 the model has a loss of 0.2729562520980835\n",
      "\u001b[1m 11/689\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m39s\u001b[0m 58ms/step - auc: 0.9753 - binary_accuracy: 0.9353 - false_negatives: 5.4545 - false_positives: 8.4545 - loss: 0.2194 - precision: 0.9239 - recall: 0.9545 - true_negatives: 82.9091 - true_positives: 95.1818\n",
      "For batch number 12 the model has a loss of 0.28020134568214417\n",
      "\u001b[1m 12/689\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m39s\u001b[0m 58ms/step - auc: 0.9738 - binary_accuracy: 0.9339 - false_negatives: 6.1667 - false_positives: 9.1667 - loss: 0.2244 - precision: 0.9235 - recall: 0.9527 - true_negatives: 89.3333 - true_positives: 103.3333\n",
      "For batch number 13 the model has a loss of 0.28851401805877686\n",
      "\u001b[1m 13/689\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m38s\u001b[0m 58ms/step - auc: 0.9723 - binary_accuracy: 0.9326 - false_negatives: 6.9231 - false_positives: 9.9231 - loss: 0.2294 - precision: 0.9229 - recall: 0.9508 - true_negatives: 95.8462 - true_positives: 111.3077\n",
      "For batch number 14 the model has a loss of 0.28466010093688965\n",
      "\u001b[1m 14/689\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m38s\u001b[0m 58ms/step - auc: 0.9712 - binary_accuracy: 0.9315 - false_negatives: 7.6429 - false_positives: 10.6429 - loss: 0.2333 - precision: 0.9225 - recall: 0.9492 - true_negatives: 102.5000 - true_positives: 119.2143\n",
      "For batch number 15 the model has a loss of 0.27967676520347595\n",
      "\u001b[1m 15/689\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m38s\u001b[0m 57ms/step - auc: 0.9702 - binary_accuracy: 0.9306 - false_negatives: 8.3333 - false_positives: 11.3333 - loss: 0.2364 - precision: 0.9222 - recall: 0.9479 - true_negatives: 109.4000 - true_positives: 126.9333\n",
      "For batch number 16 the model has a loss of 0.27311089634895325\n",
      "\u001b[1m 16/689\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m38s\u001b[0m 58ms/step - auc: 0.9694 - binary_accuracy: 0.9301 - false_negatives: 8.9375 - false_positives: 12.0000 - loss: 0.2387 - precision: 0.9220 - recall: 0.9469 - true_negatives: 116.5000 - true_positives: 134.5625\n",
      "For batch number 17 the model has a loss of 0.270990252494812\n",
      "\u001b[1m 17/689\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m38s\u001b[0m 58ms/step - auc: 0.9687 - binary_accuracy: 0.9297 - false_negatives: 9.5294 - false_positives: 12.6471 - loss: 0.2406 - precision: 0.9219 - recall: 0.9461 - true_negatives: 123.7059 - true_positives: 142.1176\n",
      "For batch number 18 the model has a loss of 0.2707475423812866\n",
      "\u001b[1m 18/689\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m38s\u001b[0m 58ms/step - auc: 0.9681 - binary_accuracy: 0.9292 - false_negatives: 10.0556 - false_positives: 13.3889 - loss: 0.2423 - precision: 0.9214 - recall: 0.9455 - true_negatives: 130.8889 - true_positives: 149.6667\n",
      "For batch number 19 the model has a loss of 0.2704015076160431\n",
      "\u001b[1m 19/689\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m38s\u001b[0m 58ms/step - auc: 0.9674 - binary_accuracy: 0.9289 - false_negatives: 10.5263 - false_positives: 14.1579 - loss: 0.2437 - precision: 0.9210 - recall: 0.9451 - true_negatives: 138.2105 - true_positives: 157.1053\n",
      "For batch number 20 the model has a loss of 0.26621392369270325\n",
      "\u001b[1m 20/689\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m38s\u001b[0m 58ms/step - auc: 0.9669 - binary_accuracy: 0.9285 - false_negatives: 11.0000 - false_positives: 14.9500 - loss: 0.2449 - precision: 0.9204 - recall: 0.9448 - true_negatives: 145.5500 - true_positives: 164.5000\n",
      "For batch number 21 the model has a loss of 0.25963616371154785\n",
      "\u001b[1m 21/689\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m38s\u001b[0m 58ms/step - auc: 0.9665 - binary_accuracy: 0.9284 - false_negatives: 11.4286 - false_positives: 15.6667 - loss: 0.2456 - precision: 0.9202 - recall: 0.9447 - true_negatives: 152.8095 - true_positives: 172.0952\n",
      "For batch number 22 the model has a loss of 0.2582924962043762\n",
      "\u001b[1m 22/689\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m38s\u001b[0m 58ms/step - auc: 0.9661 - binary_accuracy: 0.9282 - false_negatives: 11.8636 - false_positives: 16.4091 - loss: 0.2462 - precision: 0.9199 - recall: 0.9445 - true_negatives: 160.0455 - true_positives: 179.6818\n",
      "For batch number 23 the model has a loss of 0.2594929039478302\n",
      "\u001b[1m 23/689\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m38s\u001b[0m 58ms/step - auc: 0.9659 - binary_accuracy: 0.9280 - false_negatives: 12.3043 - false_positives: 17.1739 - loss: 0.2467 - precision: 0.9195 - recall: 0.9444 - true_negatives: 167.3478 - true_positives: 187.1739\n",
      "For batch number 24 the model has a loss of 0.26974955201148987\n",
      "\u001b[1m 24/689\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m38s\u001b[0m 58ms/step - auc: 0.9654 - binary_accuracy: 0.9279 - false_negatives: 12.7083 - false_positives: 17.9583 - loss: 0.2477 - precision: 0.9192 - recall: 0.9444 - true_negatives: 174.4583 - true_positives: 194.8750\n",
      "For batch number 25 the model has a loss of 0.26620057225227356\n",
      "\u001b[1m 25/689\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m38s\u001b[0m 58ms/step - auc: 0.9650 - binary_accuracy: 0.9278 - false_negatives: 13.0800 - false_positives: 18.7200 - loss: 0.2484 - precision: 0.9190 - recall: 0.9444 - true_negatives: 181.6800 - true_positives: 202.5200\n",
      "For batch number 26 the model has a loss of 0.26699498295783997\n",
      "\u001b[1m 26/689\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m38s\u001b[0m 58ms/step - auc: 0.9646 - binary_accuracy: 0.9277 - false_negatives: 13.4231 - false_positives: 19.5769 - loss: 0.2491 - precision: 0.9185 - recall: 0.9446 - true_negatives: 188.8846 - true_positives: 210.1154\n",
      "For batch number 27 the model has a loss of 0.268562376499176\n",
      "\u001b[1m 27/689\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m38s\u001b[0m 57ms/step - auc: 0.9642 - binary_accuracy: 0.9275 - false_negatives: 13.8148 - false_positives: 20.4444 - loss: 0.2499 - precision: 0.9180 - recall: 0.9446 - true_negatives: 196.1111 - true_positives: 217.6296\n",
      "For batch number 28 the model has a loss of 0.2667216360569\n",
      "\u001b[1m 28/689\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m37s\u001b[0m 57ms/step - auc: 0.9638 - binary_accuracy: 0.9272 - false_negatives: 14.2500 - false_positives: 21.3214 - loss: 0.2505 - precision: 0.9176 - recall: 0.9445 - true_negatives: 203.2857 - true_positives: 225.1429\n",
      "For batch number 29 the model has a loss of 0.26653626561164856\n",
      "\u001b[1m 29/689\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m37s\u001b[0m 58ms/step - auc: 0.9635 - binary_accuracy: 0.9269 - false_negatives: 14.6897 - false_positives: 22.2759 - loss: 0.2510 - precision: 0.9170 - recall: 0.9445 - true_negatives: 210.3448 - true_positives: 232.6897\n",
      "For batch number 30 the model has a loss of 0.2736516296863556\n",
      "\u001b[1m 30/689\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m38s\u001b[0m 58ms/step - auc: 0.9632 - binary_accuracy: 0.9267 - false_negatives: 15.1333 - false_positives: 23.2000 - loss: 0.2518 - precision: 0.9165 - recall: 0.9444 - true_negatives: 217.4667 - true_positives: 240.2000\n",
      "For batch number 31 the model has a loss of 0.26899024844169617\n",
      "\u001b[1m 31/689\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m38s\u001b[0m 58ms/step - auc: 0.9630 - binary_accuracy: 0.9265 - false_negatives: 15.5484 - false_positives: 24.0968 - loss: 0.2523 - precision: 0.9160 - recall: 0.9444 - true_negatives: 224.5484 - true_positives: 247.8065\n",
      "For batch number 32 the model has a loss of 0.2679767906665802\n",
      "\u001b[1m 32/689\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m38s\u001b[0m 58ms/step - auc: 0.9627 - binary_accuracy: 0.9263 - false_negatives: 15.9688 - false_positives: 24.9688 - loss: 0.2528 - precision: 0.9157 - recall: 0.9444 - true_negatives: 231.5000 - true_positives: 255.5625\n",
      "For batch number 33 the model has a loss of 0.2723836600780487\n",
      "\u001b[1m 33/689\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m37s\u001b[0m 58ms/step - auc: 0.9625 - binary_accuracy: 0.9260 - false_negatives: 16.3636 - false_positives: 25.9394 - loss: 0.2534 - precision: 0.9152 - recall: 0.9445 - true_negatives: 238.3636 - true_positives: 263.3333\n",
      "For batch number 34 the model has a loss of 0.2735495865345001\n",
      "\u001b[1m 34/689\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m37s\u001b[0m 58ms/step - auc: 0.9622 - binary_accuracy: 0.9258 - false_negatives: 16.7353 - false_positives: 27.0000 - loss: 0.2540 - precision: 0.9146 - recall: 0.9446 - true_negatives: 245.2059 - true_positives: 271.0588\n",
      "For batch number 35 the model has a loss of 0.28311896324157715\n",
      "\u001b[1m 35/689\u001b[0m \u001b[32m━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m37s\u001b[0m 58ms/step - auc: 0.9619 - binary_accuracy: 0.9254 - false_negatives: 17.1714 - false_positives: 28.0571 - loss: 0.2548 - precision: 0.9140 - recall: 0.9446 - true_negatives: 251.9429 - true_positives: 278.8286\n",
      "For batch number 36 the model has a loss of 0.28182756900787354\n",
      "\u001b[1m 36/689\u001b[0m \u001b[32m━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m37s\u001b[0m 58ms/step - auc: 0.9617 - binary_accuracy: 0.9251 - false_negatives: 17.6667 - false_positives: 29.0833 - loss: 0.2556 - precision: 0.9135 - recall: 0.9445 - true_negatives: 258.6667 - true_positives: 286.5833\n",
      "For batch number 37 the model has a loss of 0.28081071376800537\n",
      "\u001b[1m 37/689\u001b[0m \u001b[32m━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m37s\u001b[0m 58ms/step - auc: 0.9615 - binary_accuracy: 0.9248 - false_negatives: 18.2162 - false_positives: 30.0541 - loss: 0.2563 - precision: 0.9131 - recall: 0.9443 - true_negatives: 265.2973 - true_positives: 294.4324\n",
      "For batch number 38 the model has a loss of 0.2813413739204407\n",
      "\u001b[1m 38/689\u001b[0m \u001b[32m━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m37s\u001b[0m 58ms/step - auc: 0.9612 - binary_accuracy: 0.9245 - false_negatives: 18.7895 - false_positives: 31.0000 - loss: 0.2569 - precision: 0.9128 - recall: 0.9441 - true_negatives: 271.8421 - true_positives: 302.3684\n",
      "For batch number 39 the model has a loss of 0.28472429513931274\n",
      "\u001b[1m 39/689\u001b[0m \u001b[32m━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m37s\u001b[0m 58ms/step - auc: 0.9610 - binary_accuracy: 0.9241 - false_negatives: 19.3590 - false_positives: 31.9744 - loss: 0.2576 - precision: 0.9124 - recall: 0.9439 - true_negatives: 278.4359 - true_positives: 310.2308\n",
      "For batch number 40 the model has a loss of 0.2858735918998718\n",
      "\u001b[1m 40/689\u001b[0m \u001b[32m━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m37s\u001b[0m 58ms/step - auc: 0.9608 - binary_accuracy: 0.9238 - false_negatives: 19.9500 - false_positives: 32.9000 - loss: 0.2583 - precision: 0.9121 - recall: 0.9437 - true_negatives: 284.9500 - true_positives: 318.2000\n",
      "For batch number 41 the model has a loss of 0.284525603055954\n",
      "\u001b[1m 41/689\u001b[0m \u001b[32m━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m37s\u001b[0m 58ms/step - auc: 0.9606 - binary_accuracy: 0.9236 - false_negatives: 20.5122 - false_positives: 33.8049 - loss: 0.2590 - precision: 0.9119 - recall: 0.9436 - true_negatives: 291.5610 - true_positives: 326.1219\n",
      "For batch number 42 the model has a loss of 0.2810478210449219\n",
      "\u001b[1m 42/689\u001b[0m \u001b[32m━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m37s\u001b[0m 58ms/step - auc: 0.9604 - binary_accuracy: 0.9234 - false_negatives: 21.0714 - false_positives: 34.6905 - loss: 0.2595 - precision: 0.9117 - recall: 0.9434 - true_negatives: 298.1190 - true_positives: 334.1190\n",
      "For batch number 43 the model has a loss of 0.27683281898498535\n",
      "\u001b[1m 43/689\u001b[0m \u001b[32m━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m37s\u001b[0m 58ms/step - auc: 0.9602 - binary_accuracy: 0.9232 - false_negatives: 21.6047 - false_positives: 35.5349 - loss: 0.2599 - precision: 0.9115 - recall: 0.9433 - true_negatives: 304.7442 - true_positives: 342.1163\n",
      "For batch number 44 the model has a loss of 0.2764209806919098\n",
      "\u001b[1m 44/689\u001b[0m \u001b[32m━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m37s\u001b[0m 58ms/step - auc: 0.9601 - binary_accuracy: 0.9231 - false_negatives: 22.1136 - false_positives: 36.4091 - loss: 0.2603 - precision: 0.9113 - recall: 0.9433 - true_negatives: 311.4318 - true_positives: 350.0454\n",
      "For batch number 45 the model has a loss of 0.2776305675506592\n",
      "\u001b[1m 45/689\u001b[0m \u001b[32m━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m37s\u001b[0m 58ms/step - auc: 0.9599 - binary_accuracy: 0.9229 - false_negatives: 22.6444 - false_positives: 37.2667 - loss: 0.2607 - precision: 0.9112 - recall: 0.9432 - true_negatives: 318.0889 - true_positives: 358.0000\n",
      "For batch number 46 the model has a loss of 0.279191255569458\n",
      "\u001b[1m 46/689\u001b[0m \u001b[32m━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m37s\u001b[0m 58ms/step - auc: 0.9598 - binary_accuracy: 0.9227 - false_negatives: 23.1739 - false_positives: 38.1522 - loss: 0.2611 - precision: 0.9110 - recall: 0.9431 - true_negatives: 324.7391 - true_positives: 365.9348\n",
      "For batch number 47 the model has a loss of 0.28209465742111206\n",
      "\u001b[1m 47/689\u001b[0m \u001b[32m━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m37s\u001b[0m 58ms/step - auc: 0.9596 - binary_accuracy: 0.9226 - false_negatives: 23.7021 - false_positives: 39.0638 - loss: 0.2615 - precision: 0.9108 - recall: 0.9430 - true_negatives: 331.3192 - true_positives: 373.9149\n",
      "For batch number 48 the model has a loss of 0.2808765470981598\n",
      "\u001b[1m 48/689\u001b[0m \u001b[32m━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m37s\u001b[0m 58ms/step - auc: 0.9595 - binary_accuracy: 0.9223 - false_negatives: 24.2083 - false_positives: 40.0417 - loss: 0.2619 - precision: 0.9105 - recall: 0.9429 - true_negatives: 337.9583 - true_positives: 381.7917\n",
      "For batch number 49 the model has a loss of 0.2778718173503876\n",
      "\u001b[1m 49/689\u001b[0m \u001b[32m━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m37s\u001b[0m 58ms/step - auc: 0.9594 - binary_accuracy: 0.9222 - false_negatives: 24.7143 - false_positives: 41.0000 - loss: 0.2623 - precision: 0.9103 - recall: 0.9429 - true_negatives: 344.6327 - true_positives: 389.6531\n",
      "For batch number 50 the model has a loss of 0.2745085060596466\n",
      "\u001b[1m 50/689\u001b[0m \u001b[32m━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m37s\u001b[0m 58ms/step - auc: 0.9593 - binary_accuracy: 0.9220 - false_negatives: 25.2000 - false_positives: 41.9200 - loss: 0.2625 - precision: 0.9101 - recall: 0.9428 - true_negatives: 351.2600 - true_positives: 397.6200\n",
      "For batch number 51 the model has a loss of 0.2727428376674652\n",
      "\u001b[1m 51/689\u001b[0m \u001b[32m━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m36s\u001b[0m 58ms/step - auc: 0.9592 - binary_accuracy: 0.9219 - false_negatives: 25.6667 - false_positives: 42.8431 - loss: 0.2627 - precision: 0.9099 - recall: 0.9428 - true_negatives: 357.8824 - true_positives: 405.6078\n",
      "For batch number 52 the model has a loss of 0.2723877727985382\n",
      "\u001b[1m 52/689\u001b[0m \u001b[32m━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m36s\u001b[0m 58ms/step - auc: 0.9591 - binary_accuracy: 0.9218 - false_negatives: 26.1346 - false_positives: 43.7500 - loss: 0.2629 - precision: 0.9097 - recall: 0.9428 - true_negatives: 364.5577 - true_positives: 413.5577\n",
      "For batch number 53 the model has a loss of 0.2688766121864319\n",
      "\u001b[1m 53/689\u001b[0m \u001b[32m━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m36s\u001b[0m 58ms/step - auc: 0.9591 - binary_accuracy: 0.9217 - false_negatives: 26.5849 - false_positives: 44.6226 - loss: 0.2630 - precision: 0.9096 - recall: 0.9428 - true_negatives: 371.2453 - true_positives: 421.5472\n",
      "For batch number 54 the model has a loss of 0.2679188847541809\n",
      "\u001b[1m 54/689\u001b[0m \u001b[32m━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m36s\u001b[0m 58ms/step - auc: 0.9591 - binary_accuracy: 0.9216 - false_negatives: 27.0185 - false_positives: 45.5000 - loss: 0.2631 - precision: 0.9094 - recall: 0.9429 - true_negatives: 377.9259 - true_positives: 429.5555\n",
      "For batch number 55 the model has a loss of 0.2712404727935791\n",
      "\u001b[1m 55/689\u001b[0m \u001b[32m━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m36s\u001b[0m 58ms/step - auc: 0.9590 - binary_accuracy: 0.9215 - false_negatives: 27.4545 - false_positives: 46.4000 - loss: 0.2632 - precision: 0.9093 - recall: 0.9429 - true_negatives: 384.6000 - true_positives: 437.5454\n",
      "For batch number 56 the model has a loss of 0.268932968378067\n",
      "\u001b[1m 56/689\u001b[0m \u001b[32m━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m36s\u001b[0m 58ms/step - auc: 0.9589 - binary_accuracy: 0.9215 - false_negatives: 27.8750 - false_positives: 47.2857 - loss: 0.2633 - precision: 0.9092 - recall: 0.9429 - true_negatives: 391.2857 - true_positives: 445.5536\n",
      "For batch number 57 the model has a loss of 0.26723363995552063\n",
      "\u001b[1m 57/689\u001b[0m \u001b[32m━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m36s\u001b[0m 58ms/step - auc: 0.9589 - binary_accuracy: 0.9214 - false_negatives: 28.2807 - false_positives: 48.1754 - loss: 0.2634 - precision: 0.9090 - recall: 0.9430 - true_negatives: 397.9649 - true_positives: 453.5789\n",
      "For batch number 58 the model has a loss of 0.2708302438259125\n",
      "\u001b[1m 58/689\u001b[0m \u001b[32m━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m36s\u001b[0m 58ms/step - auc: 0.9588 - binary_accuracy: 0.9213 - false_negatives: 28.6724 - false_positives: 49.1552 - loss: 0.2635 - precision: 0.9088 - recall: 0.9431 - true_negatives: 404.7069 - true_positives: 461.4655\n",
      "For batch number 59 the model has a loss of 0.2713295519351959\n",
      "\u001b[1m 59/689\u001b[0m \u001b[32m━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m36s\u001b[0m 58ms/step - auc: 0.9587 - binary_accuracy: 0.9212 - false_negatives: 29.0678 - false_positives: 50.1695 - loss: 0.2637 - precision: 0.9086 - recall: 0.9431 - true_negatives: 411.4576 - true_positives: 469.3051\n",
      "For batch number 60 the model has a loss of 0.27017679810523987\n",
      "\u001b[1m 60/689\u001b[0m \u001b[32m━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m36s\u001b[0m 58ms/step - auc: 0.9587 - binary_accuracy: 0.9211 - false_negatives: 29.4500 - false_positives: 51.1833 - loss: 0.2638 - precision: 0.9084 - recall: 0.9432 - true_negatives: 418.2500 - true_positives: 477.1167\n",
      "For batch number 61 the model has a loss of 0.27320969104766846\n",
      "\u001b[1m 61/689\u001b[0m \u001b[32m━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m36s\u001b[0m 58ms/step - auc: 0.9586 - binary_accuracy: 0.9210 - false_negatives: 29.8525 - false_positives: 52.2131 - loss: 0.2639 - precision: 0.9081 - recall: 0.9433 - true_negatives: 425.0000 - true_positives: 484.9344\n",
      "For batch number 62 the model has a loss of 0.2714751362800598\n",
      "\u001b[1m 62/689\u001b[0m \u001b[32m━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m36s\u001b[0m 58ms/step - auc: 0.9585 - binary_accuracy: 0.9209 - false_negatives: 30.2419 - false_positives: 53.2419 - loss: 0.2641 - precision: 0.9079 - recall: 0.9433 - true_negatives: 431.7581 - true_positives: 492.7581\n",
      "For batch number 63 the model has a loss of 0.2711970806121826\n",
      "\u001b[1m 63/689\u001b[0m \u001b[32m━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m36s\u001b[0m 58ms/step - auc: 0.9584 - binary_accuracy: 0.9208 - false_negatives: 30.6349 - false_positives: 54.2698 - loss: 0.2642 - precision: 0.9076 - recall: 0.9434 - true_negatives: 438.5079 - true_positives: 500.5873\n",
      "For batch number 64 the model has a loss of 0.2804838716983795\n",
      "\u001b[1m 64/689\u001b[0m \u001b[32m━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m36s\u001b[0m 58ms/step - auc: 0.9583 - binary_accuracy: 0.9206 - false_negatives: 31.0781 - false_positives: 55.3281 - loss: 0.2644 - precision: 0.9074 - recall: 0.9434 - true_negatives: 445.2188 - true_positives: 508.3750\n",
      "For batch number 65 the model has a loss of 0.2790977358818054\n",
      "\u001b[1m 65/689\u001b[0m \u001b[32m━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m36s\u001b[0m 58ms/step - auc: 0.9583 - binary_accuracy: 0.9205 - false_negatives: 31.5385 - false_positives: 56.3538 - loss: 0.2646 - precision: 0.9071 - recall: 0.9434 - true_negatives: 451.9231 - true_positives: 516.1846\n",
      "For batch number 66 the model has a loss of 0.27949801087379456\n",
      "\u001b[1m 66/689\u001b[0m \u001b[32m━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m36s\u001b[0m 58ms/step - auc: 0.9582 - binary_accuracy: 0.9204 - false_negatives: 32.0000 - false_positives: 57.3636 - loss: 0.2649 - precision: 0.9069 - recall: 0.9434 - true_negatives: 458.5909 - true_positives: 524.0455\n",
      "For batch number 67 the model has a loss of 0.2787454426288605\n",
      "\u001b[1m 67/689\u001b[0m \u001b[32m━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m35s\u001b[0m 58ms/step - auc: 0.9581 - binary_accuracy: 0.9203 - false_negatives: 32.4776 - false_positives: 58.3731 - loss: 0.2651 - precision: 0.9067 - recall: 0.9434 - true_negatives: 465.2388 - true_positives: 531.9105\n",
      "For batch number 68 the model has a loss of 0.279457688331604\n",
      "\u001b[1m 68/689\u001b[0m \u001b[32m━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m35s\u001b[0m 58ms/step - auc: 0.9580 - binary_accuracy: 0.9201 - false_negatives: 32.9412 - false_positives: 59.4412 - loss: 0.2653 - precision: 0.9065 - recall: 0.9434 - true_negatives: 471.8529 - true_positives: 539.7647\n",
      "For batch number 69 the model has a loss of 0.2784406542778015\n",
      "\u001b[1m 69/689\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m35s\u001b[0m 58ms/step - auc: 0.9579 - binary_accuracy: 0.9200 - false_negatives: 33.4058 - false_positives: 60.5072 - loss: 0.2655 - precision: 0.9063 - recall: 0.9434 - true_negatives: 478.4928 - true_positives: 547.5942\n",
      "For batch number 70 the model has a loss of 0.2805357277393341\n",
      "\u001b[1m 70/689\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m35s\u001b[0m 58ms/step - auc: 0.9578 - binary_accuracy: 0.9198 - false_negatives: 33.8714 - false_positives: 61.5571 - loss: 0.2657 - precision: 0.9061 - recall: 0.9434 - true_negatives: 485.1429 - true_positives: 555.4286\n",
      "For batch number 71 the model has a loss of 0.2860800325870514\n",
      "\u001b[1m 71/689\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m35s\u001b[0m 58ms/step - auc: 0.9578 - binary_accuracy: 0.9197 - false_negatives: 34.3521 - false_positives: 62.6056 - loss: 0.2660 - precision: 0.9058 - recall: 0.9434 - true_negatives: 491.8028 - true_positives: 563.2394\n",
      "For batch number 72 the model has a loss of 0.2887454330921173\n",
      "\u001b[1m 72/689\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m35s\u001b[0m 58ms/step - auc: 0.9577 - binary_accuracy: 0.9196 - false_negatives: 34.8333 - false_positives: 63.6389 - loss: 0.2663 - precision: 0.9056 - recall: 0.9434 - true_negatives: 498.4167 - true_positives: 571.1111\n",
      "For batch number 73 the model has a loss of 0.295654833316803\n",
      "\u001b[1m 73/689\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m35s\u001b[0m 58ms/step - auc: 0.9576 - binary_accuracy: 0.9194 - false_negatives: 35.3151 - false_positives: 64.6849 - loss: 0.2667 - precision: 0.9054 - recall: 0.9433 - true_negatives: 505.0685 - true_positives: 578.9315\n",
      "For batch number 74 the model has a loss of 0.2994183599948883\n",
      "\u001b[1m 74/689\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m35s\u001b[0m 58ms/step - auc: 0.9575 - binary_accuracy: 0.9193 - false_negatives: 35.8108 - false_positives: 65.7027 - loss: 0.2671 - precision: 0.9053 - recall: 0.9433 - true_negatives: 511.7027 - true_positives: 586.7838\n",
      "For batch number 75 the model has a loss of 0.2996366620063782\n",
      "\u001b[1m 75/689\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m35s\u001b[0m 58ms/step - auc: 0.9574 - binary_accuracy: 0.9192 - false_negatives: 36.3200 - false_positives: 66.7067 - loss: 0.2676 - precision: 0.9051 - recall: 0.9433 - true_negatives: 518.3467 - true_positives: 594.6266\n",
      "For batch number 76 the model has a loss of 0.299084335565567\n",
      "\u001b[1m 76/689\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m35s\u001b[0m 58ms/step - auc: 0.9573 - binary_accuracy: 0.9191 - false_negatives: 36.8289 - false_positives: 67.7105 - loss: 0.2680 - precision: 0.9049 - recall: 0.9433 - true_negatives: 524.9868 - true_positives: 602.4737\n",
      "For batch number 77 the model has a loss of 0.29859691858291626\n",
      "\u001b[1m 77/689\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m35s\u001b[0m 58ms/step - auc: 0.9572 - binary_accuracy: 0.9190 - false_negatives: 37.3377 - false_positives: 68.7143 - loss: 0.2684 - precision: 0.9048 - recall: 0.9432 - true_negatives: 531.6494 - true_positives: 610.2987\n",
      "For batch number 78 the model has a loss of 0.29790815711021423\n",
      "\u001b[1m 78/689\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m35s\u001b[0m 58ms/step - auc: 0.9571 - binary_accuracy: 0.9189 - false_negatives: 37.8333 - false_positives: 69.7436 - loss: 0.2688 - precision: 0.9046 - recall: 0.9432 - true_negatives: 538.3205 - true_positives: 618.1025\n",
      "For batch number 79 the model has a loss of 0.29646384716033936\n",
      "\u001b[1m 79/689\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m35s\u001b[0m 58ms/step - auc: 0.9571 - binary_accuracy: 0.9188 - false_negatives: 38.3165 - false_positives: 70.7468 - loss: 0.2691 - precision: 0.9045 - recall: 0.9432 - true_negatives: 544.9747 - true_positives: 625.9620\n",
      "For batch number 80 the model has a loss of 0.2948015332221985\n",
      "\u001b[1m 80/689\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m35s\u001b[0m 58ms/step - auc: 0.9570 - binary_accuracy: 0.9187 - false_negatives: 38.7875 - false_positives: 71.7500 - loss: 0.2694 - precision: 0.9043 - recall: 0.9432 - true_negatives: 551.6250 - true_positives: 633.8375\n",
      "For batch number 81 the model has a loss of 0.2943076491355896\n",
      "\u001b[1m 81/689\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m35s\u001b[0m 58ms/step - auc: 0.9570 - binary_accuracy: 0.9186 - false_negatives: 39.2469 - false_positives: 72.7407 - loss: 0.2697 - precision: 0.9042 - recall: 0.9432 - true_negatives: 558.2839 - true_positives: 641.7284\n",
      "For batch number 82 the model has a loss of 0.29296234250068665\n",
      "\u001b[1m 82/689\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m35s\u001b[0m 58ms/step - auc: 0.9569 - binary_accuracy: 0.9185 - false_negatives: 39.6951 - false_positives: 73.7195 - loss: 0.2700 - precision: 0.9040 - recall: 0.9432 - true_negatives: 564.9512 - true_positives: 649.6342\n",
      "For batch number 83 the model has a loss of 0.29175740480422974\n",
      "\u001b[1m 83/689\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m34s\u001b[0m 58ms/step - auc: 0.9569 - binary_accuracy: 0.9184 - false_negatives: 40.1325 - false_positives: 74.6867 - loss: 0.2703 - precision: 0.9039 - recall: 0.9432 - true_negatives: 571.6385 - true_positives: 657.5422\n",
      "For batch number 84 the model has a loss of 0.2917500436306\n",
      "\u001b[1m 84/689\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m34s\u001b[0m 58ms/step - auc: 0.9568 - binary_accuracy: 0.9184 - false_negatives: 40.5595 - false_positives: 75.6548 - loss: 0.2705 - precision: 0.9038 - recall: 0.9433 - true_negatives: 578.3333 - true_positives: 665.4524\n",
      "For batch number 85 the model has a loss of 0.29128405451774597\n",
      "\u001b[1m 85/689\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m34s\u001b[0m 58ms/step - auc: 0.9568 - binary_accuracy: 0.9183 - false_negatives: 40.9882 - false_positives: 76.6353 - loss: 0.2708 - precision: 0.9037 - recall: 0.9433 - true_negatives: 585.0236 - true_positives: 673.3530\n",
      "For batch number 86 the model has a loss of 0.2903417944908142\n",
      "\u001b[1m 86/689\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m34s\u001b[0m 58ms/step - auc: 0.9567 - binary_accuracy: 0.9183 - false_negatives: 41.4070 - false_positives: 77.6163 - loss: 0.2710 - precision: 0.9036 - recall: 0.9433 - true_negatives: 591.7442 - true_positives: 681.2325\n",
      "For batch number 87 the model has a loss of 0.28971219062805176\n",
      "\u001b[1m 87/689\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m34s\u001b[0m 58ms/step - auc: 0.9567 - binary_accuracy: 0.9182 - false_negatives: 41.8161 - false_positives: 78.6207 - loss: 0.2712 - precision: 0.9035 - recall: 0.9433 - true_negatives: 598.4598 - true_positives: 689.1035\n",
      "For batch number 88 the model has a loss of 0.28838497400283813\n",
      "\u001b[1m 88/689\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m34s\u001b[0m 58ms/step - auc: 0.9567 - binary_accuracy: 0.9182 - false_negatives: 42.2273 - false_positives: 79.6136 - loss: 0.2714 - precision: 0.9033 - recall: 0.9434 - true_negatives: 605.2045 - true_positives: 696.9545\n",
      "For batch number 89 the model has a loss of 0.28910449147224426\n",
      "\u001b[1m 89/689\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m34s\u001b[0m 58ms/step - auc: 0.9566 - binary_accuracy: 0.9181 - false_negatives: 42.6292 - false_positives: 80.6404 - loss: 0.2716 - precision: 0.9032 - recall: 0.9434 - true_negatives: 612.0000 - true_positives: 704.7303\n",
      "For batch number 90 the model has a loss of 0.28839099407196045\n",
      "\u001b[1m 90/689\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m34s\u001b[0m 58ms/step - auc: 0.9566 - binary_accuracy: 0.9180 - false_negatives: 43.0222 - false_positives: 81.6556 - loss: 0.2718 - precision: 0.9031 - recall: 0.9435 - true_negatives: 618.7556 - true_positives: 712.5667\n",
      "For batch number 91 the model has a loss of 0.28752854466438293\n",
      "\u001b[1m 91/689\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m34s\u001b[0m 58ms/step - auc: 0.9566 - binary_accuracy: 0.9180 - false_negatives: 43.4066 - false_positives: 82.6703 - loss: 0.2720 - precision: 0.9029 - recall: 0.9435 - true_negatives: 625.5385 - true_positives: 720.3846\n",
      "For batch number 92 the model has a loss of 0.2873082458972931\n",
      "\u001b[1m 92/689\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m34s\u001b[0m 58ms/step - auc: 0.9566 - binary_accuracy: 0.9179 - false_negatives: 43.7826 - false_positives: 83.6957 - loss: 0.2722 - precision: 0.9028 - recall: 0.9436 - true_negatives: 632.3587 - true_positives: 728.1630\n",
      "For batch number 93 the model has a loss of 0.28857773542404175\n",
      "\u001b[1m 93/689\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m34s\u001b[0m 58ms/step - auc: 0.9565 - binary_accuracy: 0.9179 - false_negatives: 44.1613 - false_positives: 84.7312 - loss: 0.2723 - precision: 0.9026 - recall: 0.9436 - true_negatives: 639.1828 - true_positives: 735.9247\n",
      "For batch number 94 the model has a loss of 0.2881794273853302\n",
      "\u001b[1m 94/689\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m34s\u001b[0m 58ms/step - auc: 0.9565 - binary_accuracy: 0.9178 - false_negatives: 44.5319 - false_positives: 85.7660 - loss: 0.2725 - precision: 0.9025 - recall: 0.9437 - true_negatives: 645.9787 - true_positives: 743.7234\n",
      "For batch number 95 the model has a loss of 0.28855204582214355\n",
      "\u001b[1m 95/689\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m34s\u001b[0m 58ms/step - auc: 0.9565 - binary_accuracy: 0.9178 - false_negatives: 44.9053 - false_positives: 86.8105 - loss: 0.2727 - precision: 0.9024 - recall: 0.9437 - true_negatives: 652.7579 - true_positives: 751.5263\n",
      "For batch number 96 the model has a loss of 0.2888030707836151\n",
      "\u001b[1m 96/689\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m34s\u001b[0m 58ms/step - auc: 0.9565 - binary_accuracy: 0.9177 - false_negatives: 45.2812 - false_positives: 87.8854 - loss: 0.2728 - precision: 0.9022 - recall: 0.9438 - true_negatives: 659.5417 - true_positives: 759.2917\n",
      "For batch number 97 the model has a loss of 0.289655476808548\n",
      "\u001b[1m 97/689\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m34s\u001b[0m 58ms/step - auc: 0.9564 - binary_accuracy: 0.9177 - false_negatives: 45.6495 - false_positives: 88.9897 - loss: 0.2730 - precision: 0.9020 - recall: 0.9438 - true_negatives: 666.3299 - true_positives: 767.0309\n",
      "For batch number 98 the model has a loss of 0.28907859325408936\n",
      "\u001b[1m 98/689\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m34s\u001b[0m 58ms/step - auc: 0.9564 - binary_accuracy: 0.9176 - false_negatives: 46.0102 - false_positives: 90.1020 - loss: 0.2732 - precision: 0.9019 - recall: 0.9439 - true_negatives: 673.1735 - true_positives: 774.7143\n",
      "For batch number 99 the model has a loss of 0.2882990837097168\n",
      "\u001b[1m 99/689\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m34s\u001b[0m 58ms/step - auc: 0.9564 - binary_accuracy: 0.9175 - false_negatives: 46.3737 - false_positives: 91.1919 - loss: 0.2733 - precision: 0.9017 - recall: 0.9440 - true_negatives: 680.0101 - true_positives: 782.4243\n",
      "For batch number 100 the model has a loss of 0.2884426414966583\n",
      "\u001b[1m100/689\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m34s\u001b[0m 58ms/step - auc: 0.9564 - binary_accuracy: 0.9175 - false_negatives: 46.7500 - false_positives: 92.2700 - loss: 0.2735 - precision: 0.9015 - recall: 0.9440 - true_negatives: 686.8600 - true_positives: 790.1200\n",
      "For batch number 101 the model has a loss of 0.2875244915485382\n",
      "\u001b[1m101/689\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m33s\u001b[0m 58ms/step - auc: 0.9563 - binary_accuracy: 0.9174 - false_negatives: 47.1188 - false_positives: 93.3366 - loss: 0.2736 - precision: 0.9014 - recall: 0.9441 - true_negatives: 693.6832 - true_positives: 797.8614\n",
      "For batch number 102 the model has a loss of 0.28608036041259766\n",
      "\u001b[1m102/689\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m33s\u001b[0m 58ms/step - auc: 0.9563 - binary_accuracy: 0.9174 - false_negatives: 47.4804 - false_positives: 94.3922 - loss: 0.2737 - precision: 0.9013 - recall: 0.9441 - true_negatives: 700.5490 - true_positives: 805.5784\n",
      "For batch number 103 the model has a loss of 0.28732335567474365\n",
      "\u001b[1m103/689\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m33s\u001b[0m 58ms/step - auc: 0.9563 - binary_accuracy: 0.9173 - false_negatives: 47.8447 - false_positives: 95.4757 - loss: 0.2739 - precision: 0.9011 - recall: 0.9442 - true_negatives: 707.4272 - true_positives: 813.2524\n",
      "For batch number 104 the model has a loss of 0.28764015436172485\n",
      "\u001b[1m104/689\u001b[0m \u001b[32m━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m33s\u001b[0m 58ms/step - auc: 0.9563 - binary_accuracy: 0.9173 - false_negatives: 48.2019 - false_positives: 96.5865 - loss: 0.2740 - precision: 0.9010 - recall: 0.9442 - true_negatives: 714.3077 - true_positives: 820.9039\n",
      "For batch number 105 the model has a loss of 0.28792494535446167\n",
      "\u001b[1m105/689\u001b[0m \u001b[32m━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m33s\u001b[0m 58ms/step - auc: 0.9562 - binary_accuracy: 0.9172 - false_negatives: 48.5524 - false_positives: 97.6952 - loss: 0.2741 - precision: 0.9008 - recall: 0.9443 - true_negatives: 721.1810 - true_positives: 828.5714\n",
      "For batch number 106 the model has a loss of 0.2874907851219177\n",
      "\u001b[1m106/689\u001b[0m \u001b[32m━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m33s\u001b[0m 58ms/step - auc: 0.9562 - binary_accuracy: 0.9171 - false_negatives: 48.9057 - false_positives: 98.7924 - loss: 0.2743 - precision: 0.9006 - recall: 0.9443 - true_negatives: 728.0377 - true_positives: 836.2642\n",
      "For batch number 107 the model has a loss of 0.28694966435432434\n",
      "\u001b[1m107/689\u001b[0m \u001b[32m━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m33s\u001b[0m 58ms/step - auc: 0.9562 - binary_accuracy: 0.9171 - false_negatives: 49.2523 - false_positives: 99.8972 - loss: 0.2744 - precision: 0.9005 - recall: 0.9444 - true_negatives: 734.9346 - true_positives: 843.9159\n",
      "For batch number 108 the model has a loss of 0.2849993407726288\n",
      "\u001b[1m108/689\u001b[0m \u001b[32m━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m33s\u001b[0m 58ms/step - auc: 0.9562 - binary_accuracy: 0.9170 - false_negatives: 49.5926 - false_positives: 100.9815 - loss: 0.2745 - precision: 0.9003 - recall: 0.9445 - true_negatives: 741.8611 - true_positives: 851.5648\n",
      "For batch number 109 the model has a loss of 0.28623974323272705\n",
      "\u001b[1m109/689\u001b[0m \u001b[32m━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m33s\u001b[0m 58ms/step - auc: 0.9562 - binary_accuracy: 0.9170 - false_negatives: 49.9358 - false_positives: 102.0459 - loss: 0.2746 - precision: 0.9002 - recall: 0.9445 - true_negatives: 748.7706 - true_positives: 859.2477\n",
      "For batch number 110 the model has a loss of 0.2881915271282196\n",
      "\u001b[1m110/689\u001b[0m \u001b[32m━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m33s\u001b[0m 58ms/step - auc: 0.9561 - binary_accuracy: 0.9170 - false_negatives: 50.3000 - false_positives: 103.1091 - loss: 0.2747 - precision: 0.9001 - recall: 0.9446 - true_negatives: 755.6909 - true_positives: 866.9000\n",
      "For batch number 111 the model has a loss of 0.2883431315422058\n",
      "\u001b[1m111/689\u001b[0m \u001b[32m━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m33s\u001b[0m 58ms/step - auc: 0.9561 - binary_accuracy: 0.9169 - false_negatives: 50.6667 - false_positives: 104.1622 - loss: 0.2748 - precision: 0.9000 - recall: 0.9446 - true_negatives: 762.6396 - true_positives: 874.5316\n",
      "For batch number 112 the model has a loss of 0.2881506085395813\n",
      "\u001b[1m112/689\u001b[0m \u001b[32m━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m33s\u001b[0m 58ms/step - auc: 0.9561 - binary_accuracy: 0.9169 - false_negatives: 51.0357 - false_positives: 105.2232 - loss: 0.2749 - precision: 0.8998 - recall: 0.9446 - true_negatives: 769.6071 - true_positives: 882.1339\n",
      "For batch number 113 the model has a loss of 0.2881990671157837\n",
      "\u001b[1m113/689\u001b[0m \u001b[32m━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m33s\u001b[0m 58ms/step - auc: 0.9561 - binary_accuracy: 0.9168 - false_negatives: 51.4071 - false_positives: 106.2743 - loss: 0.2751 - precision: 0.8997 - recall: 0.9447 - true_negatives: 776.5929 - true_positives: 889.7256\n",
      "For batch number 114 the model has a loss of 0.28733041882514954\n",
      "\u001b[1m114/689\u001b[0m \u001b[32m━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m33s\u001b[0m 58ms/step - auc: 0.9560 - binary_accuracy: 0.9168 - false_negatives: 51.7807 - false_positives: 107.3246 - loss: 0.2752 - precision: 0.8996 - recall: 0.9447 - true_negatives: 783.5702 - true_positives: 897.3246\n",
      "For batch number 115 the model has a loss of 0.2898448407649994\n",
      "\u001b[1m115/689\u001b[0m \u001b[32m━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m33s\u001b[0m 58ms/step - auc: 0.9560 - binary_accuracy: 0.9168 - false_negatives: 52.1739 - false_positives: 108.3652 - loss: 0.2753 - precision: 0.8995 - recall: 0.9448 - true_negatives: 790.5305 - true_positives: 904.9304\n",
      "For batch number 116 the model has a loss of 0.28913140296936035\n",
      "\u001b[1m116/689\u001b[0m \u001b[32m━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m33s\u001b[0m 58ms/step - auc: 0.9560 - binary_accuracy: 0.9167 - false_negatives: 52.5603 - false_positives: 109.4052 - loss: 0.2754 - precision: 0.8994 - recall: 0.9448 - true_negatives: 797.5000 - true_positives: 912.5345\n",
      "For batch number 117 the model has a loss of 0.288479208946228\n",
      "\u001b[1m117/689\u001b[0m \u001b[32m━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m32s\u001b[0m 58ms/step - auc: 0.9560 - binary_accuracy: 0.9167 - false_negatives: 52.9402 - false_positives: 110.4444 - loss: 0.2755 - precision: 0.8992 - recall: 0.9448 - true_negatives: 804.4615 - true_positives: 920.1539\n",
      "For batch number 118 the model has a loss of 0.2902319133281708\n",
      "\u001b[1m118/689\u001b[0m \u001b[32m━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m32s\u001b[0m 58ms/step - auc: 0.9559 - binary_accuracy: 0.9166 - false_negatives: 53.3305 - false_positives: 111.4746 - loss: 0.2757 - precision: 0.8991 - recall: 0.9449 - true_negatives: 811.4237 - true_positives: 927.7712\n",
      "For batch number 119 the model has a loss of 0.2909383177757263\n",
      "\u001b[1m119/689\u001b[0m \u001b[32m━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m32s\u001b[0m 58ms/step - auc: 0.9559 - binary_accuracy: 0.9166 - false_negatives: 53.7311 - false_positives: 112.5210 - loss: 0.2758 - precision: 0.8990 - recall: 0.9449 - true_negatives: 818.4117 - true_positives: 935.3361\n",
      "For batch number 120 the model has a loss of 0.2905258536338806\n",
      "\u001b[1m120/689\u001b[0m \u001b[32m━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m32s\u001b[0m 58ms/step - auc: 0.9559 - binary_accuracy: 0.9166 - false_negatives: 54.1250 - false_positives: 113.5833 - loss: 0.2759 - precision: 0.8989 - recall: 0.9449 - true_negatives: 825.4250 - true_positives: 942.8666\n",
      "For batch number 121 the model has a loss of 0.28999924659729004\n",
      "\u001b[1m121/689\u001b[0m \u001b[32m━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m32s\u001b[0m 58ms/step - auc: 0.9559 - binary_accuracy: 0.9165 - false_negatives: 54.5124 - false_positives: 114.6446 - loss: 0.2760 - precision: 0.8988 - recall: 0.9449 - true_negatives: 832.4297 - true_positives: 950.4132\n",
      "For batch number 122 the model has a loss of 0.29292944073677063\n",
      "\u001b[1m122/689\u001b[0m \u001b[32m━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m32s\u001b[0m 58ms/step - auc: 0.9558 - binary_accuracy: 0.9165 - false_negatives: 54.9426 - false_positives: 115.6967 - loss: 0.2762 - precision: 0.8987 - recall: 0.9449 - true_negatives: 839.4180 - true_positives: 957.9426\n",
      "For batch number 123 the model has a loss of 0.293866366147995\n",
      "\u001b[1m123/689\u001b[0m \u001b[32m━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m32s\u001b[0m 58ms/step - auc: 0.9558 - binary_accuracy: 0.9164 - false_negatives: 55.3659 - false_positives: 116.7805 - loss: 0.2763 - precision: 0.8986 - recall: 0.9449 - true_negatives: 846.4471 - true_positives: 965.4065\n",
      "For batch number 124 the model has a loss of 0.2924487590789795\n",
      "\u001b[1m124/689\u001b[0m \u001b[32m━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m32s\u001b[0m 58ms/step - auc: 0.9558 - binary_accuracy: 0.9164 - false_negatives: 55.7903 - false_positives: 117.8468 - loss: 0.2764 - precision: 0.8984 - recall: 0.9450 - true_negatives: 853.4919 - true_positives: 972.8710\n",
      "For batch number 125 the model has a loss of 0.2914165258407593\n",
      "\u001b[1m125/689\u001b[0m \u001b[32m━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m32s\u001b[0m 58ms/step - auc: 0.9558 - binary_accuracy: 0.9163 - false_negatives: 56.2080 - false_positives: 118.9040 - loss: 0.2766 - precision: 0.8983 - recall: 0.9450 - true_negatives: 860.5600 - true_positives: 980.3280\n",
      "For batch number 126 the model has a loss of 0.29004111886024475\n",
      "\u001b[1m126/689\u001b[0m \u001b[32m━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m32s\u001b[0m 58ms/step - auc: 0.9557 - binary_accuracy: 0.9163 - false_negatives: 56.6190 - false_positives: 119.9524 - loss: 0.2767 - precision: 0.8982 - recall: 0.9450 - true_negatives: 867.6349 - true_positives: 987.7936\n",
      "For batch number 127 the model has a loss of 0.289978563785553\n",
      "\u001b[1m127/689\u001b[0m \u001b[32m━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m32s\u001b[0m 58ms/step - auc: 0.9557 - binary_accuracy: 0.9162 - false_negatives: 57.0315 - false_positives: 121.0000 - loss: 0.2768 - precision: 0.8981 - recall: 0.9450 - true_negatives: 874.7087 - true_positives: 995.2598\n",
      "For batch number 128 the model has a loss of 0.28874140977859497\n",
      "\u001b[1m128/689\u001b[0m \u001b[32m━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m32s\u001b[0m 58ms/step - auc: 0.9557 - binary_accuracy: 0.9162 - false_negatives: 57.4453 - false_positives: 122.0312 - loss: 0.2769 - precision: 0.8980 - recall: 0.9450 - true_negatives: 881.8047 - true_positives: 1002.7188\n",
      "For batch number 129 the model has a loss of 0.28804656863212585\n",
      "\u001b[1m129/689\u001b[0m \u001b[32m━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m32s\u001b[0m 58ms/step - auc: 0.9557 - binary_accuracy: 0.9162 - false_negatives: 57.8605 - false_positives: 123.0543 - loss: 0.2769 - precision: 0.8979 - recall: 0.9450 - true_negatives: 888.9070 - true_positives: 1010.1783\n",
      "For batch number 130 the model has a loss of 0.2873740792274475\n",
      "\u001b[1m130/689\u001b[0m \u001b[32m━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m32s\u001b[0m 58ms/step - auc: 0.9557 - binary_accuracy: 0.9161 - false_negatives: 58.2846 - false_positives: 124.0615 - loss: 0.2770 - precision: 0.8978 - recall: 0.9450 - true_negatives: 895.9923 - true_positives: 1017.6616\n",
      "For batch number 131 the model has a loss of 0.2860754132270813\n",
      "\u001b[1m131/689\u001b[0m \u001b[32m━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m32s\u001b[0m 58ms/step - auc: 0.9557 - binary_accuracy: 0.9161 - false_negatives: 58.7023 - false_positives: 125.0534 - loss: 0.2771 - precision: 0.8977 - recall: 0.9450 - true_negatives: 903.0992 - true_positives: 1025.1450\n",
      "For batch number 132 the model has a loss of 0.285011351108551\n",
      "\u001b[1m132/689\u001b[0m \u001b[32m━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m32s\u001b[0m 58ms/step - auc: 0.9556 - binary_accuracy: 0.9161 - false_negatives: 59.1136 - false_positives: 126.0303 - loss: 0.2772 - precision: 0.8977 - recall: 0.9451 - true_negatives: 910.2424 - true_positives: 1032.6136\n",
      "For batch number 133 the model has a loss of 0.2839384078979492\n",
      "\u001b[1m133/689\u001b[0m \u001b[32m━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m32s\u001b[0m 58ms/step - auc: 0.9556 - binary_accuracy: 0.9160 - false_negatives: 59.5188 - false_positives: 127.0000 - loss: 0.2772 - precision: 0.8976 - recall: 0.9451 - true_negatives: 917.3985 - true_positives: 1040.0828\n",
      "For batch number 134 the model has a loss of 0.2826983332633972\n",
      "\u001b[1m134/689\u001b[0m \u001b[32m━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m31s\u001b[0m 58ms/step - auc: 0.9556 - binary_accuracy: 0.9160 - false_negatives: 59.9179 - false_positives: 127.9552 - loss: 0.2773 - precision: 0.8975 - recall: 0.9451 - true_negatives: 924.5821 - true_positives: 1047.5448\n",
      "For batch number 135 the model has a loss of 0.28254926204681396\n",
      "\u001b[1m135/689\u001b[0m \u001b[32m━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m31s\u001b[0m 58ms/step - auc: 0.9556 - binary_accuracy: 0.9160 - false_negatives: 60.3185 - false_positives: 128.9037 - loss: 0.2773 - precision: 0.8975 - recall: 0.9451 - true_negatives: 931.7482 - true_positives: 1055.0297\n",
      "For batch number 136 the model has a loss of 0.28186333179473877\n",
      "\u001b[1m136/689\u001b[0m \u001b[32m━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m31s\u001b[0m 58ms/step - auc: 0.9556 - binary_accuracy: 0.9160 - false_negatives: 60.7206 - false_positives: 129.8382 - loss: 0.2773 - precision: 0.8974 - recall: 0.9451 - true_negatives: 938.9265 - true_positives: 1062.5146\n",
      "For batch number 137 the model has a loss of 0.28322580456733704\n",
      "\u001b[1m137/689\u001b[0m \u001b[32m━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m31s\u001b[0m 58ms/step - auc: 0.9556 - binary_accuracy: 0.9160 - false_negatives: 61.1314 - false_positives: 130.7591 - loss: 0.2774 - precision: 0.8974 - recall: 0.9451 - true_negatives: 946.0803 - true_positives: 1070.0292\n",
      "For batch number 138 the model has a loss of 0.28239867091178894\n",
      "\u001b[1m138/689\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m31s\u001b[0m 58ms/step - auc: 0.9556 - binary_accuracy: 0.9160 - false_negatives: 61.5435 - false_positives: 131.6667 - loss: 0.2774 - precision: 0.8973 - recall: 0.9451 - true_negatives: 953.2391 - true_positives: 1077.5508\n",
      "For batch number 139 the model has a loss of 0.2823463976383209\n",
      "\u001b[1m139/689\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m31s\u001b[0m 58ms/step - auc: 0.9556 - binary_accuracy: 0.9160 - false_negatives: 61.9496 - false_positives: 132.5827 - loss: 0.2774 - precision: 0.8973 - recall: 0.9452 - true_negatives: 960.3813 - true_positives: 1085.0863\n",
      "For batch number 140 the model has a loss of 0.28354933857917786\n",
      "\u001b[1m140/689\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m31s\u001b[0m 58ms/step - auc: 0.9556 - binary_accuracy: 0.9160 - false_negatives: 62.3643 - false_positives: 133.5000 - loss: 0.2775 - precision: 0.8972 - recall: 0.9452 - true_negatives: 967.5214 - true_positives: 1092.6143\n",
      "For batch number 141 the model has a loss of 0.28461265563964844\n",
      "\u001b[1m141/689\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m31s\u001b[0m 58ms/step - auc: 0.9556 - binary_accuracy: 0.9160 - false_negatives: 62.7872 - false_positives: 134.4326 - loss: 0.2775 - precision: 0.8972 - recall: 0.9452 - true_negatives: 974.6738 - true_positives: 1100.1063\n",
      "For batch number 142 the model has a loss of 0.2835676372051239\n",
      "\u001b[1m142/689\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m31s\u001b[0m 58ms/step - auc: 0.9556 - binary_accuracy: 0.9160 - false_negatives: 63.2113 - false_positives: 135.3592 - loss: 0.2776 - precision: 0.8971 - recall: 0.9452 - true_negatives: 981.8239 - true_positives: 1107.6056\n",
      "For batch number 143 the model has a loss of 0.2836081087589264\n",
      "\u001b[1m143/689\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m31s\u001b[0m 58ms/step - auc: 0.9556 - binary_accuracy: 0.9160 - false_negatives: 63.6294 - false_positives: 136.2937 - loss: 0.2776 - precision: 0.8971 - recall: 0.9452 - true_negatives: 988.9650 - true_positives: 1115.1119\n",
      "For batch number 144 the model has a loss of 0.2829325795173645\n",
      "\u001b[1m144/689\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m31s\u001b[0m 58ms/step - auc: 0.9556 - binary_accuracy: 0.9159 - false_negatives: 64.0417 - false_positives: 137.2292 - loss: 0.2777 - precision: 0.8970 - recall: 0.9452 - true_negatives: 996.0972 - true_positives: 1122.6320\n",
      "For batch number 145 the model has a loss of 0.28381088376045227\n",
      "\u001b[1m145/689\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m31s\u001b[0m 58ms/step - auc: 0.9556 - binary_accuracy: 0.9159 - false_negatives: 64.4483 - false_positives: 138.1931 - loss: 0.2777 - precision: 0.8970 - recall: 0.9452 - true_negatives: 1003.2138 - true_positives: 1130.1448\n",
      "For batch number 146 the model has a loss of 0.28400561213493347\n",
      "\u001b[1m146/689\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m31s\u001b[0m 58ms/step - auc: 0.9556 - binary_accuracy: 0.9159 - false_negatives: 64.8493 - false_positives: 139.1575 - loss: 0.2777 - precision: 0.8969 - recall: 0.9452 - true_negatives: 1010.3082 - true_positives: 1137.6849\n",
      "For batch number 147 the model has a loss of 0.2835911214351654\n",
      "\u001b[1m147/689\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m31s\u001b[0m 58ms/step - auc: 0.9556 - binary_accuracy: 0.9159 - false_negatives: 65.2517 - false_positives: 140.1156 - loss: 0.2778 - precision: 0.8969 - recall: 0.9453 - true_negatives: 1017.4218 - true_positives: 1145.2109\n",
      "For batch number 148 the model has a loss of 0.28411245346069336\n",
      "\u001b[1m148/689\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m31s\u001b[0m 58ms/step - auc: 0.9556 - binary_accuracy: 0.9159 - false_negatives: 65.6554 - false_positives: 141.0743 - loss: 0.2778 - precision: 0.8968 - recall: 0.9453 - true_negatives: 1024.5270 - true_positives: 1152.7433\n",
      "For batch number 149 the model has a loss of 0.2835080325603485\n",
      "\u001b[1m149/689\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m31s\u001b[0m 58ms/step - auc: 0.9555 - binary_accuracy: 0.9159 - false_negatives: 66.0537 - false_positives: 142.0268 - loss: 0.2779 - precision: 0.8968 - recall: 0.9453 - true_negatives: 1031.6443 - true_positives: 1160.2751\n",
      "For batch number 150 the model has a loss of 0.282577246427536\n",
      "\u001b[1m150/689\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m31s\u001b[0m 58ms/step - auc: 0.9555 - binary_accuracy: 0.9159 - false_negatives: 66.4467 - false_positives: 142.9733 - loss: 0.2779 - precision: 0.8967 - recall: 0.9453 - true_negatives: 1038.7467 - true_positives: 1167.8334\n",
      "For batch number 151 the model has a loss of 0.28210771083831787\n",
      "\u001b[1m151/689\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m30s\u001b[0m 58ms/step - auc: 0.9555 - binary_accuracy: 0.9159 - false_negatives: 66.8411 - false_positives: 143.9139 - loss: 0.2779 - precision: 0.8967 - recall: 0.9453 - true_negatives: 1045.8345 - true_positives: 1175.4106\n",
      "For batch number 152 the model has a loss of 0.28099551796913147\n",
      "\u001b[1m152/689\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m30s\u001b[0m 58ms/step - auc: 0.9555 - binary_accuracy: 0.9159 - false_negatives: 67.2303 - false_positives: 144.8553 - loss: 0.2779 - precision: 0.8966 - recall: 0.9454 - true_negatives: 1052.9078 - true_positives: 1183.0066\n",
      "For batch number 153 the model has a loss of 0.2808037996292114\n",
      "\u001b[1m153/689\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m30s\u001b[0m 58ms/step - auc: 0.9555 - binary_accuracy: 0.9159 - false_negatives: 67.6144 - false_positives: 145.7974 - loss: 0.2780 - precision: 0.8966 - recall: 0.9454 - true_negatives: 1060.0066 - true_positives: 1190.5817\n",
      "For batch number 154 the model has a loss of 0.28015708923339844\n",
      "\u001b[1m154/689\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m30s\u001b[0m 58ms/step - auc: 0.9555 - binary_accuracy: 0.9159 - false_negatives: 67.9935 - false_positives: 146.7403 - loss: 0.2780 - precision: 0.8966 - recall: 0.9454 - true_negatives: 1067.1039 - true_positives: 1198.1624\n",
      "For batch number 155 the model has a loss of 0.2807137072086334\n",
      "\u001b[1m155/689\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m30s\u001b[0m 58ms/step - auc: 0.9555 - binary_accuracy: 0.9158 - false_negatives: 68.3806 - false_positives: 147.6839 - loss: 0.2780 - precision: 0.8965 - recall: 0.9454 - true_negatives: 1074.1936 - true_positives: 1205.7419\n",
      "For batch number 156 the model has a loss of 0.28024783730506897\n",
      "\u001b[1m156/689\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m30s\u001b[0m 58ms/step - auc: 0.9555 - binary_accuracy: 0.9158 - false_negatives: 68.7628 - false_positives: 148.6346 - loss: 0.2780 - precision: 0.8965 - recall: 0.9455 - true_negatives: 1081.2821 - true_positives: 1213.3206\n",
      "For batch number 157 the model has a loss of 0.2799362540245056\n",
      "\u001b[1m157/689\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m30s\u001b[0m 58ms/step - auc: 0.9555 - binary_accuracy: 0.9158 - false_negatives: 69.1401 - false_positives: 149.5860 - loss: 0.2780 - precision: 0.8964 - recall: 0.9455 - true_negatives: 1088.3694 - true_positives: 1220.9044\n",
      "For batch number 158 the model has a loss of 0.2799714207649231\n",
      "\u001b[1m158/689\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m30s\u001b[0m 58ms/step - auc: 0.9555 - binary_accuracy: 0.9158 - false_negatives: 69.5127 - false_positives: 150.5506 - loss: 0.2780 - precision: 0.8964 - recall: 0.9455 - true_negatives: 1095.4684 - true_positives: 1228.4684\n",
      "For batch number 159 the model has a loss of 0.27911704778671265\n",
      "\u001b[1m159/689\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m30s\u001b[0m 58ms/step - auc: 0.9555 - binary_accuracy: 0.9158 - false_negatives: 69.8805 - false_positives: 151.5031 - loss: 0.2780 - precision: 0.8963 - recall: 0.9455 - true_negatives: 1102.5471 - true_positives: 1236.0692\n",
      "For batch number 160 the model has a loss of 0.27809861302375793\n",
      "\u001b[1m160/689\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m30s\u001b[0m 58ms/step - auc: 0.9555 - binary_accuracy: 0.9158 - false_negatives: 70.2438 - false_positives: 152.4438 - loss: 0.2780 - precision: 0.8963 - recall: 0.9456 - true_negatives: 1109.6438 - true_positives: 1243.6687\n",
      "For batch number 161 the model has a loss of 0.2772732377052307\n",
      "\u001b[1m161/689\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m30s\u001b[0m 58ms/step - auc: 0.9555 - binary_accuracy: 0.9158 - false_negatives: 70.6025 - false_positives: 153.3789 - loss: 0.2780 - precision: 0.8963 - recall: 0.9456 - true_negatives: 1116.7578 - true_positives: 1251.2609\n",
      "For batch number 162 the model has a loss of 0.27833855152130127\n",
      "\u001b[1m162/689\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m30s\u001b[0m 58ms/step - auc: 0.9555 - binary_accuracy: 0.9158 - false_negatives: 70.9568 - false_positives: 154.3210 - loss: 0.2780 - precision: 0.8962 - recall: 0.9456 - true_negatives: 1123.8643 - true_positives: 1258.8580\n",
      "For batch number 163 the model has a loss of 0.2790102958679199\n",
      "\u001b[1m163/689\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m30s\u001b[0m 58ms/step - auc: 0.9555 - binary_accuracy: 0.9158 - false_negatives: 71.3190 - false_positives: 155.2577 - loss: 0.2780 - precision: 0.8962 - recall: 0.9457 - true_negatives: 1130.9570 - true_positives: 1266.4663\n",
      "For batch number 164 the model has a loss of 0.28110313415527344\n",
      "\u001b[1m164/689\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m30s\u001b[0m 58ms/step - auc: 0.9555 - binary_accuracy: 0.9158 - false_negatives: 71.6951 - false_positives: 156.1890 - loss: 0.2781 - precision: 0.8962 - recall: 0.9457 - true_negatives: 1138.0183 - true_positives: 1274.0975\n",
      "For batch number 165 the model has a loss of 0.28172409534454346\n",
      "\u001b[1m165/689\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m30s\u001b[0m 58ms/step - auc: 0.9555 - binary_accuracy: 0.9158 - false_negatives: 72.0788 - false_positives: 157.1333 - loss: 0.2781 - precision: 0.8961 - recall: 0.9457 - true_negatives: 1145.0848 - true_positives: 1281.7030\n",
      "For batch number 166 the model has a loss of 0.2822065055370331\n",
      "\u001b[1m166/689\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m30s\u001b[0m 58ms/step - auc: 0.9554 - binary_accuracy: 0.9158 - false_negatives: 72.4639 - false_positives: 158.0843 - loss: 0.2781 - precision: 0.8961 - recall: 0.9457 - true_negatives: 1152.1566 - true_positives: 1289.2952\n",
      "For batch number 167 the model has a loss of 0.2839891314506531\n",
      "\u001b[1m167/689\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m30s\u001b[0m 58ms/step - auc: 0.9554 - binary_accuracy: 0.9158 - false_negatives: 72.8683 - false_positives: 159.0419 - loss: 0.2781 - precision: 0.8960 - recall: 0.9457 - true_negatives: 1159.2275 - true_positives: 1296.8623\n",
      "For batch number 168 the model has a loss of 0.2835713326931\n",
      "\u001b[1m168/689\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m29s\u001b[0m 58ms/step - auc: 0.9554 - binary_accuracy: 0.9158 - false_negatives: 73.2679 - false_positives: 160.0000 - loss: 0.2782 - precision: 0.8960 - recall: 0.9458 - true_negatives: 1166.3154 - true_positives: 1304.4166\n",
      "For batch number 169 the model has a loss of 0.2825409173965454\n",
      "\u001b[1m169/689\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m29s\u001b[0m 58ms/step - auc: 0.9554 - binary_accuracy: 0.9158 - false_negatives: 73.6627 - false_positives: 160.9527 - loss: 0.2782 - precision: 0.8960 - recall: 0.9458 - true_negatives: 1173.3965 - true_positives: 1311.9882\n",
      "For batch number 170 the model has a loss of 0.28182610869407654\n",
      "\u001b[1m170/689\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m29s\u001b[0m 58ms/step - auc: 0.9554 - binary_accuracy: 0.9158 - false_negatives: 74.0588 - false_positives: 161.9000 - loss: 0.2782 - precision: 0.8959 - recall: 0.9458 - true_negatives: 1180.4764 - true_positives: 1319.5647\n",
      "For batch number 171 the model has a loss of 0.2815278172492981\n",
      "\u001b[1m171/689\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m29s\u001b[0m 58ms/step - auc: 0.9554 - binary_accuracy: 0.9158 - false_negatives: 74.4561 - false_positives: 162.8479 - loss: 0.2782 - precision: 0.8959 - recall: 0.9458 - true_negatives: 1187.5614 - true_positives: 1327.1345\n",
      "For batch number 172 the model has a loss of 0.2809184193611145\n",
      "\u001b[1m172/689\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m29s\u001b[0m 58ms/step - auc: 0.9554 - binary_accuracy: 0.9158 - false_negatives: 74.8547 - false_positives: 163.7907 - loss: 0.2783 - precision: 0.8959 - recall: 0.9458 - true_negatives: 1194.6337 - true_positives: 1334.7209\n",
      "For batch number 173 the model has a loss of 0.2807248830795288\n",
      "\u001b[1m173/689\u001b[0m \u001b[32m━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━\u001b[0m \u001b[1m29s\u001b[0m 58ms/step - auc: 0.9554 - binary_accuracy: 0.9158 - false_negatives: 75.2543 - false_positives: 164.7399 - loss: 0.2783 - precision: 0.8958 - recall: 0.9458 - true_negatives: 1201.7168 - true_positives: 1342.2891\n",
      "For batch number 174 the model has a loss of 0.280563086271286\n",
      "\u001b[1m174/689\u001b[0m \u001b[32m━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━\u001b[0m \u001b[1m29s\u001b[0m 58ms/step - auc: 0.9554 - binary_accuracy: 0.9158 - false_negatives: 75.6552 - false_positives: 165.6839 - loss: 0.2783 - precision: 0.8958 - recall: 0.9459 - true_negatives: 1208.7988 - true_positives: 1349.8621\n",
      "For batch number 175 the model has a loss of 0.2810453176498413\n",
      "\u001b[1m175/689\u001b[0m \u001b[32m━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━\u001b[0m \u001b[1m29s\u001b[0m 58ms/step - auc: 0.9554 - binary_accuracy: 0.9158 - false_negatives: 76.0514 - false_positives: 166.6457 - loss: 0.2783 - precision: 0.8957 - recall: 0.9459 - true_negatives: 1215.8857 - true_positives: 1357.4171\n",
      "For batch number 176 the model has a loss of 0.28180593252182007\n",
      "\u001b[1m176/689\u001b[0m \u001b[32m━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━\u001b[0m \u001b[1m29s\u001b[0m 58ms/step - auc: 0.9554 - binary_accuracy: 0.9158 - false_negatives: 76.4545 - false_positives: 167.6023 - loss: 0.2783 - precision: 0.8957 - recall: 0.9459 - true_negatives: 1222.9829 - true_positives: 1364.9602\n",
      "For batch number 177 the model has a loss of 0.2811536192893982\n",
      "\u001b[1m177/689\u001b[0m \u001b[32m━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━\u001b[0m \u001b[1m29s\u001b[0m 58ms/step - auc: 0.9554 - binary_accuracy: 0.9158 - false_negatives: 76.8531 - false_positives: 168.5593 - loss: 0.2783 - precision: 0.8957 - recall: 0.9459 - true_negatives: 1230.0847 - true_positives: 1372.5028\n",
      "For batch number 178 the model has a loss of 0.28093212842941284\n",
      "\u001b[1m178/689\u001b[0m \u001b[32m━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━\u001b[0m \u001b[1m29s\u001b[0m 58ms/step - auc: 0.9554 - binary_accuracy: 0.9158 - false_negatives: 77.2584 - false_positives: 169.5225 - loss: 0.2783 - precision: 0.8956 - recall: 0.9459 - true_negatives: 1237.1910 - true_positives: 1380.0281\n",
      "For batch number 179 the model has a loss of 0.2802215814590454\n",
      "\u001b[1m179/689\u001b[0m \u001b[32m━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━\u001b[0m \u001b[1m29s\u001b[0m 58ms/step - auc: 0.9554 - binary_accuracy: 0.9157 - false_negatives: 77.6648 - false_positives: 170.4805 - loss: 0.2784 - precision: 0.8956 - recall: 0.9459 - true_negatives: 1244.3016 - true_positives: 1387.5531\n",
      "For batch number 180 the model has a loss of 0.2799723148345947\n",
      "\u001b[1m180/689\u001b[0m \u001b[32m━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━\u001b[0m \u001b[1m29s\u001b[0m 58ms/step - auc: 0.9554 - binary_accuracy: 0.9157 - false_negatives: 78.0667 - false_positives: 171.4389 - loss: 0.2784 - precision: 0.8956 - recall: 0.9459 - true_negatives: 1251.4000 - true_positives: 1395.0945\n",
      "For batch number 181 the model has a loss of 0.2792637050151825\n",
      "\u001b[1m181/689\u001b[0m \u001b[32m━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━\u001b[0m \u001b[1m29s\u001b[0m 58ms/step - auc: 0.9554 - binary_accuracy: 0.9157 - false_negatives: 78.4641 - false_positives: 172.3978 - loss: 0.2784 - precision: 0.8955 - recall: 0.9459 - true_negatives: 1258.5028 - true_positives: 1402.6354\n",
      "For batch number 182 the model has a loss of 0.27871057391166687\n",
      "\u001b[1m182/689\u001b[0m \u001b[32m━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━\u001b[0m \u001b[1m29s\u001b[0m 58ms/step - auc: 0.9553 - binary_accuracy: 0.9157 - false_negatives: 78.8626 - false_positives: 173.3517 - loss: 0.2784 - precision: 0.8955 - recall: 0.9460 - true_negatives: 1265.6099 - true_positives: 1410.1758\n",
      "For batch number 183 the model has a loss of 0.2779964506626129\n",
      "\u001b[1m183/689\u001b[0m \u001b[32m━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━\u001b[0m \u001b[1m29s\u001b[0m 58ms/step - auc: 0.9553 - binary_accuracy: 0.9157 - false_negatives: 79.2568 - false_positives: 174.2951 - loss: 0.2784 - precision: 0.8955 - recall: 0.9460 - true_negatives: 1272.7377 - true_positives: 1417.7103\n",
      "For batch number 184 the model has a loss of 0.27735435962677\n",
      "\u001b[1m184/689\u001b[0m \u001b[32m━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━\u001b[0m \u001b[1m29s\u001b[0m 58ms/step - auc: 0.9553 - binary_accuracy: 0.9157 - false_negatives: 79.6467 - false_positives: 175.2283 - loss: 0.2784 - precision: 0.8954 - recall: 0.9460 - true_negatives: 1279.8750 - true_positives: 1425.2500\n",
      "For batch number 185 the model has a loss of 0.27684521675109863\n",
      "\u001b[1m185/689\u001b[0m \u001b[32m━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━\u001b[0m \u001b[1m28s\u001b[0m 58ms/step - auc: 0.9554 - binary_accuracy: 0.9157 - false_negatives: 80.0324 - false_positives: 176.1568 - loss: 0.2784 - precision: 0.8954 - recall: 0.9460 - true_negatives: 1287.0109 - true_positives: 1432.8000\n",
      "For batch number 186 the model has a loss of 0.27654093503952026\n",
      "\u001b[1m186/689\u001b[0m \u001b[32m━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━\u001b[0m \u001b[1m28s\u001b[0m 57ms/step - auc: 0.9554 - binary_accuracy: 0.9157 - false_negatives: 80.4140 - false_positives: 177.0914 - loss: 0.2784 - precision: 0.8954 - recall: 0.9460 - true_negatives: 1294.1559 - true_positives: 1440.3387\n",
      "For batch number 187 the model has a loss of 0.2765495479106903\n",
      "\u001b[1m187/689\u001b[0m \u001b[32m━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━\u001b[0m \u001b[1m28s\u001b[0m 57ms/step - auc: 0.9554 - binary_accuracy: 0.9157 - false_negatives: 80.7968 - false_positives: 178.0214 - loss: 0.2783 - precision: 0.8953 - recall: 0.9460 - true_negatives: 1301.2781 - true_positives: 1447.9037\n",
      "For batch number 188 the model has a loss of 0.2773321568965912\n",
      "\u001b[1m188/689\u001b[0m \u001b[32m━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━\u001b[0m \u001b[1m28s\u001b[0m 57ms/step - auc: 0.9554 - binary_accuracy: 0.9157 - false_negatives: 81.1809 - false_positives: 178.9415 - loss: 0.2783 - precision: 0.8953 - recall: 0.9461 - true_negatives: 1308.3989 - true_positives: 1455.4788\n",
      "For batch number 189 the model has a loss of 0.2768554985523224\n",
      "\u001b[1m189/689\u001b[0m \u001b[32m━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━\u001b[0m \u001b[1m28s\u001b[0m 57ms/step - auc: 0.9554 - binary_accuracy: 0.9157 - false_negatives: 81.5608 - false_positives: 179.8571 - loss: 0.2783 - precision: 0.8953 - recall: 0.9461 - true_negatives: 1315.5027 - true_positives: 1463.0793\n",
      "For batch number 190 the model has a loss of 0.27584418654441833\n",
      "\u001b[1m190/689\u001b[0m \u001b[32m━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━\u001b[0m \u001b[1m28s\u001b[0m 58ms/step - auc: 0.9554 - binary_accuracy: 0.9157 - false_negatives: 81.9368 - false_positives: 180.7632 - loss: 0.2783 - precision: 0.8953 - recall: 0.9461 - true_negatives: 1322.6158 - true_positives: 1470.6842\n",
      "For batch number 191 the model has a loss of 0.2753642499446869\n",
      "\u001b[1m191/689\u001b[0m \u001b[32m━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━\u001b[0m \u001b[1m28s\u001b[0m 58ms/step - auc: 0.9554 - binary_accuracy: 0.9157 - false_negatives: 82.3141 - false_positives: 181.6649 - loss: 0.2783 - precision: 0.8953 - recall: 0.9461 - true_negatives: 1329.7225 - true_positives: 1478.2985\n",
      "For batch number 192 the model has a loss of 0.2756982147693634\n",
      "\u001b[1m192/689\u001b[0m \u001b[32m━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━\u001b[0m \u001b[1m28s\u001b[0m 58ms/step - auc: 0.9554 - binary_accuracy: 0.9158 - false_negatives: 82.6927 - false_positives: 182.5573 - loss: 0.2783 - precision: 0.8953 - recall: 0.9461 - true_negatives: 1336.8385 - true_positives: 1485.9115\n",
      "For batch number 193 the model has a loss of 0.2758805751800537\n",
      "\u001b[1m193/689\u001b[0m \u001b[32m━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━\u001b[0m \u001b[1m28s\u001b[0m 58ms/step - auc: 0.9554 - binary_accuracy: 0.9158 - false_negatives: 83.0777 - false_positives: 183.4404 - loss: 0.2783 - precision: 0.8952 - recall: 0.9462 - true_negatives: 1343.9379 - true_positives: 1493.5441\n",
      "For batch number 194 the model has a loss of 0.2757316529750824\n",
      "\u001b[1m194/689\u001b[0m \u001b[32m━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━\u001b[0m \u001b[1m28s\u001b[0m 57ms/step - auc: 0.9554 - binary_accuracy: 0.9158 - false_negatives: 83.4588 - false_positives: 184.3402 - loss: 0.2783 - precision: 0.8952 - recall: 0.9462 - true_negatives: 1351.0464 - true_positives: 1501.1547\n",
      "For batch number 195 the model has a loss of 0.27658572793006897\n",
      "\u001b[1m195/689\u001b[0m \u001b[32m━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━\u001b[0m \u001b[1m28s\u001b[0m 57ms/step - auc: 0.9554 - binary_accuracy: 0.9158 - false_negatives: 83.8462 - false_positives: 185.2410 - loss: 0.2783 - precision: 0.8952 - recall: 0.9462 - true_negatives: 1358.1436 - true_positives: 1508.7693\n",
      "For batch number 196 the model has a loss of 0.2766675353050232\n",
      "\u001b[1m196/689\u001b[0m \u001b[32m━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━\u001b[0m \u001b[1m28s\u001b[0m 57ms/step - auc: 0.9554 - binary_accuracy: 0.9158 - false_negatives: 84.2347 - false_positives: 186.1327 - loss: 0.2782 - precision: 0.8952 - recall: 0.9462 - true_negatives: 1365.2397 - true_positives: 1516.3928\n",
      "For batch number 197 the model has a loss of 0.2773858606815338\n",
      "\u001b[1m197/689\u001b[0m \u001b[32m━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━\u001b[0m \u001b[1m28s\u001b[0m 58ms/step - auc: 0.9554 - binary_accuracy: 0.9158 - false_negatives: 84.6244 - false_positives: 187.0254 - loss: 0.2782 - precision: 0.8952 - recall: 0.9462 - true_negatives: 1372.3147 - true_positives: 1524.0355\n",
      "For batch number 198 the model has a loss of 0.2785911560058594\n",
      "\u001b[1m198/689\u001b[0m \u001b[32m━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━\u001b[0m \u001b[1m28s\u001b[0m 57ms/step - auc: 0.9554 - binary_accuracy: 0.9158 - false_negatives: 85.0202 - false_positives: 187.9192 - loss: 0.2782 - precision: 0.8952 - recall: 0.9462 - true_negatives: 1379.3939 - true_positives: 1531.6666\n",
      "For batch number 199 the model has a loss of 0.27947157621383667\n",
      "\u001b[1m199/689\u001b[0m \u001b[32m━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━\u001b[0m \u001b[1m28s\u001b[0m 57ms/step - auc: 0.9554 - binary_accuracy: 0.9158 - false_negatives: 85.4171 - false_positives: 188.8241 - loss: 0.2782 - precision: 0.8952 - recall: 0.9463 - true_negatives: 1386.4523 - true_positives: 1539.3065\n",
      "For batch number 200 the model has a loss of 0.2788042724132538\n",
      "\u001b[1m200/689\u001b[0m \u001b[32m━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━\u001b[0m \u001b[1m28s\u001b[0m 57ms/step - auc: 0.9554 - binary_accuracy: 0.9158 - false_negatives: 85.8100 - false_positives: 189.7250 - loss: 0.2782 - precision: 0.8952 - recall: 0.9463 - true_negatives: 1393.5200 - true_positives: 1546.9449\n",
      "For batch number 201 the model has a loss of 0.27810657024383545\n",
      "\u001b[1m201/689\u001b[0m \u001b[32m━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━\u001b[0m \u001b[1m28s\u001b[0m 57ms/step - auc: 0.9554 - binary_accuracy: 0.9158 - false_negatives: 86.1990 - false_positives: 190.6219 - loss: 0.2782 - precision: 0.8951 - recall: 0.9463 - true_negatives: 1400.6119 - true_positives: 1554.5671\n",
      "For batch number 202 the model has a loss of 0.27727800607681274\n",
      "\u001b[1m202/689\u001b[0m \u001b[32m━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━\u001b[0m \u001b[1m27s\u001b[0m 57ms/step - auc: 0.9554 - binary_accuracy: 0.9158 - false_negatives: 86.5842 - false_positives: 191.5148 - loss: 0.2782 - precision: 0.8951 - recall: 0.9463 - true_negatives: 1407.6980 - true_positives: 1562.2030\n",
      "For batch number 203 the model has a loss of 0.27680858969688416\n",
      "\u001b[1m203/689\u001b[0m \u001b[32m━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━\u001b[0m \u001b[1m27s\u001b[0m 57ms/step - auc: 0.9554 - binary_accuracy: 0.9158 - false_negatives: 86.9754 - false_positives: 192.3990 - loss: 0.2782 - precision: 0.8951 - recall: 0.9463 - true_negatives: 1414.7882 - true_positives: 1569.8374\n",
      "For batch number 204 the model has a loss of 0.27600952982902527\n",
      "\u001b[1m204/689\u001b[0m \u001b[32m━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━\u001b[0m \u001b[1m27s\u001b[0m 57ms/step - auc: 0.9554 - binary_accuracy: 0.9158 - false_negatives: 87.3627 - false_positives: 193.2794 - loss: 0.2782 - precision: 0.8951 - recall: 0.9463 - true_negatives: 1421.8677 - true_positives: 1577.4902\n",
      "For batch number 205 the model has a loss of 0.2754752039909363\n",
      "\u001b[1m205/689\u001b[0m \u001b[32m━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━\u001b[0m \u001b[1m27s\u001b[0m 57ms/step - auc: 0.9554 - binary_accuracy: 0.9158 - false_negatives: 87.7463 - false_positives: 194.1610 - loss: 0.2782 - precision: 0.8951 - recall: 0.9463 - true_negatives: 1428.9512 - true_positives: 1585.1415\n",
      "For batch number 206 the model has a loss of 0.2753874659538269\n",
      "\u001b[1m206/689\u001b[0m \u001b[32m━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━\u001b[0m \u001b[1m27s\u001b[0m 57ms/step - auc: 0.9554 - binary_accuracy: 0.9158 - false_negatives: 88.1311 - false_positives: 195.0388 - loss: 0.2782 - precision: 0.8951 - recall: 0.9464 - true_negatives: 1436.0339 - true_positives: 1592.7961\n",
      "For batch number 207 the model has a loss of 0.2755192518234253\n",
      "\u001b[1m207/689\u001b[0m \u001b[32m━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━\u001b[0m \u001b[1m27s\u001b[0m 57ms/step - auc: 0.9554 - binary_accuracy: 0.9159 - false_negatives: 88.5169 - false_positives: 195.9227 - loss: 0.2782 - precision: 0.8951 - recall: 0.9464 - true_negatives: 1443.1256 - true_positives: 1600.4348\n",
      "For batch number 208 the model has a loss of 0.2762395143508911\n",
      "\u001b[1m208/689\u001b[0m \u001b[32m━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━\u001b[0m \u001b[1m27s\u001b[0m 57ms/step - auc: 0.9554 - binary_accuracy: 0.9159 - false_negatives: 88.8990 - false_positives: 196.8173 - loss: 0.2782 - precision: 0.8951 - recall: 0.9464 - true_negatives: 1450.2356 - true_positives: 1608.0481\n",
      "For batch number 209 the model has a loss of 0.27642112970352173\n",
      "\u001b[1m209/689\u001b[0m \u001b[32m━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━\u001b[0m \u001b[1m27s\u001b[0m 57ms/step - auc: 0.9554 - binary_accuracy: 0.9159 - false_negatives: 89.2823 - false_positives: 197.7177 - loss: 0.2782 - precision: 0.8951 - recall: 0.9464 - true_negatives: 1457.3445 - true_positives: 1615.6555\n",
      "For batch number 210 the model has a loss of 0.2777952551841736\n",
      "\u001b[1m210/689\u001b[0m \u001b[32m━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━\u001b[0m \u001b[1m27s\u001b[0m 57ms/step - auc: 0.9554 - binary_accuracy: 0.9159 - false_negatives: 89.6667 - false_positives: 198.6286 - loss: 0.2782 - precision: 0.8951 - recall: 0.9464 - true_negatives: 1464.4476 - true_positives: 1623.2572\n",
      "For batch number 211 the model has a loss of 0.27848780155181885\n",
      "\u001b[1m211/689\u001b[0m \u001b[32m━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━\u001b[0m \u001b[1m27s\u001b[0m 57ms/step - auc: 0.9554 - binary_accuracy: 0.9159 - false_negatives: 90.0521 - false_positives: 199.5403 - loss: 0.2782 - precision: 0.8950 - recall: 0.9464 - true_negatives: 1471.5403 - true_positives: 1630.8673\n",
      "For batch number 212 the model has a loss of 0.2789841592311859\n",
      "\u001b[1m212/689\u001b[0m \u001b[32m━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━\u001b[0m \u001b[1m27s\u001b[0m 57ms/step - auc: 0.9554 - binary_accuracy: 0.9159 - false_negatives: 90.4387 - false_positives: 200.4670 - loss: 0.2782 - precision: 0.8950 - recall: 0.9465 - true_negatives: 1478.6462 - true_positives: 1638.4481\n",
      "For batch number 213 the model has a loss of 0.2793435752391815\n",
      "\u001b[1m213/689\u001b[0m \u001b[32m━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━\u001b[0m \u001b[1m27s\u001b[0m 57ms/step - auc: 0.9554 - binary_accuracy: 0.9159 - false_negatives: 90.8263 - false_positives: 201.3944 - loss: 0.2782 - precision: 0.8950 - recall: 0.9465 - true_negatives: 1485.7699 - true_positives: 1646.0094\n",
      "For batch number 214 the model has a loss of 0.27955788373947144\n",
      "\u001b[1m214/689\u001b[0m \u001b[32m━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━\u001b[0m \u001b[1m27s\u001b[0m 57ms/step - auc: 0.9554 - binary_accuracy: 0.9159 - false_negatives: 91.2103 - false_positives: 202.3318 - loss: 0.2782 - precision: 0.8950 - recall: 0.9465 - true_negatives: 1492.8878 - true_positives: 1653.5701\n",
      "For batch number 215 the model has a loss of 0.2793503403663635\n",
      "\u001b[1m215/689\u001b[0m \u001b[32m━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━\u001b[0m \u001b[1m27s\u001b[0m 57ms/step - auc: 0.9554 - binary_accuracy: 0.9159 - false_negatives: 91.5954 - false_positives: 203.2651 - loss: 0.2782 - precision: 0.8950 - recall: 0.9465 - true_negatives: 1500.0139 - true_positives: 1661.1256\n",
      "For batch number 216 the model has a loss of 0.2801651060581207\n",
      "\u001b[1m216/689\u001b[0m \u001b[32m━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━\u001b[0m \u001b[1m27s\u001b[0m 57ms/step - auc: 0.9554 - binary_accuracy: 0.9159 - false_negatives: 91.9769 - false_positives: 204.1991 - loss: 0.2782 - precision: 0.8949 - recall: 0.9465 - true_negatives: 1507.1389 - true_positives: 1668.6852\n",
      "For batch number 217 the model has a loss of 0.2798076570034027\n",
      "\u001b[1m217/689\u001b[0m \u001b[32m━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━\u001b[0m \u001b[1m27s\u001b[0m 57ms/step - auc: 0.9554 - binary_accuracy: 0.9159 - false_negatives: 92.3594 - false_positives: 205.1290 - loss: 0.2782 - precision: 0.8949 - recall: 0.9465 - true_negatives: 1514.2765 - true_positives: 1676.2350\n",
      "For batch number 218 the model has a loss of 0.27908504009246826\n",
      "\u001b[1m218/689\u001b[0m \u001b[32m━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━\u001b[0m \u001b[1m27s\u001b[0m 57ms/step - auc: 0.9554 - binary_accuracy: 0.9159 - false_negatives: 92.7385 - false_positives: 206.0505 - loss: 0.2782 - precision: 0.8949 - recall: 0.9466 - true_negatives: 1521.4220 - true_positives: 1683.7889\n",
      "For batch number 219 the model has a loss of 0.27917176485061646\n",
      "\u001b[1m219/689\u001b[0m \u001b[32m━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━\u001b[0m \u001b[1m26s\u001b[0m 57ms/step - auc: 0.9554 - binary_accuracy: 0.9159 - false_negatives: 93.1187 - false_positives: 206.9772 - loss: 0.2782 - precision: 0.8949 - recall: 0.9466 - true_negatives: 1528.5616 - true_positives: 1691.3424\n",
      "For batch number 220 the model has a loss of 0.27915263175964355\n",
      "\u001b[1m220/689\u001b[0m \u001b[32m━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━\u001b[0m \u001b[1m26s\u001b[0m 57ms/step - auc: 0.9554 - binary_accuracy: 0.9159 - false_negatives: 93.5000 - false_positives: 207.9045 - loss: 0.2782 - precision: 0.8949 - recall: 0.9466 - true_negatives: 1535.6864 - true_positives: 1698.9091\n",
      "For batch number 221 the model has a loss of 0.27916428446769714\n",
      "\u001b[1m221/689\u001b[0m \u001b[32m━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━\u001b[0m \u001b[1m26s\u001b[0m 57ms/step - auc: 0.9554 - binary_accuracy: 0.9159 - false_negatives: 93.8869 - false_positives: 208.8235 - loss: 0.2782 - precision: 0.8949 - recall: 0.9466 - true_negatives: 1542.7919 - true_positives: 1706.4977\n",
      "For batch number 222 the model has a loss of 0.2786833345890045\n",
      "\u001b[1m222/689\u001b[0m \u001b[32m━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━\u001b[0m \u001b[1m26s\u001b[0m 57ms/step - auc: 0.9554 - binary_accuracy: 0.9159 - false_negatives: 94.2703 - false_positives: 209.7387 - loss: 0.2782 - precision: 0.8948 - recall: 0.9466 - true_negatives: 1549.9009 - true_positives: 1714.0901\n",
      "For batch number 223 the model has a loss of 0.2790759801864624\n",
      "\u001b[1m223/689\u001b[0m \u001b[32m━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━\u001b[0m \u001b[1m26s\u001b[0m 57ms/step - auc: 0.9554 - binary_accuracy: 0.9159 - false_negatives: 94.6547 - false_positives: 210.6592 - loss: 0.2782 - precision: 0.8948 - recall: 0.9466 - true_negatives: 1557.0089 - true_positives: 1721.6771\n",
      "For batch number 224 the model has a loss of 0.2798435091972351\n",
      "\u001b[1m224/689\u001b[0m \u001b[32m━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━\u001b[0m \u001b[1m26s\u001b[0m 57ms/step - auc: 0.9554 - binary_accuracy: 0.9159 - false_negatives: 95.0446 - false_positives: 211.5804 - loss: 0.2782 - precision: 0.8948 - recall: 0.9466 - true_negatives: 1564.1072 - true_positives: 1729.2678\n",
      "For batch number 225 the model has a loss of 0.2793572247028351\n",
      "\u001b[1m225/689\u001b[0m \u001b[32m━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━\u001b[0m \u001b[1m26s\u001b[0m 57ms/step - auc: 0.9554 - binary_accuracy: 0.9159 - false_negatives: 95.4444 - false_positives: 212.4933 - loss: 0.2782 - precision: 0.8948 - recall: 0.9466 - true_negatives: 1571.2000 - true_positives: 1736.8622\n",
      "For batch number 226 the model has a loss of 0.2789382040500641\n",
      "\u001b[1m226/689\u001b[0m \u001b[32m━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━\u001b[0m \u001b[1m26s\u001b[0m 57ms/step - auc: 0.9554 - binary_accuracy: 0.9159 - false_negatives: 95.8451 - false_positives: 213.4071 - loss: 0.2782 - precision: 0.8948 - recall: 0.9467 - true_negatives: 1578.2920 - true_positives: 1744.4558\n",
      "For batch number 227 the model has a loss of 0.2785018980503082\n",
      "\u001b[1m227/689\u001b[0m \u001b[32m━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━\u001b[0m \u001b[1m26s\u001b[0m 57ms/step - auc: 0.9554 - binary_accuracy: 0.9159 - false_negatives: 96.2423 - false_positives: 214.3216 - loss: 0.2782 - precision: 0.8948 - recall: 0.9467 - true_negatives: 1585.4053 - true_positives: 1752.0309\n",
      "For batch number 228 the model has a loss of 0.2790191173553467\n",
      "\u001b[1m228/689\u001b[0m \u001b[32m━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━\u001b[0m \u001b[1m26s\u001b[0m 57ms/step - auc: 0.9554 - binary_accuracy: 0.9159 - false_negatives: 96.6360 - false_positives: 215.2456 - loss: 0.2782 - precision: 0.8948 - recall: 0.9467 - true_negatives: 1592.5350 - true_positives: 1759.5834\n",
      "For batch number 229 the model has a loss of 0.27954140305519104\n",
      "\u001b[1m229/689\u001b[0m \u001b[32m━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━\u001b[0m \u001b[1m26s\u001b[0m 57ms/step - auc: 0.9554 - binary_accuracy: 0.9159 - false_negatives: 97.0262 - false_positives: 216.1790 - loss: 0.2783 - precision: 0.8947 - recall: 0.9467 - true_negatives: 1599.6725 - true_positives: 1767.1223\n",
      "For batch number 230 the model has a loss of 0.28022995591163635\n",
      "\u001b[1m230/689\u001b[0m \u001b[32m━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━\u001b[0m \u001b[1m26s\u001b[0m 57ms/step - auc: 0.9554 - binary_accuracy: 0.9159 - false_negatives: 97.4174 - false_positives: 217.1087 - loss: 0.2783 - precision: 0.8947 - recall: 0.9467 - true_negatives: 1606.8043 - true_positives: 1774.6696\n",
      "For batch number 231 the model has a loss of 0.27974891662597656\n",
      "\u001b[1m231/689\u001b[0m \u001b[32m━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━\u001b[0m \u001b[1m26s\u001b[0m 57ms/step - auc: 0.9553 - binary_accuracy: 0.9159 - false_negatives: 97.8052 - false_positives: 218.0390 - loss: 0.2783 - precision: 0.8947 - recall: 0.9467 - true_negatives: 1613.9308 - true_positives: 1782.2251\n",
      "For batch number 232 the model has a loss of 0.27993807196617126\n",
      "\u001b[1m232/689\u001b[0m \u001b[32m━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━\u001b[0m \u001b[1m26s\u001b[0m 57ms/step - auc: 0.9553 - binary_accuracy: 0.9159 - false_negatives: 98.1897 - false_positives: 218.9785 - loss: 0.2783 - precision: 0.8947 - recall: 0.9467 - true_negatives: 1621.0647 - true_positives: 1789.7672\n",
      "For batch number 233 the model has a loss of 0.2804245054721832\n",
      "\u001b[1m233/689\u001b[0m \u001b[32m━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━\u001b[0m \u001b[1m26s\u001b[0m 57ms/step - auc: 0.9553 - binary_accuracy: 0.9159 - false_negatives: 98.5751 - false_positives: 219.9185 - loss: 0.2783 - precision: 0.8947 - recall: 0.9467 - true_negatives: 1628.1974 - true_positives: 1797.3090\n",
      "For batch number 234 the model has a loss of 0.2814406752586365\n",
      "\u001b[1m234/689\u001b[0m \u001b[32m━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━\u001b[0m \u001b[1m26s\u001b[0m 57ms/step - auc: 0.9553 - binary_accuracy: 0.9159 - false_negatives: 98.9658 - false_positives: 220.8633 - loss: 0.2783 - precision: 0.8946 - recall: 0.9468 - true_negatives: 1635.3291 - true_positives: 1804.8419\n",
      "For batch number 235 the model has a loss of 0.2810702323913574\n",
      "\u001b[1m235/689\u001b[0m \u001b[32m━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━\u001b[0m \u001b[1m26s\u001b[0m 57ms/step - auc: 0.9553 - binary_accuracy: 0.9159 - false_negatives: 99.3574 - false_positives: 221.8043 - loss: 0.2783 - precision: 0.8946 - recall: 0.9468 - true_negatives: 1642.4553 - true_positives: 1812.3829\n",
      "For batch number 236 the model has a loss of 0.2817429006099701\n",
      "\u001b[1m236/689\u001b[0m \u001b[32m━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━\u001b[0m \u001b[1m26s\u001b[0m 57ms/step - auc: 0.9553 - binary_accuracy: 0.9159 - false_negatives: 99.7585 - false_positives: 222.7373 - loss: 0.2783 - precision: 0.8946 - recall: 0.9468 - true_negatives: 1649.5763 - true_positives: 1819.9280\n",
      "For batch number 237 the model has a loss of 0.28163376450538635\n",
      "\u001b[1m237/689\u001b[0m \u001b[32m━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━\u001b[0m \u001b[1m25s\u001b[0m 57ms/step - auc: 0.9553 - binary_accuracy: 0.9159 - false_negatives: 100.1603 - false_positives: 223.6751 - loss: 0.2783 - precision: 0.8946 - recall: 0.9468 - true_negatives: 1656.6836 - true_positives: 1827.4810\n",
      "For batch number 238 the model has a loss of 0.28155261278152466\n",
      "\u001b[1m238/689\u001b[0m \u001b[32m━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━\u001b[0m \u001b[1m25s\u001b[0m 57ms/step - auc: 0.9553 - binary_accuracy: 0.9159 - false_negatives: 100.5630 - false_positives: 224.6134 - loss: 0.2784 - precision: 0.8946 - recall: 0.9468 - true_negatives: 1663.7941 - true_positives: 1835.0294\n",
      "For batch number 239 the model has a loss of 0.2809700071811676\n",
      "\u001b[1m239/689\u001b[0m \u001b[32m━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━\u001b[0m \u001b[1m25s\u001b[0m 57ms/step - auc: 0.9553 - binary_accuracy: 0.9159 - false_negatives: 100.9623 - false_positives: 225.5481 - loss: 0.2784 - precision: 0.8945 - recall: 0.9468 - true_negatives: 1670.8995 - true_positives: 1842.5900\n",
      "For batch number 240 the model has a loss of 0.2810612916946411\n",
      "\u001b[1m240/689\u001b[0m \u001b[32m━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━\u001b[0m \u001b[1m25s\u001b[0m 57ms/step - auc: 0.9553 - binary_accuracy: 0.9159 - false_negatives: 101.3583 - false_positives: 226.4833 - loss: 0.2784 - precision: 0.8945 - recall: 0.9468 - true_negatives: 1678.0042 - true_positives: 1850.1542\n",
      "For batch number 241 the model has a loss of 0.28057965636253357\n",
      "\u001b[1m241/689\u001b[0m \u001b[32m━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━\u001b[0m \u001b[1m25s\u001b[0m 57ms/step - auc: 0.9553 - binary_accuracy: 0.9159 - false_negatives: 101.7552 - false_positives: 227.4149 - loss: 0.2784 - precision: 0.8945 - recall: 0.9468 - true_negatives: 1685.1162 - true_positives: 1857.7137\n",
      "For batch number 242 the model has a loss of 0.2802874445915222\n",
      "\u001b[1m242/689\u001b[0m \u001b[32m━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━\u001b[0m \u001b[1m25s\u001b[0m 57ms/step - auc: 0.9553 - binary_accuracy: 0.9159 - false_negatives: 102.1488 - false_positives: 228.3512 - loss: 0.2784 - precision: 0.8945 - recall: 0.9468 - true_negatives: 1692.2396 - true_positives: 1865.2604\n",
      "For batch number 243 the model has a loss of 0.2796754539012909\n",
      "\u001b[1m243/689\u001b[0m \u001b[32m━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━\u001b[0m \u001b[1m25s\u001b[0m 57ms/step - auc: 0.9553 - binary_accuracy: 0.9159 - false_negatives: 102.5432 - false_positives: 229.2798 - loss: 0.2784 - precision: 0.8945 - recall: 0.9468 - true_negatives: 1699.3622 - true_positives: 1872.8148\n",
      "For batch number 244 the model has a loss of 0.27934062480926514\n",
      "\u001b[1m244/689\u001b[0m \u001b[32m━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━\u001b[0m \u001b[1m25s\u001b[0m 57ms/step - auc: 0.9553 - binary_accuracy: 0.9159 - false_negatives: 102.9385 - false_positives: 230.2049 - loss: 0.2784 - precision: 0.8945 - recall: 0.9468 - true_negatives: 1706.4836 - true_positives: 1880.3729\n",
      "For batch number 245 the model has a loss of 0.2790054678916931\n",
      "\u001b[1m245/689\u001b[0m \u001b[32m━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━\u001b[0m \u001b[1m25s\u001b[0m 57ms/step - auc: 0.9553 - binary_accuracy: 0.9159 - false_negatives: 103.3347 - false_positives: 231.1265 - loss: 0.2784 - precision: 0.8945 - recall: 0.9469 - true_negatives: 1713.6000 - true_positives: 1887.9387\n",
      "For batch number 246 the model has a loss of 0.2793944478034973\n",
      "\u001b[1m246/689\u001b[0m \u001b[32m━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━\u001b[0m \u001b[1m25s\u001b[0m 57ms/step - auc: 0.9553 - binary_accuracy: 0.9159 - false_negatives: 103.7317 - false_positives: 232.0447 - loss: 0.2784 - precision: 0.8944 - recall: 0.9469 - true_negatives: 1720.7073 - true_positives: 1895.5162\n",
      "For batch number 247 the model has a loss of 0.27905428409576416\n",
      "\u001b[1m247/689\u001b[0m \u001b[32m━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━\u001b[0m \u001b[1m25s\u001b[0m 57ms/step - auc: 0.9552 - binary_accuracy: 0.9159 - false_negatives: 104.1296 - false_positives: 232.9595 - loss: 0.2784 - precision: 0.8944 - recall: 0.9469 - true_negatives: 1727.8259 - true_positives: 1903.0850\n",
      "For batch number 248 the model has a loss of 0.2795161008834839\n",
      "\u001b[1m248/689\u001b[0m \u001b[32m━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━\u001b[0m \u001b[1m25s\u001b[0m 57ms/step - auc: 0.9552 - binary_accuracy: 0.9159 - false_negatives: 104.5282 - false_positives: 233.8710 - loss: 0.2784 - precision: 0.8944 - recall: 0.9469 - true_negatives: 1734.9597 - true_positives: 1910.6411\n",
      "For batch number 249 the model has a loss of 0.27915579080581665\n",
      "\u001b[1m249/689\u001b[0m \u001b[32m━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━\u001b[0m \u001b[1m25s\u001b[0m 57ms/step - auc: 0.9552 - binary_accuracy: 0.9159 - false_negatives: 104.9277 - false_positives: 234.7751 - loss: 0.2784 - precision: 0.8944 - recall: 0.9469 - true_negatives: 1742.0924 - true_positives: 1918.2048\n",
      "For batch number 250 the model has a loss of 0.2783614993095398\n",
      "\u001b[1m250/689\u001b[0m \u001b[32m━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━\u001b[0m \u001b[1m25s\u001b[0m 57ms/step - auc: 0.9552 - binary_accuracy: 0.9159 - false_negatives: 105.3240 - false_positives: 235.6720 - loss: 0.2784 - precision: 0.8944 - recall: 0.9469 - true_negatives: 1749.2360 - true_positives: 1925.7679\n",
      "For batch number 251 the model has a loss of 0.27785083651542664\n",
      "\u001b[1m251/689\u001b[0m \u001b[32m━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━\u001b[0m \u001b[1m25s\u001b[0m 57ms/step - auc: 0.9552 - binary_accuracy: 0.9159 - false_negatives: 105.7171 - false_positives: 236.5618 - loss: 0.2784 - precision: 0.8944 - recall: 0.9469 - true_negatives: 1756.3705 - true_positives: 1933.3506\n",
      "For batch number 252 the model has a loss of 0.2781171202659607\n",
      "\u001b[1m252/689\u001b[0m \u001b[32m━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━\u001b[0m \u001b[1m25s\u001b[0m 57ms/step - auc: 0.9552 - binary_accuracy: 0.9159 - false_negatives: 106.1071 - false_positives: 237.4563 - loss: 0.2784 - precision: 0.8944 - recall: 0.9469 - true_negatives: 1763.5040 - true_positives: 1940.9325\n",
      "For batch number 253 the model has a loss of 0.27903419733047485\n",
      "\u001b[1m253/689\u001b[0m \u001b[32m━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━\u001b[0m \u001b[1m25s\u001b[0m 57ms/step - auc: 0.9552 - binary_accuracy: 0.9159 - false_negatives: 106.4941 - false_positives: 238.3834 - loss: 0.2784 - precision: 0.8944 - recall: 0.9469 - true_negatives: 1770.6324 - true_positives: 1948.4901\n",
      "For batch number 254 the model has a loss of 0.27973443269729614\n",
      "\u001b[1m254/689\u001b[0m \u001b[32m━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━\u001b[0m \u001b[1m24s\u001b[0m 57ms/step - auc: 0.9552 - binary_accuracy: 0.9159 - false_negatives: 106.8898 - false_positives: 239.3150 - loss: 0.2784 - precision: 0.8944 - recall: 0.9469 - true_negatives: 1777.7559 - true_positives: 1956.0394\n",
      "For batch number 255 the model has a loss of 0.27937114238739014\n",
      "\u001b[1m255/689\u001b[0m \u001b[32m━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━\u001b[0m \u001b[1m24s\u001b[0m 57ms/step - auc: 0.9552 - binary_accuracy: 0.9159 - false_negatives: 107.2863 - false_positives: 240.2471 - loss: 0.2784 - precision: 0.8943 - recall: 0.9469 - true_negatives: 1784.8706 - true_positives: 1963.5961\n",
      "For batch number 256 the model has a loss of 0.2786475718021393\n",
      "\u001b[1m256/689\u001b[0m \u001b[32m━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━\u001b[0m \u001b[1m24s\u001b[0m 57ms/step - auc: 0.9552 - binary_accuracy: 0.9160 - false_negatives: 107.6836 - false_positives: 241.1719 - loss: 0.2784 - precision: 0.8943 - recall: 0.9470 - true_negatives: 1791.9961 - true_positives: 1971.1484\n",
      "For batch number 257 the model has a loss of 0.2785017788410187\n",
      "\u001b[1m257/689\u001b[0m \u001b[32m━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━\u001b[0m \u001b[1m24s\u001b[0m 57ms/step - auc: 0.9552 - binary_accuracy: 0.9160 - false_negatives: 108.0778 - false_positives: 242.1051 - loss: 0.2784 - precision: 0.8943 - recall: 0.9470 - true_negatives: 1799.1090 - true_positives: 1978.7081\n",
      "For batch number 258 the model has a loss of 0.27824723720550537\n",
      "\u001b[1m258/689\u001b[0m \u001b[32m━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━\u001b[0m \u001b[1m24s\u001b[0m 57ms/step - auc: 0.9552 - binary_accuracy: 0.9160 - false_negatives: 108.4690 - false_positives: 243.0388 - loss: 0.2784 - precision: 0.8943 - recall: 0.9470 - true_negatives: 1806.2249 - true_positives: 1986.2675\n",
      "For batch number 259 the model has a loss of 0.27804115414619446\n",
      "\u001b[1m259/689\u001b[0m \u001b[32m━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━\u001b[0m \u001b[1m24s\u001b[0m 57ms/step - auc: 0.9552 - binary_accuracy: 0.9160 - false_negatives: 108.8610 - false_positives: 243.9730 - loss: 0.2784 - precision: 0.8943 - recall: 0.9470 - true_negatives: 1813.3513 - true_positives: 1993.8147\n",
      "For batch number 260 the model has a loss of 0.2779200077056885\n",
      "\u001b[1m260/689\u001b[0m \u001b[32m━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━\u001b[0m \u001b[1m24s\u001b[0m 57ms/step - auc: 0.9552 - binary_accuracy: 0.9160 - false_negatives: 109.2538 - false_positives: 244.9038 - loss: 0.2784 - precision: 0.8943 - recall: 0.9470 - true_negatives: 1820.4653 - true_positives: 2001.3770\n",
      "For batch number 261 the model has a loss of 0.27791404724121094\n",
      "\u001b[1m261/689\u001b[0m \u001b[32m━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━\u001b[0m \u001b[1m24s\u001b[0m 57ms/step - auc: 0.9552 - binary_accuracy: 0.9160 - false_negatives: 109.6475 - false_positives: 245.8353 - loss: 0.2784 - precision: 0.8943 - recall: 0.9470 - true_negatives: 1827.5785 - true_positives: 2008.9387\n",
      "For batch number 262 the model has a loss of 0.2779538631439209\n",
      "\u001b[1m262/689\u001b[0m \u001b[32m━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━\u001b[0m \u001b[1m24s\u001b[0m 57ms/step - auc: 0.9552 - binary_accuracy: 0.9160 - false_negatives: 110.0382 - false_positives: 246.7748 - loss: 0.2784 - precision: 0.8942 - recall: 0.9470 - true_negatives: 1834.6985 - true_positives: 2016.4885\n",
      "For batch number 263 the model has a loss of 0.2780890166759491\n",
      "\u001b[1m263/689\u001b[0m \u001b[32m━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━\u001b[0m \u001b[1m24s\u001b[0m 57ms/step - auc: 0.9552 - binary_accuracy: 0.9160 - false_negatives: 110.4297 - false_positives: 247.7148 - loss: 0.2784 - precision: 0.8942 - recall: 0.9470 - true_negatives: 1841.8289 - true_positives: 2024.0266\n",
      "For batch number 264 the model has a loss of 0.27865174412727356\n",
      "\u001b[1m264/689\u001b[0m \u001b[32m━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━\u001b[0m \u001b[1m24s\u001b[0m 57ms/step - auc: 0.9552 - binary_accuracy: 0.9160 - false_negatives: 110.8258 - false_positives: 248.6477 - loss: 0.2784 - precision: 0.8942 - recall: 0.9470 - true_negatives: 1848.9508 - true_positives: 2031.5758\n",
      "For batch number 265 the model has a loss of 0.2790932059288025\n",
      "\u001b[1m265/689\u001b[0m \u001b[32m━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━\u001b[0m \u001b[1m24s\u001b[0m 57ms/step - auc: 0.9552 - binary_accuracy: 0.9160 - false_negatives: 111.2226 - false_positives: 249.5774 - loss: 0.2784 - precision: 0.8942 - recall: 0.9470 - true_negatives: 1856.0868 - true_positives: 2039.1132\n",
      "For batch number 266 the model has a loss of 0.27936023473739624\n",
      "\u001b[1m266/689\u001b[0m \u001b[32m━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━\u001b[0m \u001b[1m24s\u001b[0m 57ms/step - auc: 0.9552 - binary_accuracy: 0.9160 - false_negatives: 111.6241 - false_positives: 250.5000 - loss: 0.2784 - precision: 0.8942 - recall: 0.9470 - true_negatives: 1863.2106 - true_positives: 2046.6654\n",
      "For batch number 267 the model has a loss of 0.27894774079322815\n",
      "\u001b[1m267/689\u001b[0m \u001b[32m━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━\u001b[0m \u001b[1m24s\u001b[0m 57ms/step - auc: 0.9552 - binary_accuracy: 0.9160 - false_negatives: 112.0225 - false_positives: 251.4195 - loss: 0.2784 - precision: 0.8942 - recall: 0.9471 - true_negatives: 1870.3370 - true_positives: 2054.2209\n",
      "For batch number 268 the model has a loss of 0.2792186439037323\n",
      "\u001b[1m268/689\u001b[0m \u001b[32m━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━\u001b[0m \u001b[1m24s\u001b[0m 57ms/step - auc: 0.9552 - binary_accuracy: 0.9160 - false_negatives: 112.4254 - false_positives: 252.3433 - loss: 0.2784 - precision: 0.8942 - recall: 0.9471 - true_negatives: 1877.4664 - true_positives: 2061.7649\n",
      "For batch number 269 the model has a loss of 0.27925872802734375\n",
      "\u001b[1m269/689\u001b[0m \u001b[32m━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━\u001b[0m \u001b[1m24s\u001b[0m 57ms/step - auc: 0.9552 - binary_accuracy: 0.9160 - false_negatives: 112.8290 - false_positives: 253.2677 - loss: 0.2784 - precision: 0.8942 - recall: 0.9471 - true_negatives: 1884.6022 - true_positives: 2069.3010\n",
      "For batch number 270 the model has a loss of 0.27871763706207275\n",
      "\u001b[1m270/689\u001b[0m \u001b[32m━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━\u001b[0m \u001b[1m24s\u001b[0m 57ms/step - auc: 0.9552 - binary_accuracy: 0.9160 - false_negatives: 113.2333 - false_positives: 254.1889 - loss: 0.2784 - precision: 0.8941 - recall: 0.9471 - true_negatives: 1891.7260 - true_positives: 2076.8518\n",
      "For batch number 271 the model has a loss of 0.27834051847457886\n",
      "\u001b[1m271/689\u001b[0m \u001b[32m━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━\u001b[0m \u001b[1m23s\u001b[0m 57ms/step - auc: 0.9552 - binary_accuracy: 0.9160 - false_negatives: 113.6347 - false_positives: 255.1033 - loss: 0.2784 - precision: 0.8941 - recall: 0.9471 - true_negatives: 1898.8413 - true_positives: 2084.4207\n",
      "For batch number 272 the model has a loss of 0.2787199318408966\n",
      "\u001b[1m272/689\u001b[0m \u001b[32m━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━\u001b[0m \u001b[1m23s\u001b[0m 57ms/step - auc: 0.9552 - binary_accuracy: 0.9160 - false_negatives: 114.0404 - false_positives: 256.0184 - loss: 0.2784 - precision: 0.8941 - recall: 0.9471 - true_negatives: 1905.9448 - true_positives: 2091.9963\n",
      "For batch number 273 the model has a loss of 0.2789185643196106\n",
      "\u001b[1m273/689\u001b[0m \u001b[32m━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━\u001b[0m \u001b[1m23s\u001b[0m 57ms/step - auc: 0.9552 - binary_accuracy: 0.9160 - false_negatives: 114.4469 - false_positives: 256.9267 - loss: 0.2784 - precision: 0.8941 - recall: 0.9471 - true_negatives: 1913.0549 - true_positives: 2099.5715\n",
      "For batch number 274 the model has a loss of 0.27846759557724\n",
      "\u001b[1m274/689\u001b[0m \u001b[32m━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━\u001b[0m \u001b[1m23s\u001b[0m 57ms/step - auc: 0.9552 - binary_accuracy: 0.9160 - false_negatives: 114.8504 - false_positives: 257.8358 - loss: 0.2784 - precision: 0.8941 - recall: 0.9471 - true_negatives: 1920.1678 - true_positives: 2107.1460\n",
      "For batch number 275 the model has a loss of 0.27888983488082886\n",
      "\u001b[1m275/689\u001b[0m \u001b[32m━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━\u001b[0m \u001b[1m23s\u001b[0m 57ms/step - auc: 0.9552 - binary_accuracy: 0.9160 - false_negatives: 115.2509 - false_positives: 258.7564 - loss: 0.2784 - precision: 0.8941 - recall: 0.9471 - true_negatives: 1927.2909 - true_positives: 2114.7019\n",
      "For batch number 276 the model has a loss of 0.27980339527130127\n",
      "\u001b[1m276/689\u001b[0m \u001b[32m━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━\u001b[0m \u001b[1m23s\u001b[0m 57ms/step - auc: 0.9552 - binary_accuracy: 0.9160 - false_negatives: 115.6667 - false_positives: 259.6703 - loss: 0.2784 - precision: 0.8941 - recall: 0.9471 - true_negatives: 1934.3986 - true_positives: 2122.2644\n",
      "For batch number 277 the model has a loss of 0.279927521944046\n",
      "\u001b[1m277/689\u001b[0m \u001b[32m━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━\u001b[0m \u001b[1m23s\u001b[0m 57ms/step - auc: 0.9551 - binary_accuracy: 0.9160 - false_negatives: 116.0830 - false_positives: 260.5921 - loss: 0.2784 - precision: 0.8941 - recall: 0.9471 - true_negatives: 1941.5054 - true_positives: 2129.8196\n",
      "For batch number 278 the model has a loss of 0.279611736536026\n",
      "\u001b[1m278/689\u001b[0m \u001b[32m━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━\u001b[0m \u001b[1m23s\u001b[0m 57ms/step - auc: 0.9551 - binary_accuracy: 0.9160 - false_negatives: 116.4964 - false_positives: 261.5144 - loss: 0.2785 - precision: 0.8941 - recall: 0.9471 - true_negatives: 1948.6223 - true_positives: 2137.3669\n",
      "For batch number 279 the model has a loss of 0.2805974781513214\n",
      "\u001b[1m279/689\u001b[0m \u001b[32m━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━\u001b[0m \u001b[1m23s\u001b[0m 57ms/step - auc: 0.9551 - binary_accuracy: 0.9160 - false_negatives: 116.9140 - false_positives: 262.4409 - loss: 0.2785 - precision: 0.8941 - recall: 0.9471 - true_negatives: 1955.7384 - true_positives: 2144.9067\n",
      "For batch number 280 the model has a loss of 0.28062453866004944\n",
      "\u001b[1m280/689\u001b[0m \u001b[32m━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━\u001b[0m \u001b[1m23s\u001b[0m 57ms/step - auc: 0.9551 - binary_accuracy: 0.9160 - false_negatives: 117.3321 - false_positives: 263.3607 - loss: 0.2785 - precision: 0.8940 - recall: 0.9471 - true_negatives: 1962.8607 - true_positives: 2152.4465\n",
      "For batch number 281 the model has a loss of 0.28036558628082275\n",
      "\u001b[1m281/689\u001b[0m \u001b[32m━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━\u001b[0m \u001b[1m23s\u001b[0m 57ms/step - auc: 0.9551 - binary_accuracy: 0.9160 - false_negatives: 117.7544 - false_positives: 264.2740 - loss: 0.2785 - precision: 0.8940 - recall: 0.9471 - true_negatives: 1969.9929 - true_positives: 2159.9788\n",
      "For batch number 282 the model has a loss of 0.2805394232273102\n",
      "\u001b[1m282/689\u001b[0m \u001b[32m━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━\u001b[0m \u001b[1m23s\u001b[0m 57ms/step - auc: 0.9551 - binary_accuracy: 0.9160 - false_negatives: 118.1773 - false_positives: 265.1844 - loss: 0.2785 - precision: 0.8940 - recall: 0.9471 - true_negatives: 1977.1206 - true_positives: 2167.5178\n",
      "For batch number 283 the model has a loss of 0.2800861895084381\n",
      "\u001b[1m283/689\u001b[0m \u001b[32m━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━\u001b[0m \u001b[1m23s\u001b[0m 57ms/step - auc: 0.9551 - binary_accuracy: 0.9160 - false_negatives: 118.6007 - false_positives: 266.0883 - loss: 0.2785 - precision: 0.8940 - recall: 0.9471 - true_negatives: 1984.2473 - true_positives: 2175.0637\n",
      "For batch number 284 the model has a loss of 0.2802175283432007\n",
      "\u001b[1m284/689\u001b[0m \u001b[32m━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━\u001b[0m \u001b[1m23s\u001b[0m 57ms/step - auc: 0.9551 - binary_accuracy: 0.9160 - false_negatives: 119.0247 - false_positives: 266.9965 - loss: 0.2785 - precision: 0.8940 - recall: 0.9471 - true_negatives: 1991.3767 - true_positives: 2182.6021\n",
      "For batch number 285 the model has a loss of 0.2802821099758148\n",
      "\u001b[1m285/689\u001b[0m \u001b[32m━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━\u001b[0m \u001b[1m23s\u001b[0m 57ms/step - auc: 0.9551 - binary_accuracy: 0.9160 - false_negatives: 119.4491 - false_positives: 267.9018 - loss: 0.2785 - precision: 0.8940 - recall: 0.9471 - true_negatives: 1998.5088 - true_positives: 2190.1404\n",
      "For batch number 286 the model has a loss of 0.28005415201187134\n",
      "\u001b[1m286/689\u001b[0m \u001b[32m━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━\u001b[0m \u001b[1m23s\u001b[0m 57ms/step - auc: 0.9551 - binary_accuracy: 0.9160 - false_negatives: 119.8706 - false_positives: 268.8077 - loss: 0.2785 - precision: 0.8940 - recall: 0.9471 - true_negatives: 2005.6433 - true_positives: 2197.6782\n",
      "For batch number 287 the model has a loss of 0.2800048887729645\n",
      "\u001b[1m287/689\u001b[0m \u001b[32m━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━\u001b[0m \u001b[1m23s\u001b[0m 57ms/step - auc: 0.9551 - binary_accuracy: 0.9160 - false_negatives: 120.2892 - false_positives: 269.7108 - loss: 0.2785 - precision: 0.8940 - recall: 0.9471 - true_negatives: 2012.7666 - true_positives: 2205.2334\n",
      "For batch number 288 the model has a loss of 0.27976569533348083\n",
      "\u001b[1m288/689\u001b[0m \u001b[32m━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━\u001b[0m \u001b[1m22s\u001b[0m 57ms/step - auc: 0.9551 - binary_accuracy: 0.9160 - false_negatives: 120.7049 - false_positives: 270.6180 - loss: 0.2785 - precision: 0.8940 - recall: 0.9471 - true_negatives: 2019.8959 - true_positives: 2212.7812\n",
      "For batch number 289 the model has a loss of 0.2793527841567993\n",
      "\u001b[1m289/689\u001b[0m \u001b[32m━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━\u001b[0m \u001b[1m22s\u001b[0m 57ms/step - auc: 0.9551 - binary_accuracy: 0.9160 - false_negatives: 121.1211 - false_positives: 271.5190 - loss: 0.2785 - precision: 0.8940 - recall: 0.9471 - true_negatives: 2027.0381 - true_positives: 2220.3218\n",
      "For batch number 290 the model has a loss of 0.2792818546295166\n",
      "\u001b[1m290/689\u001b[0m \u001b[32m━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━\u001b[0m \u001b[1m22s\u001b[0m 57ms/step - auc: 0.9551 - binary_accuracy: 0.9160 - false_negatives: 121.5345 - false_positives: 272.4241 - loss: 0.2785 - precision: 0.8940 - recall: 0.9471 - true_negatives: 2034.1897 - true_positives: 2227.8518\n",
      "For batch number 291 the model has a loss of 0.2788543403148651\n",
      "\u001b[1m291/689\u001b[0m \u001b[32m━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━\u001b[0m \u001b[1m22s\u001b[0m 57ms/step - auc: 0.9551 - binary_accuracy: 0.9160 - false_negatives: 121.9450 - false_positives: 273.3333 - loss: 0.2785 - precision: 0.8940 - recall: 0.9471 - true_negatives: 2041.3505 - true_positives: 2235.3711\n",
      "For batch number 292 the model has a loss of 0.2787799835205078\n",
      "\u001b[1m292/689\u001b[0m \u001b[32m━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━\u001b[0m \u001b[1m22s\u001b[0m 57ms/step - auc: 0.9551 - binary_accuracy: 0.9160 - false_negatives: 122.3562 - false_positives: 274.2397 - loss: 0.2785 - precision: 0.8940 - recall: 0.9471 - true_negatives: 2048.5103 - true_positives: 2242.8938\n",
      "For batch number 293 the model has a loss of 0.27886316180229187\n",
      "\u001b[1m293/689\u001b[0m \u001b[32m━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━\u001b[0m \u001b[1m22s\u001b[0m 57ms/step - auc: 0.9551 - binary_accuracy: 0.9160 - false_negatives: 122.7679 - false_positives: 275.1468 - loss: 0.2785 - precision: 0.8939 - recall: 0.9472 - true_negatives: 2055.6621 - true_positives: 2250.4231\n",
      "For batch number 294 the model has a loss of 0.2789517343044281\n",
      "\u001b[1m294/689\u001b[0m \u001b[32m━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━\u001b[0m \u001b[1m22s\u001b[0m 57ms/step - auc: 0.9551 - binary_accuracy: 0.9160 - false_negatives: 123.1837 - false_positives: 276.0510 - loss: 0.2785 - precision: 0.8939 - recall: 0.9472 - true_negatives: 2062.8027 - true_positives: 2257.9626\n",
      "For batch number 295 the model has a loss of 0.27904990315437317\n",
      "\u001b[1m295/689\u001b[0m \u001b[32m━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━\u001b[0m \u001b[1m22s\u001b[0m 57ms/step - auc: 0.9551 - binary_accuracy: 0.9160 - false_negatives: 123.6000 - false_positives: 276.9525 - loss: 0.2785 - precision: 0.8939 - recall: 0.9472 - true_negatives: 2069.9458 - true_positives: 2265.5017\n",
      "For batch number 296 the model has a loss of 0.27905380725860596\n",
      "\u001b[1m296/689\u001b[0m \u001b[32m━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━\u001b[0m \u001b[1m22s\u001b[0m 57ms/step - auc: 0.9551 - binary_accuracy: 0.9160 - false_negatives: 124.0135 - false_positives: 277.8615 - loss: 0.2785 - precision: 0.8939 - recall: 0.9472 - true_negatives: 2077.0945 - true_positives: 2273.0305\n",
      "For batch number 297 the model has a loss of 0.27892449498176575\n",
      "\u001b[1m297/689\u001b[0m \u001b[32m━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━\u001b[0m \u001b[1m22s\u001b[0m 57ms/step - auc: 0.9551 - binary_accuracy: 0.9160 - false_negatives: 124.4242 - false_positives: 278.7711 - loss: 0.2785 - precision: 0.8939 - recall: 0.9472 - true_negatives: 2084.2493 - true_positives: 2280.5557\n",
      "For batch number 298 the model has a loss of 0.2799699902534485\n",
      "\u001b[1m298/689\u001b[0m \u001b[32m━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━\u001b[0m \u001b[1m22s\u001b[0m 57ms/step - auc: 0.9551 - binary_accuracy: 0.9160 - false_negatives: 124.8389 - false_positives: 279.6879 - loss: 0.2785 - precision: 0.8939 - recall: 0.9472 - true_negatives: 2091.4060 - true_positives: 2288.0671\n",
      "For batch number 299 the model has a loss of 0.2793195843696594\n",
      "\u001b[1m299/689\u001b[0m \u001b[32m━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━\u001b[0m \u001b[1m22s\u001b[0m 57ms/step - auc: 0.9551 - binary_accuracy: 0.9160 - false_negatives: 125.2508 - false_positives: 280.5987 - loss: 0.2785 - precision: 0.8939 - recall: 0.9472 - true_negatives: 2098.5586 - true_positives: 2295.5920\n",
      "For batch number 300 the model has a loss of 0.2789219319820404\n",
      "\u001b[1m300/689\u001b[0m \u001b[32m━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━\u001b[0m \u001b[1m22s\u001b[0m 57ms/step - auc: 0.9551 - binary_accuracy: 0.9160 - false_negatives: 125.6600 - false_positives: 281.5100 - loss: 0.2785 - precision: 0.8939 - recall: 0.9472 - true_negatives: 2105.7034 - true_positives: 2303.1267\n",
      "For batch number 301 the model has a loss of 0.2789846360683441\n",
      "\u001b[1m301/689\u001b[0m \u001b[32m━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━\u001b[0m \u001b[1m22s\u001b[0m 57ms/step - auc: 0.9551 - binary_accuracy: 0.9160 - false_negatives: 126.0797 - false_positives: 282.4153 - loss: 0.2785 - precision: 0.8939 - recall: 0.9472 - true_negatives: 2112.8340 - true_positives: 2310.6711\n",
      "For batch number 302 the model has a loss of 0.27832287549972534\n",
      "\u001b[1m302/689\u001b[0m \u001b[32m━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━\u001b[0m \u001b[1m22s\u001b[0m 57ms/step - auc: 0.9551 - binary_accuracy: 0.9160 - false_negatives: 126.4967 - false_positives: 283.3146 - loss: 0.2785 - precision: 0.8939 - recall: 0.9472 - true_negatives: 2119.9668 - true_positives: 2318.2219\n",
      "For batch number 303 the model has a loss of 0.2778983414173126\n",
      "\u001b[1m303/689\u001b[0m \u001b[32m━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━\u001b[0m \u001b[1m22s\u001b[0m 57ms/step - auc: 0.9551 - binary_accuracy: 0.9160 - false_negatives: 126.9109 - false_positives: 284.2112 - loss: 0.2785 - precision: 0.8939 - recall: 0.9472 - true_negatives: 2127.1057 - true_positives: 2325.7722\n",
      "For batch number 304 the model has a loss of 0.27786505222320557\n",
      "\u001b[1m304/689\u001b[0m \u001b[32m━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━\u001b[0m \u001b[1m22s\u001b[0m 57ms/step - auc: 0.9551 - binary_accuracy: 0.9160 - false_negatives: 127.3257 - false_positives: 285.1053 - loss: 0.2785 - precision: 0.8939 - recall: 0.9472 - true_negatives: 2134.2434 - true_positives: 2333.3257\n",
      "For batch number 305 the model has a loss of 0.27805274724960327\n",
      "\u001b[1m305/689\u001b[0m \u001b[32m━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━\u001b[0m \u001b[1m22s\u001b[0m 57ms/step - auc: 0.9551 - binary_accuracy: 0.9160 - false_negatives: 127.7410 - false_positives: 286.0066 - loss: 0.2785 - precision: 0.8939 - recall: 0.9472 - true_negatives: 2141.3901 - true_positives: 2340.8623\n",
      "For batch number 306 the model has a loss of 0.27773791551589966\n",
      "\u001b[1m306/689\u001b[0m \u001b[32m━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━\u001b[0m \u001b[1m21s\u001b[0m 57ms/step - auc: 0.9551 - binary_accuracy: 0.9160 - false_negatives: 128.1536 - false_positives: 286.9118 - loss: 0.2785 - precision: 0.8939 - recall: 0.9472 - true_negatives: 2148.5457 - true_positives: 2348.3889\n",
      "For batch number 307 the model has a loss of 0.27732303738594055\n",
      "\u001b[1m307/689\u001b[0m \u001b[32m━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━\u001b[0m \u001b[1m21s\u001b[0m 57ms/step - auc: 0.9551 - binary_accuracy: 0.9160 - false_negatives: 128.5635 - false_positives: 287.8143 - loss: 0.2785 - precision: 0.8939 - recall: 0.9472 - true_negatives: 2155.7102 - true_positives: 2355.9121\n",
      "For batch number 308 the model has a loss of 0.27697378396987915\n",
      "\u001b[1m308/689\u001b[0m \u001b[32m━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━\u001b[0m \u001b[1m21s\u001b[0m 57ms/step - auc: 0.9551 - binary_accuracy: 0.9160 - false_negatives: 128.9740 - false_positives: 288.7143 - loss: 0.2785 - precision: 0.8938 - recall: 0.9472 - true_negatives: 2162.8733 - true_positives: 2363.4382\n",
      "For batch number 309 the model has a loss of 0.2769475281238556\n",
      "\u001b[1m309/689\u001b[0m \u001b[32m━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━\u001b[0m \u001b[1m21s\u001b[0m 57ms/step - auc: 0.9551 - binary_accuracy: 0.9160 - false_negatives: 129.3851 - false_positives: 289.6149 - loss: 0.2785 - precision: 0.8938 - recall: 0.9472 - true_negatives: 2170.0518 - true_positives: 2370.9482\n",
      "For batch number 310 the model has a loss of 0.27692851424217224\n",
      "\u001b[1m310/689\u001b[0m \u001b[32m━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━\u001b[0m \u001b[1m21s\u001b[0m 57ms/step - auc: 0.9551 - binary_accuracy: 0.9160 - false_negatives: 129.7968 - false_positives: 290.5161 - loss: 0.2785 - precision: 0.8938 - recall: 0.9472 - true_negatives: 2177.2192 - true_positives: 2378.4678\n",
      "For batch number 311 the model has a loss of 0.2763262093067169\n",
      "\u001b[1m311/689\u001b[0m \u001b[32m━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━\u001b[0m \u001b[1m21s\u001b[0m 57ms/step - auc: 0.9551 - binary_accuracy: 0.9160 - false_negatives: 130.2090 - false_positives: 291.4116 - loss: 0.2785 - precision: 0.8938 - recall: 0.9472 - true_negatives: 2184.3794 - true_positives: 2386.0000\n",
      "For batch number 312 the model has a loss of 0.276373028755188\n",
      "\u001b[1m312/689\u001b[0m \u001b[32m━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━\u001b[0m \u001b[1m21s\u001b[0m 57ms/step - auc: 0.9551 - binary_accuracy: 0.9160 - false_negatives: 130.6186 - false_positives: 292.3109 - loss: 0.2785 - precision: 0.8938 - recall: 0.9472 - true_negatives: 2191.5352 - true_positives: 2393.5352\n",
      "For batch number 313 the model has a loss of 0.27644482254981995\n",
      "\u001b[1m313/689\u001b[0m \u001b[32m━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━\u001b[0m \u001b[1m21s\u001b[0m 57ms/step - auc: 0.9551 - binary_accuracy: 0.9160 - false_negatives: 131.0287 - false_positives: 293.2109 - loss: 0.2785 - precision: 0.8938 - recall: 0.9472 - true_negatives: 2198.6902 - true_positives: 2401.0703\n",
      "For batch number 314 the model has a loss of 0.2763984501361847\n",
      "\u001b[1m314/689\u001b[0m \u001b[32m━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━\u001b[0m \u001b[1m21s\u001b[0m 57ms/step - auc: 0.9551 - binary_accuracy: 0.9160 - false_negatives: 131.4427 - false_positives: 294.1083 - loss: 0.2785 - precision: 0.8938 - recall: 0.9472 - true_negatives: 2205.8408 - true_positives: 2408.6084\n",
      "For batch number 315 the model has a loss of 0.27639472484588623\n",
      "\u001b[1m315/689\u001b[0m \u001b[32m━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━\u001b[0m \u001b[1m21s\u001b[0m 57ms/step - auc: 0.9551 - binary_accuracy: 0.9160 - false_negatives: 131.8571 - false_positives: 295.0095 - loss: 0.2785 - precision: 0.8938 - recall: 0.9472 - true_negatives: 2212.9873 - true_positives: 2416.1460\n",
      "For batch number 316 the model has a loss of 0.2766803801059723\n",
      "\u001b[1m316/689\u001b[0m \u001b[32m━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━\u001b[0m \u001b[1m21s\u001b[0m 57ms/step - auc: 0.9551 - binary_accuracy: 0.9160 - false_negatives: 132.2722 - false_positives: 295.9082 - loss: 0.2785 - precision: 0.8938 - recall: 0.9472 - true_negatives: 2220.1328 - true_positives: 2423.6868\n",
      "For batch number 317 the model has a loss of 0.2762675881385803\n",
      "\u001b[1m317/689\u001b[0m \u001b[32m━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━\u001b[0m \u001b[1m21s\u001b[0m 57ms/step - auc: 0.9551 - binary_accuracy: 0.9160 - false_negatives: 132.6845 - false_positives: 296.8013 - loss: 0.2785 - precision: 0.8938 - recall: 0.9472 - true_negatives: 2227.2839 - true_positives: 2431.2302\n",
      "For batch number 318 the model has a loss of 0.2759478688240051\n",
      "\u001b[1m318/689\u001b[0m \u001b[32m━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━\u001b[0m \u001b[1m21s\u001b[0m 57ms/step - auc: 0.9551 - binary_accuracy: 0.9160 - false_negatives: 133.1006 - false_positives: 297.6918 - loss: 0.2785 - precision: 0.8938 - recall: 0.9472 - true_negatives: 2234.4370 - true_positives: 2438.7705\n",
      "For batch number 319 the model has a loss of 0.2755144238471985\n",
      "\u001b[1m319/689\u001b[0m \u001b[32m━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━\u001b[0m \u001b[1m21s\u001b[0m 57ms/step - auc: 0.9551 - binary_accuracy: 0.9160 - false_negatives: 133.5141 - false_positives: 298.5768 - loss: 0.2784 - precision: 0.8938 - recall: 0.9472 - true_negatives: 2241.5925 - true_positives: 2446.3167\n",
      "For batch number 320 the model has a loss of 0.2758205831050873\n",
      "\u001b[1m320/689\u001b[0m \u001b[32m━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━\u001b[0m \u001b[1m21s\u001b[0m 57ms/step - auc: 0.9551 - binary_accuracy: 0.9160 - false_negatives: 133.9312 - false_positives: 299.4594 - loss: 0.2784 - precision: 0.8938 - recall: 0.9472 - true_negatives: 2248.7437 - true_positives: 2453.8657\n",
      "For batch number 321 the model has a loss of 0.2772936224937439\n",
      "\u001b[1m321/689\u001b[0m \u001b[32m━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━\u001b[0m \u001b[1m21s\u001b[0m 57ms/step - auc: 0.9551 - binary_accuracy: 0.9160 - false_negatives: 134.3551 - false_positives: 300.3396 - loss: 0.2784 - precision: 0.8938 - recall: 0.9472 - true_negatives: 2255.9033 - true_positives: 2461.4019\n",
      "For batch number 322 the model has a loss of 0.27701905369758606\n",
      "\u001b[1m322/689\u001b[0m \u001b[32m━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━\u001b[0m \u001b[1m21s\u001b[0m 57ms/step - auc: 0.9551 - binary_accuracy: 0.9160 - false_negatives: 134.7795 - false_positives: 301.2174 - loss: 0.2784 - precision: 0.8938 - recall: 0.9472 - true_negatives: 2263.0591 - true_positives: 2468.9441\n",
      "For batch number 323 the model has a loss of 0.27714329957962036\n",
      "\u001b[1m323/689\u001b[0m \u001b[32m━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━\u001b[0m \u001b[1m21s\u001b[0m 57ms/step - auc: 0.9551 - binary_accuracy: 0.9161 - false_negatives: 135.2074 - false_positives: 302.0991 - loss: 0.2784 - precision: 0.8938 - recall: 0.9472 - true_negatives: 2270.2197 - true_positives: 2476.4736\n",
      "For batch number 324 the model has a loss of 0.2776491045951843\n",
      "\u001b[1m324/689\u001b[0m \u001b[32m━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━\u001b[0m \u001b[1m20s\u001b[0m 57ms/step - auc: 0.9551 - binary_accuracy: 0.9161 - false_negatives: 135.6389 - false_positives: 302.9846 - loss: 0.2784 - precision: 0.8938 - recall: 0.9472 - true_negatives: 2277.3857 - true_positives: 2483.9907\n",
      "For batch number 325 the model has a loss of 0.27800989151000977\n",
      "\u001b[1m325/689\u001b[0m \u001b[32m━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━\u001b[0m \u001b[1m20s\u001b[0m 57ms/step - auc: 0.9551 - binary_accuracy: 0.9161 - false_negatives: 136.0769 - false_positives: 303.8708 - loss: 0.2784 - precision: 0.8938 - recall: 0.9472 - true_negatives: 2284.5354 - true_positives: 2491.5168\n",
      "For batch number 326 the model has a loss of 0.27760758996009827\n",
      "\u001b[1m326/689\u001b[0m \u001b[32m━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━\u001b[0m \u001b[1m20s\u001b[0m 57ms/step - auc: 0.9551 - binary_accuracy: 0.9161 - false_negatives: 136.5123 - false_positives: 304.7546 - loss: 0.2784 - precision: 0.8938 - recall: 0.9472 - true_negatives: 2291.6902 - true_positives: 2499.0430\n",
      "For batch number 327 the model has a loss of 0.2777026891708374\n",
      "\u001b[1m327/689\u001b[0m \u001b[32m━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━\u001b[0m \u001b[1m20s\u001b[0m 57ms/step - auc: 0.9551 - binary_accuracy: 0.9161 - false_negatives: 136.9511 - false_positives: 305.6391 - loss: 0.2784 - precision: 0.8938 - recall: 0.9472 - true_negatives: 2298.8501 - true_positives: 2506.5596\n",
      "For batch number 328 the model has a loss of 0.27751466631889343\n",
      "\u001b[1m328/689\u001b[0m \u001b[32m━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━\u001b[0m \u001b[1m20s\u001b[0m 57ms/step - auc: 0.9551 - binary_accuracy: 0.9161 - false_negatives: 137.3872 - false_positives: 306.5244 - loss: 0.2784 - precision: 0.8938 - recall: 0.9472 - true_negatives: 2306.0244 - true_positives: 2514.0640\n",
      "For batch number 329 the model has a loss of 0.27740374207496643\n",
      "\u001b[1m329/689\u001b[0m \u001b[32m━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━\u001b[0m \u001b[1m20s\u001b[0m 57ms/step - auc: 0.9551 - binary_accuracy: 0.9161 - false_negatives: 137.8207 - false_positives: 307.4164 - loss: 0.2784 - precision: 0.8938 - recall: 0.9472 - true_negatives: 2313.2097 - true_positives: 2521.5532\n",
      "For batch number 330 the model has a loss of 0.27737879753112793\n",
      "\u001b[1m330/689\u001b[0m \u001b[32m━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━\u001b[0m \u001b[1m20s\u001b[0m 57ms/step - auc: 0.9551 - binary_accuracy: 0.9161 - false_negatives: 138.2515 - false_positives: 308.3091 - loss: 0.2784 - precision: 0.8938 - recall: 0.9472 - true_negatives: 2320.4031 - true_positives: 2529.0364\n",
      "For batch number 331 the model has a loss of 0.27723780274391174\n",
      "\u001b[1m331/689\u001b[0m \u001b[32m━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━\u001b[0m \u001b[1m20s\u001b[0m 57ms/step - auc: 0.9551 - binary_accuracy: 0.9161 - false_negatives: 138.6798 - false_positives: 309.2024 - loss: 0.2784 - precision: 0.8938 - recall: 0.9472 - true_negatives: 2327.5920 - true_positives: 2536.5256\n",
      "For batch number 332 the model has a loss of 0.2769487798213959\n",
      "\u001b[1m332/689\u001b[0m \u001b[32m━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━\u001b[0m \u001b[1m20s\u001b[0m 57ms/step - auc: 0.9551 - binary_accuracy: 0.9161 - false_negatives: 139.1084 - false_positives: 310.0904 - loss: 0.2784 - precision: 0.8938 - recall: 0.9472 - true_negatives: 2334.7771 - true_positives: 2544.0242\n",
      "For batch number 333 the model has a loss of 0.2769348621368408\n",
      "\u001b[1m333/689\u001b[0m \u001b[32m━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━\u001b[0m \u001b[1m20s\u001b[0m 57ms/step - auc: 0.9551 - binary_accuracy: 0.9161 - false_negatives: 139.5375 - false_positives: 310.9730 - loss: 0.2784 - precision: 0.8938 - recall: 0.9472 - true_negatives: 2341.9639 - true_positives: 2551.5256\n",
      "For batch number 334 the model has a loss of 0.2772725522518158\n",
      "\u001b[1m334/689\u001b[0m \u001b[32m━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━\u001b[0m \u001b[1m20s\u001b[0m 57ms/step - auc: 0.9551 - binary_accuracy: 0.9161 - false_negatives: 139.9701 - false_positives: 311.8533 - loss: 0.2784 - precision: 0.8938 - recall: 0.9472 - true_negatives: 2349.1467 - true_positives: 2559.0300\n",
      "For batch number 335 the model has a loss of 0.27745935320854187\n",
      "\u001b[1m335/689\u001b[0m \u001b[32m━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━\u001b[0m \u001b[1m20s\u001b[0m 57ms/step - auc: 0.9551 - binary_accuracy: 0.9161 - false_negatives: 140.4030 - false_positives: 312.7373 - loss: 0.2784 - precision: 0.8938 - recall: 0.9472 - true_negatives: 2356.3284 - true_positives: 2566.5312\n",
      "For batch number 336 the model has a loss of 0.27802619338035583\n",
      "\u001b[1m336/689\u001b[0m \u001b[32m━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━\u001b[0m \u001b[1m20s\u001b[0m 57ms/step - auc: 0.9551 - binary_accuracy: 0.9161 - false_negatives: 140.8333 - false_positives: 313.6220 - loss: 0.2784 - precision: 0.8938 - recall: 0.9472 - true_negatives: 2363.5059 - true_positives: 2574.0386\n",
      "For batch number 337 the model has a loss of 0.27819180488586426\n",
      "\u001b[1m337/689\u001b[0m \u001b[32m━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━\u001b[0m \u001b[1m20s\u001b[0m 57ms/step - auc: 0.9551 - binary_accuracy: 0.9161 - false_negatives: 141.2611 - false_positives: 314.5074 - loss: 0.2784 - precision: 0.8938 - recall: 0.9472 - true_negatives: 2370.6885 - true_positives: 2581.5430\n",
      "For batch number 338 the model has a loss of 0.2777089774608612\n",
      "\u001b[1m338/689\u001b[0m \u001b[32m━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━\u001b[0m \u001b[1m20s\u001b[0m 57ms/step - auc: 0.9551 - binary_accuracy: 0.9161 - false_negatives: 141.6864 - false_positives: 315.3876 - loss: 0.2784 - precision: 0.8938 - recall: 0.9472 - true_negatives: 2377.8787 - true_positives: 2589.0474\n",
      "For batch number 339 the model has a loss of 0.27806782722473145\n",
      "\u001b[1m339/689\u001b[0m \u001b[32m━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━\u001b[0m \u001b[1m20s\u001b[0m 57ms/step - auc: 0.9551 - binary_accuracy: 0.9161 - false_negatives: 142.1151 - false_positives: 316.2714 - loss: 0.2784 - precision: 0.8937 - recall: 0.9472 - true_negatives: 2385.0562 - true_positives: 2596.5576\n",
      "For batch number 340 the model has a loss of 0.2777010202407837\n",
      "\u001b[1m340/689\u001b[0m \u001b[32m━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━\u001b[0m \u001b[1m20s\u001b[0m 57ms/step - auc: 0.9551 - binary_accuracy: 0.9161 - false_negatives: 142.5412 - false_positives: 317.1559 - loss: 0.2784 - precision: 0.8937 - recall: 0.9472 - true_negatives: 2392.2236 - true_positives: 2604.0793\n",
      "For batch number 341 the model has a loss of 0.27727845311164856\n",
      "\u001b[1m341/689\u001b[0m \u001b[32m━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━\u001b[0m \u001b[1m19s\u001b[0m 57ms/step - auc: 0.9551 - binary_accuracy: 0.9161 - false_negatives: 142.9648 - false_positives: 318.0352 - loss: 0.2784 - precision: 0.8937 - recall: 0.9472 - true_negatives: 2399.3843 - true_positives: 2611.6157\n",
      "For batch number 342 the model has a loss of 0.2777380049228668\n",
      "\u001b[1m342/689\u001b[0m \u001b[32m━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━\u001b[0m \u001b[1m19s\u001b[0m 57ms/step - auc: 0.9551 - binary_accuracy: 0.9161 - false_negatives: 143.3889 - false_positives: 318.9211 - loss: 0.2784 - precision: 0.8937 - recall: 0.9472 - true_negatives: 2406.5527 - true_positives: 2619.1375\n",
      "For batch number 343 the model has a loss of 0.27894946932792664\n",
      "\u001b[1m343/689\u001b[0m \u001b[32m━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━\u001b[0m \u001b[1m19s\u001b[0m 57ms/step - auc: 0.9551 - binary_accuracy: 0.9161 - false_negatives: 143.8192 - false_positives: 319.8076 - loss: 0.2784 - precision: 0.8937 - recall: 0.9472 - true_negatives: 2413.7173 - true_positives: 2626.6560\n",
      "For batch number 344 the model has a loss of 0.2790141999721527\n",
      "\u001b[1m344/689\u001b[0m \u001b[32m━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━\u001b[0m \u001b[1m19s\u001b[0m 57ms/step - auc: 0.9551 - binary_accuracy: 0.9161 - false_negatives: 144.2471 - false_positives: 320.6977 - loss: 0.2784 - precision: 0.8937 - recall: 0.9472 - true_negatives: 2420.8896 - true_positives: 2634.1658\n",
      "For batch number 345 the model has a loss of 0.27878594398498535\n",
      "\u001b[1m345/689\u001b[0m \u001b[32m━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━\u001b[0m \u001b[1m19s\u001b[0m 57ms/step - auc: 0.9551 - binary_accuracy: 0.9161 - false_negatives: 144.6754 - false_positives: 321.5884 - loss: 0.2784 - precision: 0.8937 - recall: 0.9472 - true_negatives: 2428.0608 - true_positives: 2641.6753\n",
      "For batch number 346 the model has a loss of 0.2796953618526459\n",
      "\u001b[1m346/689\u001b[0m \u001b[32m━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━\u001b[0m \u001b[1m19s\u001b[0m 57ms/step - auc: 0.9551 - binary_accuracy: 0.9161 - false_negatives: 145.1069 - false_positives: 322.4798 - loss: 0.2784 - precision: 0.8937 - recall: 0.9472 - true_negatives: 2435.2400 - true_positives: 2649.1733\n",
      "For batch number 347 the model has a loss of 0.2800643742084503\n",
      "\u001b[1m347/689\u001b[0m \u001b[32m━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━\u001b[0m \u001b[1m19s\u001b[0m 57ms/step - auc: 0.9550 - binary_accuracy: 0.9161 - false_negatives: 145.5389 - false_positives: 323.3718 - loss: 0.2784 - precision: 0.8937 - recall: 0.9472 - true_negatives: 2442.4324 - true_positives: 2656.6570\n",
      "For batch number 348 the model has a loss of 0.2810705602169037\n",
      "\u001b[1m348/689\u001b[0m \u001b[32m━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━\u001b[0m \u001b[1m19s\u001b[0m 57ms/step - auc: 0.9550 - binary_accuracy: 0.9161 - false_negatives: 145.9741 - false_positives: 324.2644 - loss: 0.2784 - precision: 0.8937 - recall: 0.9472 - true_negatives: 2449.6350 - true_positives: 2664.1265\n",
      "For batch number 349 the model has a loss of 0.2807595431804657\n",
      "\u001b[1m349/689\u001b[0m \u001b[32m━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━\u001b[0m \u001b[1m19s\u001b[0m 57ms/step - auc: 0.9550 - binary_accuracy: 0.9161 - false_negatives: 146.4069 - false_positives: 325.1547 - loss: 0.2784 - precision: 0.8937 - recall: 0.9472 - true_negatives: 2456.8396 - true_positives: 2671.5989\n",
      "For batch number 350 the model has a loss of 0.28093889355659485\n",
      "\u001b[1m350/689\u001b[0m \u001b[32m━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━\u001b[0m \u001b[1m19s\u001b[0m 57ms/step - auc: 0.9550 - binary_accuracy: 0.9161 - false_negatives: 146.8371 - false_positives: 326.0428 - loss: 0.2784 - precision: 0.8937 - recall: 0.9472 - true_negatives: 2464.0457 - true_positives: 2679.0742\n",
      "For batch number 351 the model has a loss of 0.2811354398727417\n",
      "\u001b[1m351/689\u001b[0m \u001b[32m━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━\u001b[0m \u001b[1m19s\u001b[0m 57ms/step - auc: 0.9550 - binary_accuracy: 0.9161 - false_negatives: 147.2650 - false_positives: 326.9316 - loss: 0.2784 - precision: 0.8937 - recall: 0.9472 - true_negatives: 2471.2478 - true_positives: 2686.5557\n",
      "For batch number 352 the model has a loss of 0.2807720899581909\n",
      "\u001b[1m352/689\u001b[0m \u001b[32m━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━\u001b[0m \u001b[1m19s\u001b[0m 57ms/step - auc: 0.9550 - binary_accuracy: 0.9161 - false_negatives: 147.6903 - false_positives: 327.8182 - loss: 0.2784 - precision: 0.8937 - recall: 0.9472 - true_negatives: 2478.4573 - true_positives: 2694.0342\n",
      "For batch number 353 the model has a loss of 0.2808905243873596\n",
      "\u001b[1m353/689\u001b[0m \u001b[32m━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━\u001b[0m \u001b[1m19s\u001b[0m 57ms/step - auc: 0.9550 - binary_accuracy: 0.9161 - false_negatives: 148.1161 - false_positives: 328.6997 - loss: 0.2784 - precision: 0.8937 - recall: 0.9472 - true_negatives: 2485.6602 - true_positives: 2701.5242\n",
      "For batch number 354 the model has a loss of 0.28096508979797363\n",
      "\u001b[1m354/689\u001b[0m \u001b[32m━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━\u001b[0m \u001b[1m19s\u001b[0m 57ms/step - auc: 0.9550 - binary_accuracy: 0.9161 - false_negatives: 148.5424 - false_positives: 329.5819 - loss: 0.2784 - precision: 0.8937 - recall: 0.9472 - true_negatives: 2492.8645 - true_positives: 2709.0112\n",
      "For batch number 355 the model has a loss of 0.2806028127670288\n",
      "\u001b[1m355/689\u001b[0m \u001b[32m━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━\u001b[0m \u001b[1m19s\u001b[0m 57ms/step - auc: 0.9550 - binary_accuracy: 0.9161 - false_negatives: 148.9662 - false_positives: 330.4592 - loss: 0.2784 - precision: 0.8937 - recall: 0.9472 - true_negatives: 2500.0732 - true_positives: 2716.5015\n",
      "For batch number 356 the model has a loss of 0.2807406187057495\n",
      "\u001b[1m356/689\u001b[0m \u001b[32m━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━\u001b[0m \u001b[1m19s\u001b[0m 57ms/step - auc: 0.9550 - binary_accuracy: 0.9161 - false_negatives: 149.3876 - false_positives: 331.3371 - loss: 0.2785 - precision: 0.8937 - recall: 0.9472 - true_negatives: 2507.2810 - true_positives: 2723.9944\n",
      "For batch number 357 the model has a loss of 0.2814047336578369\n",
      "\u001b[1m357/689\u001b[0m \u001b[32m━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━\u001b[0m \u001b[1m19s\u001b[0m 57ms/step - auc: 0.9550 - binary_accuracy: 0.9161 - false_negatives: 149.8179 - false_positives: 332.2101 - loss: 0.2785 - precision: 0.8937 - recall: 0.9472 - true_negatives: 2514.4817 - true_positives: 2731.4902\n",
      "For batch number 358 the model has a loss of 0.28127753734588623\n",
      "\u001b[1m358/689\u001b[0m \u001b[32m━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━\u001b[0m \u001b[1m18s\u001b[0m 57ms/step - auc: 0.9550 - binary_accuracy: 0.9161 - false_negatives: 150.2486 - false_positives: 333.0810 - loss: 0.2785 - precision: 0.8937 - recall: 0.9472 - true_negatives: 2521.6843 - true_positives: 2738.9861\n",
      "For batch number 359 the model has a loss of 0.28127893805503845\n",
      "\u001b[1m359/689\u001b[0m \u001b[32m━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━\u001b[0m \u001b[1m18s\u001b[0m 57ms/step - auc: 0.9550 - binary_accuracy: 0.9161 - false_negatives: 150.6797 - false_positives: 333.9471 - loss: 0.2785 - precision: 0.8937 - recall: 0.9472 - true_negatives: 2528.8748 - true_positives: 2746.4985\n",
      "For batch number 360 the model has a loss of 0.28098487854003906\n",
      "\u001b[1m360/689\u001b[0m \u001b[32m━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━\u001b[0m \u001b[1m18s\u001b[0m 57ms/step - auc: 0.9550 - binary_accuracy: 0.9161 - false_negatives: 151.1111 - false_positives: 334.8083 - loss: 0.2785 - precision: 0.8937 - recall: 0.9472 - true_negatives: 2536.0640 - true_positives: 2754.0166\n",
      "For batch number 361 the model has a loss of 0.28084903955459595\n",
      "\u001b[1m361/689\u001b[0m \u001b[32m━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━\u001b[0m \u001b[1m18s\u001b[0m 57ms/step - auc: 0.9550 - binary_accuracy: 0.9161 - false_negatives: 151.5402 - false_positives: 335.6703 - loss: 0.2785 - precision: 0.8937 - recall: 0.9472 - true_negatives: 2543.2522 - true_positives: 2761.5374\n",
      "For batch number 362 the model has a loss of 0.28082576394081116\n",
      "\u001b[1m362/689\u001b[0m \u001b[32m━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━\u001b[0m \u001b[1m18s\u001b[0m 57ms/step - auc: 0.9550 - binary_accuracy: 0.9161 - false_negatives: 151.9669 - false_positives: 336.5359 - loss: 0.2785 - precision: 0.8937 - recall: 0.9472 - true_negatives: 2550.4309 - true_positives: 2769.0664\n",
      "For batch number 363 the model has a loss of 0.2808857560157776\n",
      "\u001b[1m363/689\u001b[0m \u001b[32m━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━\u001b[0m \u001b[1m18s\u001b[0m 57ms/step - auc: 0.9550 - binary_accuracy: 0.9161 - false_negatives: 152.3912 - false_positives: 337.4132 - loss: 0.2785 - precision: 0.8937 - recall: 0.9472 - true_negatives: 2557.6089 - true_positives: 2776.5867\n",
      "For batch number 364 the model has a loss of 0.28151825070381165\n",
      "\u001b[1m364/689\u001b[0m \u001b[32m━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━\u001b[0m \u001b[1m18s\u001b[0m 57ms/step - auc: 0.9550 - binary_accuracy: 0.9161 - false_negatives: 152.8159 - false_positives: 338.2939 - loss: 0.2785 - precision: 0.8937 - recall: 0.9472 - true_negatives: 2564.7830 - true_positives: 2784.1072\n",
      "For batch number 365 the model has a loss of 0.28150710463523865\n",
      "\u001b[1m365/689\u001b[0m \u001b[32m━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━\u001b[0m \u001b[1m18s\u001b[0m 57ms/step - auc: 0.9550 - binary_accuracy: 0.9161 - false_negatives: 153.2384 - false_positives: 339.1863 - loss: 0.2785 - precision: 0.8937 - recall: 0.9472 - true_negatives: 2571.9534 - true_positives: 2791.6218\n",
      "For batch number 366 the model has a loss of 0.2821151316165924\n",
      "\u001b[1m366/689\u001b[0m \u001b[32m━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━\u001b[0m \u001b[1m18s\u001b[0m 57ms/step - auc: 0.9550 - binary_accuracy: 0.9161 - false_negatives: 153.6639 - false_positives: 340.0874 - loss: 0.2785 - precision: 0.8937 - recall: 0.9472 - true_negatives: 2579.1284 - true_positives: 2799.1201\n",
      "For batch number 367 the model has a loss of 0.28172141313552856\n",
      "\u001b[1m367/689\u001b[0m \u001b[32m━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━\u001b[0m \u001b[1m18s\u001b[0m 57ms/step - auc: 0.9550 - binary_accuracy: 0.9161 - false_negatives: 154.0872 - false_positives: 340.9891 - loss: 0.2785 - precision: 0.8937 - recall: 0.9472 - true_negatives: 2586.3025 - true_positives: 2806.6213\n",
      "For batch number 368 the model has a loss of 0.28189441561698914\n",
      "\u001b[1m368/689\u001b[0m \u001b[32m━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━\u001b[0m \u001b[1m18s\u001b[0m 57ms/step - auc: 0.9550 - binary_accuracy: 0.9161 - false_negatives: 154.5081 - false_positives: 341.8967 - loss: 0.2786 - precision: 0.8937 - recall: 0.9472 - true_negatives: 2593.4702 - true_positives: 2814.1250\n",
      "For batch number 369 the model has a loss of 0.28142571449279785\n",
      "\u001b[1m369/689\u001b[0m \u001b[32m━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━\u001b[0m \u001b[1m18s\u001b[0m 57ms/step - auc: 0.9550 - binary_accuracy: 0.9162 - false_negatives: 154.9295 - false_positives: 342.7995 - loss: 0.2786 - precision: 0.8937 - recall: 0.9472 - true_negatives: 2600.6396 - true_positives: 2821.6313\n",
      "For batch number 370 the model has a loss of 0.2809334695339203\n",
      "\u001b[1m370/689\u001b[0m \u001b[32m━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━\u001b[0m \u001b[1m18s\u001b[0m 57ms/step - auc: 0.9550 - binary_accuracy: 0.9162 - false_negatives: 155.3486 - false_positives: 343.7000 - loss: 0.2786 - precision: 0.8937 - recall: 0.9472 - true_negatives: 2607.7974 - true_positives: 2829.1541\n",
      "For batch number 371 the model has a loss of 0.28098344802856445\n",
      "\u001b[1m371/689\u001b[0m \u001b[32m━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━\u001b[0m \u001b[1m18s\u001b[0m 57ms/step - auc: 0.9550 - binary_accuracy: 0.9162 - false_negatives: 155.7655 - false_positives: 344.6065 - loss: 0.2786 - precision: 0.8937 - recall: 0.9472 - true_negatives: 2614.9595 - true_positives: 2836.6685\n",
      "For batch number 372 the model has a loss of 0.28065094351768494\n",
      "\u001b[1m372/689\u001b[0m \u001b[32m━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━\u001b[0m \u001b[1m18s\u001b[0m 57ms/step - auc: 0.9550 - binary_accuracy: 0.9162 - false_negatives: 156.1801 - false_positives: 345.5107 - loss: 0.2786 - precision: 0.8937 - recall: 0.9472 - true_negatives: 2622.1182 - true_positives: 2844.1909\n",
      "For batch number 373 the model has a loss of 0.2817549407482147\n",
      "\u001b[1m373/689\u001b[0m \u001b[32m━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━\u001b[0m \u001b[1m18s\u001b[0m 57ms/step - auc: 0.9549 - binary_accuracy: 0.9162 - false_negatives: 156.5979 - false_positives: 346.4182 - loss: 0.2786 - precision: 0.8937 - recall: 0.9472 - true_negatives: 2629.2734 - true_positives: 2851.7104\n",
      "For batch number 374 the model has a loss of 0.2816789448261261\n",
      "\u001b[1m374/689\u001b[0m \u001b[32m━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━\u001b[0m \u001b[1m18s\u001b[0m 57ms/step - auc: 0.9549 - binary_accuracy: 0.9162 - false_negatives: 157.0134 - false_positives: 347.3315 - loss: 0.2786 - precision: 0.8937 - recall: 0.9472 - true_negatives: 2636.4277 - true_positives: 2859.2273\n",
      "For batch number 375 the model has a loss of 0.28163501620292664\n",
      "\u001b[1m375/689\u001b[0m \u001b[32m━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━\u001b[0m \u001b[1m18s\u001b[0m 57ms/step - auc: 0.9549 - binary_accuracy: 0.9162 - false_negatives: 157.4320 - false_positives: 348.2427 - loss: 0.2786 - precision: 0.8937 - recall: 0.9472 - true_negatives: 2643.5706 - true_positives: 2866.7546\n",
      "For batch number 376 the model has a loss of 0.28144702315330505\n",
      "\u001b[1m376/689\u001b[0m \u001b[32m━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━\u001b[0m \u001b[1m17s\u001b[0m 57ms/step - auc: 0.9549 - binary_accuracy: 0.9162 - false_negatives: 157.8484 - false_positives: 349.1543 - loss: 0.2786 - precision: 0.8937 - recall: 0.9472 - true_negatives: 2650.7102 - true_positives: 2874.2874\n",
      "For batch number 377 the model has a loss of 0.28104984760284424\n",
      "\u001b[1m377/689\u001b[0m \u001b[32m━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━\u001b[0m \u001b[1m17s\u001b[0m 57ms/step - auc: 0.9549 - binary_accuracy: 0.9162 - false_negatives: 158.2626 - false_positives: 350.0637 - loss: 0.2786 - precision: 0.8937 - recall: 0.9472 - true_negatives: 2657.8540 - true_positives: 2881.8196\n",
      "For batch number 378 the model has a loss of 0.2808791697025299\n",
      "\u001b[1m378/689\u001b[0m \u001b[32m━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━\u001b[0m \u001b[1m17s\u001b[0m 57ms/step - auc: 0.9549 - binary_accuracy: 0.9162 - false_negatives: 158.6772 - false_positives: 350.9683 - loss: 0.2786 - precision: 0.8937 - recall: 0.9472 - true_negatives: 2664.9973 - true_positives: 2889.3572\n",
      "For batch number 379 the model has a loss of 0.28077083826065063\n",
      "\u001b[1m379/689\u001b[0m \u001b[32m━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━\u001b[0m \u001b[1m17s\u001b[0m 57ms/step - auc: 0.9549 - binary_accuracy: 0.9162 - false_negatives: 159.0923 - false_positives: 351.8734 - loss: 0.2786 - precision: 0.8936 - recall: 0.9472 - true_negatives: 2672.1399 - true_positives: 2896.8945\n",
      "For batch number 380 the model has a loss of 0.28069964051246643\n",
      "\u001b[1m380/689\u001b[0m \u001b[32m━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━\u001b[0m \u001b[1m17s\u001b[0m 57ms/step - auc: 0.9549 - binary_accuracy: 0.9162 - false_negatives: 159.5053 - false_positives: 352.7816 - loss: 0.2786 - precision: 0.8936 - recall: 0.9472 - true_negatives: 2679.2815 - true_positives: 2904.4316\n",
      "For batch number 381 the model has a loss of 0.2804374694824219\n",
      "\u001b[1m381/689\u001b[0m \u001b[32m━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━\u001b[0m \u001b[1m17s\u001b[0m 57ms/step - auc: 0.9549 - binary_accuracy: 0.9162 - false_negatives: 159.9160 - false_positives: 353.6929 - loss: 0.2786 - precision: 0.8936 - recall: 0.9472 - true_negatives: 2686.4226 - true_positives: 2911.9685\n",
      "For batch number 382 the model has a loss of 0.28072473406791687\n",
      "\u001b[1m382/689\u001b[0m \u001b[32m━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━\u001b[0m \u001b[1m17s\u001b[0m 58ms/step - auc: 0.9549 - binary_accuracy: 0.9162 - false_negatives: 160.3272 - false_positives: 354.6100 - loss: 0.2786 - precision: 0.8936 - recall: 0.9472 - true_negatives: 2693.5681 - true_positives: 2919.4949\n",
      "For batch number 383 the model has a loss of 0.2803601324558258\n",
      "\u001b[1m383/689\u001b[0m \u001b[32m━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━\u001b[0m \u001b[1m17s\u001b[0m 58ms/step - auc: 0.9549 - binary_accuracy: 0.9162 - false_negatives: 160.7363 - false_positives: 355.5222 - loss: 0.2786 - precision: 0.8936 - recall: 0.9472 - true_negatives: 2700.7102 - true_positives: 2927.0312\n",
      "For batch number 384 the model has a loss of 0.2802068889141083\n",
      "\u001b[1m384/689\u001b[0m \u001b[32m━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━\u001b[0m \u001b[1m17s\u001b[0m 58ms/step - auc: 0.9549 - binary_accuracy: 0.9162 - false_negatives: 161.1432 - false_positives: 356.4427 - loss: 0.2787 - precision: 0.8936 - recall: 0.9472 - true_negatives: 2707.8567 - true_positives: 2934.5574\n",
      "For batch number 385 the model has a loss of 0.27998965978622437\n",
      "\u001b[1m385/689\u001b[0m \u001b[32m━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━\u001b[0m \u001b[1m17s\u001b[0m 58ms/step - auc: 0.9549 - binary_accuracy: 0.9162 - false_negatives: 161.5506 - false_positives: 357.3636 - loss: 0.2787 - precision: 0.8936 - recall: 0.9472 - true_negatives: 2714.9973 - true_positives: 2942.0884\n",
      "For batch number 386 the model has a loss of 0.2798202335834503\n",
      "\u001b[1m386/689\u001b[0m \u001b[32m━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━\u001b[0m \u001b[1m17s\u001b[0m 58ms/step - auc: 0.9549 - binary_accuracy: 0.9162 - false_negatives: 161.9585 - false_positives: 358.2824 - loss: 0.2787 - precision: 0.8936 - recall: 0.9472 - true_negatives: 2722.1477 - true_positives: 2949.6113\n",
      "For batch number 387 the model has a loss of 0.2794460952281952\n",
      "\u001b[1m387/689\u001b[0m \u001b[32m━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━\u001b[0m \u001b[1m17s\u001b[0m 58ms/step - auc: 0.9549 - binary_accuracy: 0.9162 - false_negatives: 162.3669 - false_positives: 359.1964 - loss: 0.2787 - precision: 0.8936 - recall: 0.9472 - true_negatives: 2729.2971 - true_positives: 2957.1396\n",
      "For batch number 388 the model has a loss of 0.27969902753829956\n",
      "\u001b[1m388/689\u001b[0m \u001b[32m━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━\u001b[0m \u001b[1m17s\u001b[0m 58ms/step - auc: 0.9549 - binary_accuracy: 0.9162 - false_negatives: 162.7758 - false_positives: 360.1108 - loss: 0.2787 - precision: 0.8936 - recall: 0.9472 - true_negatives: 2736.4509 - true_positives: 2964.6624\n",
      "For batch number 389 the model has a loss of 0.27954068779945374\n",
      "\u001b[1m389/689\u001b[0m \u001b[32m━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━\u001b[0m \u001b[1m17s\u001b[0m 58ms/step - auc: 0.9549 - binary_accuracy: 0.9162 - false_negatives: 163.1825 - false_positives: 361.0283 - loss: 0.2787 - precision: 0.8936 - recall: 0.9472 - true_negatives: 2743.6145 - true_positives: 2972.1748\n",
      "For batch number 390 the model has a loss of 0.27955371141433716\n",
      "\u001b[1m390/689\u001b[0m \u001b[32m━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━\u001b[0m \u001b[1m17s\u001b[0m 58ms/step - auc: 0.9549 - binary_accuracy: 0.9162 - false_negatives: 163.5872 - false_positives: 361.9487 - loss: 0.2787 - precision: 0.8936 - recall: 0.9473 - true_negatives: 2750.7744 - true_positives: 2979.6897\n",
      "For batch number 391 the model has a loss of 0.27920427918434143\n",
      "\u001b[1m391/689\u001b[0m \u001b[32m━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━\u001b[0m \u001b[1m17s\u001b[0m 58ms/step - auc: 0.9549 - binary_accuracy: 0.9162 - false_negatives: 163.9898 - false_positives: 362.8670 - loss: 0.2787 - precision: 0.8936 - recall: 0.9473 - true_negatives: 2757.9258 - true_positives: 2987.2173\n",
      "For batch number 392 the model has a loss of 0.2791772782802582\n",
      "\u001b[1m392/689\u001b[0m \u001b[32m━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━\u001b[0m \u001b[1m17s\u001b[0m 58ms/step - auc: 0.9549 - binary_accuracy: 0.9162 - false_negatives: 164.3929 - false_positives: 363.7806 - loss: 0.2787 - precision: 0.8936 - recall: 0.9473 - true_negatives: 2765.0715 - true_positives: 2994.7551\n",
      "For batch number 393 the model has a loss of 0.2791513502597809\n",
      "\u001b[1m393/689\u001b[0m \u001b[32m━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━\u001b[0m \u001b[1m17s\u001b[0m 58ms/step - auc: 0.9549 - binary_accuracy: 0.9162 - false_negatives: 164.7990 - false_positives: 364.6896 - loss: 0.2787 - precision: 0.8936 - recall: 0.9473 - true_negatives: 2772.2136 - true_positives: 3002.2976\n",
      "For batch number 394 the model has a loss of 0.2789941430091858\n",
      "\u001b[1m394/689\u001b[0m \u001b[32m━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━\u001b[0m \u001b[1m16s\u001b[0m 58ms/step - auc: 0.9549 - binary_accuracy: 0.9162 - false_negatives: 165.2081 - false_positives: 365.5964 - loss: 0.2787 - precision: 0.8936 - recall: 0.9473 - true_negatives: 2779.3604 - true_positives: 3009.8350\n",
      "For batch number 395 the model has a loss of 0.27929913997650146\n",
      "\u001b[1m395/689\u001b[0m \u001b[32m━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━\u001b[0m \u001b[1m16s\u001b[0m 58ms/step - auc: 0.9549 - binary_accuracy: 0.9162 - false_negatives: 165.6177 - false_positives: 366.5013 - loss: 0.2787 - precision: 0.8936 - recall: 0.9473 - true_negatives: 2786.5164 - true_positives: 3017.3645\n",
      "For batch number 396 the model has a loss of 0.2789258360862732\n",
      "\u001b[1m396/689\u001b[0m \u001b[32m━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━\u001b[0m \u001b[1m16s\u001b[0m 58ms/step - auc: 0.9549 - binary_accuracy: 0.9162 - false_negatives: 166.0253 - false_positives: 367.4015 - loss: 0.2787 - precision: 0.8936 - recall: 0.9473 - true_negatives: 2793.6743 - true_positives: 3024.8989\n",
      "For batch number 397 the model has a loss of 0.2790781557559967\n",
      "\u001b[1m397/689\u001b[0m \u001b[32m━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━\u001b[0m \u001b[1m16s\u001b[0m 58ms/step - auc: 0.9549 - binary_accuracy: 0.9162 - false_negatives: 166.4433 - false_positives: 368.2972 - loss: 0.2787 - precision: 0.8936 - recall: 0.9473 - true_negatives: 2800.8137 - true_positives: 3032.4458\n",
      "For batch number 398 the model has a loss of 0.2791239023208618\n",
      "\u001b[1m398/689\u001b[0m \u001b[32m━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━\u001b[0m \u001b[1m16s\u001b[0m 58ms/step - auc: 0.9549 - binary_accuracy: 0.9162 - false_negatives: 166.8593 - false_positives: 369.2035 - loss: 0.2787 - precision: 0.8936 - recall: 0.9473 - true_negatives: 2807.9497 - true_positives: 3039.9875\n",
      "For batch number 399 the model has a loss of 0.2791054844856262\n",
      "\u001b[1m399/689\u001b[0m \u001b[32m━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━\u001b[0m \u001b[1m16s\u001b[0m 58ms/step - auc: 0.9549 - binary_accuracy: 0.9162 - false_negatives: 167.2732 - false_positives: 370.1078 - loss: 0.2787 - precision: 0.8936 - recall: 0.9473 - true_negatives: 2815.0803 - true_positives: 3047.5388\n",
      "For batch number 400 the model has a loss of 0.27893561124801636\n",
      "\u001b[1m400/689\u001b[0m \u001b[32m━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━\u001b[0m \u001b[1m16s\u001b[0m 58ms/step - auc: 0.9549 - binary_accuracy: 0.9162 - false_negatives: 167.6875 - false_positives: 371.0125 - loss: 0.2787 - precision: 0.8936 - recall: 0.9473 - true_negatives: 2822.2075 - true_positives: 3055.0925\n",
      "For batch number 401 the model has a loss of 0.27911436557769775\n",
      "\u001b[1m401/689\u001b[0m \u001b[32m━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━\u001b[0m \u001b[1m16s\u001b[0m 58ms/step - auc: 0.9549 - binary_accuracy: 0.9162 - false_negatives: 168.0997 - false_positives: 371.9252 - loss: 0.2787 - precision: 0.8936 - recall: 0.9473 - true_negatives: 2829.3342 - true_positives: 3062.6409\n",
      "For batch number 402 the model has a loss of 0.2791319191455841\n",
      "\u001b[1m402/689\u001b[0m \u001b[32m━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━\u001b[0m \u001b[1m16s\u001b[0m 58ms/step - auc: 0.9549 - binary_accuracy: 0.9162 - false_negatives: 168.5149 - false_positives: 372.8383 - loss: 0.2787 - precision: 0.8935 - recall: 0.9473 - true_negatives: 2836.4602 - true_positives: 3070.1865\n",
      "For batch number 403 the model has a loss of 0.2796472907066345\n",
      "\u001b[1m403/689\u001b[0m \u001b[32m━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━\u001b[0m \u001b[1m16s\u001b[0m 58ms/step - auc: 0.9549 - binary_accuracy: 0.9162 - false_negatives: 168.9305 - false_positives: 373.7568 - loss: 0.2787 - precision: 0.8935 - recall: 0.9473 - true_negatives: 2843.5857 - true_positives: 3077.7271\n",
      "For batch number 404 the model has a loss of 0.27924132347106934\n",
      "\u001b[1m404/689\u001b[0m \u001b[32m━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━\u001b[0m \u001b[1m16s\u001b[0m 58ms/step - auc: 0.9549 - binary_accuracy: 0.9162 - false_negatives: 169.3441 - false_positives: 374.6708 - loss: 0.2787 - precision: 0.8935 - recall: 0.9473 - true_negatives: 2850.7129 - true_positives: 3085.2722\n",
      "For batch number 405 the model has a loss of 0.27979546785354614\n",
      "\u001b[1m405/689\u001b[0m \u001b[32m━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━\u001b[0m \u001b[1m16s\u001b[0m 58ms/step - auc: 0.9549 - binary_accuracy: 0.9162 - false_negatives: 169.7580 - false_positives: 375.6000 - loss: 0.2787 - precision: 0.8935 - recall: 0.9473 - true_negatives: 2857.8372 - true_positives: 3092.8049\n",
      "For batch number 406 the model has a loss of 0.2801609933376312\n",
      "\u001b[1m406/689\u001b[0m \u001b[32m━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━\u001b[0m \u001b[1m16s\u001b[0m 58ms/step - auc: 0.9549 - binary_accuracy: 0.9162 - false_negatives: 170.1724 - false_positives: 376.5246 - loss: 0.2787 - precision: 0.8935 - recall: 0.9473 - true_negatives: 2864.9580 - true_positives: 3100.3447\n",
      "For batch number 407 the model has a loss of 0.2806398272514343\n",
      "\u001b[1m407/689\u001b[0m \u001b[32m━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━\u001b[0m \u001b[1m16s\u001b[0m 58ms/step - auc: 0.9549 - binary_accuracy: 0.9162 - false_negatives: 170.5872 - false_positives: 377.4546 - loss: 0.2787 - precision: 0.8935 - recall: 0.9473 - true_negatives: 2872.0762 - true_positives: 3107.8821\n",
      "For batch number 408 the model has a loss of 0.2803889214992523\n",
      "\u001b[1m408/689\u001b[0m \u001b[32m━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━\u001b[0m \u001b[1m16s\u001b[0m 58ms/step - auc: 0.9549 - binary_accuracy: 0.9162 - false_negatives: 171.0000 - false_positives: 378.3848 - loss: 0.2787 - precision: 0.8935 - recall: 0.9473 - true_negatives: 2879.1960 - true_positives: 3115.4192\n",
      "For batch number 409 the model has a loss of 0.2804614007472992\n",
      "\u001b[1m409/689\u001b[0m \u001b[32m━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━\u001b[0m \u001b[1m16s\u001b[0m 58ms/step - auc: 0.9549 - binary_accuracy: 0.9162 - false_negatives: 171.4132 - false_positives: 379.3154 - loss: 0.2787 - precision: 0.8935 - recall: 0.9473 - true_negatives: 2886.3081 - true_positives: 3122.9634\n",
      "For batch number 410 the model has a loss of 0.2801416218280792\n",
      "\u001b[1m410/689\u001b[0m \u001b[32m━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━\u001b[0m \u001b[1m16s\u001b[0m 58ms/step - auc: 0.9549 - binary_accuracy: 0.9162 - false_negatives: 171.8244 - false_positives: 380.2463 - loss: 0.2787 - precision: 0.8935 - recall: 0.9473 - true_negatives: 2893.4219 - true_positives: 3130.5073\n",
      "For batch number 411 the model has a loss of 0.28001388907432556\n",
      "\u001b[1m411/689\u001b[0m \u001b[32m━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━\u001b[0m \u001b[1m16s\u001b[0m 58ms/step - auc: 0.9549 - binary_accuracy: 0.9162 - false_negatives: 172.2336 - false_positives: 381.1801 - loss: 0.2787 - precision: 0.8935 - recall: 0.9473 - true_negatives: 2900.5376 - true_positives: 3138.0486\n",
      "For batch number 412 the model has a loss of 0.2797389030456543\n",
      "\u001b[1m412/689\u001b[0m \u001b[32m━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━\u001b[0m \u001b[1m15s\u001b[0m 58ms/step - auc: 0.9549 - binary_accuracy: 0.9162 - false_negatives: 172.6432 - false_positives: 382.1117 - loss: 0.2787 - precision: 0.8935 - recall: 0.9473 - true_negatives: 2907.6553 - true_positives: 3145.5898\n",
      "For batch number 413 the model has a loss of 0.2795645296573639\n",
      "\u001b[1m413/689\u001b[0m \u001b[32m━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━\u001b[0m \u001b[1m15s\u001b[0m 58ms/step - auc: 0.9549 - binary_accuracy: 0.9162 - false_negatives: 173.0557 - false_positives: 383.0387 - loss: 0.2787 - precision: 0.8935 - recall: 0.9473 - true_negatives: 2914.7725 - true_positives: 3153.1331\n",
      "For batch number 414 the model has a loss of 0.2795816659927368\n",
      "\u001b[1m414/689\u001b[0m \u001b[32m━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━\u001b[0m \u001b[1m15s\u001b[0m 58ms/step - auc: 0.9549 - binary_accuracy: 0.9162 - false_negatives: 173.4686 - false_positives: 383.9638 - loss: 0.2787 - precision: 0.8935 - recall: 0.9473 - true_negatives: 2921.8914 - true_positives: 3160.6763\n",
      "For batch number 415 the model has a loss of 0.2794254720211029\n",
      "\u001b[1m415/689\u001b[0m \u001b[32m━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━\u001b[0m \u001b[1m15s\u001b[0m 58ms/step - auc: 0.9549 - binary_accuracy: 0.9162 - false_negatives: 173.8795 - false_positives: 384.8916 - loss: 0.2787 - precision: 0.8935 - recall: 0.9473 - true_negatives: 2929.0095 - true_positives: 3168.2192\n",
      "For batch number 416 the model has a loss of 0.2793506979942322\n",
      "\u001b[1m416/689\u001b[0m \u001b[32m━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━\u001b[0m \u001b[1m15s\u001b[0m 58ms/step - auc: 0.9549 - binary_accuracy: 0.9162 - false_negatives: 174.2909 - false_positives: 385.8197 - loss: 0.2787 - precision: 0.8935 - recall: 0.9473 - true_negatives: 2936.1274 - true_positives: 3175.7620\n",
      "For batch number 417 the model has a loss of 0.28022000193595886\n",
      "\u001b[1m417/689\u001b[0m \u001b[32m━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━\u001b[0m \u001b[1m15s\u001b[0m 58ms/step - auc: 0.9549 - binary_accuracy: 0.9162 - false_negatives: 174.7026 - false_positives: 386.7578 - loss: 0.2787 - precision: 0.8935 - recall: 0.9473 - true_negatives: 2943.2542 - true_positives: 3183.2854\n",
      "For batch number 418 the model has a loss of 0.2815561890602112\n",
      "\u001b[1m418/689\u001b[0m \u001b[32m━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━\u001b[0m \u001b[1m15s\u001b[0m 58ms/step - auc: 0.9549 - binary_accuracy: 0.9162 - false_negatives: 175.1483 - false_positives: 387.6938 - loss: 0.2787 - precision: 0.8934 - recall: 0.9473 - true_negatives: 2950.3613 - true_positives: 3190.7966\n",
      "For batch number 419 the model has a loss of 0.2819226384162903\n",
      "\u001b[1m419/689\u001b[0m \u001b[32m━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━\u001b[0m \u001b[1m15s\u001b[0m 58ms/step - auc: 0.9549 - binary_accuracy: 0.9162 - false_negatives: 175.6014 - false_positives: 388.6253 - loss: 0.2787 - precision: 0.8934 - recall: 0.9473 - true_negatives: 2957.4583 - true_positives: 3198.3149\n",
      "For batch number 420 the model has a loss of 0.28204819560050964\n",
      "\u001b[1m420/689\u001b[0m \u001b[32m━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━\u001b[0m \u001b[1m15s\u001b[0m 58ms/step - auc: 0.9549 - binary_accuracy: 0.9162 - false_negatives: 176.0524 - false_positives: 389.5619 - loss: 0.2787 - precision: 0.8934 - recall: 0.9473 - true_negatives: 2964.5547 - true_positives: 3205.8311\n",
      "For batch number 421 the model has a loss of 0.28165027499198914\n",
      "\u001b[1m421/689\u001b[0m \u001b[32m━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━\u001b[0m \u001b[1m15s\u001b[0m 58ms/step - auc: 0.9548 - binary_accuracy: 0.9162 - false_negatives: 176.5012 - false_positives: 390.4964 - loss: 0.2788 - precision: 0.8934 - recall: 0.9473 - true_negatives: 2971.6555 - true_positives: 3213.3467\n",
      "For batch number 422 the model has a loss of 0.281630277633667\n",
      "\u001b[1m422/689\u001b[0m \u001b[32m━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━\u001b[0m \u001b[1m15s\u001b[0m 58ms/step - auc: 0.9548 - binary_accuracy: 0.9162 - false_negatives: 176.9479 - false_positives: 391.4337 - loss: 0.2788 - precision: 0.8934 - recall: 0.9473 - true_negatives: 2978.7607 - true_positives: 3220.8579\n",
      "For batch number 423 the model has a loss of 0.2814061939716339\n",
      "\u001b[1m423/689\u001b[0m \u001b[32m━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━\u001b[0m \u001b[1m15s\u001b[0m 58ms/step - auc: 0.9548 - binary_accuracy: 0.9162 - false_negatives: 177.3948 - false_positives: 392.3664 - loss: 0.2788 - precision: 0.8934 - recall: 0.9473 - true_negatives: 2985.8628 - true_positives: 3228.3760\n",
      "For batch number 424 the model has a loss of 0.28206169605255127\n",
      "\u001b[1m424/689\u001b[0m \u001b[32m━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━\u001b[0m \u001b[1m15s\u001b[0m 58ms/step - auc: 0.9548 - binary_accuracy: 0.9162 - false_negatives: 177.8420 - false_positives: 393.2995 - loss: 0.2788 - precision: 0.8934 - recall: 0.9473 - true_negatives: 2992.9622 - true_positives: 3235.8962\n",
      "For batch number 425 the model has a loss of 0.2820632755756378\n",
      "\u001b[1m425/689\u001b[0m \u001b[32m━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━\u001b[0m \u001b[1m15s\u001b[0m 58ms/step - auc: 0.9548 - binary_accuracy: 0.9162 - false_negatives: 178.2894 - false_positives: 394.2282 - loss: 0.2788 - precision: 0.8934 - recall: 0.9473 - true_negatives: 3000.0635 - true_positives: 3243.4189\n",
      "For batch number 426 the model has a loss of 0.2821977436542511\n",
      "\u001b[1m426/689\u001b[0m \u001b[32m━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━\u001b[0m \u001b[1m15s\u001b[0m 58ms/step - auc: 0.9548 - binary_accuracy: 0.9162 - false_negatives: 178.7371 - false_positives: 395.1573 - loss: 0.2788 - precision: 0.8934 - recall: 0.9473 - true_negatives: 3007.1667 - true_positives: 3250.9390\n",
      "For batch number 427 the model has a loss of 0.28237342834472656\n",
      "\u001b[1m427/689\u001b[0m \u001b[32m━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━\u001b[0m \u001b[1m15s\u001b[0m 58ms/step - auc: 0.9548 - binary_accuracy: 0.9162 - false_negatives: 179.1920 - false_positives: 396.0890 - loss: 0.2788 - precision: 0.8934 - recall: 0.9473 - true_negatives: 3014.2622 - true_positives: 3258.4568\n",
      "For batch number 428 the model has a loss of 0.28337621688842773\n",
      "\u001b[1m428/689\u001b[0m \u001b[32m━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━\u001b[0m \u001b[1m15s\u001b[0m 58ms/step - auc: 0.9548 - binary_accuracy: 0.9162 - false_negatives: 179.6519 - false_positives: 397.0257 - loss: 0.2788 - precision: 0.8934 - recall: 0.9473 - true_negatives: 3021.3528 - true_positives: 3265.9697\n",
      "For batch number 429 the model has a loss of 0.2834596633911133\n",
      "\u001b[1m429/689\u001b[0m \u001b[32m━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━\u001b[0m \u001b[1m14s\u001b[0m 58ms/step - auc: 0.9548 - binary_accuracy: 0.9162 - false_negatives: 180.1142 - false_positives: 397.9650 - loss: 0.2788 - precision: 0.8934 - recall: 0.9473 - true_negatives: 3028.4358 - true_positives: 3273.4849\n",
      "For batch number 430 the model has a loss of 0.2832120358943939\n",
      "\u001b[1m430/689\u001b[0m \u001b[32m━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━\u001b[0m \u001b[1m14s\u001b[0m 58ms/step - auc: 0.9548 - binary_accuracy: 0.9161 - false_negatives: 180.5744 - false_positives: 398.9047 - loss: 0.2788 - precision: 0.8934 - recall: 0.9473 - true_negatives: 3035.5164 - true_positives: 3281.0046\n",
      "For batch number 431 the model has a loss of 0.28334903717041016\n",
      "\u001b[1m431/689\u001b[0m \u001b[32m━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━\u001b[0m \u001b[1m14s\u001b[0m 58ms/step - auc: 0.9548 - binary_accuracy: 0.9161 - false_negatives: 181.0348 - false_positives: 399.8492 - loss: 0.2788 - precision: 0.8934 - recall: 0.9473 - true_negatives: 3042.5940 - true_positives: 3288.5220\n",
      "For batch number 432 the model has a loss of 0.28424641489982605\n",
      "\u001b[1m432/689\u001b[0m \u001b[32m━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━\u001b[0m \u001b[1m14s\u001b[0m 58ms/step - auc: 0.9548 - binary_accuracy: 0.9161 - false_negatives: 181.5000 - false_positives: 400.7917 - loss: 0.2789 - precision: 0.8934 - recall: 0.9472 - true_negatives: 3049.6643 - true_positives: 3296.0439\n",
      "For batch number 433 the model has a loss of 0.2845595180988312\n",
      "\u001b[1m433/689\u001b[0m \u001b[32m━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━\u001b[0m \u001b[1m14s\u001b[0m 58ms/step - auc: 0.9548 - binary_accuracy: 0.9161 - false_negatives: 181.9654 - false_positives: 401.7344 - loss: 0.2789 - precision: 0.8933 - recall: 0.9472 - true_negatives: 3056.7322 - true_positives: 3303.5681\n",
      "For batch number 434 the model has a loss of 0.2845984399318695\n",
      "\u001b[1m434/689\u001b[0m \u001b[32m━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━\u001b[0m \u001b[1m14s\u001b[0m 58ms/step - auc: 0.9548 - binary_accuracy: 0.9161 - false_negatives: 182.4355 - false_positives: 402.6774 - loss: 0.2789 - precision: 0.8933 - recall: 0.9472 - true_negatives: 3063.8018 - true_positives: 3311.0852\n",
      "For batch number 435 the model has a loss of 0.2845311462879181\n",
      "\u001b[1m435/689\u001b[0m \u001b[32m━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━\u001b[0m \u001b[1m14s\u001b[0m 58ms/step - auc: 0.9548 - binary_accuracy: 0.9161 - false_negatives: 182.9034 - false_positives: 403.6276 - loss: 0.2789 - precision: 0.8933 - recall: 0.9472 - true_negatives: 3070.8689 - true_positives: 3318.6001\n",
      "For batch number 436 the model has a loss of 0.28438955545425415\n",
      "\u001b[1m436/689\u001b[0m \u001b[32m━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━\u001b[0m \u001b[1m14s\u001b[0m 58ms/step - auc: 0.9548 - binary_accuracy: 0.9161 - false_negatives: 183.3716 - false_positives: 404.5757 - loss: 0.2789 - precision: 0.8933 - recall: 0.9472 - true_negatives: 3077.9336 - true_positives: 3326.1194\n",
      "For batch number 437 the model has a loss of 0.2841866612434387\n",
      "\u001b[1m437/689\u001b[0m \u001b[32m━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━\u001b[0m \u001b[1m14s\u001b[0m 58ms/step - auc: 0.9548 - binary_accuracy: 0.9161 - false_negatives: 183.8398 - false_positives: 405.5263 - loss: 0.2789 - precision: 0.8933 - recall: 0.9472 - true_negatives: 3084.9978 - true_positives: 3333.6362\n",
      "For batch number 438 the model has a loss of 0.28394216299057007\n",
      "\u001b[1m438/689\u001b[0m \u001b[32m━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━\u001b[0m \u001b[1m14s\u001b[0m 58ms/step - auc: 0.9548 - binary_accuracy: 0.9161 - false_negatives: 184.3059 - false_positives: 406.4795 - loss: 0.2789 - precision: 0.8933 - recall: 0.9472 - true_negatives: 3092.0525 - true_positives: 3341.1621\n",
      "For batch number 439 the model has a loss of 0.2837018370628357\n",
      "\u001b[1m439/689\u001b[0m \u001b[32m━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━\u001b[0m \u001b[1m14s\u001b[0m 58ms/step - auc: 0.9548 - binary_accuracy: 0.9161 - false_negatives: 184.7722 - false_positives: 407.4328 - loss: 0.2789 - precision: 0.8933 - recall: 0.9472 - true_negatives: 3099.1116 - true_positives: 3348.6833\n",
      "For batch number 440 the model has a loss of 0.28368517756462097\n",
      "\u001b[1m440/689\u001b[0m \u001b[32m━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━\u001b[0m \u001b[1m14s\u001b[0m 58ms/step - auc: 0.9548 - binary_accuracy: 0.9161 - false_negatives: 185.2364 - false_positives: 408.3909 - loss: 0.2790 - precision: 0.8933 - recall: 0.9472 - true_negatives: 3106.1750 - true_positives: 3356.1978\n",
      "For batch number 441 the model has a loss of 0.2831707000732422\n",
      "\u001b[1m441/689\u001b[0m \u001b[32m━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━\u001b[0m \u001b[1m14s\u001b[0m 58ms/step - auc: 0.9548 - binary_accuracy: 0.9161 - false_negatives: 185.6984 - false_positives: 409.3447 - loss: 0.2790 - precision: 0.8933 - recall: 0.9472 - true_negatives: 3113.2380 - true_positives: 3363.7188\n",
      "For batch number 442 the model has a loss of 0.28288283944129944\n",
      "\u001b[1m442/689\u001b[0m \u001b[32m━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━\u001b[0m \u001b[1m14s\u001b[0m 58ms/step - auc: 0.9548 - binary_accuracy: 0.9161 - false_negatives: 186.1584 - false_positives: 410.2964 - loss: 0.2790 - precision: 0.8933 - recall: 0.9472 - true_negatives: 3120.3032 - true_positives: 3371.2422\n",
      "For batch number 443 the model has a loss of 0.2829663157463074\n",
      "\u001b[1m443/689\u001b[0m \u001b[32m━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━\u001b[0m \u001b[1m14s\u001b[0m 58ms/step - auc: 0.9548 - binary_accuracy: 0.9161 - false_negatives: 186.6163 - false_positives: 411.2528 - loss: 0.2790 - precision: 0.8933 - recall: 0.9472 - true_negatives: 3127.3679 - true_positives: 3378.7629\n",
      "For batch number 444 the model has a loss of 0.2828007936477661\n",
      "\u001b[1m444/689\u001b[0m \u001b[32m━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━\u001b[0m \u001b[1m14s\u001b[0m 58ms/step - auc: 0.9548 - binary_accuracy: 0.9161 - false_negatives: 187.0721 - false_positives: 412.2117 - loss: 0.2790 - precision: 0.8933 - recall: 0.9472 - true_negatives: 3134.4302 - true_positives: 3386.2861\n",
      "For batch number 445 the model has a loss of 0.28261494636535645\n",
      "\u001b[1m445/689\u001b[0m \u001b[32m━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━\u001b[0m \u001b[1m14s\u001b[0m 58ms/step - auc: 0.9548 - binary_accuracy: 0.9161 - false_negatives: 187.5258 - false_positives: 413.1708 - loss: 0.2790 - precision: 0.8932 - recall: 0.9472 - true_negatives: 3141.4988 - true_positives: 3393.8044\n",
      "For batch number 446 the model has a loss of 0.28228771686553955\n",
      "\u001b[1m446/689\u001b[0m \u001b[32m━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━\u001b[0m \u001b[1m14s\u001b[0m 58ms/step - auc: 0.9548 - binary_accuracy: 0.9161 - false_negatives: 187.9776 - false_positives: 414.1278 - loss: 0.2790 - precision: 0.8932 - recall: 0.9472 - true_negatives: 3148.5674 - true_positives: 3401.3274\n",
      "For batch number 447 the model has a loss of 0.2822117209434509\n",
      "\u001b[1m447/689\u001b[0m \u001b[32m━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━\u001b[0m \u001b[1m13s\u001b[0m 58ms/step - auc: 0.9548 - binary_accuracy: 0.9161 - false_negatives: 188.4295 - false_positives: 415.0828 - loss: 0.2790 - precision: 0.8932 - recall: 0.9472 - true_negatives: 3155.6377 - true_positives: 3408.8501\n",
      "For batch number 448 the model has a loss of 0.282134473323822\n",
      "\u001b[1m448/689\u001b[0m \u001b[32m━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━\u001b[0m \u001b[1m13s\u001b[0m 58ms/step - auc: 0.9548 - binary_accuracy: 0.9161 - false_negatives: 188.8817 - false_positives: 416.0402 - loss: 0.2790 - precision: 0.8932 - recall: 0.9472 - true_negatives: 3162.7009 - true_positives: 3416.3772\n",
      "For batch number 449 the model has a loss of 0.2816595435142517\n",
      "\u001b[1m449/689\u001b[0m \u001b[32m━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━\u001b[0m \u001b[1m13s\u001b[0m 58ms/step - auc: 0.9547 - binary_accuracy: 0.9160 - false_negatives: 189.3318 - false_positives: 416.9933 - loss: 0.2790 - precision: 0.8932 - recall: 0.9472 - true_negatives: 3169.7661 - true_positives: 3423.9087\n",
      "For batch number 450 the model has a loss of 0.28172123432159424\n",
      "\u001b[1m450/689\u001b[0m \u001b[32m━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━\u001b[0m \u001b[1m13s\u001b[0m 58ms/step - auc: 0.9547 - binary_accuracy: 0.9160 - false_negatives: 189.7822 - false_positives: 417.9489 - loss: 0.2790 - precision: 0.8932 - recall: 0.9472 - true_negatives: 3176.8333 - true_positives: 3431.4355\n",
      "For batch number 451 the model has a loss of 0.28183799982070923\n",
      "\u001b[1m451/689\u001b[0m \u001b[32m━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━\u001b[0m \u001b[1m13s\u001b[0m 58ms/step - auc: 0.9547 - binary_accuracy: 0.9160 - false_negatives: 190.2306 - false_positives: 418.9069 - loss: 0.2790 - precision: 0.8932 - recall: 0.9472 - true_negatives: 3183.9070 - true_positives: 3438.9556\n",
      "For batch number 452 the model has a loss of 0.2816025912761688\n",
      "\u001b[1m452/689\u001b[0m \u001b[32m━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━\u001b[0m \u001b[1m13s\u001b[0m 58ms/step - auc: 0.9547 - binary_accuracy: 0.9160 - false_negatives: 190.6792 - false_positives: 419.8628 - loss: 0.2790 - precision: 0.8932 - recall: 0.9471 - true_negatives: 3190.9824 - true_positives: 3446.4756\n",
      "For batch number 453 the model has a loss of 0.28124091029167175\n",
      "\u001b[1m453/689\u001b[0m \u001b[32m━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━\u001b[0m \u001b[1m13s\u001b[0m 58ms/step - auc: 0.9547 - binary_accuracy: 0.9160 - false_negatives: 191.1258 - false_positives: 420.8146 - loss: 0.2790 - precision: 0.8932 - recall: 0.9471 - true_negatives: 3198.0508 - true_positives: 3454.0088\n",
      "For batch number 454 the model has a loss of 0.28125861287117004\n",
      "\u001b[1m454/689\u001b[0m \u001b[32m━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━\u001b[0m \u001b[1m13s\u001b[0m 58ms/step - auc: 0.9547 - binary_accuracy: 0.9160 - false_negatives: 191.5727 - false_positives: 421.7643 - loss: 0.2790 - precision: 0.8932 - recall: 0.9471 - true_negatives: 3205.1255 - true_positives: 3461.5374\n",
      "For batch number 455 the model has a loss of 0.2811681628227234\n",
      "\u001b[1m455/689\u001b[0m \u001b[32m━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━\u001b[0m \u001b[1m13s\u001b[0m 58ms/step - auc: 0.9547 - binary_accuracy: 0.9160 - false_negatives: 192.0242 - false_positives: 422.7121 - loss: 0.2791 - precision: 0.8932 - recall: 0.9471 - true_negatives: 3212.1868 - true_positives: 3469.0769\n",
      "For batch number 456 the model has a loss of 0.28075653314590454\n",
      "\u001b[1m456/689\u001b[0m \u001b[32m━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━\u001b[0m \u001b[1m13s\u001b[0m 58ms/step - auc: 0.9547 - binary_accuracy: 0.9160 - false_negatives: 192.4737 - false_positives: 423.6579 - loss: 0.2791 - precision: 0.8932 - recall: 0.9471 - true_negatives: 3219.2456 - true_positives: 3476.6228\n",
      "For batch number 457 the model has a loss of 0.28065529465675354\n",
      "\u001b[1m457/689\u001b[0m \u001b[32m━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━\u001b[0m \u001b[1m13s\u001b[0m 58ms/step - auc: 0.9547 - binary_accuracy: 0.9160 - false_negatives: 192.9212 - false_positives: 424.6061 - loss: 0.2791 - precision: 0.8931 - recall: 0.9471 - true_negatives: 3226.3086 - true_positives: 3484.1641\n",
      "For batch number 458 the model has a loss of 0.28079769015312195\n",
      "\u001b[1m458/689\u001b[0m \u001b[32m━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━\u001b[0m \u001b[1m13s\u001b[0m 58ms/step - auc: 0.9547 - binary_accuracy: 0.9160 - false_negatives: 193.3712 - false_positives: 425.5524 - loss: 0.2791 - precision: 0.8931 - recall: 0.9471 - true_negatives: 3233.3689 - true_positives: 3491.7075\n",
      "For batch number 459 the model has a loss of 0.2809615135192871\n",
      "\u001b[1m459/689\u001b[0m \u001b[32m━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━\u001b[0m \u001b[1m13s\u001b[0m 58ms/step - auc: 0.9547 - binary_accuracy: 0.9160 - false_negatives: 193.8192 - false_positives: 426.5011 - loss: 0.2791 - precision: 0.8931 - recall: 0.9471 - true_negatives: 3240.4358 - true_positives: 3499.2439\n",
      "For batch number 460 the model has a loss of 0.28071117401123047\n",
      "\u001b[1m460/689\u001b[0m \u001b[32m━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━\u001b[0m \u001b[1m13s\u001b[0m 58ms/step - auc: 0.9547 - binary_accuracy: 0.9160 - false_negatives: 194.2696 - false_positives: 427.4500 - loss: 0.2791 - precision: 0.8931 - recall: 0.9471 - true_negatives: 3247.5088 - true_positives: 3506.7717\n",
      "For batch number 461 the model has a loss of 0.2804124653339386\n",
      "\u001b[1m461/689\u001b[0m \u001b[32m━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━\u001b[0m \u001b[1m13s\u001b[0m 58ms/step - auc: 0.9547 - binary_accuracy: 0.9160 - false_negatives: 194.7180 - false_positives: 428.3970 - loss: 0.2791 - precision: 0.8931 - recall: 0.9471 - true_negatives: 3254.5857 - true_positives: 3514.2993\n",
      "For batch number 462 the model has a loss of 0.27998068928718567\n",
      "\u001b[1m462/689\u001b[0m \u001b[32m━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━\u001b[0m \u001b[1m13s\u001b[0m 58ms/step - auc: 0.9547 - binary_accuracy: 0.9160 - false_negatives: 195.1645 - false_positives: 429.3420 - loss: 0.2791 - precision: 0.8931 - recall: 0.9471 - true_negatives: 3261.6646 - true_positives: 3521.8291\n",
      "For batch number 463 the model has a loss of 0.2798982858657837\n",
      "\u001b[1m463/689\u001b[0m \u001b[32m━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━\u001b[0m \u001b[1m13s\u001b[0m 58ms/step - auc: 0.9547 - binary_accuracy: 0.9160 - false_negatives: 195.6091 - false_positives: 430.2873 - loss: 0.2791 - precision: 0.8931 - recall: 0.9471 - true_negatives: 3268.7451 - true_positives: 3529.3586\n",
      "For batch number 464 the model has a loss of 0.27976861596107483\n",
      "\u001b[1m464/689\u001b[0m \u001b[32m━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━\u001b[0m \u001b[1m12s\u001b[0m 58ms/step - auc: 0.9547 - binary_accuracy: 0.9160 - false_negatives: 196.0539 - false_positives: 431.2306 - loss: 0.2791 - precision: 0.8931 - recall: 0.9471 - true_negatives: 3275.8276 - true_positives: 3536.8879\n",
      "For batch number 465 the model has a loss of 0.2798260450363159\n",
      "\u001b[1m465/689\u001b[0m \u001b[32m━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━\u001b[0m \u001b[1m12s\u001b[0m 58ms/step - auc: 0.9547 - binary_accuracy: 0.9160 - false_negatives: 196.4989 - false_positives: 432.1721 - loss: 0.2791 - precision: 0.8931 - recall: 0.9471 - true_negatives: 3282.9053 - true_positives: 3544.4236\n",
      "For batch number 466 the model has a loss of 0.27957645058631897\n",
      "\u001b[1m466/689\u001b[0m \u001b[32m━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━\u001b[0m \u001b[1m12s\u001b[0m 58ms/step - auc: 0.9547 - binary_accuracy: 0.9160 - false_negatives: 196.9442 - false_positives: 433.1094 - loss: 0.2791 - precision: 0.8931 - recall: 0.9471 - true_negatives: 3289.9829 - true_positives: 3551.9636\n",
      "For batch number 467 the model has a loss of 0.2795441150665283\n",
      "\u001b[1m467/689\u001b[0m \u001b[32m━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━\u001b[0m \u001b[1m12s\u001b[0m 58ms/step - auc: 0.9547 - binary_accuracy: 0.9160 - false_negatives: 197.3876 - false_positives: 434.0493 - loss: 0.2791 - precision: 0.8931 - recall: 0.9471 - true_negatives: 3297.0642 - true_positives: 3559.4990\n",
      "For batch number 468 the model has a loss of 0.27975523471832275\n",
      "\u001b[1m468/689\u001b[0m \u001b[32m━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━\u001b[0m \u001b[1m12s\u001b[0m 58ms/step - auc: 0.9547 - binary_accuracy: 0.9160 - false_negatives: 197.8312 - false_positives: 434.9872 - loss: 0.2791 - precision: 0.8931 - recall: 0.9471 - true_negatives: 3304.1497 - true_positives: 3567.0320\n",
      "For batch number 469 the model has a loss of 0.2795528769493103\n",
      "\u001b[1m469/689\u001b[0m \u001b[32m━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━\u001b[0m \u001b[1m12s\u001b[0m 58ms/step - auc: 0.9547 - binary_accuracy: 0.9160 - false_negatives: 198.2729 - false_positives: 435.9275 - loss: 0.2791 - precision: 0.8931 - recall: 0.9471 - true_negatives: 3311.2366 - true_positives: 3574.5630\n",
      "For batch number 470 the model has a loss of 0.27937832474708557\n",
      "\u001b[1m470/689\u001b[0m \u001b[32m━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━\u001b[0m \u001b[1m12s\u001b[0m 58ms/step - auc: 0.9547 - binary_accuracy: 0.9160 - false_negatives: 198.7128 - false_positives: 436.8681 - loss: 0.2791 - precision: 0.8931 - recall: 0.9471 - true_negatives: 3318.3213 - true_positives: 3582.0979\n",
      "For batch number 471 the model has a loss of 0.2794404923915863\n",
      "\u001b[1m471/689\u001b[0m \u001b[32m━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━\u001b[0m \u001b[1m12s\u001b[0m 58ms/step - auc: 0.9547 - binary_accuracy: 0.9160 - false_negatives: 199.1507 - false_positives: 437.8110 - loss: 0.2791 - precision: 0.8931 - recall: 0.9471 - true_negatives: 3325.4077 - true_positives: 3589.6306\n",
      "For batch number 472 the model has a loss of 0.27950388193130493\n",
      "\u001b[1m472/689\u001b[0m \u001b[32m━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━\u001b[0m \u001b[1m12s\u001b[0m 58ms/step - auc: 0.9547 - binary_accuracy: 0.9160 - false_negatives: 199.5869 - false_positives: 438.7542 - loss: 0.2791 - precision: 0.8930 - recall: 0.9471 - true_negatives: 3332.4915 - true_positives: 3597.1675\n",
      "For batch number 473 the model has a loss of 0.27919575572013855\n",
      "\u001b[1m473/689\u001b[0m \u001b[32m━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━\u001b[0m \u001b[1m12s\u001b[0m 58ms/step - auc: 0.9547 - binary_accuracy: 0.9160 - false_negatives: 200.0211 - false_positives: 439.6977 - loss: 0.2791 - precision: 0.8930 - recall: 0.9471 - true_negatives: 3339.5793 - true_positives: 3604.7019\n",
      "For batch number 474 the model has a loss of 0.2789141833782196\n",
      "\u001b[1m474/689\u001b[0m \u001b[32m━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━\u001b[0m \u001b[1m12s\u001b[0m 58ms/step - auc: 0.9547 - binary_accuracy: 0.9160 - false_negatives: 200.4536 - false_positives: 440.6371 - loss: 0.2791 - precision: 0.8930 - recall: 0.9471 - true_negatives: 3346.6731 - true_positives: 3612.2363\n",
      "For batch number 475 the model has a loss of 0.27858784794807434\n",
      "\u001b[1m475/689\u001b[0m \u001b[32m━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━\u001b[0m \u001b[1m12s\u001b[0m 58ms/step - auc: 0.9547 - binary_accuracy: 0.9160 - false_negatives: 200.8863 - false_positives: 441.5726 - loss: 0.2791 - precision: 0.8930 - recall: 0.9471 - true_negatives: 3353.7664 - true_positives: 3619.7747\n",
      "For batch number 476 the model has a loss of 0.2782230079174042\n",
      "\u001b[1m476/689\u001b[0m \u001b[32m━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━\u001b[0m \u001b[1m12s\u001b[0m 58ms/step - auc: 0.9547 - binary_accuracy: 0.9160 - false_negatives: 201.3172 - false_positives: 442.5063 - loss: 0.2791 - precision: 0.8930 - recall: 0.9471 - true_negatives: 3360.8591 - true_positives: 3627.3171\n",
      "For batch number 477 the model has a loss of 0.27847814559936523\n",
      "\u001b[1m477/689\u001b[0m \u001b[32m━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━\u001b[0m \u001b[1m12s\u001b[0m 58ms/step - auc: 0.9547 - binary_accuracy: 0.9160 - false_negatives: 201.7484 - false_positives: 443.4424 - loss: 0.2791 - precision: 0.8930 - recall: 0.9471 - true_negatives: 3367.9561 - true_positives: 3634.8533\n",
      "For batch number 478 the model has a loss of 0.2787351906299591\n",
      "\u001b[1m478/689\u001b[0m \u001b[32m━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━\u001b[0m \u001b[1m12s\u001b[0m 58ms/step - auc: 0.9547 - binary_accuracy: 0.9160 - false_negatives: 202.1841 - false_positives: 444.3745 - loss: 0.2791 - precision: 0.8930 - recall: 0.9471 - true_negatives: 3375.0481 - true_positives: 3642.3933\n",
      "For batch number 479 the model has a loss of 0.2786119282245636\n",
      "\u001b[1m479/689\u001b[0m \u001b[32m━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━\u001b[0m \u001b[1m12s\u001b[0m 58ms/step - auc: 0.9547 - binary_accuracy: 0.9160 - false_negatives: 202.6200 - false_positives: 445.3069 - loss: 0.2791 - precision: 0.8930 - recall: 0.9471 - true_negatives: 3382.1504 - true_positives: 3649.9229\n",
      "For batch number 480 the model has a loss of 0.27829229831695557\n",
      "\u001b[1m480/689\u001b[0m \u001b[32m━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━\u001b[0m \u001b[1m12s\u001b[0m 58ms/step - auc: 0.9547 - binary_accuracy: 0.9159 - false_negatives: 203.0562 - false_positives: 446.2375 - loss: 0.2791 - precision: 0.8930 - recall: 0.9471 - true_negatives: 3389.2583 - true_positives: 3657.4480\n",
      "For batch number 481 the model has a loss of 0.2779695391654968\n",
      "\u001b[1m481/689\u001b[0m \u001b[32m━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━\u001b[0m \u001b[1m11s\u001b[0m 58ms/step - auc: 0.9547 - binary_accuracy: 0.9159 - false_negatives: 203.4906 - false_positives: 447.1642 - loss: 0.2791 - precision: 0.8930 - recall: 0.9471 - true_negatives: 3396.3638 - true_positives: 3664.9812\n",
      "For batch number 482 the model has a loss of 0.2779356837272644\n",
      "\u001b[1m482/689\u001b[0m \u001b[32m━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━\u001b[0m \u001b[1m11s\u001b[0m 58ms/step - auc: 0.9547 - binary_accuracy: 0.9159 - false_negatives: 203.9232 - false_positives: 448.0934 - loss: 0.2791 - precision: 0.8930 - recall: 0.9471 - true_negatives: 3403.4690 - true_positives: 3672.5144\n",
      "For batch number 483 the model has a loss of 0.2779328525066376\n",
      "\u001b[1m483/689\u001b[0m \u001b[32m━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━\u001b[0m \u001b[1m11s\u001b[0m 58ms/step - auc: 0.9547 - binary_accuracy: 0.9159 - false_negatives: 204.3582 - false_positives: 449.0207 - loss: 0.2791 - precision: 0.8930 - recall: 0.9471 - true_negatives: 3410.5674 - true_positives: 3680.0537\n",
      "For batch number 484 the model has a loss of 0.2777796685695648\n",
      "\u001b[1m484/689\u001b[0m \u001b[32m━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━\u001b[0m \u001b[1m11s\u001b[0m 58ms/step - auc: 0.9547 - binary_accuracy: 0.9159 - false_negatives: 204.7913 - false_positives: 449.9483 - loss: 0.2791 - precision: 0.8930 - recall: 0.9471 - true_negatives: 3417.6653 - true_positives: 3687.5950\n",
      "For batch number 485 the model has a loss of 0.2774643003940582\n",
      "\u001b[1m485/689\u001b[0m \u001b[32m━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━\u001b[0m \u001b[1m11s\u001b[0m 58ms/step - auc: 0.9547 - binary_accuracy: 0.9159 - false_negatives: 205.2227 - false_positives: 450.8763 - loss: 0.2791 - precision: 0.8930 - recall: 0.9471 - true_negatives: 3424.7690 - true_positives: 3695.1321\n",
      "For batch number 486 the model has a loss of 0.2781148850917816\n",
      "\u001b[1m486/689\u001b[0m \u001b[32m━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━\u001b[0m \u001b[1m11s\u001b[0m 58ms/step - auc: 0.9547 - binary_accuracy: 0.9159 - false_negatives: 205.6564 - false_positives: 451.8004 - loss: 0.2791 - precision: 0.8930 - recall: 0.9471 - true_negatives: 3431.8765 - true_positives: 3702.6667\n",
      "For batch number 487 the model has a loss of 0.2785196304321289\n",
      "\u001b[1m487/689\u001b[0m \u001b[32m━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━\u001b[0m \u001b[1m11s\u001b[0m 58ms/step - auc: 0.9547 - binary_accuracy: 0.9159 - false_negatives: 206.0924 - false_positives: 452.7228 - loss: 0.2791 - precision: 0.8930 - recall: 0.9471 - true_negatives: 3438.9917 - true_positives: 3710.1931\n",
      "For batch number 488 the model has a loss of 0.2785896956920624\n",
      "\u001b[1m488/689\u001b[0m \u001b[32m━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━\u001b[0m \u001b[1m11s\u001b[0m 58ms/step - auc: 0.9547 - binary_accuracy: 0.9159 - false_negatives: 206.5307 - false_positives: 453.6414 - loss: 0.2791 - precision: 0.8930 - recall: 0.9471 - true_negatives: 3446.1003 - true_positives: 3717.7275\n",
      "For batch number 489 the model has a loss of 0.2784268260002136\n",
      "\u001b[1m489/689\u001b[0m \u001b[32m━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━\u001b[0m \u001b[1m11s\u001b[0m 58ms/step - auc: 0.9547 - binary_accuracy: 0.9159 - false_negatives: 206.9693 - false_positives: 454.5583 - loss: 0.2791 - precision: 0.8930 - recall: 0.9471 - true_negatives: 3453.2046 - true_positives: 3725.2678\n",
      "For batch number 490 the model has a loss of 0.27845054864883423\n",
      "\u001b[1m490/689\u001b[0m \u001b[32m━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━\u001b[0m \u001b[1m11s\u001b[0m 58ms/step - auc: 0.9547 - binary_accuracy: 0.9159 - false_negatives: 207.4082 - false_positives: 455.4735 - loss: 0.2791 - precision: 0.8930 - recall: 0.9471 - true_negatives: 3460.3020 - true_positives: 3732.8164\n",
      "For batch number 491 the model has a loss of 0.27861014008522034\n",
      "\u001b[1m491/689\u001b[0m \u001b[32m━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━\u001b[0m \u001b[1m11s\u001b[0m 58ms/step - auc: 0.9547 - binary_accuracy: 0.9159 - false_negatives: 207.8493 - false_positives: 456.3911 - loss: 0.2791 - precision: 0.8930 - recall: 0.9471 - true_negatives: 3467.3972 - true_positives: 3740.3625\n",
      "For batch number 492 the model has a loss of 0.27868521213531494\n",
      "\u001b[1m492/689\u001b[0m \u001b[32m━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━\u001b[0m \u001b[1m11s\u001b[0m 58ms/step - auc: 0.9547 - binary_accuracy: 0.9159 - false_negatives: 208.2906 - false_positives: 457.3110 - loss: 0.2791 - precision: 0.8930 - recall: 0.9471 - true_negatives: 3474.4919 - true_positives: 3747.9065\n",
      "For batch number 493 the model has a loss of 0.279155969619751\n",
      "\u001b[1m493/689\u001b[0m \u001b[32m━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━\u001b[0m \u001b[1m11s\u001b[0m 58ms/step - auc: 0.9547 - binary_accuracy: 0.9159 - false_negatives: 208.7323 - false_positives: 458.2312 - loss: 0.2791 - precision: 0.8930 - recall: 0.9471 - true_negatives: 3481.5820 - true_positives: 3755.4543\n",
      "For batch number 494 the model has a loss of 0.27911821007728577\n",
      "\u001b[1m494/689\u001b[0m \u001b[32m━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━\u001b[0m \u001b[1m11s\u001b[0m 58ms/step - auc: 0.9547 - binary_accuracy: 0.9159 - false_negatives: 209.1741 - false_positives: 459.1518 - loss: 0.2791 - precision: 0.8930 - recall: 0.9471 - true_negatives: 3488.6821 - true_positives: 3762.9919\n",
      "For batch number 495 the model has a loss of 0.27928873896598816\n",
      "\u001b[1m495/689\u001b[0m \u001b[32m━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━\u001b[0m \u001b[1m11s\u001b[0m 58ms/step - auc: 0.9547 - binary_accuracy: 0.9159 - false_negatives: 209.6162 - false_positives: 460.0727 - loss: 0.2791 - precision: 0.8929 - recall: 0.9471 - true_negatives: 3495.7817 - true_positives: 3770.5293\n",
      "For batch number 496 the model has a loss of 0.27924951910972595\n",
      "\u001b[1m496/689\u001b[0m \u001b[32m━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━\u001b[0m \u001b[1m11s\u001b[0m 58ms/step - auc: 0.9547 - binary_accuracy: 0.9159 - false_negatives: 210.0565 - false_positives: 460.9960 - loss: 0.2791 - precision: 0.8929 - recall: 0.9471 - true_negatives: 3502.8850 - true_positives: 3778.0625\n",
      "For batch number 497 the model has a loss of 0.27962303161621094\n",
      "\u001b[1m497/689\u001b[0m \u001b[32m━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━\u001b[0m \u001b[1m11s\u001b[0m 58ms/step - auc: 0.9547 - binary_accuracy: 0.9159 - false_negatives: 210.4970 - false_positives: 461.9195 - loss: 0.2791 - precision: 0.8929 - recall: 0.9471 - true_negatives: 3509.9919 - true_positives: 3785.5916\n",
      "For batch number 498 the model has a loss of 0.2796003222465515\n",
      "\u001b[1m498/689\u001b[0m \u001b[32m━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━\u001b[0m \u001b[1m11s\u001b[0m 58ms/step - auc: 0.9547 - binary_accuracy: 0.9159 - false_negatives: 210.9398 - false_positives: 462.8394 - loss: 0.2791 - precision: 0.8929 - recall: 0.9471 - true_negatives: 3517.0984 - true_positives: 3793.1226\n",
      "For batch number 499 the model has a loss of 0.27944955229759216\n",
      "\u001b[1m499/689\u001b[0m \u001b[32m━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━\u001b[0m \u001b[1m10s\u001b[0m 58ms/step - auc: 0.9547 - binary_accuracy: 0.9159 - false_negatives: 211.3828 - false_positives: 463.7595 - loss: 0.2791 - precision: 0.8929 - recall: 0.9471 - true_negatives: 3524.2063 - true_positives: 3800.6514\n",
      "For batch number 500 the model has a loss of 0.2791154980659485\n",
      "\u001b[1m500/689\u001b[0m \u001b[32m━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━\u001b[0m \u001b[1m10s\u001b[0m 58ms/step - auc: 0.9547 - binary_accuracy: 0.9159 - false_negatives: 211.8240 - false_positives: 464.6760 - loss: 0.2791 - precision: 0.8929 - recall: 0.9470 - true_negatives: 3531.3120 - true_positives: 3808.1880\n",
      "For batch number 501 the model has a loss of 0.27871373295783997\n",
      "\u001b[1m501/689\u001b[0m \u001b[32m━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━\u001b[0m \u001b[1m10s\u001b[0m 58ms/step - auc: 0.9547 - binary_accuracy: 0.9159 - false_negatives: 212.2635 - false_positives: 465.5908 - loss: 0.2791 - precision: 0.8929 - recall: 0.9470 - true_negatives: 3538.4172 - true_positives: 3815.7285\n",
      "For batch number 502 the model has a loss of 0.2786562144756317\n",
      "\u001b[1m502/689\u001b[0m \u001b[32m━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━\u001b[0m \u001b[1m10s\u001b[0m 58ms/step - auc: 0.9547 - binary_accuracy: 0.9159 - false_negatives: 212.7052 - false_positives: 466.5020 - loss: 0.2791 - precision: 0.8929 - recall: 0.9470 - true_negatives: 3545.5278 - true_positives: 3823.2649\n",
      "For batch number 503 the model has a loss of 0.27839797735214233\n",
      "\u001b[1m503/689\u001b[0m \u001b[32m━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━\u001b[0m \u001b[1m10s\u001b[0m 58ms/step - auc: 0.9547 - binary_accuracy: 0.9159 - false_negatives: 213.1451 - false_positives: 467.4115 - loss: 0.2791 - precision: 0.8929 - recall: 0.9470 - true_negatives: 3552.6462 - true_positives: 3830.7971\n",
      "For batch number 504 the model has a loss of 0.27841418981552124\n",
      "\u001b[1m504/689\u001b[0m \u001b[32m━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━\u001b[0m \u001b[1m10s\u001b[0m 58ms/step - auc: 0.9547 - binary_accuracy: 0.9159 - false_negatives: 213.5873 - false_positives: 468.3175 - loss: 0.2791 - precision: 0.8929 - recall: 0.9470 - true_negatives: 3559.7539 - true_positives: 3838.3413\n",
      "For batch number 505 the model has a loss of 0.2781856060028076\n",
      "\u001b[1m505/689\u001b[0m \u001b[32m━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━\u001b[0m \u001b[1m10s\u001b[0m 58ms/step - auc: 0.9547 - binary_accuracy: 0.9159 - false_negatives: 214.0277 - false_positives: 469.2218 - loss: 0.2791 - precision: 0.8929 - recall: 0.9470 - true_negatives: 3566.8652 - true_positives: 3845.8853\n",
      "For batch number 506 the model has a loss of 0.27822187542915344\n",
      "\u001b[1m506/689\u001b[0m \u001b[32m━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━\u001b[0m \u001b[1m10s\u001b[0m 58ms/step - auc: 0.9547 - binary_accuracy: 0.9159 - false_negatives: 214.4664 - false_positives: 470.1304 - loss: 0.2791 - precision: 0.8929 - recall: 0.9470 - true_negatives: 3573.9724 - true_positives: 3853.4309\n",
      "For batch number 507 the model has a loss of 0.27884143590927124\n",
      "\u001b[1m507/689\u001b[0m \u001b[32m━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━\u001b[0m \u001b[1m10s\u001b[0m 58ms/step - auc: 0.9547 - binary_accuracy: 0.9159 - false_negatives: 214.9112 - false_positives: 471.0355 - loss: 0.2791 - precision: 0.8929 - recall: 0.9470 - true_negatives: 3581.0691 - true_positives: 3860.9841\n",
      "For batch number 508 the model has a loss of 0.2786608934402466\n",
      "\u001b[1m508/689\u001b[0m \u001b[32m━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━\u001b[0m \u001b[1m10s\u001b[0m 58ms/step - auc: 0.9547 - binary_accuracy: 0.9159 - false_negatives: 215.3543 - false_positives: 471.9409 - loss: 0.2791 - precision: 0.8929 - recall: 0.9470 - true_negatives: 3588.1692 - true_positives: 3868.5354\n",
      "For batch number 509 the model has a loss of 0.27886131405830383\n",
      "\u001b[1m509/689\u001b[0m \u001b[32m━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━\u001b[0m \u001b[1m10s\u001b[0m 58ms/step - auc: 0.9547 - binary_accuracy: 0.9159 - false_negatives: 215.7996 - false_positives: 472.8487 - loss: 0.2791 - precision: 0.8929 - recall: 0.9470 - true_negatives: 3595.2690 - true_positives: 3876.0825\n",
      "For batch number 510 the model has a loss of 0.2787894606590271\n",
      "\u001b[1m510/689\u001b[0m \u001b[32m━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━\u001b[0m \u001b[1m10s\u001b[0m 58ms/step - auc: 0.9547 - binary_accuracy: 0.9159 - false_negatives: 216.2431 - false_positives: 473.7588 - loss: 0.2791 - precision: 0.8929 - recall: 0.9470 - true_negatives: 3602.3706 - true_positives: 3883.6274\n",
      "For batch number 511 the model has a loss of 0.27879220247268677\n",
      "\u001b[1m511/689\u001b[0m \u001b[32m━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━\u001b[0m \u001b[1m10s\u001b[0m 58ms/step - auc: 0.9547 - binary_accuracy: 0.9159 - false_negatives: 216.6869 - false_positives: 474.6673 - loss: 0.2791 - precision: 0.8929 - recall: 0.9470 - true_negatives: 3609.4678 - true_positives: 3891.1780\n",
      "For batch number 512 the model has a loss of 0.27845311164855957\n",
      "\u001b[1m512/689\u001b[0m \u001b[32m━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━\u001b[0m \u001b[1m10s\u001b[0m 58ms/step - auc: 0.9547 - binary_accuracy: 0.9159 - false_negatives: 217.1289 - false_positives: 475.5723 - loss: 0.2791 - precision: 0.8929 - recall: 0.9470 - true_negatives: 3616.5684 - true_positives: 3898.7305\n",
      "For batch number 513 the model has a loss of 0.2782401442527771\n",
      "\u001b[1m513/689\u001b[0m \u001b[32m━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━\u001b[0m \u001b[1m10s\u001b[0m 58ms/step - auc: 0.9547 - binary_accuracy: 0.9159 - false_negatives: 217.5692 - false_positives: 476.4776 - loss: 0.2791 - precision: 0.8929 - recall: 0.9470 - true_negatives: 3623.6726 - true_positives: 3906.2808\n",
      "For batch number 514 the model has a loss of 0.27816593647003174\n",
      "\u001b[1m514/689\u001b[0m \u001b[32m━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━\u001b[0m \u001b[1m10s\u001b[0m 58ms/step - auc: 0.9547 - binary_accuracy: 0.9159 - false_negatives: 218.0097 - false_positives: 477.3852 - loss: 0.2791 - precision: 0.8929 - recall: 0.9470 - true_negatives: 3630.7783 - true_positives: 3913.8269\n",
      "For batch number 515 the model has a loss of 0.278268039226532\n",
      "\u001b[1m515/689\u001b[0m \u001b[32m━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━\u001b[0m \u001b[1m10s\u001b[0m 58ms/step - auc: 0.9547 - binary_accuracy: 0.9159 - false_negatives: 218.4485 - false_positives: 478.2990 - loss: 0.2790 - precision: 0.8929 - recall: 0.9470 - true_negatives: 3637.8914 - true_positives: 3921.3611\n",
      "For batch number 516 the model has a loss of 0.27852562069892883\n",
      "\u001b[1m516/689\u001b[0m \u001b[32m━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━\u001b[0m \u001b[1m9s\u001b[0m 58ms/step - auc: 0.9547 - binary_accuracy: 0.9159 - false_negatives: 218.8857 - false_positives: 479.2151 - loss: 0.2790 - precision: 0.8929 - recall: 0.9470 - true_negatives: 3645.0137 - true_positives: 3928.8857 \n",
      "For batch number 517 the model has a loss of 0.27817127108573914\n",
      "\u001b[1m517/689\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m9s\u001b[0m 58ms/step - auc: 0.9547 - binary_accuracy: 0.9159 - false_negatives: 219.3211 - false_positives: 480.1296 - loss: 0.2790 - precision: 0.8929 - recall: 0.9470 - true_negatives: 3652.1411 - true_positives: 3936.4082\n",
      "For batch number 518 the model has a loss of 0.27813810110092163\n",
      "\u001b[1m518/689\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m9s\u001b[0m 58ms/step - auc: 0.9547 - binary_accuracy: 0.9159 - false_negatives: 219.7548 - false_positives: 481.0463 - loss: 0.2790 - precision: 0.8929 - recall: 0.9470 - true_negatives: 3659.2703 - true_positives: 3943.9285\n",
      "For batch number 519 the model has a loss of 0.2779412865638733\n",
      "\u001b[1m519/689\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m9s\u001b[0m 58ms/step - auc: 0.9547 - binary_accuracy: 0.9159 - false_negatives: 220.1869 - false_positives: 481.9615 - loss: 0.2790 - precision: 0.8929 - recall: 0.9470 - true_negatives: 3666.4026 - true_positives: 3951.4490\n",
      "For batch number 520 the model has a loss of 0.27781999111175537\n",
      "\u001b[1m520/689\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m9s\u001b[0m 58ms/step - auc: 0.9547 - binary_accuracy: 0.9159 - false_negatives: 220.6173 - false_positives: 482.8750 - loss: 0.2790 - precision: 0.8929 - recall: 0.9470 - true_negatives: 3673.5308 - true_positives: 3958.9768\n",
      "For batch number 521 the model has a loss of 0.27773356437683105\n",
      "\u001b[1m521/689\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m9s\u001b[0m 58ms/step - auc: 0.9547 - binary_accuracy: 0.9159 - false_negatives: 221.0461 - false_positives: 483.7870 - loss: 0.2790 - precision: 0.8929 - recall: 0.9470 - true_negatives: 3680.6545 - true_positives: 3966.5125\n",
      "For batch number 522 the model has a loss of 0.2774620056152344\n",
      "\u001b[1m522/689\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m9s\u001b[0m 58ms/step - auc: 0.9547 - binary_accuracy: 0.9159 - false_negatives: 221.4732 - false_positives: 484.6973 - loss: 0.2790 - precision: 0.8929 - recall: 0.9470 - true_negatives: 3687.7834 - true_positives: 3974.0459\n",
      "For batch number 523 the model has a loss of 0.27766284346580505\n",
      "\u001b[1m523/689\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m9s\u001b[0m 58ms/step - auc: 0.9547 - binary_accuracy: 0.9159 - false_negatives: 221.9006 - false_positives: 485.6138 - loss: 0.2790 - precision: 0.8929 - recall: 0.9470 - true_negatives: 3694.9158 - true_positives: 3981.5698\n",
      "For batch number 524 the model has a loss of 0.27742308378219604\n",
      "\u001b[1m524/689\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m9s\u001b[0m 58ms/step - auc: 0.9547 - binary_accuracy: 0.9159 - false_negatives: 222.3282 - false_positives: 486.5267 - loss: 0.2790 - precision: 0.8929 - recall: 0.9470 - true_negatives: 3702.0610 - true_positives: 3989.0840\n",
      "For batch number 525 the model has a loss of 0.2773299217224121\n",
      "\u001b[1m525/689\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m9s\u001b[0m 58ms/step - auc: 0.9547 - binary_accuracy: 0.9159 - false_negatives: 222.7543 - false_positives: 487.4381 - loss: 0.2790 - precision: 0.8929 - recall: 0.9470 - true_negatives: 3709.2058 - true_positives: 3996.6018\n",
      "For batch number 526 the model has a loss of 0.27719494700431824\n",
      "\u001b[1m526/689\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m9s\u001b[0m 58ms/step - auc: 0.9547 - binary_accuracy: 0.9159 - false_negatives: 223.1787 - false_positives: 488.3479 - loss: 0.2790 - precision: 0.8929 - recall: 0.9470 - true_negatives: 3716.3459 - true_positives: 4004.1274\n",
      "For batch number 527 the model has a loss of 0.2771027982234955\n",
      "\u001b[1m527/689\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m9s\u001b[0m 58ms/step - auc: 0.9547 - binary_accuracy: 0.9159 - false_negatives: 223.6015 - false_positives: 489.2581 - loss: 0.2790 - precision: 0.8929 - recall: 0.9470 - true_negatives: 3723.4915 - true_positives: 4011.6489\n",
      "For batch number 528 the model has a loss of 0.27702367305755615\n",
      "\u001b[1m528/689\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m9s\u001b[0m 58ms/step - auc: 0.9547 - binary_accuracy: 0.9159 - false_negatives: 224.0246 - false_positives: 490.1685 - loss: 0.2790 - precision: 0.8929 - recall: 0.9470 - true_negatives: 3730.6326 - true_positives: 4019.1743\n",
      "For batch number 529 the model has a loss of 0.2769402265548706\n",
      "\u001b[1m529/689\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m9s\u001b[0m 58ms/step - auc: 0.9547 - binary_accuracy: 0.9159 - false_negatives: 224.4518 - false_positives: 491.0756 - loss: 0.2790 - precision: 0.8929 - recall: 0.9470 - true_negatives: 3737.7751 - true_positives: 4026.6975\n",
      "For batch number 530 the model has a loss of 0.2768537402153015\n",
      "\u001b[1m530/689\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m9s\u001b[0m 58ms/step - auc: 0.9547 - binary_accuracy: 0.9159 - false_negatives: 224.8792 - false_positives: 491.9849 - loss: 0.2790 - precision: 0.8929 - recall: 0.9470 - true_negatives: 3744.9226 - true_positives: 4034.2131\n",
      "For batch number 531 the model has a loss of 0.2768895626068115\n",
      "\u001b[1m531/689\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m9s\u001b[0m 58ms/step - auc: 0.9547 - binary_accuracy: 0.9159 - false_negatives: 225.3089 - false_positives: 492.8945 - loss: 0.2790 - precision: 0.8929 - recall: 0.9470 - true_negatives: 3752.0659 - true_positives: 4041.7307\n",
      "For batch number 532 the model has a loss of 0.27655652165412903\n",
      "\u001b[1m532/689\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m9s\u001b[0m 58ms/step - auc: 0.9547 - binary_accuracy: 0.9159 - false_negatives: 225.7368 - false_positives: 493.8026 - loss: 0.2790 - precision: 0.8929 - recall: 0.9470 - true_negatives: 3759.2144 - true_positives: 4049.2463\n",
      "For batch number 533 the model has a loss of 0.2767764627933502\n",
      "\u001b[1m533/689\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m9s\u001b[0m 58ms/step - auc: 0.9547 - binary_accuracy: 0.9159 - false_negatives: 226.1670 - false_positives: 494.7073 - loss: 0.2790 - precision: 0.8929 - recall: 0.9470 - true_negatives: 3766.3564 - true_positives: 4056.7693\n",
      "For batch number 534 the model has a loss of 0.27715593576431274\n",
      "\u001b[1m534/689\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m8s\u001b[0m 58ms/step - auc: 0.9547 - binary_accuracy: 0.9159 - false_negatives: 226.5974 - false_positives: 495.6161 - loss: 0.2790 - precision: 0.8929 - recall: 0.9470 - true_negatives: 3773.4944 - true_positives: 4064.2922\n",
      "For batch number 535 the model has a loss of 0.2768370807170868\n",
      "\u001b[1m535/689\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m8s\u001b[0m 58ms/step - auc: 0.9547 - binary_accuracy: 0.9159 - false_negatives: 227.0262 - false_positives: 496.5234 - loss: 0.2790 - precision: 0.8929 - recall: 0.9470 - true_negatives: 3780.6355 - true_positives: 4071.8149\n",
      "For batch number 536 the model has a loss of 0.27694565057754517\n",
      "\u001b[1m536/689\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m8s\u001b[0m 58ms/step - auc: 0.9547 - binary_accuracy: 0.9159 - false_negatives: 227.4534 - false_positives: 497.4347 - loss: 0.2790 - precision: 0.8928 - recall: 0.9470 - true_negatives: 3787.7817 - true_positives: 4079.3303\n",
      "For batch number 537 the model has a loss of 0.27711278200149536\n",
      "\u001b[1m537/689\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m8s\u001b[0m 58ms/step - auc: 0.9547 - binary_accuracy: 0.9159 - false_negatives: 227.8827 - false_positives: 498.3445 - loss: 0.2790 - precision: 0.8928 - recall: 0.9470 - true_negatives: 3794.9275 - true_positives: 4086.8455\n",
      "For batch number 538 the model has a loss of 0.27709829807281494\n",
      "\u001b[1m538/689\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m8s\u001b[0m 58ms/step - auc: 0.9547 - binary_accuracy: 0.9159 - false_negatives: 228.3123 - false_positives: 499.2509 - loss: 0.2790 - precision: 0.8928 - recall: 0.9470 - true_negatives: 3802.0706 - true_positives: 4094.3662\n",
      "For batch number 539 the model has a loss of 0.2772267758846283\n",
      "\u001b[1m539/689\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m8s\u001b[0m 58ms/step - auc: 0.9547 - binary_accuracy: 0.9159 - false_negatives: 228.7458 - false_positives: 500.1540 - loss: 0.2790 - precision: 0.8928 - recall: 0.9470 - true_negatives: 3809.2134 - true_positives: 4101.8867\n",
      "For batch number 540 the model has a loss of 0.2773399353027344\n",
      "\u001b[1m540/689\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m8s\u001b[0m 58ms/step - auc: 0.9547 - binary_accuracy: 0.9159 - false_negatives: 229.1833 - false_positives: 501.0537 - loss: 0.2790 - precision: 0.8928 - recall: 0.9470 - true_negatives: 3816.3574 - true_positives: 4109.4058\n",
      "For batch number 541 the model has a loss of 0.27710938453674316\n",
      "\u001b[1m541/689\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m8s\u001b[0m 58ms/step - auc: 0.9547 - binary_accuracy: 0.9159 - false_negatives: 229.6192 - false_positives: 501.9519 - loss: 0.2790 - precision: 0.8928 - recall: 0.9470 - true_negatives: 3823.4990 - true_positives: 4116.9297\n",
      "For batch number 542 the model has a loss of 0.2777787148952484\n",
      "\u001b[1m542/689\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m8s\u001b[0m 58ms/step - auc: 0.9547 - binary_accuracy: 0.9159 - false_negatives: 230.0572 - false_positives: 502.8506 - loss: 0.2790 - precision: 0.8928 - recall: 0.9470 - true_negatives: 3830.6438 - true_positives: 4124.4482\n",
      "For batch number 543 the model has a loss of 0.277744323015213\n",
      "\u001b[1m543/689\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m8s\u001b[0m 58ms/step - auc: 0.9547 - binary_accuracy: 0.9159 - false_negatives: 230.4972 - false_positives: 503.7495 - loss: 0.2790 - precision: 0.8928 - recall: 0.9470 - true_negatives: 3837.7864 - true_positives: 4131.9668\n",
      "For batch number 544 the model has a loss of 0.27804872393608093\n",
      "\u001b[1m544/689\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m8s\u001b[0m 58ms/step - auc: 0.9547 - binary_accuracy: 0.9159 - false_negatives: 230.9393 - false_positives: 504.6507 - loss: 0.2790 - precision: 0.8928 - recall: 0.9470 - true_negatives: 3844.9229 - true_positives: 4139.4873\n",
      "For batch number 545 the model has a loss of 0.2780572772026062\n",
      "\u001b[1m545/689\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m8s\u001b[0m 58ms/step - auc: 0.9547 - binary_accuracy: 0.9159 - false_negatives: 231.3817 - false_positives: 505.5523 - loss: 0.2790 - precision: 0.8928 - recall: 0.9470 - true_negatives: 3852.0605 - true_positives: 4147.0054\n",
      "For batch number 546 the model has a loss of 0.2781369686126709\n",
      "\u001b[1m546/689\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m8s\u001b[0m 58ms/step - auc: 0.9547 - binary_accuracy: 0.9159 - false_negatives: 231.8223 - false_positives: 506.4561 - loss: 0.2790 - precision: 0.8928 - recall: 0.9470 - true_negatives: 3859.1941 - true_positives: 4154.5273\n",
      "For batch number 547 the model has a loss of 0.27838167548179626\n",
      "\u001b[1m547/689\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m8s\u001b[0m 58ms/step - auc: 0.9547 - binary_accuracy: 0.9159 - false_negatives: 232.2651 - false_positives: 507.3620 - loss: 0.2790 - precision: 0.8928 - recall: 0.9470 - true_negatives: 3866.3291 - true_positives: 4162.0439\n",
      "For batch number 548 the model has a loss of 0.2781507670879364\n",
      "\u001b[1m548/689\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m8s\u001b[0m 58ms/step - auc: 0.9547 - binary_accuracy: 0.9159 - false_negatives: 232.7062 - false_positives: 508.2664 - loss: 0.2790 - precision: 0.8928 - recall: 0.9470 - true_negatives: 3873.4600 - true_positives: 4169.5674\n",
      "For batch number 549 the model has a loss of 0.27791276574134827\n",
      "\u001b[1m549/689\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m8s\u001b[0m 58ms/step - auc: 0.9547 - binary_accuracy: 0.9159 - false_negatives: 233.1457 - false_positives: 509.1694 - loss: 0.2790 - precision: 0.8928 - recall: 0.9470 - true_negatives: 3880.5884 - true_positives: 4177.0967\n",
      "For batch number 550 the model has a loss of 0.2784615457057953\n",
      "\u001b[1m550/689\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m8s\u001b[0m 58ms/step - auc: 0.9547 - binary_accuracy: 0.9159 - false_negatives: 233.5891 - false_positives: 510.0727 - loss: 0.2790 - precision: 0.8928 - recall: 0.9470 - true_negatives: 3887.7163 - true_positives: 4184.6216\n",
      "For batch number 551 the model has a loss of 0.2785884141921997\n",
      "\u001b[1m551/689\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m8s\u001b[0m 58ms/step - auc: 0.9547 - binary_accuracy: 0.9159 - false_negatives: 234.0345 - false_positives: 510.9728 - loss: 0.2790 - precision: 0.8928 - recall: 0.9470 - true_negatives: 3894.8350 - true_positives: 4192.1577\n",
      "For batch number 552 the model has a loss of 0.278516948223114\n",
      "\u001b[1m552/689\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m7s\u001b[0m 58ms/step - auc: 0.9547 - binary_accuracy: 0.9159 - false_negatives: 234.4801 - false_positives: 511.8696 - loss: 0.2789 - precision: 0.8928 - recall: 0.9470 - true_negatives: 3901.9529 - true_positives: 4199.6973\n",
      "For batch number 553 the model has a loss of 0.2787352204322815\n",
      "\u001b[1m553/689\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m7s\u001b[0m 58ms/step - auc: 0.9547 - binary_accuracy: 0.9159 - false_negatives: 234.9259 - false_positives: 512.7722 - loss: 0.2789 - precision: 0.8928 - recall: 0.9470 - true_negatives: 3909.0723 - true_positives: 4207.2295\n",
      "For batch number 554 the model has a loss of 0.2785366475582123\n",
      "\u001b[1m554/689\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m7s\u001b[0m 58ms/step - auc: 0.9547 - binary_accuracy: 0.9159 - false_negatives: 235.3718 - false_positives: 513.6715 - loss: 0.2789 - precision: 0.8928 - recall: 0.9470 - true_negatives: 3916.1895 - true_positives: 4214.7671\n",
      "For batch number 555 the model has a loss of 0.2784564197063446\n",
      "\u001b[1m555/689\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m7s\u001b[0m 58ms/step - auc: 0.9547 - binary_accuracy: 0.9159 - false_negatives: 235.8162 - false_positives: 514.5730 - loss: 0.2789 - precision: 0.8928 - recall: 0.9470 - true_negatives: 3923.3064 - true_positives: 4222.3047\n",
      "For batch number 556 the model has a loss of 0.27840059995651245\n",
      "\u001b[1m556/689\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m7s\u001b[0m 58ms/step - auc: 0.9547 - binary_accuracy: 0.9159 - false_negatives: 236.2590 - false_positives: 515.4748 - loss: 0.2789 - precision: 0.8928 - recall: 0.9470 - true_negatives: 3930.4209 - true_positives: 4229.8452\n",
      "For batch number 557 the model has a loss of 0.2786346971988678\n",
      "\u001b[1m557/689\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m7s\u001b[0m 58ms/step - auc: 0.9547 - binary_accuracy: 0.9159 - false_negatives: 236.7038 - false_positives: 516.3770 - loss: 0.2789 - precision: 0.8928 - recall: 0.9470 - true_negatives: 3937.5439 - true_positives: 4237.3750\n",
      "For batch number 558 the model has a loss of 0.2783924639225006\n",
      "\u001b[1m558/689\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m7s\u001b[0m 58ms/step - auc: 0.9547 - binary_accuracy: 0.9159 - false_negatives: 237.1470 - false_positives: 517.2778 - loss: 0.2789 - precision: 0.8928 - recall: 0.9470 - true_negatives: 3944.6667 - true_positives: 4244.9087\n",
      "For batch number 559 the model has a loss of 0.2785586714744568\n",
      "\u001b[1m559/689\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m7s\u001b[0m 58ms/step - auc: 0.9547 - binary_accuracy: 0.9159 - false_negatives: 237.5903 - false_positives: 518.1807 - loss: 0.2789 - precision: 0.8928 - recall: 0.9470 - true_negatives: 3951.7961 - true_positives: 4252.4331\n",
      "For batch number 560 the model has a loss of 0.2785560190677643\n",
      "\u001b[1m560/689\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m7s\u001b[0m 58ms/step - auc: 0.9547 - binary_accuracy: 0.9159 - false_negatives: 238.0322 - false_positives: 519.0857 - loss: 0.2789 - precision: 0.8928 - recall: 0.9470 - true_negatives: 3958.9268 - true_positives: 4259.9556\n",
      "For batch number 561 the model has a loss of 0.2789228558540344\n",
      "\u001b[1m561/689\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m7s\u001b[0m 58ms/step - auc: 0.9547 - binary_accuracy: 0.9159 - false_negatives: 238.4724 - false_positives: 519.9911 - loss: 0.2789 - precision: 0.8928 - recall: 0.9470 - true_negatives: 3966.0571 - true_positives: 4267.4795\n",
      "For batch number 562 the model has a loss of 0.27927061915397644\n",
      "\u001b[1m562/689\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m7s\u001b[0m 58ms/step - auc: 0.9547 - binary_accuracy: 0.9159 - false_negatives: 238.9110 - false_positives: 520.9075 - loss: 0.2789 - precision: 0.8928 - recall: 0.9470 - true_negatives: 3973.1814 - true_positives: 4275.0000\n",
      "For batch number 563 the model has a loss of 0.27932286262512207\n",
      "\u001b[1m563/689\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m7s\u001b[0m 58ms/step - auc: 0.9547 - binary_accuracy: 0.9159 - false_negatives: 239.3481 - false_positives: 521.8277 - loss: 0.2789 - precision: 0.8928 - recall: 0.9470 - true_negatives: 3980.3108 - true_positives: 4282.5132\n",
      "For batch number 564 the model has a loss of 0.2796236276626587\n",
      "\u001b[1m564/689\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m7s\u001b[0m 58ms/step - auc: 0.9547 - binary_accuracy: 0.9159 - false_negatives: 239.7855 - false_positives: 522.7482 - loss: 0.2789 - precision: 0.8928 - recall: 0.9470 - true_negatives: 3987.4363 - true_positives: 4290.0303\n",
      "For batch number 565 the model has a loss of 0.2799820303916931\n",
      "\u001b[1m565/689\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m7s\u001b[0m 58ms/step - auc: 0.9547 - binary_accuracy: 0.9159 - false_negatives: 240.2248 - false_positives: 523.6690 - loss: 0.2789 - precision: 0.8928 - recall: 0.9469 - true_negatives: 3994.5610 - true_positives: 4297.5449\n",
      "For batch number 566 the model has a loss of 0.2801119387149811\n",
      "\u001b[1m566/689\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m7s\u001b[0m 58ms/step - auc: 0.9547 - binary_accuracy: 0.9159 - false_negatives: 240.6643 - false_positives: 524.5919 - loss: 0.2789 - precision: 0.8928 - recall: 0.9469 - true_negatives: 4001.6909 - true_positives: 4305.0532\n",
      "For batch number 567 the model has a loss of 0.2803654372692108\n",
      "\u001b[1m567/689\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m7s\u001b[0m 58ms/step - auc: 0.9546 - binary_accuracy: 0.9159 - false_negatives: 241.1023 - false_positives: 525.5150 - loss: 0.2790 - precision: 0.8928 - recall: 0.9469 - true_negatives: 4008.8096 - true_positives: 4312.5732\n",
      "For batch number 568 the model has a loss of 0.2805553078651428\n",
      "\u001b[1m568/689\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m7s\u001b[0m 58ms/step - auc: 0.9546 - binary_accuracy: 0.9159 - false_negatives: 241.5423 - false_positives: 526.4384 - loss: 0.2790 - precision: 0.8928 - recall: 0.9469 - true_negatives: 4015.9277 - true_positives: 4320.0913\n",
      "For batch number 569 the model has a loss of 0.28069448471069336\n",
      "\u001b[1m569/689\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m6s\u001b[0m 58ms/step - auc: 0.9546 - binary_accuracy: 0.9159 - false_negatives: 241.9824 - false_positives: 527.3621 - loss: 0.2790 - precision: 0.8928 - recall: 0.9469 - true_negatives: 4023.0510 - true_positives: 4327.6045\n",
      "For batch number 570 the model has a loss of 0.2804390490055084\n",
      "\u001b[1m570/689\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m6s\u001b[0m 58ms/step - auc: 0.9546 - binary_accuracy: 0.9159 - false_negatives: 242.4211 - false_positives: 528.2859 - loss: 0.2790 - precision: 0.8928 - recall: 0.9469 - true_negatives: 4030.1772 - true_positives: 4335.1157\n",
      "For batch number 571 the model has a loss of 0.28038489818573\n",
      "\u001b[1m571/689\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m6s\u001b[0m 58ms/step - auc: 0.9546 - binary_accuracy: 0.9159 - false_negatives: 242.8599 - false_positives: 529.2084 - loss: 0.2790 - precision: 0.8928 - recall: 0.9469 - true_negatives: 4037.2925 - true_positives: 4342.6392\n",
      "For batch number 572 the model has a loss of 0.2800431549549103\n",
      "\u001b[1m572/689\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m6s\u001b[0m 58ms/step - auc: 0.9546 - binary_accuracy: 0.9159 - false_negatives: 243.2972 - false_positives: 530.1294 - loss: 0.2790 - precision: 0.8928 - recall: 0.9469 - true_negatives: 4044.4109 - true_positives: 4350.1626\n",
      "For batch number 573 the model has a loss of 0.279674232006073\n",
      "\u001b[1m573/689\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m6s\u001b[0m 58ms/step - auc: 0.9546 - binary_accuracy: 0.9159 - false_negatives: 243.7330 - false_positives: 531.0489 - loss: 0.2790 - precision: 0.8928 - recall: 0.9469 - true_negatives: 4051.5339 - true_positives: 4357.6841\n",
      "For batch number 574 the model has a loss of 0.27977699041366577\n",
      "\u001b[1m574/689\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m6s\u001b[0m 58ms/step - auc: 0.9546 - binary_accuracy: 0.9159 - false_negatives: 244.1707 - false_positives: 531.9669 - loss: 0.2790 - precision: 0.8928 - recall: 0.9469 - true_negatives: 4058.6584 - true_positives: 4365.2036\n",
      "For batch number 575 the model has a loss of 0.27968987822532654\n",
      "\u001b[1m575/689\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m6s\u001b[0m 58ms/step - auc: 0.9546 - binary_accuracy: 0.9159 - false_negatives: 244.6070 - false_positives: 532.8852 - loss: 0.2790 - precision: 0.8928 - recall: 0.9469 - true_negatives: 4065.7844 - true_positives: 4372.7236\n",
      "For batch number 576 the model has a loss of 0.2796836793422699\n",
      "\u001b[1m576/689\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m6s\u001b[0m 58ms/step - auc: 0.9546 - binary_accuracy: 0.9159 - false_negatives: 245.0434 - false_positives: 533.8038 - loss: 0.2790 - precision: 0.8928 - recall: 0.9469 - true_negatives: 4072.9131 - true_positives: 4380.2397\n",
      "For batch number 577 the model has a loss of 0.2797047793865204\n",
      "\u001b[1m577/689\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m6s\u001b[0m 58ms/step - auc: 0.9546 - binary_accuracy: 0.9159 - false_negatives: 245.4818 - false_positives: 534.7227 - loss: 0.2790 - precision: 0.8928 - recall: 0.9469 - true_negatives: 4080.0347 - true_positives: 4387.7607\n",
      "For batch number 578 the model has a loss of 0.2798348665237427\n",
      "\u001b[1m578/689\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m6s\u001b[0m 58ms/step - auc: 0.9546 - binary_accuracy: 0.9159 - false_negatives: 245.9187 - false_positives: 535.6436 - loss: 0.2790 - precision: 0.8928 - recall: 0.9469 - true_negatives: 4087.1609 - true_positives: 4395.2769\n",
      "For batch number 579 the model has a loss of 0.2799222767353058\n",
      "\u001b[1m579/689\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m6s\u001b[0m 58ms/step - auc: 0.9546 - binary_accuracy: 0.9159 - false_negatives: 246.3575 - false_positives: 536.5613 - loss: 0.2790 - precision: 0.8928 - recall: 0.9469 - true_negatives: 4094.2849 - true_positives: 4402.7964\n",
      "For batch number 580 the model has a loss of 0.27989545464515686\n",
      "\u001b[1m580/689\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m6s\u001b[0m 58ms/step - auc: 0.9546 - binary_accuracy: 0.9159 - false_negatives: 246.7948 - false_positives: 537.4810 - loss: 0.2790 - precision: 0.8928 - recall: 0.9469 - true_negatives: 4101.4087 - true_positives: 4410.3154\n",
      "For batch number 581 the model has a loss of 0.28007757663726807\n",
      "\u001b[1m581/689\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m6s\u001b[0m 58ms/step - auc: 0.9546 - binary_accuracy: 0.9159 - false_negatives: 247.2358 - false_positives: 538.3976 - loss: 0.2790 - precision: 0.8928 - recall: 0.9469 - true_negatives: 4108.5283 - true_positives: 4417.8384\n",
      "For batch number 582 the model has a loss of 0.28007999062538147\n",
      "\u001b[1m582/689\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m6s\u001b[0m 58ms/step - auc: 0.9546 - binary_accuracy: 0.9159 - false_negatives: 247.6770 - false_positives: 539.3110 - loss: 0.2790 - precision: 0.8928 - recall: 0.9469 - true_negatives: 4115.6392 - true_positives: 4425.3730\n",
      "For batch number 583 the model has a loss of 0.2801585793495178\n",
      "\u001b[1m583/689\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m6s\u001b[0m 58ms/step - auc: 0.9546 - binary_accuracy: 0.9159 - false_negatives: 248.1183 - false_positives: 540.2230 - loss: 0.2790 - precision: 0.8928 - recall: 0.9469 - true_negatives: 4122.7495 - true_positives: 4432.9092\n",
      "For batch number 584 the model has a loss of 0.27993786334991455\n",
      "\u001b[1m584/689\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m6s\u001b[0m 58ms/step - auc: 0.9546 - binary_accuracy: 0.9159 - false_negatives: 248.5599 - false_positives: 541.1318 - loss: 0.2790 - precision: 0.8928 - recall: 0.9469 - true_negatives: 4129.8682 - true_positives: 4440.4399\n",
      "For batch number 585 the model has a loss of 0.27971258759498596\n",
      "\u001b[1m585/689\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m6s\u001b[0m 58ms/step - auc: 0.9546 - binary_accuracy: 0.9159 - false_negatives: 249.0000 - false_positives: 542.0393 - loss: 0.2790 - precision: 0.8928 - recall: 0.9469 - true_negatives: 4136.9878 - true_positives: 4447.9727\n",
      "For batch number 586 the model has a loss of 0.27949878573417664\n",
      "\u001b[1m586/689\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m5s\u001b[0m 58ms/step - auc: 0.9546 - binary_accuracy: 0.9159 - false_negatives: 249.4386 - false_positives: 542.9437 - loss: 0.2790 - precision: 0.8928 - recall: 0.9469 - true_negatives: 4144.1040 - true_positives: 4455.5137\n",
      "For batch number 587 the model has a loss of 0.27961334586143494\n",
      "\u001b[1m587/689\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m5s\u001b[0m 58ms/step - auc: 0.9546 - binary_accuracy: 0.9159 - false_negatives: 249.8790 - false_positives: 543.8501 - loss: 0.2790 - precision: 0.8928 - recall: 0.9469 - true_negatives: 4151.2163 - true_positives: 4463.0547\n",
      "For batch number 588 the model has a loss of 0.27926722168922424\n",
      "\u001b[1m588/689\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m5s\u001b[0m 58ms/step - auc: 0.9546 - binary_accuracy: 0.9159 - false_negatives: 250.3180 - false_positives: 544.7534 - loss: 0.2790 - precision: 0.8928 - recall: 0.9469 - true_negatives: 4158.3335 - true_positives: 4470.5952\n",
      "For batch number 589 the model has a loss of 0.28018808364868164\n",
      "\u001b[1m589/689\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m5s\u001b[0m 58ms/step - auc: 0.9546 - binary_accuracy: 0.9159 - false_negatives: 250.7589 - false_positives: 545.6588 - loss: 0.2790 - precision: 0.8927 - recall: 0.9469 - true_negatives: 4165.4482 - true_positives: 4478.1343\n",
      "For batch number 590 the model has a loss of 0.2799851894378662\n",
      "\u001b[1m590/689\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m5s\u001b[0m 58ms/step - auc: 0.9546 - binary_accuracy: 0.9159 - false_negatives: 251.2017 - false_positives: 546.5610 - loss: 0.2790 - precision: 0.8927 - recall: 0.9469 - true_negatives: 4172.5576 - true_positives: 4485.6797\n",
      "For batch number 591 the model has a loss of 0.2797001302242279\n",
      "\u001b[1m591/689\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m5s\u001b[0m 58ms/step - auc: 0.9546 - binary_accuracy: 0.9159 - false_negatives: 251.6430 - false_positives: 547.4619 - loss: 0.2790 - precision: 0.8927 - recall: 0.9469 - true_negatives: 4179.6699 - true_positives: 4493.2251\n",
      "For batch number 592 the model has a loss of 0.27956318855285645\n",
      "\u001b[1m592/689\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m5s\u001b[0m 58ms/step - auc: 0.9546 - binary_accuracy: 0.9159 - false_negatives: 252.0845 - false_positives: 548.3615 - loss: 0.2790 - precision: 0.8927 - recall: 0.9469 - true_negatives: 4186.7856 - true_positives: 4500.7686\n",
      "For batch number 593 the model has a loss of 0.2793898582458496\n",
      "\u001b[1m593/689\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m5s\u001b[0m 58ms/step - auc: 0.9546 - binary_accuracy: 0.9159 - false_negatives: 252.5261 - false_positives: 549.2580 - loss: 0.2790 - precision: 0.8927 - recall: 0.9469 - true_negatives: 4193.9038 - true_positives: 4508.3120\n",
      "For batch number 594 the model has a loss of 0.27924415469169617\n",
      "\u001b[1m594/689\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m5s\u001b[0m 58ms/step - auc: 0.9546 - binary_accuracy: 0.9159 - false_negatives: 252.9663 - false_positives: 550.1549 - loss: 0.2790 - precision: 0.8927 - recall: 0.9469 - true_negatives: 4201.0220 - true_positives: 4515.8569\n",
      "For batch number 595 the model has a loss of 0.27924826741218567\n",
      "\u001b[1m595/689\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m5s\u001b[0m 58ms/step - auc: 0.9546 - binary_accuracy: 0.9159 - false_negatives: 253.4050 - false_positives: 551.0521 - loss: 0.2790 - precision: 0.8927 - recall: 0.9469 - true_negatives: 4208.1445 - true_positives: 4523.3984\n",
      "For batch number 596 the model has a loss of 0.27910616993904114\n",
      "\u001b[1m596/689\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m5s\u001b[0m 58ms/step - auc: 0.9546 - binary_accuracy: 0.9159 - false_negatives: 253.8423 - false_positives: 551.9480 - loss: 0.2790 - precision: 0.8927 - recall: 0.9469 - true_negatives: 4215.2754 - true_positives: 4530.9346\n",
      "For batch number 597 the model has a loss of 0.27911439538002014\n",
      "\u001b[1m597/689\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m5s\u001b[0m 58ms/step - auc: 0.9546 - binary_accuracy: 0.9159 - false_negatives: 254.2797 - false_positives: 552.8442 - loss: 0.2790 - precision: 0.8927 - recall: 0.9469 - true_negatives: 4222.4004 - true_positives: 4538.4756\n",
      "For batch number 598 the model has a loss of 0.2790701389312744\n",
      "\u001b[1m598/689\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m5s\u001b[0m 58ms/step - auc: 0.9546 - binary_accuracy: 0.9159 - false_negatives: 254.7157 - false_positives: 553.7425 - loss: 0.2790 - precision: 0.8927 - recall: 0.9469 - true_negatives: 4229.5269 - true_positives: 4546.0151\n",
      "For batch number 599 the model has a loss of 0.2788289487361908\n",
      "\u001b[1m599/689\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m5s\u001b[0m 58ms/step - auc: 0.9546 - binary_accuracy: 0.9159 - false_negatives: 255.1503 - false_positives: 554.6411 - loss: 0.2790 - precision: 0.8927 - recall: 0.9469 - true_negatives: 4236.6562 - true_positives: 4553.5527\n",
      "For batch number 600 the model has a loss of 0.2788780927658081\n",
      "\u001b[1m600/689\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m5s\u001b[0m 58ms/step - auc: 0.9546 - binary_accuracy: 0.9159 - false_negatives: 255.5867 - false_positives: 555.5400 - loss: 0.2790 - precision: 0.8927 - recall: 0.9469 - true_negatives: 4243.7886 - true_positives: 4561.0850\n",
      "For batch number 601 the model has a loss of 0.27882638573646545\n",
      "\u001b[1m601/689\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m5s\u001b[0m 58ms/step - auc: 0.9546 - binary_accuracy: 0.9159 - false_negatives: 256.0216 - false_positives: 556.4393 - loss: 0.2790 - precision: 0.8927 - recall: 0.9469 - true_negatives: 4250.9150 - true_positives: 4568.6240\n",
      "For batch number 602 the model has a loss of 0.2787543833255768\n",
      "\u001b[1m602/689\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m5s\u001b[0m 58ms/step - auc: 0.9546 - binary_accuracy: 0.9159 - false_negatives: 256.4551 - false_positives: 557.3389 - loss: 0.2790 - precision: 0.8927 - recall: 0.9469 - true_negatives: 4258.0498 - true_positives: 4576.1562\n",
      "For batch number 603 the model has a loss of 0.27867215871810913\n",
      "\u001b[1m603/689\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m4s\u001b[0m 58ms/step - auc: 0.9546 - binary_accuracy: 0.9159 - false_negatives: 256.8889 - false_positives: 558.2371 - loss: 0.2790 - precision: 0.8927 - recall: 0.9469 - true_negatives: 4265.1841 - true_positives: 4583.6899\n",
      "For batch number 604 the model has a loss of 0.27850714325904846\n",
      "\u001b[1m604/689\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m4s\u001b[0m 58ms/step - auc: 0.9546 - binary_accuracy: 0.9159 - false_negatives: 257.3212 - false_positives: 559.1324 - loss: 0.2790 - precision: 0.8927 - recall: 0.9469 - true_negatives: 4272.3110 - true_positives: 4591.2349\n",
      "For batch number 605 the model has a loss of 0.2782353162765503\n",
      "\u001b[1m605/689\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m4s\u001b[0m 58ms/step - auc: 0.9546 - binary_accuracy: 0.9159 - false_negatives: 257.7521 - false_positives: 560.0264 - loss: 0.2790 - precision: 0.8927 - recall: 0.9469 - true_negatives: 4279.4380 - true_positives: 4598.7837\n",
      "For batch number 606 the model has a loss of 0.2781113088130951\n",
      "\u001b[1m606/689\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m4s\u001b[0m 58ms/step - auc: 0.9546 - binary_accuracy: 0.9159 - false_negatives: 258.1815 - false_positives: 560.9208 - loss: 0.2790 - precision: 0.8927 - recall: 0.9469 - true_negatives: 4286.5610 - true_positives: 4606.3364\n",
      "For batch number 607 the model has a loss of 0.2786356210708618\n",
      "\u001b[1m607/689\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m4s\u001b[0m 58ms/step - auc: 0.9546 - binary_accuracy: 0.9159 - false_negatives: 258.6112 - false_positives: 561.8138 - loss: 0.2790 - precision: 0.8927 - recall: 0.9469 - true_negatives: 4293.6836 - true_positives: 4613.8911\n",
      "For batch number 608 the model has a loss of 0.2785842716693878\n",
      "\u001b[1m608/689\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m4s\u001b[0m 58ms/step - auc: 0.9546 - binary_accuracy: 0.9159 - false_negatives: 259.0411 - false_positives: 562.7039 - loss: 0.2790 - precision: 0.8927 - recall: 0.9469 - true_negatives: 4300.8062 - true_positives: 4621.4492\n",
      "For batch number 609 the model has a loss of 0.2784614562988281\n",
      "\u001b[1m609/689\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m4s\u001b[0m 58ms/step - auc: 0.9546 - binary_accuracy: 0.9159 - false_negatives: 259.4696 - false_positives: 563.5928 - loss: 0.2790 - precision: 0.8927 - recall: 0.9469 - true_negatives: 4307.9312 - true_positives: 4629.0063\n",
      "For batch number 610 the model has a loss of 0.2781800329685211\n",
      "\u001b[1m610/689\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m4s\u001b[0m 58ms/step - auc: 0.9546 - binary_accuracy: 0.9159 - false_negatives: 259.8967 - false_positives: 564.4787 - loss: 0.2790 - precision: 0.8927 - recall: 0.9469 - true_negatives: 4315.0576 - true_positives: 4636.5674\n",
      "For batch number 611 the model has a loss of 0.2783706784248352\n",
      "\u001b[1m611/689\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m4s\u001b[0m 58ms/step - auc: 0.9546 - binary_accuracy: 0.9159 - false_negatives: 260.3257 - false_positives: 565.3666 - loss: 0.2790 - precision: 0.8927 - recall: 0.9469 - true_negatives: 4322.1851 - true_positives: 4644.1226\n",
      "For batch number 612 the model has a loss of 0.27830132842063904\n",
      "\u001b[1m612/689\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m4s\u001b[0m 58ms/step - auc: 0.9546 - binary_accuracy: 0.9159 - false_negatives: 260.7533 - false_positives: 566.2565 - loss: 0.2790 - precision: 0.8927 - recall: 0.9469 - true_negatives: 4329.3154 - true_positives: 4651.6748\n",
      "For batch number 613 the model has a loss of 0.278908371925354\n",
      "\u001b[1m613/689\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m4s\u001b[0m 58ms/step - auc: 0.9546 - binary_accuracy: 0.9159 - false_negatives: 261.1827 - false_positives: 567.1452 - loss: 0.2790 - precision: 0.8927 - recall: 0.9469 - true_negatives: 4336.4419 - true_positives: 4659.2300\n",
      "For batch number 614 the model has a loss of 0.2791059613227844\n",
      "\u001b[1m614/689\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m4s\u001b[0m 58ms/step - auc: 0.9546 - binary_accuracy: 0.9159 - false_negatives: 261.6124 - false_positives: 568.0342 - loss: 0.2790 - precision: 0.8927 - recall: 0.9469 - true_negatives: 4343.5684 - true_positives: 4666.7852\n",
      "For batch number 615 the model has a loss of 0.2789500057697296\n",
      "\u001b[1m615/689\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m4s\u001b[0m 58ms/step - auc: 0.9546 - binary_accuracy: 0.9159 - false_negatives: 262.0423 - false_positives: 568.9219 - loss: 0.2790 - precision: 0.8927 - recall: 0.9469 - true_negatives: 4350.6909 - true_positives: 4674.3447\n",
      "For batch number 616 the model has a loss of 0.2787777781486511\n",
      "\u001b[1m616/689\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m4s\u001b[0m 58ms/step - auc: 0.9546 - binary_accuracy: 0.9159 - false_negatives: 262.4708 - false_positives: 569.8068 - loss: 0.2790 - precision: 0.8927 - recall: 0.9469 - true_negatives: 4357.8135 - true_positives: 4681.9092\n",
      "For batch number 617 the model has a loss of 0.27923548221588135\n",
      "\u001b[1m617/689\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m4s\u001b[0m 58ms/step - auc: 0.9546 - binary_accuracy: 0.9159 - false_negatives: 262.9011 - false_positives: 570.6937 - loss: 0.2790 - precision: 0.8927 - recall: 0.9469 - true_negatives: 4364.9287 - true_positives: 4689.4766\n",
      "For batch number 618 the model has a loss of 0.27950745820999146\n",
      "\u001b[1m618/689\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m4s\u001b[0m 58ms/step - auc: 0.9546 - binary_accuracy: 0.9159 - false_negatives: 263.3317 - false_positives: 571.5809 - loss: 0.2790 - precision: 0.8928 - recall: 0.9469 - true_negatives: 4372.0420 - true_positives: 4697.0454\n",
      "For batch number 619 the model has a loss of 0.2794511914253235\n",
      "\u001b[1m619/689\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m4s\u001b[0m 58ms/step - auc: 0.9546 - binary_accuracy: 0.9159 - false_negatives: 263.7625 - false_positives: 572.4685 - loss: 0.2790 - precision: 0.8928 - recall: 0.9469 - true_negatives: 4379.1484 - true_positives: 4704.6201\n",
      "For batch number 620 the model has a loss of 0.2794846296310425\n",
      "\u001b[1m620/689\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m4s\u001b[0m 58ms/step - auc: 0.9546 - binary_accuracy: 0.9159 - false_negatives: 264.1935 - false_positives: 573.3532 - loss: 0.2790 - precision: 0.8928 - recall: 0.9469 - true_negatives: 4386.2583 - true_positives: 4712.1953\n",
      "For batch number 621 the model has a loss of 0.27955472469329834\n",
      "\u001b[1m621/689\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m3s\u001b[0m 58ms/step - auc: 0.9546 - binary_accuracy: 0.9159 - false_negatives: 264.6248 - false_positives: 574.2415 - loss: 0.2790 - precision: 0.8928 - recall: 0.9469 - true_negatives: 4393.3672 - true_positives: 4719.7666\n",
      "For batch number 622 the model has a loss of 0.2793983221054077\n",
      "\u001b[1m622/689\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m3s\u001b[0m 58ms/step - auc: 0.9546 - binary_accuracy: 0.9159 - false_negatives: 265.0547 - false_positives: 575.1302 - loss: 0.2790 - precision: 0.8928 - recall: 0.9469 - true_negatives: 4400.4741 - true_positives: 4727.3408\n",
      "For batch number 623 the model has a loss of 0.27944421768188477\n",
      "\u001b[1m623/689\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m3s\u001b[0m 58ms/step - auc: 0.9546 - binary_accuracy: 0.9159 - false_negatives: 265.4832 - false_positives: 576.0209 - loss: 0.2790 - precision: 0.8928 - recall: 0.9469 - true_negatives: 4407.5859 - true_positives: 4734.9102\n",
      "For batch number 624 the model has a loss of 0.279175728559494\n",
      "\u001b[1m624/689\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m3s\u001b[0m 58ms/step - auc: 0.9546 - binary_accuracy: 0.9159 - false_negatives: 265.9102 - false_positives: 576.9086 - loss: 0.2790 - precision: 0.8928 - recall: 0.9469 - true_negatives: 4414.6924 - true_positives: 4742.4888\n",
      "For batch number 625 the model has a loss of 0.279766708612442\n",
      "\u001b[1m625/689\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m3s\u001b[0m 58ms/step - auc: 0.9546 - binary_accuracy: 0.9159 - false_negatives: 266.3392 - false_positives: 577.7984 - loss: 0.2790 - precision: 0.8928 - recall: 0.9469 - true_negatives: 4421.7969 - true_positives: 4750.0654\n",
      "For batch number 626 the model has a loss of 0.2794920802116394\n",
      "\u001b[1m626/689\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m3s\u001b[0m 58ms/step - auc: 0.9546 - binary_accuracy: 0.9159 - false_negatives: 266.7668 - false_positives: 578.6869 - loss: 0.2790 - precision: 0.8928 - recall: 0.9469 - true_negatives: 4428.8994 - true_positives: 4757.6470\n",
      "For batch number 627 the model has a loss of 0.27923211455345154\n",
      "\u001b[1m627/689\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m3s\u001b[0m 58ms/step - auc: 0.9546 - binary_accuracy: 0.9159 - false_negatives: 267.1930 - false_positives: 579.5742 - loss: 0.2790 - precision: 0.8928 - recall: 0.9469 - true_negatives: 4436.0015 - true_positives: 4765.2314\n",
      "For batch number 628 the model has a loss of 0.2794835865497589\n",
      "\u001b[1m628/689\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m3s\u001b[0m 58ms/step - auc: 0.9546 - binary_accuracy: 0.9159 - false_negatives: 267.6210 - false_positives: 580.4650 - loss: 0.2790 - precision: 0.8928 - recall: 0.9469 - true_negatives: 4443.0957 - true_positives: 4772.8184\n",
      "For batch number 629 the model has a loss of 0.28002500534057617\n",
      "\u001b[1m629/689\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m3s\u001b[0m 58ms/step - auc: 0.9546 - binary_accuracy: 0.9159 - false_negatives: 268.0525 - false_positives: 581.3530 - loss: 0.2790 - precision: 0.8928 - recall: 0.9469 - true_negatives: 4450.1826 - true_positives: 4780.4116\n",
      "For batch number 630 the model has a loss of 0.2800244987010956\n",
      "\u001b[1m630/689\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m3s\u001b[0m 58ms/step - auc: 0.9546 - binary_accuracy: 0.9159 - false_negatives: 268.4841 - false_positives: 582.2444 - loss: 0.2790 - precision: 0.8928 - recall: 0.9469 - true_negatives: 4457.2700 - true_positives: 4788.0015\n",
      "For batch number 631 the model has a loss of 0.27976149320602417\n",
      "\u001b[1m631/689\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m3s\u001b[0m 58ms/step - auc: 0.9546 - binary_accuracy: 0.9159 - false_negatives: 268.9144 - false_positives: 583.1331 - loss: 0.2790 - precision: 0.8928 - recall: 0.9469 - true_negatives: 4464.3584 - true_positives: 4795.5942\n",
      "For batch number 632 the model has a loss of 0.27961695194244385\n",
      "\u001b[1m632/689\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m3s\u001b[0m 58ms/step - auc: 0.9546 - binary_accuracy: 0.9159 - false_negatives: 269.3449 - false_positives: 584.0206 - loss: 0.2790 - precision: 0.8928 - recall: 0.9469 - true_negatives: 4471.4414 - true_positives: 4803.1929\n",
      "For batch number 633 the model has a loss of 0.27989259362220764\n",
      "\u001b[1m633/689\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m3s\u001b[0m 58ms/step - auc: 0.9546 - binary_accuracy: 0.9159 - false_negatives: 269.7757 - false_positives: 584.9052 - loss: 0.2790 - precision: 0.8928 - recall: 0.9469 - true_negatives: 4478.5195 - true_positives: 4810.7993\n",
      "For batch number 634 the model has a loss of 0.2802671194076538\n",
      "\u001b[1m634/689\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m3s\u001b[0m 58ms/step - auc: 0.9546 - binary_accuracy: 0.9159 - false_negatives: 270.2082 - false_positives: 585.7902 - loss: 0.2790 - precision: 0.8928 - recall: 0.9469 - true_negatives: 4485.5977 - true_positives: 4818.4038\n",
      "For batch number 635 the model has a loss of 0.280156672000885\n",
      "\u001b[1m635/689\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m3s\u001b[0m 58ms/step - auc: 0.9546 - binary_accuracy: 0.9159 - false_negatives: 270.6394 - false_positives: 586.6756 - loss: 0.2790 - precision: 0.8928 - recall: 0.9469 - true_negatives: 4492.6787 - true_positives: 4826.0063\n",
      "For batch number 636 the model has a loss of 0.2801707684993744\n",
      "\u001b[1m636/689\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m3s\u001b[0m 58ms/step - auc: 0.9546 - binary_accuracy: 0.9159 - false_negatives: 271.0692 - false_positives: 587.5660 - loss: 0.2790 - precision: 0.8928 - recall: 0.9469 - true_negatives: 4499.7549 - true_positives: 4833.6099\n",
      "For batch number 637 the model has a loss of 0.28000739216804504\n",
      "\u001b[1m637/689\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m3s\u001b[0m 58ms/step - auc: 0.9546 - binary_accuracy: 0.9159 - false_negatives: 271.4977 - false_positives: 588.4568 - loss: 0.2790 - precision: 0.8928 - recall: 0.9469 - true_negatives: 4506.8242 - true_positives: 4841.2212\n",
      "For batch number 638 the model has a loss of 0.27994754910469055\n",
      "\u001b[1m638/689\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m2s\u001b[0m 58ms/step - auc: 0.9546 - binary_accuracy: 0.9159 - false_negatives: 271.9263 - false_positives: 589.3480 - loss: 0.2790 - precision: 0.8928 - recall: 0.9469 - true_negatives: 4513.8887 - true_positives: 4848.8369\n",
      "For batch number 639 the model has a loss of 0.27992281317710876\n",
      "\u001b[1m639/689\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m2s\u001b[0m 58ms/step - auc: 0.9546 - binary_accuracy: 0.9159 - false_negatives: 272.3537 - false_positives: 590.2426 - loss: 0.2790 - precision: 0.8928 - recall: 0.9469 - true_negatives: 4520.9531 - true_positives: 4856.4507\n",
      "For batch number 640 the model has a loss of 0.27987390756607056\n",
      "\u001b[1m640/689\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m2s\u001b[0m 58ms/step - auc: 0.9546 - binary_accuracy: 0.9159 - false_negatives: 272.7797 - false_positives: 591.1375 - loss: 0.2790 - precision: 0.8928 - recall: 0.9469 - true_negatives: 4528.0220 - true_positives: 4864.0610\n",
      "For batch number 641 the model has a loss of 0.2799302041530609\n",
      "\u001b[1m641/689\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m2s\u001b[0m 58ms/step - auc: 0.9546 - binary_accuracy: 0.9159 - false_negatives: 273.2044 - false_positives: 592.0390 - loss: 0.2790 - precision: 0.8928 - recall: 0.9469 - true_negatives: 4535.0903 - true_positives: 4871.6660\n",
      "For batch number 642 the model has a loss of 0.279741108417511\n",
      "\u001b[1m642/689\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m2s\u001b[0m 58ms/step - auc: 0.9546 - binary_accuracy: 0.9159 - false_negatives: 273.6277 - false_positives: 592.9408 - loss: 0.2790 - precision: 0.8928 - recall: 0.9469 - true_negatives: 4542.1572 - true_positives: 4879.2739\n",
      "For batch number 643 the model has a loss of 0.27964821457862854\n",
      "\u001b[1m643/689\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m2s\u001b[0m 58ms/step - auc: 0.9546 - binary_accuracy: 0.9159 - false_negatives: 274.0498 - false_positives: 593.8429 - loss: 0.2790 - precision: 0.8928 - recall: 0.9469 - true_negatives: 4549.2271 - true_positives: 4886.8804\n",
      "For batch number 644 the model has a loss of 0.27968546748161316\n",
      "\u001b[1m644/689\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m2s\u001b[0m 58ms/step - auc: 0.9546 - binary_accuracy: 0.9159 - false_negatives: 274.4752 - false_positives: 594.7422 - loss: 0.2790 - precision: 0.8928 - recall: 0.9469 - true_negatives: 4556.2920 - true_positives: 4894.4907\n",
      "For batch number 645 the model has a loss of 0.27943310141563416\n",
      "\u001b[1m645/689\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m2s\u001b[0m 58ms/step - auc: 0.9546 - binary_accuracy: 0.9159 - false_negatives: 274.8992 - false_positives: 595.6403 - loss: 0.2790 - precision: 0.8928 - recall: 0.9469 - true_negatives: 4563.3564 - true_positives: 4902.1040\n",
      "For batch number 646 the model has a loss of 0.27930721640586853\n",
      "\u001b[1m646/689\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m2s\u001b[0m 58ms/step - auc: 0.9546 - binary_accuracy: 0.9159 - false_negatives: 275.3220 - false_positives: 596.5387 - loss: 0.2790 - precision: 0.8928 - recall: 0.9469 - true_negatives: 4570.4243 - true_positives: 4909.7153\n",
      "For batch number 647 the model has a loss of 0.27929338812828064\n",
      "\u001b[1m647/689\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m2s\u001b[0m 58ms/step - auc: 0.9546 - binary_accuracy: 0.9159 - false_negatives: 275.7434 - false_positives: 597.4390 - loss: 0.2790 - precision: 0.8928 - recall: 0.9469 - true_negatives: 4577.4932 - true_positives: 4917.3247\n",
      "For batch number 648 the model has a loss of 0.2790897786617279\n",
      "\u001b[1m648/689\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m2s\u001b[0m 58ms/step - auc: 0.9546 - binary_accuracy: 0.9159 - false_negatives: 276.1636 - false_positives: 598.3380 - loss: 0.2790 - precision: 0.8928 - recall: 0.9469 - true_negatives: 4584.5615 - true_positives: 4924.9365\n",
      "For batch number 649 the model has a loss of 0.27903011441230774\n",
      "\u001b[1m649/689\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m2s\u001b[0m 58ms/step - auc: 0.9546 - binary_accuracy: 0.9159 - false_negatives: 276.5840 - false_positives: 599.2373 - loss: 0.2790 - precision: 0.8928 - recall: 0.9469 - true_negatives: 4591.6333 - true_positives: 4932.5454\n",
      "For batch number 650 the model has a loss of 0.27905362844467163\n",
      "\u001b[1m650/689\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m2s\u001b[0m 58ms/step - auc: 0.9546 - binary_accuracy: 0.9159 - false_negatives: 277.0046 - false_positives: 600.1385 - loss: 0.2790 - precision: 0.8928 - recall: 0.9469 - true_negatives: 4598.7046 - true_positives: 4940.1523\n",
      "For batch number 651 the model has a loss of 0.2788618206977844\n",
      "\u001b[1m651/689\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m2s\u001b[0m 58ms/step - auc: 0.9546 - binary_accuracy: 0.9159 - false_negatives: 277.4240 - false_positives: 601.0384 - loss: 0.2790 - precision: 0.8928 - recall: 0.9469 - true_negatives: 4605.7759 - true_positives: 4947.7617\n",
      "For batch number 652 the model has a loss of 0.27902862429618835\n",
      "\u001b[1m652/689\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m2s\u001b[0m 58ms/step - auc: 0.9546 - binary_accuracy: 0.9159 - false_negatives: 277.8451 - false_positives: 601.9371 - loss: 0.2790 - precision: 0.8928 - recall: 0.9469 - true_negatives: 4612.8481 - true_positives: 4955.3696\n",
      "For batch number 653 the model has a loss of 0.27899619936943054\n",
      "\u001b[1m653/689\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m2s\u001b[0m 58ms/step - auc: 0.9546 - binary_accuracy: 0.9160 - false_negatives: 278.2649 - false_positives: 602.8392 - loss: 0.2790 - precision: 0.8928 - recall: 0.9469 - true_negatives: 4619.9233 - true_positives: 4962.9727\n",
      "For batch number 654 the model has a loss of 0.27892589569091797\n",
      "\u001b[1m654/689\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m2s\u001b[0m 58ms/step - auc: 0.9546 - binary_accuracy: 0.9160 - false_negatives: 278.6835 - false_positives: 603.7416 - loss: 0.2790 - precision: 0.8928 - recall: 0.9469 - true_negatives: 4626.9971 - true_positives: 4970.5781\n",
      "For batch number 655 the model has a loss of 0.27891024947166443\n",
      "\u001b[1m655/689\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m1s\u001b[0m 58ms/step - auc: 0.9546 - binary_accuracy: 0.9160 - false_negatives: 279.1038 - false_positives: 604.6458 - loss: 0.2790 - precision: 0.8928 - recall: 0.9469 - true_negatives: 4634.0703 - true_positives: 4978.1802\n",
      "For batch number 656 the model has a loss of 0.2787489891052246\n",
      "\u001b[1m656/689\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m1s\u001b[0m 58ms/step - auc: 0.9546 - binary_accuracy: 0.9160 - false_negatives: 279.5229 - false_positives: 605.5488 - loss: 0.2790 - precision: 0.8928 - recall: 0.9469 - true_negatives: 4641.1465 - true_positives: 4985.7822\n",
      "For batch number 657 the model has a loss of 0.27879735827445984\n",
      "\u001b[1m657/689\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m1s\u001b[0m 58ms/step - auc: 0.9546 - binary_accuracy: 0.9160 - false_negatives: 279.9422 - false_positives: 606.4520 - loss: 0.2790 - precision: 0.8928 - recall: 0.9469 - true_negatives: 4648.2236 - true_positives: 4993.3818\n",
      "For batch number 658 the model has a loss of 0.2786770761013031\n",
      "\u001b[1m658/689\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m1s\u001b[0m 58ms/step - auc: 0.9546 - binary_accuracy: 0.9160 - false_negatives: 280.3617 - false_positives: 607.3541 - loss: 0.2790 - precision: 0.8928 - recall: 0.9469 - true_negatives: 4655.3022 - true_positives: 5000.9819\n",
      "For batch number 659 the model has a loss of 0.27849429845809937\n",
      "\u001b[1m659/689\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m1s\u001b[0m 58ms/step - auc: 0.9546 - binary_accuracy: 0.9160 - false_negatives: 280.7800 - false_positives: 608.2549 - loss: 0.2790 - precision: 0.8928 - recall: 0.9469 - true_negatives: 4662.3838 - true_positives: 5008.5811\n",
      "For batch number 660 the model has a loss of 0.27837780117988586\n",
      "\u001b[1m660/689\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m1s\u001b[0m 58ms/step - auc: 0.9546 - binary_accuracy: 0.9160 - false_negatives: 281.1970 - false_positives: 609.1561 - loss: 0.2790 - precision: 0.8928 - recall: 0.9469 - true_negatives: 4669.4727 - true_positives: 5016.1743\n",
      "For batch number 661 the model has a loss of 0.2783145308494568\n",
      "\u001b[1m661/689\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m1s\u001b[0m 58ms/step - auc: 0.9546 - binary_accuracy: 0.9160 - false_negatives: 281.6127 - false_positives: 610.0590 - loss: 0.2790 - precision: 0.8928 - recall: 0.9469 - true_negatives: 4676.5645 - true_positives: 5023.7642\n",
      "For batch number 662 the model has a loss of 0.2780812084674835\n",
      "\u001b[1m662/689\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m1s\u001b[0m 58ms/step - auc: 0.9546 - binary_accuracy: 0.9160 - false_negatives: 282.0272 - false_positives: 610.9592 - loss: 0.2790 - precision: 0.8928 - recall: 0.9469 - true_negatives: 4683.6572 - true_positives: 5031.3564\n",
      "For batch number 663 the model has a loss of 0.27806469798088074\n",
      "\u001b[1m663/689\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m1s\u001b[0m 58ms/step - auc: 0.9546 - binary_accuracy: 0.9160 - false_negatives: 282.4404 - false_positives: 611.8627 - loss: 0.2790 - precision: 0.8928 - recall: 0.9469 - true_negatives: 4690.7559 - true_positives: 5038.9414\n",
      "For batch number 664 the model has a loss of 0.27792420983314514\n",
      "\u001b[1m664/689\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m1s\u001b[0m 58ms/step - auc: 0.9546 - binary_accuracy: 0.9160 - false_negatives: 282.8524 - false_positives: 612.7651 - loss: 0.2790 - precision: 0.8928 - recall: 0.9469 - true_negatives: 4697.8525 - true_positives: 5046.5303\n",
      "For batch number 665 the model has a loss of 0.27804824709892273\n",
      "\u001b[1m665/689\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m1s\u001b[0m 58ms/step - auc: 0.9546 - binary_accuracy: 0.9160 - false_negatives: 283.2646 - false_positives: 613.6661 - loss: 0.2790 - precision: 0.8928 - recall: 0.9469 - true_negatives: 4704.9517 - true_positives: 5054.1172\n",
      "For batch number 666 the model has a loss of 0.2781558930873871\n",
      "\u001b[1m666/689\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m1s\u001b[0m 58ms/step - auc: 0.9546 - binary_accuracy: 0.9160 - false_negatives: 283.6772 - false_positives: 614.5646 - loss: 0.2790 - precision: 0.8928 - recall: 0.9469 - true_negatives: 4712.0493 - true_positives: 5061.7085\n",
      "For batch number 667 the model has a loss of 0.27839377522468567\n",
      "\u001b[1m667/689\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m1s\u001b[0m 58ms/step - auc: 0.9546 - binary_accuracy: 0.9160 - false_negatives: 284.0900 - false_positives: 615.4633 - loss: 0.2790 - precision: 0.8928 - recall: 0.9469 - true_negatives: 4719.1484 - true_positives: 5069.2983\n",
      "For batch number 668 the model has a loss of 0.27815237641334534\n",
      "\u001b[1m668/689\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m1s\u001b[0m 58ms/step - auc: 0.9546 - binary_accuracy: 0.9160 - false_negatives: 284.5015 - false_positives: 616.3608 - loss: 0.2790 - precision: 0.8928 - recall: 0.9469 - true_negatives: 4726.2471 - true_positives: 5076.8906\n",
      "For batch number 669 the model has a loss of 0.27798911929130554\n",
      "\u001b[1m669/689\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m1s\u001b[0m 58ms/step - auc: 0.9546 - binary_accuracy: 0.9160 - false_negatives: 284.9118 - false_positives: 617.2586 - loss: 0.2790 - precision: 0.8928 - recall: 0.9469 - true_negatives: 4733.3511 - true_positives: 5084.4785\n",
      "For batch number 670 the model has a loss of 0.27783623337745667\n",
      "\u001b[1m670/689\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m1s\u001b[0m 58ms/step - auc: 0.9546 - binary_accuracy: 0.9160 - false_negatives: 285.3209 - false_positives: 618.1552 - loss: 0.2790 - precision: 0.8928 - recall: 0.9469 - true_negatives: 4740.4565 - true_positives: 5092.0674\n",
      "For batch number 671 the model has a loss of 0.27770179510116577\n",
      "\u001b[1m671/689\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m1s\u001b[0m 58ms/step - auc: 0.9546 - binary_accuracy: 0.9160 - false_negatives: 285.7288 - false_positives: 619.0492 - loss: 0.2790 - precision: 0.8928 - recall: 0.9469 - true_negatives: 4747.5649 - true_positives: 5099.6572\n",
      "For batch number 672 the model has a loss of 0.2776391804218292\n",
      "\u001b[1m672/689\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 58ms/step - auc: 0.9546 - binary_accuracy: 0.9160 - false_negatives: 286.1369 - false_positives: 619.9420 - loss: 0.2790 - precision: 0.8928 - recall: 0.9469 - true_negatives: 4754.6758 - true_positives: 5107.2456\n",
      "For batch number 673 the model has a loss of 0.27749237418174744\n",
      "\u001b[1m673/689\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 58ms/step - auc: 0.9546 - binary_accuracy: 0.9160 - false_negatives: 286.5438 - false_positives: 620.8351 - loss: 0.2790 - precision: 0.8928 - recall: 0.9469 - true_negatives: 4761.7891 - true_positives: 5114.8320\n",
      "For batch number 674 the model has a loss of 0.27741992473602295\n",
      "\u001b[1m674/689\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 58ms/step - auc: 0.9546 - binary_accuracy: 0.9160 - false_negatives: 286.9510 - false_positives: 621.7285 - loss: 0.2790 - precision: 0.8928 - recall: 0.9469 - true_negatives: 4768.9067 - true_positives: 5122.4141\n",
      "For batch number 675 the model has a loss of 0.2772064805030823\n",
      "\u001b[1m675/689\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 58ms/step - auc: 0.9546 - binary_accuracy: 0.9160 - false_negatives: 287.3570 - false_positives: 622.6207 - loss: 0.2790 - precision: 0.8928 - recall: 0.9469 - true_negatives: 4776.0239 - true_positives: 5129.9985\n",
      "For batch number 676 the model has a loss of 0.277119904756546\n",
      "\u001b[1m676/689\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 58ms/step - auc: 0.9546 - binary_accuracy: 0.9160 - false_negatives: 287.7633 - false_positives: 623.5133 - loss: 0.2790 - precision: 0.8928 - recall: 0.9469 - true_negatives: 4783.1392 - true_positives: 5137.5845\n",
      "For batch number 677 the model has a loss of 0.27733325958251953\n",
      "\u001b[1m677/689\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 58ms/step - auc: 0.9546 - binary_accuracy: 0.9160 - false_negatives: 288.1714 - false_positives: 624.4047 - loss: 0.2790 - precision: 0.8928 - recall: 0.9469 - true_negatives: 4790.2466 - true_positives: 5145.1772\n",
      "For batch number 678 the model has a loss of 0.27738526463508606\n",
      "\u001b[1m678/689\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 58ms/step - auc: 0.9546 - binary_accuracy: 0.9160 - false_negatives: 288.5782 - false_positives: 625.2994 - loss: 0.2790 - precision: 0.8928 - recall: 0.9469 - true_negatives: 4797.3584 - true_positives: 5152.7642\n",
      "For batch number 679 the model has a loss of 0.2772060036659241\n",
      "\u001b[1m679/689\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 58ms/step - auc: 0.9546 - binary_accuracy: 0.9160 - false_negatives: 288.9838 - false_positives: 626.1929 - loss: 0.2790 - precision: 0.8928 - recall: 0.9469 - true_negatives: 4804.4741 - true_positives: 5160.3491\n",
      "For batch number 680 the model has a loss of 0.2771064341068268\n",
      "\u001b[1m680/689\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 58ms/step - auc: 0.9546 - binary_accuracy: 0.9160 - false_negatives: 289.3897 - false_positives: 627.0838 - loss: 0.2790 - precision: 0.8928 - recall: 0.9469 - true_negatives: 4811.5869 - true_positives: 5167.9399\n",
      "For batch number 681 the model has a loss of 0.2774336040019989\n",
      "\u001b[1m681/689\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 58ms/step - auc: 0.9546 - binary_accuracy: 0.9160 - false_negatives: 289.7974 - false_positives: 627.9765 - loss: 0.2790 - precision: 0.8928 - recall: 0.9469 - true_negatives: 4818.7065 - true_positives: 5175.5200\n",
      "For batch number 682 the model has a loss of 0.2773793339729309\n",
      "\u001b[1m682/689\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 58ms/step - auc: 0.9546 - binary_accuracy: 0.9160 - false_negatives: 290.2038 - false_positives: 628.8695 - loss: 0.2790 - precision: 0.8928 - recall: 0.9469 - true_negatives: 4825.8242 - true_positives: 5183.1025\n",
      "For batch number 683 the model has a loss of 0.27729472517967224\n",
      "\u001b[1m683/689\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 58ms/step - auc: 0.9546 - binary_accuracy: 0.9160 - false_negatives: 290.6091 - false_positives: 629.7628 - loss: 0.2790 - precision: 0.8928 - recall: 0.9469 - true_negatives: 4832.9429 - true_positives: 5190.6851\n",
      "For batch number 684 the model has a loss of 0.27728915214538574\n",
      "\u001b[1m684/689\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 58ms/step - auc: 0.9546 - binary_accuracy: 0.9160 - false_negatives: 291.0146 - false_positives: 630.6564 - loss: 0.2790 - precision: 0.8928 - recall: 0.9469 - true_negatives: 4840.0557 - true_positives: 5198.2734\n",
      "For batch number 685 the model has a loss of 0.27720510959625244\n",
      "\u001b[1m685/689\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 58ms/step - auc: 0.9546 - binary_accuracy: 0.9160 - false_negatives: 291.4190 - false_positives: 631.5489 - loss: 0.2790 - precision: 0.8928 - recall: 0.9469 - true_negatives: 4847.1709 - true_positives: 5205.8613\n",
      "For batch number 686 the model has a loss of 0.2771328389644623\n",
      "\u001b[1m686/689\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 58ms/step - auc: 0.9546 - binary_accuracy: 0.9160 - false_negatives: 291.8236 - false_positives: 632.4402 - loss: 0.2790 - precision: 0.8928 - recall: 0.9469 - true_negatives: 4854.2871 - true_positives: 5213.4492\n",
      "For batch number 687 the model has a loss of 0.27703601121902466\n",
      "\u001b[1m687/689\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 58ms/step - auc: 0.9546 - binary_accuracy: 0.9160 - false_negatives: 292.2271 - false_positives: 633.3304 - loss: 0.2790 - precision: 0.8928 - recall: 0.9469 - true_negatives: 4861.4092 - true_positives: 5221.0337\n",
      "For batch number 688 the model has a loss of 0.27696049213409424\n",
      "\u001b[1m688/689\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 58ms/step - auc: 0.9546 - binary_accuracy: 0.9160 - false_negatives: 292.6294 - false_positives: 634.2195 - loss: 0.2790 - precision: 0.8928 - recall: 0.9469 - true_negatives: 4868.5332 - true_positives: 5228.6177\n",
      "For batch number 689 the model has a loss of 0.2767498791217804\n",
      "\u001b[1m689/689\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 58ms/step - auc: 0.9546 - binary_accuracy: 0.9160 - false_negatives: 293.0305 - false_positives: 635.1074 - loss: 0.2790 - precision: 0.8928 - recall: 0.9469 - true_negatives: 4875.6577 - true_positives: 5236.2017\n",
      "For epoch number 3 the model has a loss of 0.2767498791217804\n",
      "\u001b[1m689/689\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m42s\u001b[0m 61ms/step - auc: 0.9546 - binary_accuracy: 0.9160 - false_negatives: 293.4304 - false_positives: 635.9927 - loss: 0.2789 - precision: 0.8928 - recall: 0.9469 - true_negatives: 4882.7607 - true_positives: 5243.7637 - val_auc: 0.9479 - val_binary_accuracy: 0.9056 - val_false_negatives: 224.0000 - val_false_positives: 36.0000 - val_loss: 0.6606 - val_precision: 0.9702 - val_recall: 0.8395 - val_true_negatives: 1323.0000 - val_true_positives: 1172.0000\n",
      "Epoch 4/5\n",
      "\n",
      "For batch number 1 the model has a loss of 0.20407789945602417\n",
      "\u001b[1m  1/689\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m53s\u001b[0m 78ms/step - auc: 0.9921 - binary_accuracy: 0.9375 - false_negatives: 0.0000e+00 - false_positives: 2.0000 - loss: 0.2041 - precision: 0.9000 - recall: 1.0000 - true_negatives: 12.0000 - true_positives: 18.0000\n",
      "For batch number 2 the model has a loss of 0.1422961950302124\n",
      "\u001b[1m  2/689\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m45s\u001b[0m 66ms/step - auc: 0.9951 - binary_accuracy: 0.9531 - false_negatives: 0.0000e+00 - false_positives: 2.0000 - loss: 0.1732 - precision: 0.9214 - recall: 1.0000 - true_negatives: 20.5000 - true_positives: 25.5000\n",
      "For batch number 3 the model has a loss of 0.17977185547351837\n",
      "\u001b[1m  3/689\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m43s\u001b[0m 63ms/step - auc: 0.9927 - binary_accuracy: 0.9583 - false_negatives: 0.3333 - false_positives: 2.0000 - loss: 0.1754 - precision: 0.9350 - recall: 0.9936 - true_negatives: 27.6667 - true_positives: 34.0000    \n",
      "For batch number 4 the model has a loss of 0.20181027054786682\n",
      "\u001b[1m  4/689\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m41s\u001b[0m 60ms/step - auc: 0.9898 - binary_accuracy: 0.9551 - false_negatives: 0.5000 - false_positives: 3.0000 - loss: 0.1820 - precision: 0.9302 - recall: 0.9914 - true_negatives: 34.7500 - true_positives: 41.7500\n",
      "For batch number 5 the model has a loss of 0.19184517860412598\n",
      "\u001b[1m  5/689\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m40s\u001b[0m 59ms/step - auc: 0.9885 - binary_accuracy: 0.9516 - false_negatives: 1.2000 - false_positives: 3.6000 - loss: 0.1840 - precision: 0.9303 - recall: 0.9837 - true_negatives: 41.6000 - true_positives: 49.6000\n",
      "For batch number 6 the model has a loss of 0.18766506016254425\n",
      "\u001b[1m  6/689\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m40s\u001b[0m 60ms/step - auc: 0.9879 - binary_accuracy: 0.9492 - false_negatives: 1.8333 - false_positives: 4.1667 - loss: 0.1846 - precision: 0.9309 - recall: 0.9784 - true_negatives: 48.1667 - true_positives: 57.8333\n",
      "For batch number 7 the model has a loss of 0.19490011036396027\n",
      "\u001b[1m  7/689\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m41s\u001b[0m 61ms/step - auc: 0.9870 - binary_accuracy: 0.9463 - false_negatives: 2.4286 - false_positives: 5.0000 - loss: 0.1861 - precision: 0.9290 - recall: 0.9742 - true_negatives: 55.1429 - true_positives: 65.4286\n",
      "For batch number 8 the model has a loss of 0.23454175889492035\n",
      "\u001b[1m  8/689\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m41s\u001b[0m 61ms/step - auc: 0.9859 - binary_accuracy: 0.9442 - false_negatives: 3.0000 - false_positives: 5.7500 - loss: 0.1921 - precision: 0.9279 - recall: 0.9709 - true_negatives: 62.1250 - true_positives: 73.1250\n",
      "For batch number 9 the model has a loss of 0.2287498265504837\n",
      "\u001b[1m  9/689\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m40s\u001b[0m 60ms/step - auc: 0.9851 - binary_accuracy: 0.9427 - false_negatives: 3.4444 - false_positives: 6.5556 - loss: 0.1962 - precision: 0.9264 - recall: 0.9688 - true_negatives: 69.5556 - true_positives: 80.4444\n",
      "For batch number 10 the model has a loss of 0.2619338929653168\n",
      "\u001b[1m 10/689\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m41s\u001b[0m 60ms/step - auc: 0.9834 - binary_accuracy: 0.9400 - false_negatives: 4.2000 - false_positives: 7.5000 - loss: 0.2028 - precision: 0.9242 - recall: 0.9652 - true_negatives: 76.7000 - true_positives: 87.6000\n",
      "For batch number 11 the model has a loss of 0.266094833612442\n",
      "\u001b[1m 11/689\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m40s\u001b[0m 60ms/step - auc: 0.9820 - binary_accuracy: 0.9372 - false_negatives: 5.0909 - false_positives: 8.4545 - loss: 0.2085 - precision: 0.9225 - recall: 0.9614 - true_negatives: 83.2727 - true_positives: 95.1818\n",
      "For batch number 12 the model has a loss of 0.27216199040412903\n",
      "\u001b[1m 12/689\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m40s\u001b[0m 60ms/step - auc: 0.9806 - binary_accuracy: 0.9352 - false_negatives: 5.9167 - false_positives: 9.2500 - loss: 0.2138 - precision: 0.9217 - recall: 0.9585 - true_negatives: 89.9167 - true_positives: 102.9167\n",
      "For batch number 13 the model has a loss of 0.2751934230327606\n",
      "\u001b[1m 13/689\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m40s\u001b[0m 60ms/step - auc: 0.9794 - binary_accuracy: 0.9339 - false_negatives: 6.6923 - false_positives: 9.9231 - loss: 0.2185 - precision: 0.9216 - recall: 0.9562 - true_negatives: 96.3077 - true_positives: 111.0769\n",
      "For batch number 14 the model has a loss of 0.27531757950782776\n",
      "\u001b[1m 14/689\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m41s\u001b[0m 61ms/step - auc: 0.9784 - binary_accuracy: 0.9328 - false_negatives: 7.4286 - false_positives: 10.6429 - loss: 0.2226 - precision: 0.9213 - recall: 0.9543 - true_negatives: 102.9286 - true_positives: 119.0000\n",
      "For batch number 15 the model has a loss of 0.2728528380393982\n",
      "\u001b[1m 15/689\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m40s\u001b[0m 61ms/step - auc: 0.9775 - binary_accuracy: 0.9317 - false_negatives: 8.1333 - false_positives: 11.4000 - loss: 0.2259 - precision: 0.9208 - recall: 0.9526 - true_negatives: 109.7333 - true_positives: 126.7333\n",
      "For batch number 16 the model has a loss of 0.26923179626464844\n",
      "\u001b[1m 16/689\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m40s\u001b[0m 61ms/step - auc: 0.9767 - binary_accuracy: 0.9307 - false_negatives: 8.8125 - false_positives: 12.1875 - loss: 0.2287 - precision: 0.9203 - recall: 0.9511 - true_negatives: 116.5625 - true_positives: 134.4375\n",
      "For batch number 17 the model has a loss of 0.2696552872657776\n",
      "\u001b[1m 17/689\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m40s\u001b[0m 61ms/step - auc: 0.9759 - binary_accuracy: 0.9298 - false_negatives: 9.4706 - false_positives: 13.0000 - loss: 0.2311 - precision: 0.9197 - recall: 0.9499 - true_negatives: 123.4706 - true_positives: 142.0588\n",
      "For batch number 18 the model has a loss of 0.27280786633491516\n",
      "\u001b[1m 18/689\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m40s\u001b[0m 60ms/step - auc: 0.9750 - binary_accuracy: 0.9289 - false_negatives: 10.0556 - false_positives: 13.9444 - loss: 0.2334 - precision: 0.9187 - recall: 0.9489 - true_negatives: 130.5000 - true_positives: 149.5000\n",
      "For batch number 19 the model has a loss of 0.26905933022499084\n",
      "\u001b[1m 19/689\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m40s\u001b[0m 60ms/step - auc: 0.9743 - binary_accuracy: 0.9282 - false_negatives: 10.5789 - false_positives: 14.8421 - loss: 0.2353 - precision: 0.9179 - recall: 0.9482 - true_negatives: 137.6316 - true_positives: 156.9474\n",
      "For batch number 20 the model has a loss of 0.2617129683494568\n",
      "\u001b[1m 20/689\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m40s\u001b[0m 60ms/step - auc: 0.9737 - binary_accuracy: 0.9277 - false_negatives: 11.0500 - false_positives: 15.7000 - loss: 0.2366 - precision: 0.9173 - recall: 0.9477 - true_negatives: 144.9000 - true_positives: 164.3500\n",
      "For batch number 21 the model has a loss of 0.2594931125640869\n",
      "\u001b[1m 21/689\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m39s\u001b[0m 60ms/step - auc: 0.9732 - binary_accuracy: 0.9273 - false_negatives: 11.5238 - false_positives: 16.5714 - loss: 0.2377 - precision: 0.9167 - recall: 0.9473 - true_negatives: 152.1429 - true_positives: 171.7619\n",
      "For batch number 22 the model has a loss of 0.25237759947776794\n",
      "\u001b[1m 22/689\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m39s\u001b[0m 60ms/step - auc: 0.9728 - binary_accuracy: 0.9270 - false_negatives: 11.9545 - false_positives: 17.3636 - loss: 0.2383 - precision: 0.9163 - recall: 0.9470 - true_negatives: 159.2727 - true_positives: 179.4091\n",
      "For batch number 23 the model has a loss of 0.2509475648403168\n",
      "\u001b[1m 23/689\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m39s\u001b[0m 60ms/step - auc: 0.9724 - binary_accuracy: 0.9268 - false_negatives: 12.3478 - false_positives: 18.1739 - loss: 0.2389 - precision: 0.9159 - recall: 0.9469 - true_negatives: 166.4783 - true_positives: 187.0000\n",
      "For batch number 24 the model has a loss of 0.2500283420085907\n",
      "\u001b[1m 24/689\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m39s\u001b[0m 59ms/step - auc: 0.9721 - binary_accuracy: 0.9267 - false_negatives: 12.7083 - false_positives: 18.9583 - loss: 0.2394 - precision: 0.9157 - recall: 0.9469 - true_negatives: 173.6250 - true_positives: 194.7083\n",
      "For batch number 25 the model has a loss of 0.24520009756088257\n",
      "\u001b[1m 25/689\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m39s\u001b[0m 59ms/step - auc: 0.9718 - binary_accuracy: 0.9267 - false_negatives: 13.0400 - false_positives: 19.7200 - loss: 0.2396 - precision: 0.9155 - recall: 0.9469 - true_negatives: 180.8800 - true_positives: 202.3600\n",
      "For batch number 26 the model has a loss of 0.24396854639053345\n",
      "\u001b[1m 26/689\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m39s\u001b[0m 59ms/step - auc: 0.9716 - binary_accuracy: 0.9267 - false_negatives: 13.3846 - false_positives: 20.4615 - loss: 0.2398 - precision: 0.9153 - recall: 0.9470 - true_negatives: 188.1923 - true_positives: 209.9615\n",
      "For batch number 27 the model has a loss of 0.24006950855255127\n",
      "\u001b[1m 27/689\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m39s\u001b[0m 59ms/step - auc: 0.9714 - binary_accuracy: 0.9268 - false_negatives: 13.7037 - false_positives: 21.1852 - loss: 0.2398 - precision: 0.9152 - recall: 0.9471 - true_negatives: 195.5556 - true_positives: 217.5556\n",
      "For batch number 28 the model has a loss of 0.2461797297000885\n",
      "\u001b[1m 28/689\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m38s\u001b[0m 59ms/step - auc: 0.9711 - binary_accuracy: 0.9268 - false_negatives: 14.0714 - false_positives: 21.8929 - loss: 0.2400 - precision: 0.9151 - recall: 0.9471 - true_negatives: 202.8929 - true_positives: 225.1429\n",
      "For batch number 29 the model has a loss of 0.24699349701404572\n",
      "\u001b[1m 29/689\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m38s\u001b[0m 59ms/step - auc: 0.9709 - binary_accuracy: 0.9269 - false_negatives: 14.4483 - false_positives: 22.5517 - loss: 0.2402 - precision: 0.9151 - recall: 0.9471 - true_negatives: 210.2069 - true_positives: 232.7931\n",
      "For batch number 30 the model has a loss of 0.24641472101211548\n",
      "\u001b[1m 30/689\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m38s\u001b[0m 59ms/step - auc: 0.9707 - binary_accuracy: 0.9270 - false_negatives: 14.8000 - false_positives: 23.2333 - loss: 0.2404 - precision: 0.9151 - recall: 0.9471 - true_negatives: 217.4667 - true_positives: 240.5000\n",
      "For batch number 31 the model has a loss of 0.24553896486759186\n",
      "\u001b[1m 31/689\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m38s\u001b[0m 59ms/step - auc: 0.9705 - binary_accuracy: 0.9270 - false_negatives: 15.1613 - false_positives: 23.9032 - loss: 0.2406 - precision: 0.9151 - recall: 0.9472 - true_negatives: 224.8065 - true_positives: 248.1290\n",
      "For batch number 32 the model has a loss of 0.24256466329097748\n",
      "\u001b[1m 32/689\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m38s\u001b[0m 59ms/step - auc: 0.9704 - binary_accuracy: 0.9272 - false_negatives: 15.5000 - false_positives: 24.5312 - loss: 0.2407 - precision: 0.9153 - recall: 0.9473 - true_negatives: 232.0312 - true_positives: 255.9375\n",
      "For batch number 33 the model has a loss of 0.23857903480529785\n",
      "\u001b[1m 33/689\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m38s\u001b[0m 59ms/step - auc: 0.9703 - binary_accuracy: 0.9273 - false_negatives: 15.8182 - false_positives: 25.1515 - loss: 0.2406 - precision: 0.9154 - recall: 0.9474 - true_negatives: 239.3030 - true_positives: 263.7273\n",
      "For batch number 34 the model has a loss of 0.2425452619791031\n",
      "\u001b[1m 34/689\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m38s\u001b[0m 59ms/step - auc: 0.9701 - binary_accuracy: 0.9274 - false_negatives: 16.1471 - false_positives: 25.8529 - loss: 0.2407 - precision: 0.9154 - recall: 0.9475 - true_negatives: 246.5000 - true_positives: 271.5000\n",
      "For batch number 35 the model has a loss of 0.24493765830993652\n",
      "\u001b[1m 35/689\u001b[0m \u001b[32m━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m38s\u001b[0m 59ms/step - auc: 0.9700 - binary_accuracy: 0.9275 - false_negatives: 16.4857 - false_positives: 26.5429 - loss: 0.2408 - precision: 0.9154 - recall: 0.9476 - true_negatives: 253.6000 - true_positives: 279.3714\n",
      "For batch number 36 the model has a loss of 0.2450835108757019\n",
      "\u001b[1m 36/689\u001b[0m \u001b[32m━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m38s\u001b[0m 59ms/step - auc: 0.9699 - binary_accuracy: 0.9275 - false_negatives: 16.8889 - false_positives: 27.2222 - loss: 0.2409 - precision: 0.9154 - recall: 0.9476 - true_negatives: 260.6111 - true_positives: 287.2778\n",
      "For batch number 37 the model has a loss of 0.24106287956237793\n",
      "\u001b[1m 37/689\u001b[0m \u001b[32m━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m38s\u001b[0m 59ms/step - auc: 0.9698 - binary_accuracy: 0.9276 - false_negatives: 17.2973 - false_positives: 27.8649 - loss: 0.2409 - precision: 0.9155 - recall: 0.9476 - true_negatives: 267.6216 - true_positives: 295.2162\n",
      "For batch number 38 the model has a loss of 0.2389824539422989\n",
      "\u001b[1m 38/689\u001b[0m \u001b[32m━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m38s\u001b[0m 58ms/step - auc: 0.9697 - binary_accuracy: 0.9277 - false_negatives: 17.7105 - false_positives: 28.4737 - loss: 0.2409 - precision: 0.9157 - recall: 0.9477 - true_negatives: 274.5000 - true_positives: 303.3158\n",
      "For batch number 39 the model has a loss of 0.24640701711177826\n",
      "\u001b[1m 39/689\u001b[0m \u001b[32m━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m37s\u001b[0m 58ms/step - auc: 0.9696 - binary_accuracy: 0.9277 - false_negatives: 18.1795 - false_positives: 29.0769 - loss: 0.2410 - precision: 0.9159 - recall: 0.9476 - true_negatives: 281.2820 - true_positives: 311.4615\n",
      "For batch number 40 the model has a loss of 0.24632474780082703\n",
      "\u001b[1m 40/689\u001b[0m \u001b[32m━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m37s\u001b[0m 58ms/step - auc: 0.9694 - binary_accuracy: 0.9278 - false_negatives: 18.6500 - false_positives: 29.7000 - loss: 0.2411 - precision: 0.9160 - recall: 0.9475 - true_negatives: 288.1250 - true_positives: 319.5250\n",
      "For batch number 41 the model has a loss of 0.24503712356090546\n",
      "\u001b[1m 41/689\u001b[0m \u001b[32m━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m37s\u001b[0m 58ms/step - auc: 0.9693 - binary_accuracy: 0.9278 - false_negatives: 19.0976 - false_positives: 30.3171 - loss: 0.2412 - precision: 0.9162 - recall: 0.9475 - true_negatives: 295.0244 - true_positives: 327.5610\n",
      "For batch number 42 the model has a loss of 0.24195393919944763\n",
      "\u001b[1m 42/689\u001b[0m \u001b[32m━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m37s\u001b[0m 58ms/step - auc: 0.9692 - binary_accuracy: 0.9279 - false_negatives: 19.5476 - false_positives: 30.9048 - loss: 0.2412 - precision: 0.9164 - recall: 0.9475 - true_negatives: 301.8810 - true_positives: 335.6667\n",
      "For batch number 43 the model has a loss of 0.24029968678951263\n",
      "\u001b[1m 43/689\u001b[0m \u001b[32m━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m37s\u001b[0m 58ms/step - auc: 0.9691 - binary_accuracy: 0.9280 - false_negatives: 20.0000 - false_positives: 31.4884 - loss: 0.2412 - precision: 0.9166 - recall: 0.9474 - true_negatives: 308.7442 - true_positives: 343.7675\n",
      "For batch number 44 the model has a loss of 0.23899485170841217\n",
      "\u001b[1m 44/689\u001b[0m \u001b[32m━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m37s\u001b[0m 59ms/step - auc: 0.9690 - binary_accuracy: 0.9281 - false_negatives: 20.4318 - false_positives: 32.0682 - loss: 0.2412 - precision: 0.9167 - recall: 0.9474 - true_negatives: 315.6364 - true_positives: 351.8636\n",
      "For batch number 45 the model has a loss of 0.23793676495552063\n",
      "\u001b[1m 45/689\u001b[0m \u001b[32m━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m37s\u001b[0m 59ms/step - auc: 0.9690 - binary_accuracy: 0.9282 - false_negatives: 20.8444 - false_positives: 32.6444 - loss: 0.2411 - precision: 0.9169 - recall: 0.9474 - true_negatives: 322.6222 - true_positives: 359.8889\n",
      "For batch number 46 the model has a loss of 0.2422981858253479\n",
      "\u001b[1m 46/689\u001b[0m \u001b[32m━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m37s\u001b[0m 59ms/step - auc: 0.9689 - binary_accuracy: 0.9282 - false_negatives: 21.2826 - false_positives: 33.2174 - loss: 0.2411 - precision: 0.9171 - recall: 0.9474 - true_negatives: 329.5435 - true_positives: 367.9565\n",
      "For batch number 47 the model has a loss of 0.2416369467973709\n",
      "\u001b[1m 47/689\u001b[0m \u001b[32m━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m37s\u001b[0m 59ms/step - auc: 0.9688 - binary_accuracy: 0.9283 - false_negatives: 21.7021 - false_positives: 33.8511 - loss: 0.2411 - precision: 0.9172 - recall: 0.9474 - true_negatives: 336.4468 - true_positives: 376.0000\n",
      "For batch number 48 the model has a loss of 0.24341772496700287\n",
      "\u001b[1m 48/689\u001b[0m \u001b[32m━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m37s\u001b[0m 59ms/step - auc: 0.9687 - binary_accuracy: 0.9283 - false_negatives: 22.1250 - false_positives: 34.5417 - loss: 0.2412 - precision: 0.9172 - recall: 0.9474 - true_negatives: 343.4167 - true_positives: 383.9167\n",
      "For batch number 49 the model has a loss of 0.24033667147159576\n",
      "\u001b[1m 49/689\u001b[0m \u001b[32m━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m37s\u001b[0m 59ms/step - auc: 0.9687 - binary_accuracy: 0.9284 - false_negatives: 22.5306 - false_positives: 35.2041 - loss: 0.2412 - precision: 0.9173 - recall: 0.9475 - true_negatives: 350.4082 - true_positives: 391.8571\n",
      "For batch number 50 the model has a loss of 0.2379179745912552\n",
      "\u001b[1m 50/689\u001b[0m \u001b[32m━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m37s\u001b[0m 59ms/step - auc: 0.9686 - binary_accuracy: 0.9284 - false_negatives: 22.9200 - false_positives: 35.8400 - loss: 0.2411 - precision: 0.9174 - recall: 0.9475 - true_negatives: 357.3200 - true_positives: 399.9200\n",
      "For batch number 51 the model has a loss of 0.23790542781352997\n",
      "\u001b[1m 51/689\u001b[0m \u001b[32m━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m37s\u001b[0m 59ms/step - auc: 0.9686 - binary_accuracy: 0.9285 - false_negatives: 23.2941 - false_positives: 36.5098 - loss: 0.2410 - precision: 0.9175 - recall: 0.9476 - true_negatives: 364.1961 - true_positives: 408.0000\n",
      "For batch number 52 the model has a loss of 0.23541337251663208\n",
      "\u001b[1m 52/689\u001b[0m \u001b[32m━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m37s\u001b[0m 59ms/step - auc: 0.9686 - binary_accuracy: 0.9286 - false_negatives: 23.6538 - false_positives: 37.1538 - loss: 0.2409 - precision: 0.9175 - recall: 0.9476 - true_negatives: 371.0962 - true_positives: 416.0962\n",
      "For batch number 53 the model has a loss of 0.23445214331150055\n",
      "\u001b[1m 53/689\u001b[0m \u001b[32m━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m37s\u001b[0m 59ms/step - auc: 0.9685 - binary_accuracy: 0.9286 - false_negatives: 24.0000 - false_positives: 37.8113 - loss: 0.2408 - precision: 0.9176 - recall: 0.9477 - true_negatives: 377.9811 - true_positives: 424.2076\n",
      "For batch number 54 the model has a loss of 0.23223407566547394\n",
      "\u001b[1m 54/689\u001b[0m \u001b[32m━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m37s\u001b[0m 59ms/step - auc: 0.9685 - binary_accuracy: 0.9287 - false_negatives: 24.3333 - false_positives: 38.4444 - loss: 0.2407 - precision: 0.9177 - recall: 0.9478 - true_negatives: 384.9259 - true_positives: 432.2963\n",
      "For batch number 55 the model has a loss of 0.2339114397764206\n",
      "\u001b[1m 55/689\u001b[0m \u001b[32m━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m37s\u001b[0m 59ms/step - auc: 0.9685 - binary_accuracy: 0.9288 - false_negatives: 24.6727 - false_positives: 39.0909 - loss: 0.2405 - precision: 0.9178 - recall: 0.9479 - true_negatives: 391.8727 - true_positives: 440.3636\n",
      "For batch number 56 the model has a loss of 0.23296795785427094\n",
      "\u001b[1m 56/689\u001b[0m \u001b[32m━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m37s\u001b[0m 59ms/step - auc: 0.9685 - binary_accuracy: 0.9289 - false_negatives: 25.0000 - false_positives: 39.7321 - loss: 0.2404 - precision: 0.9179 - recall: 0.9481 - true_negatives: 398.8036 - true_positives: 448.4643\n",
      "For batch number 57 the model has a loss of 0.23473003506660461\n",
      "\u001b[1m 57/689\u001b[0m \u001b[32m━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m37s\u001b[0m 59ms/step - auc: 0.9685 - binary_accuracy: 0.9290 - false_negatives: 25.3158 - false_positives: 40.4386 - loss: 0.2403 - precision: 0.9179 - recall: 0.9482 - true_negatives: 405.7544 - true_positives: 456.4912\n",
      "For batch number 58 the model has a loss of 0.23321941494941711\n",
      "\u001b[1m 58/689\u001b[0m \u001b[32m━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m36s\u001b[0m 59ms/step - auc: 0.9685 - binary_accuracy: 0.9291 - false_negatives: 25.6207 - false_positives: 41.1552 - loss: 0.2402 - precision: 0.9179 - recall: 0.9483 - true_negatives: 412.7242 - true_positives: 464.5000\n",
      "For batch number 59 the model has a loss of 0.23340243101119995\n",
      "\u001b[1m 59/689\u001b[0m \u001b[32m━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m36s\u001b[0m 59ms/step - auc: 0.9685 - binary_accuracy: 0.9291 - false_negatives: 25.9153 - false_positives: 41.8983 - loss: 0.2401 - precision: 0.9179 - recall: 0.9484 - true_negatives: 419.7288 - true_positives: 472.4576\n",
      "For batch number 60 the model has a loss of 0.2316455990076065\n",
      "\u001b[1m 60/689\u001b[0m \u001b[32m━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m36s\u001b[0m 59ms/step - auc: 0.9685 - binary_accuracy: 0.9292 - false_negatives: 26.2000 - false_positives: 42.6167 - loss: 0.2399 - precision: 0.9179 - recall: 0.9486 - true_negatives: 426.8000 - true_positives: 480.3833\n",
      "For batch number 61 the model has a loss of 0.2369600236415863\n",
      "\u001b[1m 61/689\u001b[0m \u001b[32m━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m36s\u001b[0m 59ms/step - auc: 0.9684 - binary_accuracy: 0.9292 - false_negatives: 26.4918 - false_positives: 43.4262 - loss: 0.2399 - precision: 0.9178 - recall: 0.9487 - true_negatives: 433.8524 - true_positives: 488.2295\n",
      "For batch number 62 the model has a loss of 0.23692399263381958\n",
      "\u001b[1m 62/689\u001b[0m \u001b[32m━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m36s\u001b[0m 59ms/step - auc: 0.9684 - binary_accuracy: 0.9292 - false_negatives: 26.7742 - false_positives: 44.2258 - loss: 0.2398 - precision: 0.9177 - recall: 0.9488 - true_negatives: 440.8226 - true_positives: 496.1774\n",
      "For batch number 63 the model has a loss of 0.2384454309940338\n",
      "\u001b[1m 63/689\u001b[0m \u001b[32m━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m36s\u001b[0m 59ms/step - auc: 0.9683 - binary_accuracy: 0.9292 - false_negatives: 27.0794 - false_positives: 45.0159 - loss: 0.2398 - precision: 0.9177 - recall: 0.9489 - true_negatives: 447.7936 - true_positives: 504.1111\n",
      "For batch number 64 the model has a loss of 0.24642620980739594\n",
      "\u001b[1m 64/689\u001b[0m \u001b[32m━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m36s\u001b[0m 59ms/step - auc: 0.9683 - binary_accuracy: 0.9292 - false_negatives: 27.4219 - false_positives: 45.8281 - loss: 0.2399 - precision: 0.9176 - recall: 0.9490 - true_negatives: 454.7188 - true_positives: 512.0312\n",
      "For batch number 65 the model has a loss of 0.24722929298877716\n",
      "\u001b[1m 65/689\u001b[0m \u001b[32m━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m36s\u001b[0m 59ms/step - auc: 0.9682 - binary_accuracy: 0.9292 - false_negatives: 27.7692 - false_positives: 46.6308 - loss: 0.2400 - precision: 0.9175 - recall: 0.9491 - true_negatives: 461.6461 - true_positives: 519.9539\n",
      "For batch number 66 the model has a loss of 0.24849992990493774\n",
      "\u001b[1m 66/689\u001b[0m \u001b[32m━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m36s\u001b[0m 58ms/step - auc: 0.9681 - binary_accuracy: 0.9292 - false_negatives: 28.1212 - false_positives: 47.4242 - loss: 0.2401 - precision: 0.9174 - recall: 0.9491 - true_negatives: 468.5000 - true_positives: 527.9545\n",
      "For batch number 67 the model has a loss of 0.24836280941963196\n",
      "\u001b[1m 67/689\u001b[0m \u001b[32m━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m36s\u001b[0m 58ms/step - auc: 0.9681 - binary_accuracy: 0.9292 - false_negatives: 28.4627 - false_positives: 48.2388 - loss: 0.2403 - precision: 0.9174 - recall: 0.9492 - true_negatives: 475.3284 - true_positives: 535.9702\n",
      "For batch number 68 the model has a loss of 0.24691402912139893\n",
      "\u001b[1m 68/689\u001b[0m \u001b[32m━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m36s\u001b[0m 58ms/step - auc: 0.9680 - binary_accuracy: 0.9292 - false_negatives: 28.7941 - false_positives: 49.0441 - loss: 0.2404 - precision: 0.9173 - recall: 0.9493 - true_negatives: 482.2206 - true_positives: 543.9412\n",
      "For batch number 69 the model has a loss of 0.24531486630439758\n",
      "\u001b[1m 69/689\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m36s\u001b[0m 58ms/step - auc: 0.9679 - binary_accuracy: 0.9292 - false_negatives: 29.1304 - false_positives: 49.8261 - loss: 0.2404 - precision: 0.9173 - recall: 0.9494 - true_negatives: 489.1159 - true_positives: 551.9276\n",
      "For batch number 70 the model has a loss of 0.2500235438346863\n",
      "\u001b[1m 70/689\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m36s\u001b[0m 58ms/step - auc: 0.9679 - binary_accuracy: 0.9292 - false_negatives: 29.4857 - false_positives: 50.5857 - loss: 0.2406 - precision: 0.9172 - recall: 0.9494 - true_negatives: 496.0428 - true_positives: 559.8857\n",
      "For batch number 71 the model has a loss of 0.24785961210727692\n",
      "\u001b[1m 71/689\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m36s\u001b[0m 58ms/step - auc: 0.9678 - binary_accuracy: 0.9293 - false_negatives: 29.8310 - false_positives: 51.3239 - loss: 0.2407 - precision: 0.9172 - recall: 0.9495 - true_negatives: 503.0000 - true_positives: 567.8451\n",
      "For batch number 72 the model has a loss of 0.25044316053390503\n",
      "\u001b[1m 72/689\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m36s\u001b[0m 59ms/step - auc: 0.9678 - binary_accuracy: 0.9293 - false_negatives: 30.1944 - false_positives: 52.0417 - loss: 0.2408 - precision: 0.9172 - recall: 0.9496 - true_negatives: 509.9305 - true_positives: 575.8333\n",
      "For batch number 73 the model has a loss of 0.25369125604629517\n",
      "\u001b[1m 73/689\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m36s\u001b[0m 59ms/step - auc: 0.9677 - binary_accuracy: 0.9293 - false_negatives: 30.5616 - false_positives: 52.7534 - loss: 0.2410 - precision: 0.9172 - recall: 0.9496 - true_negatives: 516.8904 - true_positives: 583.7945\n",
      "For batch number 74 the model has a loss of 0.25946715474128723\n",
      "\u001b[1m 74/689\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m35s\u001b[0m 59ms/step - auc: 0.9676 - binary_accuracy: 0.9293 - false_negatives: 30.9595 - false_positives: 53.4730 - loss: 0.2412 - precision: 0.9172 - recall: 0.9496 - true_negatives: 523.8243 - true_positives: 591.7432\n",
      "For batch number 75 the model has a loss of 0.2586199939250946\n",
      "\u001b[1m 75/689\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m35s\u001b[0m 58ms/step - auc: 0.9675 - binary_accuracy: 0.9293 - false_negatives: 31.3600 - false_positives: 54.1867 - loss: 0.2415 - precision: 0.9172 - recall: 0.9497 - true_negatives: 530.7466 - true_positives: 599.7067\n",
      "For batch number 76 the model has a loss of 0.25897231698036194\n",
      "\u001b[1m 76/689\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m35s\u001b[0m 58ms/step - auc: 0.9675 - binary_accuracy: 0.9293 - false_negatives: 31.7632 - false_positives: 54.8947 - loss: 0.2417 - precision: 0.9172 - recall: 0.9497 - true_negatives: 537.6842 - true_positives: 607.6579\n",
      "For batch number 77 the model has a loss of 0.25799867510795593\n",
      "\u001b[1m 77/689\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m35s\u001b[0m 58ms/step - auc: 0.9674 - binary_accuracy: 0.9293 - false_negatives: 32.1688 - false_positives: 55.5974 - loss: 0.2419 - precision: 0.9172 - recall: 0.9497 - true_negatives: 544.6494 - true_positives: 615.5844\n",
      "For batch number 78 the model has a loss of 0.25659260153770447\n",
      "\u001b[1m 78/689\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m35s\u001b[0m 58ms/step - auc: 0.9674 - binary_accuracy: 0.9294 - false_negatives: 32.5641 - false_positives: 56.3077 - loss: 0.2421 - precision: 0.9172 - recall: 0.9497 - true_negatives: 551.6411 - true_positives: 623.4872\n",
      "For batch number 79 the model has a loss of 0.2555026412010193\n",
      "\u001b[1m 79/689\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m35s\u001b[0m 58ms/step - auc: 0.9673 - binary_accuracy: 0.9294 - false_negatives: 32.9494 - false_positives: 57.0253 - loss: 0.2423 - precision: 0.9172 - recall: 0.9497 - true_negatives: 558.6076 - true_positives: 631.4177\n",
      "For batch number 80 the model has a loss of 0.2543241083621979\n",
      "\u001b[1m 80/689\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m35s\u001b[0m 58ms/step - auc: 0.9673 - binary_accuracy: 0.9294 - false_negatives: 33.3375 - false_positives: 57.7250 - loss: 0.2424 - precision: 0.9172 - recall: 0.9498 - true_negatives: 565.5750 - true_positives: 639.3625\n",
      "For batch number 81 the model has a loss of 0.255044549703598\n",
      "\u001b[1m 81/689\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m35s\u001b[0m 59ms/step - auc: 0.9672 - binary_accuracy: 0.9294 - false_negatives: 33.7160 - false_positives: 58.4568 - loss: 0.2426 - precision: 0.9172 - recall: 0.9498 - true_negatives: 572.5309 - true_positives: 647.2963\n",
      "For batch number 82 the model has a loss of 0.2538095712661743\n",
      "\u001b[1m 82/689\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m35s\u001b[0m 59ms/step - auc: 0.9672 - binary_accuracy: 0.9294 - false_negatives: 34.0976 - false_positives: 59.1707 - loss: 0.2427 - precision: 0.9172 - recall: 0.9498 - true_negatives: 579.4878 - true_positives: 655.2439\n",
      "For batch number 83 the model has a loss of 0.2528707683086395\n",
      "\u001b[1m 83/689\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m35s\u001b[0m 59ms/step - auc: 0.9671 - binary_accuracy: 0.9294 - false_negatives: 34.4699 - false_positives: 59.8795 - loss: 0.2428 - precision: 0.9172 - recall: 0.9499 - true_negatives: 586.4217 - true_positives: 663.2289\n",
      "For batch number 84 the model has a loss of 0.25212860107421875\n",
      "\u001b[1m 84/689\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m35s\u001b[0m 58ms/step - auc: 0.9671 - binary_accuracy: 0.9295 - false_negatives: 34.8333 - false_positives: 60.5952 - loss: 0.2429 - precision: 0.9172 - recall: 0.9499 - true_negatives: 593.3571 - true_positives: 671.2143\n",
      "For batch number 85 the model has a loss of 0.2507779598236084\n",
      "\u001b[1m 85/689\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m35s\u001b[0m 58ms/step - auc: 0.9671 - binary_accuracy: 0.9295 - false_negatives: 35.2000 - false_positives: 61.2941 - loss: 0.2430 - precision: 0.9173 - recall: 0.9500 - true_negatives: 600.2941 - true_positives: 679.2118\n",
      "For batch number 86 the model has a loss of 0.25180551409721375\n",
      "\u001b[1m 86/689\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m35s\u001b[0m 58ms/step - auc: 0.9670 - binary_accuracy: 0.9295 - false_negatives: 35.5581 - false_positives: 62.0465 - loss: 0.2431 - precision: 0.9172 - recall: 0.9500 - true_negatives: 607.2558 - true_positives: 687.1395\n",
      "For batch number 87 the model has a loss of 0.2521095871925354\n",
      "\u001b[1m 87/689\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m35s\u001b[0m 58ms/step - auc: 0.9670 - binary_accuracy: 0.9295 - false_negatives: 35.9195 - false_positives: 62.8046 - loss: 0.2432 - precision: 0.9172 - recall: 0.9500 - true_negatives: 614.2529 - true_positives: 695.0230\n",
      "For batch number 88 the model has a loss of 0.25167325139045715\n",
      "\u001b[1m 88/689\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m35s\u001b[0m 58ms/step - auc: 0.9670 - binary_accuracy: 0.9295 - false_negatives: 36.2727 - false_positives: 63.5682 - loss: 0.2433 - precision: 0.9172 - recall: 0.9501 - true_negatives: 621.2045 - true_positives: 702.9545\n",
      "For batch number 89 the model has a loss of 0.2519603371620178\n",
      "\u001b[1m 89/689\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m35s\u001b[0m 58ms/step - auc: 0.9669 - binary_accuracy: 0.9295 - false_negatives: 36.6180 - false_positives: 64.3483 - loss: 0.2434 - precision: 0.9171 - recall: 0.9501 - true_negatives: 628.2023 - true_positives: 710.8315\n",
      "For batch number 90 the model has a loss of 0.2530122995376587\n",
      "\u001b[1m 90/689\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m35s\u001b[0m 58ms/step - auc: 0.9669 - binary_accuracy: 0.9295 - false_negatives: 36.9667 - false_positives: 65.1222 - loss: 0.2435 - precision: 0.9171 - recall: 0.9502 - true_negatives: 635.1889 - true_positives: 718.7222\n",
      "For batch number 91 the model has a loss of 0.255196213722229\n",
      "\u001b[1m 91/689\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m34s\u001b[0m 58ms/step - auc: 0.9668 - binary_accuracy: 0.9295 - false_negatives: 37.3187 - false_positives: 65.9341 - loss: 0.2437 - precision: 0.9170 - recall: 0.9502 - true_negatives: 642.1868 - true_positives: 726.5604\n",
      "For batch number 92 the model has a loss of 0.25373438000679016\n",
      "\u001b[1m 92/689\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m34s\u001b[0m 58ms/step - auc: 0.9668 - binary_accuracy: 0.9295 - false_negatives: 37.6630 - false_positives: 66.7391 - loss: 0.2438 - precision: 0.9170 - recall: 0.9503 - true_negatives: 649.2065 - true_positives: 734.3913\n",
      "For batch number 93 the model has a loss of 0.25556379556655884\n",
      "\u001b[1m 93/689\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m34s\u001b[0m 58ms/step - auc: 0.9668 - binary_accuracy: 0.9295 - false_negatives: 38.0215 - false_positives: 67.5591 - loss: 0.2439 - precision: 0.9169 - recall: 0.9503 - true_negatives: 656.2150 - true_positives: 742.2043\n",
      "For batch number 94 the model has a loss of 0.2557254433631897\n",
      "\u001b[1m 94/689\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m34s\u001b[0m 58ms/step - auc: 0.9667 - binary_accuracy: 0.9295 - false_negatives: 38.3723 - false_positives: 68.3830 - loss: 0.2440 - precision: 0.9168 - recall: 0.9503 - true_negatives: 663.2447 - true_positives: 750.0000\n",
      "For batch number 95 the model has a loss of 0.2552993893623352\n",
      "\u001b[1m 95/689\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m34s\u001b[0m 58ms/step - auc: 0.9667 - binary_accuracy: 0.9294 - false_negatives: 38.7263 - false_positives: 69.2105 - loss: 0.2442 - precision: 0.9168 - recall: 0.9504 - true_negatives: 670.2632 - true_positives: 757.8000\n",
      "For batch number 96 the model has a loss of 0.2544075548648834\n",
      "\u001b[1m 96/689\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m34s\u001b[0m 58ms/step - auc: 0.9666 - binary_accuracy: 0.9294 - false_negatives: 39.0729 - false_positives: 70.0312 - loss: 0.2443 - precision: 0.9167 - recall: 0.9504 - true_negatives: 677.2917 - true_positives: 765.6042\n",
      "For batch number 97 the model has a loss of 0.25417211651802063\n",
      "\u001b[1m 97/689\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m34s\u001b[0m 58ms/step - auc: 0.9666 - binary_accuracy: 0.9294 - false_negatives: 39.4227 - false_positives: 70.8660 - loss: 0.2444 - precision: 0.9166 - recall: 0.9505 - true_negatives: 684.3299 - true_positives: 773.3815\n",
      "For batch number 98 the model has a loss of 0.2541075646877289\n",
      "\u001b[1m 98/689\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m34s\u001b[0m 58ms/step - auc: 0.9665 - binary_accuracy: 0.9294 - false_negatives: 39.7653 - false_positives: 71.7041 - loss: 0.2445 - precision: 0.9166 - recall: 0.9505 - true_negatives: 691.4081 - true_positives: 781.1224\n",
      "For batch number 99 the model has a loss of 0.25317123532295227\n",
      "\u001b[1m 99/689\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m34s\u001b[0m 58ms/step - auc: 0.9665 - binary_accuracy: 0.9294 - false_negatives: 40.1111 - false_positives: 72.5354 - loss: 0.2445 - precision: 0.9165 - recall: 0.9505 - true_negatives: 698.5051 - true_positives: 788.8485\n",
      "For batch number 100 the model has a loss of 0.2545710504055023\n",
      "\u001b[1m100/689\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m34s\u001b[0m 58ms/step - auc: 0.9665 - binary_accuracy: 0.9294 - false_negatives: 40.4700 - false_positives: 73.3600 - loss: 0.2446 - precision: 0.9164 - recall: 0.9506 - true_negatives: 705.6000 - true_positives: 796.5700\n",
      "For batch number 101 the model has a loss of 0.2539663314819336\n",
      "\u001b[1m101/689\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m34s\u001b[0m 58ms/step - auc: 0.9664 - binary_accuracy: 0.9294 - false_negatives: 40.8317 - false_positives: 74.1881 - loss: 0.2447 - precision: 0.9164 - recall: 0.9506 - true_negatives: 712.6931 - true_positives: 804.2871\n",
      "For batch number 102 the model has a loss of 0.25227874517440796\n",
      "\u001b[1m102/689\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m34s\u001b[0m 58ms/step - auc: 0.9664 - binary_accuracy: 0.9293 - false_negatives: 41.1863 - false_positives: 75.0000 - loss: 0.2448 - precision: 0.9163 - recall: 0.9506 - true_negatives: 719.8137 - true_positives: 812.0000\n",
      "For batch number 103 the model has a loss of 0.2526112198829651\n",
      "\u001b[1m103/689\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m34s\u001b[0m 58ms/step - auc: 0.9664 - binary_accuracy: 0.9293 - false_negatives: 41.5437 - false_positives: 75.8155 - loss: 0.2449 - precision: 0.9162 - recall: 0.9507 - true_negatives: 726.9418 - true_positives: 819.6990\n",
      "For batch number 104 the model has a loss of 0.2538108229637146\n",
      "\u001b[1m104/689\u001b[0m \u001b[32m━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m34s\u001b[0m 58ms/step - auc: 0.9663 - binary_accuracy: 0.9293 - false_negatives: 41.8942 - false_positives: 76.6635 - loss: 0.2450 - precision: 0.9162 - recall: 0.9507 - true_negatives: 734.0961 - true_positives: 827.3461\n",
      "For batch number 105 the model has a loss of 0.25345757603645325\n",
      "\u001b[1m105/689\u001b[0m \u001b[32m━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m34s\u001b[0m 58ms/step - auc: 0.9663 - binary_accuracy: 0.9293 - false_negatives: 42.2381 - false_positives: 77.5048 - loss: 0.2451 - precision: 0.9161 - recall: 0.9507 - true_negatives: 741.2381 - true_positives: 835.0190\n",
      "For batch number 106 the model has a loss of 0.25322312116622925\n",
      "\u001b[1m106/689\u001b[0m \u001b[32m━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m33s\u001b[0m 58ms/step - auc: 0.9663 - binary_accuracy: 0.9293 - false_negatives: 42.5849 - false_positives: 78.3396 - loss: 0.2451 - precision: 0.9160 - recall: 0.9508 - true_negatives: 748.3679 - true_positives: 842.7075\n",
      "For batch number 107 the model has a loss of 0.2525585889816284\n",
      "\u001b[1m107/689\u001b[0m \u001b[32m━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m33s\u001b[0m 58ms/step - auc: 0.9663 - binary_accuracy: 0.9293 - false_negatives: 42.9252 - false_positives: 79.1589 - loss: 0.2452 - precision: 0.9159 - recall: 0.9508 - true_negatives: 755.5607 - true_positives: 850.3552\n",
      "For batch number 108 the model has a loss of 0.2520141005516052\n",
      "\u001b[1m108/689\u001b[0m \u001b[32m━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m33s\u001b[0m 58ms/step - auc: 0.9662 - binary_accuracy: 0.9293 - false_negatives: 43.2593 - false_positives: 79.9630 - loss: 0.2453 - precision: 0.9159 - recall: 0.9508 - true_negatives: 762.7500 - true_positives: 858.0278\n",
      "For batch number 109 the model has a loss of 0.25208064913749695\n",
      "\u001b[1m109/689\u001b[0m \u001b[32m━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m33s\u001b[0m 58ms/step - auc: 0.9662 - binary_accuracy: 0.9293 - false_negatives: 43.6055 - false_positives: 80.7523 - loss: 0.2453 - precision: 0.9158 - recall: 0.9509 - true_negatives: 769.9541 - true_positives: 865.6880\n",
      "For batch number 110 the model has a loss of 0.2513495087623596\n",
      "\u001b[1m110/689\u001b[0m \u001b[32m━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m33s\u001b[0m 58ms/step - auc: 0.9662 - binary_accuracy: 0.9293 - false_negatives: 43.9545 - false_positives: 81.5364 - loss: 0.2454 - precision: 0.9158 - recall: 0.9509 - true_negatives: 777.1636 - true_positives: 873.3455\n",
      "For batch number 111 the model has a loss of 0.25163406133651733\n",
      "\u001b[1m111/689\u001b[0m \u001b[32m━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m33s\u001b[0m 58ms/step - auc: 0.9662 - binary_accuracy: 0.9293 - false_negatives: 44.3153 - false_positives: 82.3153 - loss: 0.2454 - precision: 0.9158 - recall: 0.9509 - true_negatives: 784.4054 - true_positives: 880.9640\n",
      "For batch number 112 the model has a loss of 0.2515229880809784\n",
      "\u001b[1m112/689\u001b[0m \u001b[32m━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m33s\u001b[0m 58ms/step - auc: 0.9661 - binary_accuracy: 0.9293 - false_negatives: 44.6786 - false_positives: 83.0982 - loss: 0.2455 - precision: 0.9157 - recall: 0.9509 - true_negatives: 791.6696 - true_positives: 888.5536\n",
      "For batch number 113 the model has a loss of 0.2506870925426483\n",
      "\u001b[1m113/689\u001b[0m \u001b[32m━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m33s\u001b[0m 58ms/step - auc: 0.9661 - binary_accuracy: 0.9293 - false_negatives: 45.0442 - false_positives: 83.8673 - loss: 0.2455 - precision: 0.9157 - recall: 0.9510 - true_negatives: 798.9292 - true_positives: 896.1593\n",
      "For batch number 114 the model has a loss of 0.25019073486328125\n",
      "\u001b[1m114/689\u001b[0m \u001b[32m━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m33s\u001b[0m 58ms/step - auc: 0.9661 - binary_accuracy: 0.9293 - false_negatives: 45.4035 - false_positives: 84.6316 - loss: 0.2456 - precision: 0.9156 - recall: 0.9510 - true_negatives: 806.1754 - true_positives: 903.7895\n",
      "For batch number 115 the model has a loss of 0.25230464339256287\n",
      "\u001b[1m115/689\u001b[0m \u001b[32m━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m33s\u001b[0m 58ms/step - auc: 0.9661 - binary_accuracy: 0.9293 - false_negatives: 45.7739 - false_positives: 85.4174 - loss: 0.2456 - precision: 0.9156 - recall: 0.9510 - true_negatives: 813.4000 - true_positives: 911.4087\n",
      "For batch number 116 the model has a loss of 0.25292885303497314\n",
      "\u001b[1m116/689\u001b[0m \u001b[32m━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m33s\u001b[0m 58ms/step - auc: 0.9660 - binary_accuracy: 0.9293 - false_negatives: 46.1466 - false_positives: 86.1897 - loss: 0.2457 - precision: 0.9155 - recall: 0.9510 - true_negatives: 820.6293 - true_positives: 919.0345\n",
      "For batch number 117 the model has a loss of 0.25464463233947754\n",
      "\u001b[1m117/689\u001b[0m \u001b[32m━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m33s\u001b[0m 58ms/step - auc: 0.9660 - binary_accuracy: 0.9293 - false_negatives: 46.5299 - false_positives: 86.9658 - loss: 0.2458 - precision: 0.9155 - recall: 0.9510 - true_negatives: 827.8461 - true_positives: 926.6581\n",
      "For batch number 118 the model has a loss of 0.25395628809928894\n",
      "\u001b[1m118/689\u001b[0m \u001b[32m━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m33s\u001b[0m 59ms/step - auc: 0.9660 - binary_accuracy: 0.9293 - false_negatives: 46.9068 - false_positives: 87.7373 - loss: 0.2458 - precision: 0.9155 - recall: 0.9510 - true_negatives: 835.0932 - true_positives: 934.2627\n",
      "For batch number 119 the model has a loss of 0.2545919418334961\n",
      "\u001b[1m119/689\u001b[0m \u001b[32m━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m33s\u001b[0m 59ms/step - auc: 0.9659 - binary_accuracy: 0.9292 - false_negatives: 47.2857 - false_positives: 88.5378 - loss: 0.2459 - precision: 0.9154 - recall: 0.9511 - true_negatives: 842.3278 - true_positives: 941.8488\n",
      "For batch number 120 the model has a loss of 0.2543073892593384\n",
      "\u001b[1m120/689\u001b[0m \u001b[32m━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m33s\u001b[0m 59ms/step - auc: 0.9659 - binary_accuracy: 0.9292 - false_negatives: 47.6583 - false_positives: 89.3417 - loss: 0.2460 - precision: 0.9154 - recall: 0.9511 - true_negatives: 849.6000 - true_positives: 949.4000\n",
      "For batch number 121 the model has a loss of 0.2539268732070923\n",
      "\u001b[1m121/689\u001b[0m \u001b[32m━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m33s\u001b[0m 59ms/step - auc: 0.9659 - binary_accuracy: 0.9292 - false_negatives: 48.0413 - false_positives: 90.1405 - loss: 0.2461 - precision: 0.9153 - recall: 0.9511 - true_negatives: 856.8678 - true_positives: 956.9504\n",
      "For batch number 122 the model has a loss of 0.25584208965301514\n",
      "\u001b[1m122/689\u001b[0m \u001b[32m━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m33s\u001b[0m 59ms/step - auc: 0.9659 - binary_accuracy: 0.9292 - false_negatives: 48.4426 - false_positives: 90.9344 - loss: 0.2461 - precision: 0.9153 - recall: 0.9511 - true_negatives: 864.1066 - true_positives: 964.5164\n",
      "For batch number 123 the model has a loss of 0.25535398721694946\n",
      "\u001b[1m123/689\u001b[0m \u001b[32m━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m33s\u001b[0m 59ms/step - auc: 0.9658 - binary_accuracy: 0.9292 - false_negatives: 48.8374 - false_positives: 91.7317 - loss: 0.2462 - precision: 0.9152 - recall: 0.9511 - true_negatives: 871.3740 - true_positives: 972.0569\n",
      "For batch number 124 the model has a loss of 0.254702091217041\n",
      "\u001b[1m124/689\u001b[0m \u001b[32m━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m33s\u001b[0m 59ms/step - auc: 0.9658 - binary_accuracy: 0.9292 - false_negatives: 49.2258 - false_positives: 92.5323 - loss: 0.2463 - precision: 0.9152 - recall: 0.9511 - true_negatives: 878.6774 - true_positives: 979.5645\n",
      "For batch number 125 the model has a loss of 0.25593405961990356\n",
      "\u001b[1m125/689\u001b[0m \u001b[32m━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m33s\u001b[0m 59ms/step - auc: 0.9658 - binary_accuracy: 0.9292 - false_negatives: 49.6080 - false_positives: 93.3520 - loss: 0.2464 - precision: 0.9151 - recall: 0.9511 - true_negatives: 885.9920 - true_positives: 987.0480\n",
      "For batch number 126 the model has a loss of 0.25502416491508484\n",
      "\u001b[1m126/689\u001b[0m \u001b[32m━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m32s\u001b[0m 59ms/step - auc: 0.9657 - binary_accuracy: 0.9291 - false_negatives: 49.9841 - false_positives: 94.1587 - loss: 0.2464 - precision: 0.9151 - recall: 0.9511 - true_negatives: 893.3174 - true_positives: 994.5397\n",
      "For batch number 127 the model has a loss of 0.25539034605026245\n",
      "\u001b[1m127/689\u001b[0m \u001b[32m━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m32s\u001b[0m 59ms/step - auc: 0.9657 - binary_accuracy: 0.9291 - false_negatives: 50.3622 - false_positives: 94.9606 - loss: 0.2465 - precision: 0.9150 - recall: 0.9511 - true_negatives: 900.6536 - true_positives: 1002.0236\n",
      "For batch number 128 the model has a loss of 0.2550038993358612\n",
      "\u001b[1m128/689\u001b[0m \u001b[32m━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m32s\u001b[0m 59ms/step - auc: 0.9657 - binary_accuracy: 0.9291 - false_negatives: 50.7500 - false_positives: 95.7500 - loss: 0.2466 - precision: 0.9150 - recall: 0.9511 - true_negatives: 907.9922 - true_positives: 1009.5078\n",
      "For batch number 129 the model has a loss of 0.2539982497692108\n",
      "\u001b[1m129/689\u001b[0m \u001b[32m━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m32s\u001b[0m 59ms/step - auc: 0.9657 - binary_accuracy: 0.9291 - false_negatives: 51.1318 - false_positives: 96.5349 - loss: 0.2466 - precision: 0.9149 - recall: 0.9511 - true_negatives: 915.3101 - true_positives: 1017.0233\n",
      "For batch number 130 the model has a loss of 0.252936452627182\n",
      "\u001b[1m130/689\u001b[0m \u001b[32m━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m32s\u001b[0m 59ms/step - auc: 0.9656 - binary_accuracy: 0.9291 - false_negatives: 51.5154 - false_positives: 97.3077 - loss: 0.2467 - precision: 0.9149 - recall: 0.9511 - true_negatives: 922.6385 - true_positives: 1024.5385\n",
      "For batch number 131 the model has a loss of 0.25196096301078796\n",
      "\u001b[1m131/689\u001b[0m \u001b[32m━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m32s\u001b[0m 59ms/step - auc: 0.9656 - binary_accuracy: 0.9291 - false_negatives: 51.8931 - false_positives: 98.0687 - loss: 0.2467 - precision: 0.9148 - recall: 0.9511 - true_negatives: 929.9466 - true_positives: 1032.0916\n",
      "For batch number 132 the model has a loss of 0.25181958079338074\n",
      "\u001b[1m132/689\u001b[0m \u001b[32m━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m32s\u001b[0m 59ms/step - auc: 0.9656 - binary_accuracy: 0.9291 - false_negatives: 52.2652 - false_positives: 98.8409 - loss: 0.2467 - precision: 0.9148 - recall: 0.9511 - true_negatives: 937.2879 - true_positives: 1039.6061\n",
      "For batch number 133 the model has a loss of 0.25099146366119385\n",
      "\u001b[1m133/689\u001b[0m \u001b[32m━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m32s\u001b[0m 59ms/step - auc: 0.9656 - binary_accuracy: 0.9291 - false_negatives: 52.6391 - false_positives: 99.6015 - loss: 0.2468 - precision: 0.9148 - recall: 0.9511 - true_negatives: 944.6391 - true_positives: 1047.1204\n",
      "For batch number 134 the model has a loss of 0.24985282123088837\n",
      "\u001b[1m134/689\u001b[0m \u001b[32m━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m32s\u001b[0m 59ms/step - auc: 0.9656 - binary_accuracy: 0.9291 - false_negatives: 53.0075 - false_positives: 100.3582 - loss: 0.2468 - precision: 0.9147 - recall: 0.9511 - true_negatives: 952.0074 - true_positives: 1054.6268\n",
      "For batch number 135 the model has a loss of 0.24957461655139923\n",
      "\u001b[1m135/689\u001b[0m \u001b[32m━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m32s\u001b[0m 58ms/step - auc: 0.9655 - binary_accuracy: 0.9291 - false_negatives: 53.3778 - false_positives: 101.1185 - loss: 0.2468 - precision: 0.9147 - recall: 0.9512 - true_negatives: 959.3778 - true_positives: 1062.1260\n",
      "For batch number 136 the model has a loss of 0.24899965524673462\n",
      "\u001b[1m136/689\u001b[0m \u001b[32m━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m32s\u001b[0m 58ms/step - auc: 0.9655 - binary_accuracy: 0.9291 - false_negatives: 53.7500 - false_positives: 101.8676 - loss: 0.2468 - precision: 0.9147 - recall: 0.9512 - true_negatives: 966.7353 - true_positives: 1069.6471\n",
      "For batch number 137 the model has a loss of 0.2480064183473587\n",
      "\u001b[1m137/689\u001b[0m \u001b[32m━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m32s\u001b[0m 59ms/step - auc: 0.9655 - binary_accuracy: 0.9291 - false_negatives: 54.1241 - false_positives: 102.6058 - loss: 0.2468 - precision: 0.9147 - recall: 0.9512 - true_negatives: 974.0876 - true_positives: 1077.1825\n",
      "For batch number 138 the model has a loss of 0.2500530779361725\n",
      "\u001b[1m138/689\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m32s\u001b[0m 59ms/step - auc: 0.9655 - binary_accuracy: 0.9291 - false_negatives: 54.5145 - false_positives: 103.3333 - loss: 0.2469 - precision: 0.9147 - recall: 0.9512 - true_negatives: 981.4203 - true_positives: 1084.7319\n",
      "For batch number 139 the model has a loss of 0.250748872756958\n",
      "\u001b[1m139/689\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m32s\u001b[0m 58ms/step - auc: 0.9655 - binary_accuracy: 0.9291 - false_negatives: 54.9065 - false_positives: 104.0576 - loss: 0.2469 - precision: 0.9146 - recall: 0.9512 - true_negatives: 988.7626 - true_positives: 1092.2734\n",
      "For batch number 140 the model has a loss of 0.252046674489975\n",
      "\u001b[1m140/689\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m32s\u001b[0m 58ms/step - auc: 0.9655 - binary_accuracy: 0.9291 - false_negatives: 55.3000 - false_positives: 104.7857 - loss: 0.2469 - precision: 0.9146 - recall: 0.9512 - true_negatives: 996.0928 - true_positives: 1099.8214\n",
      "For batch number 141 the model has a loss of 0.25151821970939636\n",
      "\u001b[1m141/689\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m32s\u001b[0m 58ms/step - auc: 0.9655 - binary_accuracy: 0.9291 - false_negatives: 55.6879 - false_positives: 105.5106 - loss: 0.2470 - precision: 0.9146 - recall: 0.9512 - true_negatives: 1003.4397 - true_positives: 1107.3617\n",
      "For batch number 142 the model has a loss of 0.25108346343040466\n",
      "\u001b[1m142/689\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m31s\u001b[0m 58ms/step - auc: 0.9654 - binary_accuracy: 0.9291 - false_negatives: 56.0775 - false_positives: 106.2324 - loss: 0.2470 - precision: 0.9146 - recall: 0.9512 - true_negatives: 1010.7958 - true_positives: 1114.8944\n",
      "For batch number 143 the model has a loss of 0.25052642822265625\n",
      "\u001b[1m143/689\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m31s\u001b[0m 58ms/step - auc: 0.9654 - binary_accuracy: 0.9291 - false_negatives: 56.4615 - false_positives: 106.9510 - loss: 0.2470 - precision: 0.9146 - recall: 0.9512 - true_negatives: 1018.1398 - true_positives: 1122.4475\n",
      "For batch number 144 the model has a loss of 0.25074949860572815\n",
      "\u001b[1m144/689\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m31s\u001b[0m 58ms/step - auc: 0.9654 - binary_accuracy: 0.9291 - false_negatives: 56.8403 - false_positives: 107.6806 - loss: 0.2470 - precision: 0.9146 - recall: 0.9512 - true_negatives: 1025.4861 - true_positives: 1129.9930\n",
      "For batch number 145 the model has a loss of 0.2509854733943939\n",
      "\u001b[1m145/689\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m31s\u001b[0m 58ms/step - auc: 0.9654 - binary_accuracy: 0.9291 - false_negatives: 57.2138 - false_positives: 108.4276 - loss: 0.2471 - precision: 0.9146 - recall: 0.9512 - true_negatives: 1032.8138 - true_positives: 1137.5448\n",
      "For batch number 146 the model has a loss of 0.25046294927597046\n",
      "\u001b[1m146/689\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m31s\u001b[0m 58ms/step - auc: 0.9654 - binary_accuracy: 0.9291 - false_negatives: 57.5822 - false_positives: 109.1781 - loss: 0.2471 - precision: 0.9145 - recall: 0.9512 - true_negatives: 1040.1438 - true_positives: 1145.0959\n",
      "For batch number 147 the model has a loss of 0.2509130537509918\n",
      "\u001b[1m147/689\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m31s\u001b[0m 58ms/step - auc: 0.9654 - binary_accuracy: 0.9291 - false_negatives: 57.9524 - false_positives: 109.9320 - loss: 0.2471 - precision: 0.9145 - recall: 0.9512 - true_negatives: 1047.4490 - true_positives: 1152.6666\n",
      "For batch number 148 the model has a loss of 0.2507631480693817\n",
      "\u001b[1m148/689\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m31s\u001b[0m 58ms/step - auc: 0.9653 - binary_accuracy: 0.9291 - false_negatives: 58.3243 - false_positives: 110.6892 - loss: 0.2471 - precision: 0.9145 - recall: 0.9512 - true_negatives: 1054.7567 - true_positives: 1160.2297\n",
      "For batch number 149 the model has a loss of 0.25017091631889343\n",
      "\u001b[1m149/689\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m31s\u001b[0m 58ms/step - auc: 0.9653 - binary_accuracy: 0.9291 - false_negatives: 58.6913 - false_positives: 111.4430 - loss: 0.2472 - precision: 0.9145 - recall: 0.9513 - true_negatives: 1062.0671 - true_positives: 1167.7987\n",
      "For batch number 150 the model has a loss of 0.2495727241039276\n",
      "\u001b[1m150/689\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m31s\u001b[0m 58ms/step - auc: 0.9653 - binary_accuracy: 0.9292 - false_negatives: 59.0533 - false_positives: 112.1933 - loss: 0.2472 - precision: 0.9144 - recall: 0.9513 - true_negatives: 1069.3534 - true_positives: 1175.4000\n",
      "For batch number 151 the model has a loss of 0.248628631234169\n",
      "\u001b[1m151/689\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m31s\u001b[0m 58ms/step - auc: 0.9653 - binary_accuracy: 0.9292 - false_negatives: 59.4106 - false_positives: 112.9404 - loss: 0.2472 - precision: 0.9144 - recall: 0.9513 - true_negatives: 1076.6490 - true_positives: 1183.0000\n",
      "For batch number 152 the model has a loss of 0.24840186536312103\n",
      "\u001b[1m152/689\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m31s\u001b[0m 58ms/step - auc: 0.9653 - binary_accuracy: 0.9292 - false_negatives: 59.7697 - false_positives: 113.6842 - loss: 0.2472 - precision: 0.9144 - recall: 0.9513 - true_negatives: 1083.9276 - true_positives: 1190.6184\n",
      "For batch number 153 the model has a loss of 0.24841345846652985\n",
      "\u001b[1m153/689\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m31s\u001b[0m 58ms/step - auc: 0.9653 - binary_accuracy: 0.9292 - false_negatives: 60.1242 - false_positives: 114.4444 - loss: 0.2472 - precision: 0.9144 - recall: 0.9513 - true_negatives: 1091.1896 - true_positives: 1198.2418\n",
      "For batch number 154 the model has a loss of 0.24829599261283875\n",
      "\u001b[1m154/689\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m31s\u001b[0m 58ms/step - auc: 0.9653 - binary_accuracy: 0.9292 - false_negatives: 60.4740 - false_positives: 115.2078 - loss: 0.2472 - precision: 0.9144 - recall: 0.9513 - true_negatives: 1098.4611 - true_positives: 1205.8572\n",
      "For batch number 155 the model has a loss of 0.2502000331878662\n",
      "\u001b[1m155/689\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m31s\u001b[0m 58ms/step - auc: 0.9653 - binary_accuracy: 0.9292 - false_negatives: 60.8387 - false_positives: 115.9806 - loss: 0.2472 - precision: 0.9143 - recall: 0.9513 - true_negatives: 1105.7162 - true_positives: 1213.4645\n",
      "For batch number 156 the model has a loss of 0.24944153428077698\n",
      "\u001b[1m156/689\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m31s\u001b[0m 58ms/step - auc: 0.9653 - binary_accuracy: 0.9292 - false_negatives: 61.2051 - false_positives: 116.7500 - loss: 0.2473 - precision: 0.9143 - recall: 0.9514 - true_negatives: 1112.9807 - true_positives: 1221.0641\n",
      "For batch number 157 the model has a loss of 0.24993568658828735\n",
      "\u001b[1m157/689\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m31s\u001b[0m 58ms/step - auc: 0.9652 - binary_accuracy: 0.9292 - false_negatives: 61.5669 - false_positives: 117.5414 - loss: 0.2473 - precision: 0.9143 - recall: 0.9514 - true_negatives: 1120.2548 - true_positives: 1228.6370\n",
      "For batch number 158 the model has a loss of 0.24924828112125397\n",
      "\u001b[1m158/689\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m31s\u001b[0m 58ms/step - auc: 0.9652 - binary_accuracy: 0.9292 - false_negatives: 61.9240 - false_positives: 118.3354 - loss: 0.2473 - precision: 0.9142 - recall: 0.9514 - true_negatives: 1127.5253 - true_positives: 1236.2152\n",
      "For batch number 159 the model has a loss of 0.24858175218105316\n",
      "\u001b[1m159/689\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m30s\u001b[0m 58ms/step - auc: 0.9652 - binary_accuracy: 0.9292 - false_negatives: 62.2767 - false_positives: 119.1195 - loss: 0.2473 - precision: 0.9142 - recall: 0.9514 - true_negatives: 1134.7861 - true_positives: 1243.8176\n",
      "For batch number 160 the model has a loss of 0.24799220263957977\n",
      "\u001b[1m160/689\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m30s\u001b[0m 58ms/step - auc: 0.9652 - binary_accuracy: 0.9292 - false_negatives: 62.6250 - false_positives: 119.9000 - loss: 0.2473 - precision: 0.9142 - recall: 0.9514 - true_negatives: 1142.0500 - true_positives: 1251.4250\n",
      "For batch number 161 the model has a loss of 0.24709957838058472\n",
      "\u001b[1m161/689\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m30s\u001b[0m 58ms/step - auc: 0.9652 - binary_accuracy: 0.9292 - false_negatives: 62.9689 - false_positives: 120.6708 - loss: 0.2473 - precision: 0.9142 - recall: 0.9514 - true_negatives: 1149.3230 - true_positives: 1259.0372\n",
      "For batch number 162 the model has a loss of 0.2472805380821228\n",
      "\u001b[1m162/689\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m30s\u001b[0m 58ms/step - auc: 0.9652 - binary_accuracy: 0.9292 - false_negatives: 63.3086 - false_positives: 121.4506 - loss: 0.2473 - precision: 0.9141 - recall: 0.9515 - true_negatives: 1156.6050 - true_positives: 1266.6357\n",
      "For batch number 163 the model has a loss of 0.24675294756889343\n",
      "\u001b[1m163/689\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m30s\u001b[0m 58ms/step - auc: 0.9652 - binary_accuracy: 0.9292 - false_negatives: 63.6503 - false_positives: 122.2209 - loss: 0.2473 - precision: 0.9141 - recall: 0.9515 - true_negatives: 1163.8589 - true_positives: 1274.2699\n",
      "For batch number 164 the model has a loss of 0.24687838554382324\n",
      "\u001b[1m164/689\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m30s\u001b[0m 58ms/step - auc: 0.9652 - binary_accuracy: 0.9292 - false_negatives: 64.0000 - false_positives: 122.9878 - loss: 0.2473 - precision: 0.9141 - recall: 0.9515 - true_negatives: 1171.0853 - true_positives: 1281.9269\n",
      "For batch number 165 the model has a loss of 0.247943714261055\n",
      "\u001b[1m165/689\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m30s\u001b[0m 58ms/step - auc: 0.9652 - binary_accuracy: 0.9292 - false_negatives: 64.3576 - false_positives: 123.7697 - loss: 0.2473 - precision: 0.9141 - recall: 0.9515 - true_negatives: 1178.3152 - true_positives: 1289.5576\n",
      "For batch number 166 the model has a loss of 0.24828338623046875\n",
      "\u001b[1m166/689\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m30s\u001b[0m 58ms/step - auc: 0.9652 - binary_accuracy: 0.9292 - false_negatives: 64.7169 - false_positives: 124.5602 - loss: 0.2473 - precision: 0.9140 - recall: 0.9515 - true_negatives: 1185.5542 - true_positives: 1297.1687\n",
      "For batch number 167 the model has a loss of 0.24836792051792145\n",
      "\u001b[1m167/689\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m30s\u001b[0m 58ms/step - auc: 0.9652 - binary_accuracy: 0.9292 - false_negatives: 65.0898 - false_positives: 125.3473 - loss: 0.2473 - precision: 0.9140 - recall: 0.9515 - true_negatives: 1192.7904 - true_positives: 1304.7725\n",
      "For batch number 168 the model has a loss of 0.24833238124847412\n",
      "\u001b[1m168/689\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m30s\u001b[0m 58ms/step - auc: 0.9652 - binary_accuracy: 0.9292 - false_negatives: 65.4643 - false_positives: 126.1310 - loss: 0.2473 - precision: 0.9140 - recall: 0.9515 - true_negatives: 1200.0476 - true_positives: 1312.3572\n",
      "For batch number 169 the model has a loss of 0.24804897606372833\n",
      "\u001b[1m169/689\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m30s\u001b[0m 58ms/step - auc: 0.9652 - binary_accuracy: 0.9292 - false_negatives: 65.8343 - false_positives: 126.9112 - loss: 0.2473 - precision: 0.9140 - recall: 0.9515 - true_negatives: 1207.3077 - true_positives: 1319.9468\n",
      "For batch number 170 the model has a loss of 0.24755193293094635\n",
      "\u001b[1m170/689\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m30s\u001b[0m 58ms/step - auc: 0.9652 - binary_accuracy: 0.9292 - false_negatives: 66.2059 - false_positives: 127.6882 - loss: 0.2473 - precision: 0.9139 - recall: 0.9516 - true_negatives: 1214.5706 - true_positives: 1327.5353\n",
      "For batch number 171 the model has a loss of 0.24753956496715546\n",
      "\u001b[1m171/689\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m30s\u001b[0m 58ms/step - auc: 0.9652 - binary_accuracy: 0.9292 - false_negatives: 66.5789 - false_positives: 128.4620 - loss: 0.2473 - precision: 0.9139 - recall: 0.9516 - true_negatives: 1221.8187 - true_positives: 1335.1404\n",
      "For batch number 172 the model has a loss of 0.2478998601436615\n",
      "\u001b[1m172/689\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m30s\u001b[0m 58ms/step - auc: 0.9651 - binary_accuracy: 0.9292 - false_negatives: 66.9593 - false_positives: 129.2267 - loss: 0.2473 - precision: 0.9139 - recall: 0.9516 - true_negatives: 1229.0524 - true_positives: 1342.7616\n",
      "For batch number 173 the model has a loss of 0.2499293088912964\n",
      "\u001b[1m173/689\u001b[0m \u001b[32m━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━\u001b[0m \u001b[1m30s\u001b[0m 58ms/step - auc: 0.9651 - binary_accuracy: 0.9291 - false_negatives: 67.3410 - false_positives: 130.0000 - loss: 0.2473 - precision: 0.9139 - recall: 0.9516 - true_negatives: 1236.2717 - true_positives: 1350.3873\n",
      "For batch number 174 the model has a loss of 0.2500663697719574\n",
      "\u001b[1m174/689\u001b[0m \u001b[32m━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━\u001b[0m \u001b[1m30s\u001b[0m 58ms/step - auc: 0.9651 - binary_accuracy: 0.9291 - false_negatives: 67.7184 - false_positives: 130.7816 - loss: 0.2474 - precision: 0.9138 - recall: 0.9516 - true_negatives: 1243.5172 - true_positives: 1357.9828\n",
      "For batch number 175 the model has a loss of 0.25040245056152344\n",
      "\u001b[1m175/689\u001b[0m \u001b[32m━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━\u001b[0m \u001b[1m29s\u001b[0m 58ms/step - auc: 0.9651 - binary_accuracy: 0.9291 - false_negatives: 68.0914 - false_positives: 131.5771 - loss: 0.2474 - precision: 0.9138 - recall: 0.9516 - true_negatives: 1250.7771 - true_positives: 1365.5543\n",
      "For batch number 176 the model has a loss of 0.2504398226737976\n",
      "\u001b[1m176/689\u001b[0m \u001b[32m━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━\u001b[0m \u001b[1m29s\u001b[0m 58ms/step - auc: 0.9651 - binary_accuracy: 0.9291 - false_negatives: 68.4602 - false_positives: 132.3807 - loss: 0.2474 - precision: 0.9138 - recall: 0.9516 - true_negatives: 1258.0171 - true_positives: 1373.1421\n",
      "For batch number 177 the model has a loss of 0.2505743205547333\n",
      "\u001b[1m177/689\u001b[0m \u001b[32m━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━\u001b[0m \u001b[1m29s\u001b[0m 58ms/step - auc: 0.9651 - binary_accuracy: 0.9291 - false_negatives: 68.8362 - false_positives: 133.1921 - loss: 0.2474 - precision: 0.9137 - recall: 0.9516 - true_negatives: 1265.2543 - true_positives: 1380.7175\n",
      "For batch number 178 the model has a loss of 0.2507191598415375\n",
      "\u001b[1m178/689\u001b[0m \u001b[32m━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━\u001b[0m \u001b[1m29s\u001b[0m 58ms/step - auc: 0.9651 - binary_accuracy: 0.9291 - false_negatives: 69.2079 - false_positives: 134.0225 - loss: 0.2474 - precision: 0.9137 - recall: 0.9516 - true_negatives: 1272.4944 - true_positives: 1388.2753\n",
      "For batch number 179 the model has a loss of 0.25035804510116577\n",
      "\u001b[1m179/689\u001b[0m \u001b[32m━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━\u001b[0m \u001b[1m29s\u001b[0m 58ms/step - auc: 0.9651 - binary_accuracy: 0.9291 - false_negatives: 69.5810 - false_positives: 134.8548 - loss: 0.2474 - precision: 0.9137 - recall: 0.9516 - true_negatives: 1279.7262 - true_positives: 1395.8380\n",
      "For batch number 180 the model has a loss of 0.25061026215553284\n",
      "\u001b[1m180/689\u001b[0m \u001b[32m━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━\u001b[0m \u001b[1m29s\u001b[0m 58ms/step - auc: 0.9651 - binary_accuracy: 0.9291 - false_negatives: 69.9500 - false_positives: 135.6833 - loss: 0.2475 - precision: 0.9136 - recall: 0.9516 - true_negatives: 1286.9500 - true_positives: 1403.4166\n",
      "For batch number 181 the model has a loss of 0.25113406777381897\n",
      "\u001b[1m181/689\u001b[0m \u001b[32m━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━\u001b[0m \u001b[1m29s\u001b[0m 58ms/step - auc: 0.9651 - binary_accuracy: 0.9291 - false_negatives: 70.3149 - false_positives: 136.5359 - loss: 0.2475 - precision: 0.9136 - recall: 0.9516 - true_negatives: 1294.1713 - true_positives: 1410.9779\n",
      "For batch number 182 the model has a loss of 0.2507542073726654\n",
      "\u001b[1m182/689\u001b[0m \u001b[32m━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━\u001b[0m \u001b[1m29s\u001b[0m 58ms/step - auc: 0.9651 - binary_accuracy: 0.9291 - false_negatives: 70.6758 - false_positives: 137.3901 - loss: 0.2475 - precision: 0.9135 - recall: 0.9516 - true_negatives: 1301.3956 - true_positives: 1418.5385\n",
      "For batch number 183 the model has a loss of 0.25045523047447205\n",
      "\u001b[1m183/689\u001b[0m \u001b[32m━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━\u001b[0m \u001b[1m29s\u001b[0m 58ms/step - auc: 0.9650 - binary_accuracy: 0.9290 - false_negatives: 71.0437 - false_positives: 138.2404 - loss: 0.2475 - precision: 0.9135 - recall: 0.9516 - true_negatives: 1308.6174 - true_positives: 1426.0984\n",
      "For batch number 184 the model has a loss of 0.25015750527381897\n",
      "\u001b[1m184/689\u001b[0m \u001b[32m━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━\u001b[0m \u001b[1m29s\u001b[0m 58ms/step - auc: 0.9650 - binary_accuracy: 0.9290 - false_negatives: 71.4076 - false_positives: 139.0978 - loss: 0.2475 - precision: 0.9134 - recall: 0.9517 - true_negatives: 1315.8315 - true_positives: 1433.6631\n",
      "For batch number 185 the model has a loss of 0.24981851875782013\n",
      "\u001b[1m185/689\u001b[0m \u001b[32m━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━\u001b[0m \u001b[1m29s\u001b[0m 58ms/step - auc: 0.9650 - binary_accuracy: 0.9290 - false_negatives: 71.7676 - false_positives: 139.9568 - loss: 0.2475 - precision: 0.9134 - recall: 0.9517 - true_negatives: 1323.0487 - true_positives: 1441.2271\n",
      "For batch number 186 the model has a loss of 0.24991068243980408\n",
      "\u001b[1m186/689\u001b[0m \u001b[32m━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━\u001b[0m \u001b[1m29s\u001b[0m 58ms/step - auc: 0.9650 - binary_accuracy: 0.9290 - false_negatives: 72.1290 - false_positives: 140.8172 - loss: 0.2475 - precision: 0.9133 - recall: 0.9517 - true_negatives: 1330.2581 - true_positives: 1448.7957\n",
      "For batch number 187 the model has a loss of 0.24941402673721313\n",
      "\u001b[1m187/689\u001b[0m \u001b[32m━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━\u001b[0m \u001b[1m29s\u001b[0m 58ms/step - auc: 0.9650 - binary_accuracy: 0.9290 - false_negatives: 72.4920 - false_positives: 141.6738 - loss: 0.2476 - precision: 0.9133 - recall: 0.9517 - true_negatives: 1337.4598 - true_positives: 1456.3744\n",
      "For batch number 188 the model has a loss of 0.2491103857755661\n",
      "\u001b[1m188/689\u001b[0m \u001b[32m━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━\u001b[0m \u001b[1m29s\u001b[0m 58ms/step - auc: 0.9650 - binary_accuracy: 0.9290 - false_negatives: 72.8564 - false_positives: 142.5213 - loss: 0.2476 - precision: 0.9133 - recall: 0.9517 - true_negatives: 1344.6543 - true_positives: 1463.9681\n",
      "For batch number 189 the model has a loss of 0.24978791177272797\n",
      "\u001b[1m189/689\u001b[0m \u001b[32m━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━\u001b[0m \u001b[1m29s\u001b[0m 58ms/step - auc: 0.9650 - binary_accuracy: 0.9290 - false_negatives: 73.2222 - false_positives: 143.3598 - loss: 0.2476 - precision: 0.9132 - recall: 0.9517 - true_negatives: 1351.8307 - true_positives: 1471.5873\n",
      "For batch number 190 the model has a loss of 0.2493623048067093\n",
      "\u001b[1m190/689\u001b[0m \u001b[32m━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━\u001b[0m \u001b[1m29s\u001b[0m 58ms/step - auc: 0.9650 - binary_accuracy: 0.9289 - false_negatives: 73.5842 - false_positives: 144.1947 - loss: 0.2476 - precision: 0.9132 - recall: 0.9517 - true_negatives: 1359.0052 - true_positives: 1479.2158\n",
      "For batch number 191 the model has a loss of 0.24991729855537415\n",
      "\u001b[1m191/689\u001b[0m \u001b[32m━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━\u001b[0m \u001b[1m29s\u001b[0m 58ms/step - auc: 0.9650 - binary_accuracy: 0.9289 - false_negatives: 73.9529 - false_positives: 145.0209 - loss: 0.2476 - precision: 0.9132 - recall: 0.9517 - true_negatives: 1366.1832 - true_positives: 1486.8429\n",
      "For batch number 192 the model has a loss of 0.24923431873321533\n",
      "\u001b[1m192/689\u001b[0m \u001b[32m━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━\u001b[0m \u001b[1m29s\u001b[0m 58ms/step - auc: 0.9650 - binary_accuracy: 0.9289 - false_negatives: 74.3177 - false_positives: 145.8385 - loss: 0.2476 - precision: 0.9131 - recall: 0.9517 - true_negatives: 1373.3594 - true_positives: 1494.4844\n",
      "For batch number 193 the model has a loss of 0.24948027729988098\n",
      "\u001b[1m193/689\u001b[0m \u001b[32m━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━\u001b[0m \u001b[1m28s\u001b[0m 58ms/step - auc: 0.9650 - binary_accuracy: 0.9289 - false_negatives: 74.6788 - false_positives: 146.6736 - loss: 0.2476 - precision: 0.9131 - recall: 0.9517 - true_negatives: 1380.5388 - true_positives: 1502.1088\n",
      "For batch number 194 the model has a loss of 0.24993062019348145\n",
      "\u001b[1m194/689\u001b[0m \u001b[32m━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━\u001b[0m \u001b[1m28s\u001b[0m 58ms/step - auc: 0.9650 - binary_accuracy: 0.9289 - false_negatives: 75.0464 - false_positives: 147.5052 - loss: 0.2476 - precision: 0.9131 - recall: 0.9517 - true_negatives: 1387.7113 - true_positives: 1509.7371\n",
      "For batch number 195 the model has a loss of 0.24990858137607574\n",
      "\u001b[1m195/689\u001b[0m \u001b[32m━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━\u001b[0m \u001b[1m28s\u001b[0m 58ms/step - auc: 0.9650 - binary_accuracy: 0.9289 - false_negatives: 75.4205 - false_positives: 148.3282 - loss: 0.2476 - precision: 0.9130 - recall: 0.9518 - true_negatives: 1394.8770 - true_positives: 1517.3744\n",
      "For batch number 196 the model has a loss of 0.25037047266960144\n",
      "\u001b[1m196/689\u001b[0m \u001b[32m━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━\u001b[0m \u001b[1m28s\u001b[0m 58ms/step - auc: 0.9650 - binary_accuracy: 0.9289 - false_negatives: 75.8010 - false_positives: 149.1480 - loss: 0.2477 - precision: 0.9130 - recall: 0.9518 - true_negatives: 1402.0459 - true_positives: 1525.0051\n",
      "For batch number 197 the model has a loss of 0.251185804605484\n",
      "\u001b[1m197/689\u001b[0m \u001b[32m━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━\u001b[0m \u001b[1m28s\u001b[0m 58ms/step - auc: 0.9649 - binary_accuracy: 0.9289 - false_negatives: 76.1878 - false_positives: 149.9695 - loss: 0.2477 - precision: 0.9130 - recall: 0.9518 - true_negatives: 1409.2081 - true_positives: 1532.6345\n",
      "For batch number 198 the model has a loss of 0.25121867656707764\n",
      "\u001b[1m198/689\u001b[0m \u001b[32m━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━\u001b[0m \u001b[1m28s\u001b[0m 58ms/step - auc: 0.9649 - binary_accuracy: 0.9289 - false_negatives: 76.5758 - false_positives: 150.7879 - loss: 0.2477 - precision: 0.9129 - recall: 0.9518 - true_negatives: 1416.3636 - true_positives: 1540.2727\n",
      "For batch number 199 the model has a loss of 0.25064554810523987\n",
      "\u001b[1m199/689\u001b[0m \u001b[32m━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━\u001b[0m \u001b[1m28s\u001b[0m 58ms/step - auc: 0.9649 - binary_accuracy: 0.9289 - false_negatives: 76.9598 - false_positives: 151.6080 - loss: 0.2477 - precision: 0.9129 - recall: 0.9518 - true_negatives: 1423.5226 - true_positives: 1547.9095\n",
      "For batch number 200 the model has a loss of 0.2502950429916382\n",
      "\u001b[1m200/689\u001b[0m \u001b[32m━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━\u001b[0m \u001b[1m28s\u001b[0m 58ms/step - auc: 0.9649 - binary_accuracy: 0.9288 - false_negatives: 77.3400 - false_positives: 152.4300 - loss: 0.2477 - precision: 0.9129 - recall: 0.9518 - true_negatives: 1430.6851 - true_positives: 1555.5450\n",
      "For batch number 201 the model has a loss of 0.24936290085315704\n",
      "\u001b[1m201/689\u001b[0m \u001b[32m━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━\u001b[0m \u001b[1m28s\u001b[0m 58ms/step - auc: 0.9649 - binary_accuracy: 0.9288 - false_negatives: 77.7164 - false_positives: 153.2438 - loss: 0.2477 - precision: 0.9128 - recall: 0.9518 - true_negatives: 1437.8408 - true_positives: 1563.1990\n",
      "For batch number 202 the model has a loss of 0.24911680817604065\n",
      "\u001b[1m202/689\u001b[0m \u001b[32m━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━\u001b[0m \u001b[1m28s\u001b[0m 58ms/step - auc: 0.9649 - binary_accuracy: 0.9288 - false_negatives: 78.0941 - false_positives: 154.0495 - loss: 0.2477 - precision: 0.9128 - recall: 0.9518 - true_negatives: 1445.0050 - true_positives: 1570.8514\n",
      "For batch number 203 the model has a loss of 0.2483803927898407\n",
      "\u001b[1m203/689\u001b[0m \u001b[32m━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━\u001b[0m \u001b[1m28s\u001b[0m 58ms/step - auc: 0.9649 - binary_accuracy: 0.9288 - false_negatives: 78.4680 - false_positives: 154.8522 - loss: 0.2477 - precision: 0.9128 - recall: 0.9518 - true_negatives: 1452.1724 - true_positives: 1578.5074\n",
      "For batch number 204 the model has a loss of 0.2479322999715805\n",
      "\u001b[1m204/689\u001b[0m \u001b[32m━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━\u001b[0m \u001b[1m28s\u001b[0m 58ms/step - auc: 0.9649 - binary_accuracy: 0.9288 - false_negatives: 78.8431 - false_positives: 155.6471 - loss: 0.2477 - precision: 0.9128 - recall: 0.9518 - true_negatives: 1459.3235 - true_positives: 1586.1863\n",
      "For batch number 205 the model has a loss of 0.24791352450847626\n",
      "\u001b[1m205/689\u001b[0m \u001b[32m━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━\u001b[0m \u001b[1m28s\u001b[0m 58ms/step - auc: 0.9649 - binary_accuracy: 0.9288 - false_negatives: 79.2244 - false_positives: 156.4390 - loss: 0.2477 - precision: 0.9128 - recall: 0.9518 - true_negatives: 1466.4878 - true_positives: 1593.8488\n",
      "For batch number 206 the model has a loss of 0.24778322875499725\n",
      "\u001b[1m206/689\u001b[0m \u001b[32m━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━\u001b[0m \u001b[1m28s\u001b[0m 58ms/step - auc: 0.9649 - binary_accuracy: 0.9288 - false_negatives: 79.6068 - false_positives: 157.2281 - loss: 0.2477 - precision: 0.9127 - recall: 0.9518 - true_negatives: 1473.6602 - true_positives: 1601.5049\n",
      "For batch number 207 the model has a loss of 0.2480650246143341\n",
      "\u001b[1m207/689\u001b[0m \u001b[32m━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━\u001b[0m \u001b[1m28s\u001b[0m 58ms/step - auc: 0.9649 - binary_accuracy: 0.9288 - false_negatives: 79.9855 - false_positives: 158.0242 - loss: 0.2477 - precision: 0.9127 - recall: 0.9518 - true_negatives: 1480.8551 - true_positives: 1609.1353\n",
      "For batch number 208 the model has a loss of 0.24843190610408783\n",
      "\u001b[1m208/689\u001b[0m \u001b[32m━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━\u001b[0m \u001b[1m28s\u001b[0m 58ms/step - auc: 0.9649 - binary_accuracy: 0.9288 - false_negatives: 80.3606 - false_positives: 158.8365 - loss: 0.2477 - precision: 0.9127 - recall: 0.9518 - true_negatives: 1488.0481 - true_positives: 1616.7548\n",
      "For batch number 209 the model has a loss of 0.24813233315944672\n",
      "\u001b[1m209/689\u001b[0m \u001b[32m━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━\u001b[0m \u001b[1m27s\u001b[0m 58ms/step - auc: 0.9649 - binary_accuracy: 0.9288 - false_negatives: 80.7368 - false_positives: 159.6411 - loss: 0.2477 - precision: 0.9127 - recall: 0.9518 - true_negatives: 1495.2535 - true_positives: 1624.3684\n",
      "For batch number 210 the model has a loss of 0.2474883496761322\n",
      "\u001b[1m210/689\u001b[0m \u001b[32m━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━\u001b[0m \u001b[1m27s\u001b[0m 58ms/step - auc: 0.9649 - binary_accuracy: 0.9288 - false_negatives: 81.1095 - false_positives: 160.4476 - loss: 0.2477 - precision: 0.9126 - recall: 0.9518 - true_negatives: 1502.4619 - true_positives: 1631.9810\n",
      "For batch number 211 the model has a loss of 0.24760736525058746\n",
      "\u001b[1m211/689\u001b[0m \u001b[32m━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━\u001b[0m \u001b[1m27s\u001b[0m 58ms/step - auc: 0.9649 - binary_accuracy: 0.9288 - false_negatives: 81.4834 - false_positives: 161.2654 - loss: 0.2477 - precision: 0.9126 - recall: 0.9518 - true_negatives: 1509.6730 - true_positives: 1639.5782\n",
      "For batch number 212 the model has a loss of 0.24744273722171783\n",
      "\u001b[1m212/689\u001b[0m \u001b[32m━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━\u001b[0m \u001b[1m27s\u001b[0m 58ms/step - auc: 0.9649 - binary_accuracy: 0.9288 - false_negatives: 81.8585 - false_positives: 162.0802 - loss: 0.2477 - precision: 0.9126 - recall: 0.9518 - true_negatives: 1516.8962 - true_positives: 1647.1650\n",
      "For batch number 213 the model has a loss of 0.24886655807495117\n",
      "\u001b[1m213/689\u001b[0m \u001b[32m━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━\u001b[0m \u001b[1m27s\u001b[0m 58ms/step - auc: 0.9649 - binary_accuracy: 0.9287 - false_negatives: 82.2394 - false_positives: 162.9202 - loss: 0.2477 - precision: 0.9125 - recall: 0.9518 - true_negatives: 1524.1267 - true_positives: 1654.7136\n",
      "For batch number 214 the model has a loss of 0.249439537525177\n",
      "\u001b[1m214/689\u001b[0m \u001b[32m━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━\u001b[0m \u001b[1m27s\u001b[0m 58ms/step - auc: 0.9649 - binary_accuracy: 0.9287 - false_negatives: 82.6215 - false_positives: 163.7617 - loss: 0.2478 - precision: 0.9125 - recall: 0.9518 - true_negatives: 1531.3551 - true_positives: 1662.2617\n",
      "For batch number 215 the model has a loss of 0.24923187494277954\n",
      "\u001b[1m215/689\u001b[0m \u001b[32m━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━\u001b[0m \u001b[1m27s\u001b[0m 58ms/step - auc: 0.9649 - binary_accuracy: 0.9287 - false_negatives: 83.0093 - false_positives: 164.6000 - loss: 0.2478 - precision: 0.9125 - recall: 0.9518 - true_negatives: 1538.5814 - true_positives: 1669.8093\n",
      "For batch number 216 the model has a loss of 0.24961364269256592\n",
      "\u001b[1m216/689\u001b[0m \u001b[32m━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━\u001b[0m \u001b[1m27s\u001b[0m 58ms/step - auc: 0.9649 - binary_accuracy: 0.9287 - false_negatives: 83.3981 - false_positives: 165.4352 - loss: 0.2478 - precision: 0.9124 - recall: 0.9518 - true_negatives: 1545.8009 - true_positives: 1677.3657\n",
      "For batch number 217 the model has a loss of 0.24932536482810974\n",
      "\u001b[1m217/689\u001b[0m \u001b[32m━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━\u001b[0m \u001b[1m27s\u001b[0m 58ms/step - auc: 0.9649 - binary_accuracy: 0.9287 - false_negatives: 83.7880 - false_positives: 166.2719 - loss: 0.2478 - precision: 0.9124 - recall: 0.9518 - true_negatives: 1553.0231 - true_positives: 1684.9170\n",
      "For batch number 218 the model has a loss of 0.24883277714252472\n",
      "\u001b[1m218/689\u001b[0m \u001b[32m━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━\u001b[0m \u001b[1m27s\u001b[0m 58ms/step - auc: 0.9649 - binary_accuracy: 0.9287 - false_negatives: 84.1743 - false_positives: 167.1055 - loss: 0.2478 - precision: 0.9124 - recall: 0.9518 - true_negatives: 1560.2568 - true_positives: 1692.4633\n",
      "For batch number 219 the model has a loss of 0.24864594638347626\n",
      "\u001b[1m219/689\u001b[0m \u001b[32m━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━\u001b[0m \u001b[1m27s\u001b[0m 58ms/step - auc: 0.9648 - binary_accuracy: 0.9287 - false_negatives: 84.5571 - false_positives: 167.9406 - loss: 0.2478 - precision: 0.9123 - recall: 0.9518 - true_negatives: 1567.4932 - true_positives: 1700.0092\n",
      "For batch number 220 the model has a loss of 0.24878214299678802\n",
      "\u001b[1m220/689\u001b[0m \u001b[32m━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━\u001b[0m \u001b[1m27s\u001b[0m 58ms/step - auc: 0.9648 - binary_accuracy: 0.9287 - false_negatives: 84.9364 - false_positives: 168.7727 - loss: 0.2478 - precision: 0.9123 - recall: 0.9518 - true_negatives: 1574.7091 - true_positives: 1707.5818\n",
      "For batch number 221 the model has a loss of 0.24927857518196106\n",
      "\u001b[1m221/689\u001b[0m \u001b[32m━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━\u001b[0m \u001b[1m27s\u001b[0m 58ms/step - auc: 0.9648 - binary_accuracy: 0.9286 - false_negatives: 85.3213 - false_positives: 169.5973 - loss: 0.2478 - precision: 0.9123 - recall: 0.9518 - true_negatives: 1581.9050 - true_positives: 1715.1765\n",
      "For batch number 222 the model has a loss of 0.248925119638443\n",
      "\u001b[1m222/689\u001b[0m \u001b[32m━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━\u001b[0m \u001b[1m27s\u001b[0m 58ms/step - auc: 0.9648 - binary_accuracy: 0.9286 - false_negatives: 85.7072 - false_positives: 170.4144 - loss: 0.2478 - precision: 0.9123 - recall: 0.9518 - true_negatives: 1589.1171 - true_positives: 1722.7612\n",
      "For batch number 223 the model has a loss of 0.24920788407325745\n",
      "\u001b[1m223/689\u001b[0m \u001b[32m━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━\u001b[0m \u001b[1m27s\u001b[0m 58ms/step - auc: 0.9648 - binary_accuracy: 0.9286 - false_negatives: 86.0987 - false_positives: 171.2287 - loss: 0.2478 - precision: 0.9122 - recall: 0.9518 - true_negatives: 1596.3184 - true_positives: 1730.3542\n",
      "For batch number 224 the model has a loss of 0.24957451224327087\n",
      "\u001b[1m224/689\u001b[0m \u001b[32m━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━\u001b[0m \u001b[1m27s\u001b[0m 58ms/step - auc: 0.9648 - binary_accuracy: 0.9286 - false_negatives: 86.4866 - false_positives: 172.0536 - loss: 0.2478 - precision: 0.9122 - recall: 0.9518 - true_negatives: 1603.5223 - true_positives: 1737.9375\n",
      "For batch number 225 the model has a loss of 0.24958565831184387\n",
      "\u001b[1m225/689\u001b[0m \u001b[32m━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━\u001b[0m \u001b[1m27s\u001b[0m 58ms/step - auc: 0.9648 - binary_accuracy: 0.9286 - false_negatives: 86.8711 - false_positives: 172.8889 - loss: 0.2478 - precision: 0.9122 - recall: 0.9518 - true_negatives: 1610.7200 - true_positives: 1745.5200\n",
      "For batch number 226 the model has a loss of 0.24964916706085205\n",
      "\u001b[1m226/689\u001b[0m \u001b[32m━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━\u001b[0m \u001b[1m26s\u001b[0m 58ms/step - auc: 0.9648 - binary_accuracy: 0.9286 - false_negatives: 87.2655 - false_positives: 173.7168 - loss: 0.2478 - precision: 0.9121 - recall: 0.9518 - true_negatives: 1617.9071 - true_positives: 1753.1106\n",
      "For batch number 227 the model has a loss of 0.24990186095237732\n",
      "\u001b[1m227/689\u001b[0m \u001b[32m━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━\u001b[0m \u001b[1m26s\u001b[0m 58ms/step - auc: 0.9648 - binary_accuracy: 0.9286 - false_negatives: 87.6608 - false_positives: 174.5507 - loss: 0.2478 - precision: 0.9121 - recall: 0.9518 - true_negatives: 1625.0969 - true_positives: 1760.6917\n",
      "For batch number 228 the model has a loss of 0.2503798007965088\n",
      "\u001b[1m228/689\u001b[0m \u001b[32m━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━\u001b[0m \u001b[1m26s\u001b[0m 58ms/step - auc: 0.9648 - binary_accuracy: 0.9286 - false_negatives: 88.0526 - false_positives: 175.3947 - loss: 0.2479 - precision: 0.9121 - recall: 0.9518 - true_negatives: 1632.3026 - true_positives: 1768.2500\n",
      "For batch number 229 the model has a loss of 0.2504000961780548\n",
      "\u001b[1m229/689\u001b[0m \u001b[32m━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━\u001b[0m \u001b[1m26s\u001b[0m 58ms/step - auc: 0.9648 - binary_accuracy: 0.9286 - false_negatives: 88.4410 - false_positives: 176.2445 - loss: 0.2479 - precision: 0.9120 - recall: 0.9518 - true_negatives: 1639.5109 - true_positives: 1775.8035\n",
      "For batch number 230 the model has a loss of 0.251706063747406\n",
      "\u001b[1m230/689\u001b[0m \u001b[32m━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━\u001b[0m \u001b[1m26s\u001b[0m 58ms/step - auc: 0.9648 - binary_accuracy: 0.9285 - false_negatives: 88.8304 - false_positives: 177.0913 - loss: 0.2479 - precision: 0.9120 - recall: 0.9518 - true_negatives: 1646.7217 - true_positives: 1783.3566\n",
      "For batch number 231 the model has a loss of 0.2515229284763336\n",
      "\u001b[1m231/689\u001b[0m \u001b[32m━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━\u001b[0m \u001b[1m26s\u001b[0m 58ms/step - auc: 0.9648 - binary_accuracy: 0.9285 - false_negatives: 89.2165 - false_positives: 177.9481 - loss: 0.2479 - precision: 0.9120 - recall: 0.9518 - true_negatives: 1653.9264 - true_positives: 1790.9091\n",
      "For batch number 232 the model has a loss of 0.2510545551776886\n",
      "\u001b[1m232/689\u001b[0m \u001b[32m━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━\u001b[0m \u001b[1m26s\u001b[0m 58ms/step - auc: 0.9648 - binary_accuracy: 0.9285 - false_negatives: 89.5991 - false_positives: 178.8060 - loss: 0.2479 - precision: 0.9119 - recall: 0.9518 - true_negatives: 1661.1422 - true_positives: 1798.4526\n",
      "For batch number 233 the model has a loss of 0.2518385052680969\n",
      "\u001b[1m233/689\u001b[0m \u001b[32m━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━\u001b[0m \u001b[1m26s\u001b[0m 58ms/step - auc: 0.9648 - binary_accuracy: 0.9285 - false_negatives: 89.9785 - false_positives: 179.6824 - loss: 0.2479 - precision: 0.9119 - recall: 0.9518 - true_negatives: 1668.3605 - true_positives: 1805.9785\n",
      "For batch number 234 the model has a loss of 0.25367578864097595\n",
      "\u001b[1m234/689\u001b[0m \u001b[32m━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━\u001b[0m \u001b[1m26s\u001b[0m 58ms/step - auc: 0.9648 - binary_accuracy: 0.9285 - false_negatives: 90.3675 - false_positives: 180.5556 - loss: 0.2480 - precision: 0.9119 - recall: 0.9518 - true_negatives: 1675.5641 - true_positives: 1813.5128\n",
      "For batch number 235 the model has a loss of 0.25348421931266785\n",
      "\u001b[1m235/689\u001b[0m \u001b[32m━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━\u001b[0m \u001b[1m26s\u001b[0m 58ms/step - auc: 0.9647 - binary_accuracy: 0.9285 - false_negatives: 90.7574 - false_positives: 181.4255 - loss: 0.2480 - precision: 0.9118 - recall: 0.9518 - true_negatives: 1682.7532 - true_positives: 1821.0638\n",
      "For batch number 236 the model has a loss of 0.2541501224040985\n",
      "\u001b[1m236/689\u001b[0m \u001b[32m━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━\u001b[0m \u001b[1m26s\u001b[0m 58ms/step - auc: 0.9647 - binary_accuracy: 0.9284 - false_negatives: 91.1483 - false_positives: 182.3008 - loss: 0.2480 - precision: 0.9118 - recall: 0.9518 - true_negatives: 1689.9237 - true_positives: 1828.6271\n",
      "For batch number 237 the model has a loss of 0.2540060877799988\n",
      "\u001b[1m237/689\u001b[0m \u001b[32m━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━\u001b[0m \u001b[1m26s\u001b[0m 58ms/step - auc: 0.9647 - binary_accuracy: 0.9284 - false_negatives: 91.5401 - false_positives: 183.1772 - loss: 0.2480 - precision: 0.9118 - recall: 0.9518 - true_negatives: 1697.0886 - true_positives: 1836.1941\n",
      "For batch number 238 the model has a loss of 0.25412067770957947\n",
      "\u001b[1m238/689\u001b[0m \u001b[32m━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━\u001b[0m \u001b[1m26s\u001b[0m 58ms/step - auc: 0.9647 - binary_accuracy: 0.9284 - false_negatives: 91.9286 - false_positives: 184.0588 - loss: 0.2481 - precision: 0.9117 - recall: 0.9518 - true_negatives: 1704.2646 - true_positives: 1843.7479\n",
      "For batch number 239 the model has a loss of 0.25399574637413025\n",
      "\u001b[1m239/689\u001b[0m \u001b[32m━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━\u001b[0m \u001b[1m26s\u001b[0m 58ms/step - auc: 0.9647 - binary_accuracy: 0.9284 - false_negatives: 92.3138 - false_positives: 184.9456 - loss: 0.2481 - precision: 0.9117 - recall: 0.9518 - true_negatives: 1711.4268 - true_positives: 1851.3138\n",
      "For batch number 240 the model has a loss of 0.25384989380836487\n",
      "\u001b[1m240/689\u001b[0m \u001b[32m━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━\u001b[0m \u001b[1m26s\u001b[0m 58ms/step - auc: 0.9647 - binary_accuracy: 0.9284 - false_negatives: 92.6958 - false_positives: 185.8292 - loss: 0.2481 - precision: 0.9116 - recall: 0.9518 - true_negatives: 1718.5958 - true_positives: 1858.8792\n",
      "For batch number 241 the model has a loss of 0.25315994024276733\n",
      "\u001b[1m241/689\u001b[0m \u001b[32m━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━\u001b[0m \u001b[1m26s\u001b[0m 58ms/step - auc: 0.9647 - binary_accuracy: 0.9284 - false_negatives: 93.0747 - false_positives: 186.7095 - loss: 0.2481 - precision: 0.9116 - recall: 0.9518 - true_negatives: 1725.7676 - true_positives: 1866.4481\n",
      "For batch number 242 the model has a loss of 0.25307419896125793\n",
      "\u001b[1m242/689\u001b[0m \u001b[32m━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━\u001b[0m \u001b[1m25s\u001b[0m 58ms/step - auc: 0.9647 - binary_accuracy: 0.9283 - false_negatives: 93.4545 - false_positives: 187.5868 - loss: 0.2481 - precision: 0.9116 - recall: 0.9518 - true_negatives: 1732.9421 - true_positives: 1874.0165\n",
      "For batch number 243 the model has a loss of 0.25251150131225586\n",
      "\u001b[1m243/689\u001b[0m \u001b[32m━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━\u001b[0m \u001b[1m25s\u001b[0m 58ms/step - auc: 0.9647 - binary_accuracy: 0.9283 - false_negatives: 93.8313 - false_positives: 188.4568 - loss: 0.2482 - precision: 0.9115 - recall: 0.9518 - true_negatives: 1740.1317 - true_positives: 1881.5802\n",
      "For batch number 244 the model has a loss of 0.2524956166744232\n",
      "\u001b[1m244/689\u001b[0m \u001b[32m━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━\u001b[0m \u001b[1m25s\u001b[0m 58ms/step - auc: 0.9647 - binary_accuracy: 0.9283 - false_negatives: 94.2131 - false_positives: 189.3238 - loss: 0.2482 - precision: 0.9115 - recall: 0.9519 - true_negatives: 1747.3074 - true_positives: 1889.1558\n",
      "For batch number 245 the model has a loss of 0.25274908542633057\n",
      "\u001b[1m245/689\u001b[0m \u001b[32m━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━\u001b[0m \u001b[1m25s\u001b[0m 58ms/step - auc: 0.9647 - binary_accuracy: 0.9283 - false_negatives: 94.5959 - false_positives: 190.1837 - loss: 0.2482 - precision: 0.9115 - recall: 0.9519 - true_negatives: 1754.4775 - true_positives: 1896.7428\n",
      "For batch number 246 the model has a loss of 0.2525729238986969\n",
      "\u001b[1m246/689\u001b[0m \u001b[32m━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━\u001b[0m \u001b[1m25s\u001b[0m 58ms/step - auc: 0.9646 - binary_accuracy: 0.9283 - false_negatives: 94.9756 - false_positives: 191.0406 - loss: 0.2482 - precision: 0.9114 - recall: 0.9519 - true_negatives: 1761.6464 - true_positives: 1904.3374\n",
      "For batch number 247 the model has a loss of 0.2524844706058502\n",
      "\u001b[1m247/689\u001b[0m \u001b[32m━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━\u001b[0m \u001b[1m25s\u001b[0m 58ms/step - auc: 0.9646 - binary_accuracy: 0.9283 - false_negatives: 95.3563 - false_positives: 191.9028 - loss: 0.2482 - precision: 0.9114 - recall: 0.9519 - true_negatives: 1768.8219 - true_positives: 1911.9191\n",
      "For batch number 248 the model has a loss of 0.2532068192958832\n",
      "\u001b[1m248/689\u001b[0m \u001b[32m━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━\u001b[0m \u001b[1m25s\u001b[0m 58ms/step - auc: 0.9646 - binary_accuracy: 0.9283 - false_negatives: 95.7379 - false_positives: 192.7661 - loss: 0.2483 - precision: 0.9114 - recall: 0.9519 - true_negatives: 1776.0081 - true_positives: 1919.4879\n",
      "For batch number 249 the model has a loss of 0.2534353733062744\n",
      "\u001b[1m249/689\u001b[0m \u001b[32m━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━\u001b[0m \u001b[1m25s\u001b[0m 58ms/step - auc: 0.9646 - binary_accuracy: 0.9282 - false_negatives: 96.1245 - false_positives: 193.6225 - loss: 0.2483 - precision: 0.9113 - recall: 0.9519 - true_negatives: 1783.1727 - true_positives: 1927.0803\n",
      "For batch number 250 the model has a loss of 0.2528862953186035\n",
      "\u001b[1m250/689\u001b[0m \u001b[32m━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━\u001b[0m \u001b[1m25s\u001b[0m 58ms/step - auc: 0.9646 - binary_accuracy: 0.9282 - false_negatives: 96.5120 - false_positives: 194.4720 - loss: 0.2483 - precision: 0.9113 - recall: 0.9519 - true_negatives: 1790.3521 - true_positives: 1934.6639\n",
      "For batch number 251 the model has a loss of 0.25248220562934875\n",
      "\u001b[1m251/689\u001b[0m \u001b[32m━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━\u001b[0m \u001b[1m25s\u001b[0m 58ms/step - auc: 0.9646 - binary_accuracy: 0.9282 - false_negatives: 96.8964 - false_positives: 195.3187 - loss: 0.2483 - precision: 0.9113 - recall: 0.9519 - true_negatives: 1797.5378 - true_positives: 1942.2471\n",
      "For batch number 252 the model has a loss of 0.25272804498672485\n",
      "\u001b[1m252/689\u001b[0m \u001b[32m━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━\u001b[0m \u001b[1m25s\u001b[0m 58ms/step - auc: 0.9646 - binary_accuracy: 0.9282 - false_negatives: 97.2778 - false_positives: 196.1667 - loss: 0.2483 - precision: 0.9112 - recall: 0.9519 - true_negatives: 1804.7222 - true_positives: 1949.8334\n",
      "For batch number 253 the model has a loss of 0.25286588072776794\n",
      "\u001b[1m253/689\u001b[0m \u001b[32m━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━\u001b[0m \u001b[1m25s\u001b[0m 58ms/step - auc: 0.9646 - binary_accuracy: 0.9282 - false_negatives: 97.6640 - false_positives: 197.0198 - loss: 0.2483 - precision: 0.9112 - recall: 0.9519 - true_negatives: 1811.9131 - true_positives: 1957.4032\n",
      "For batch number 254 the model has a loss of 0.25401827692985535\n",
      "\u001b[1m254/689\u001b[0m \u001b[32m━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━\u001b[0m \u001b[1m25s\u001b[0m 58ms/step - auc: 0.9646 - binary_accuracy: 0.9282 - false_negatives: 98.0551 - false_positives: 197.8858 - loss: 0.2484 - precision: 0.9112 - recall: 0.9519 - true_negatives: 1819.1024 - true_positives: 1964.9567\n",
      "For batch number 255 the model has a loss of 0.25378453731536865\n",
      "\u001b[1m255/689\u001b[0m \u001b[32m━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━\u001b[0m \u001b[1m25s\u001b[0m 58ms/step - auc: 0.9646 - binary_accuracy: 0.9282 - false_negatives: 98.4471 - false_positives: 198.7490 - loss: 0.2484 - precision: 0.9111 - recall: 0.9519 - true_negatives: 1826.2902 - true_positives: 1972.5137\n",
      "For batch number 256 the model has a loss of 0.2537217438220978\n",
      "\u001b[1m256/689\u001b[0m \u001b[32m━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━\u001b[0m \u001b[1m25s\u001b[0m 58ms/step - auc: 0.9646 - binary_accuracy: 0.9282 - false_negatives: 98.8359 - false_positives: 199.6133 - loss: 0.2484 - precision: 0.9111 - recall: 0.9519 - true_negatives: 1833.4844 - true_positives: 1980.0664\n",
      "For batch number 257 the model has a loss of 0.253574937582016\n",
      "\u001b[1m257/689\u001b[0m \u001b[32m━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━\u001b[0m \u001b[1m25s\u001b[0m 58ms/step - auc: 0.9646 - binary_accuracy: 0.9281 - false_negatives: 99.2257 - false_positives: 200.4708 - loss: 0.2484 - precision: 0.9111 - recall: 0.9519 - true_negatives: 1840.6693 - true_positives: 1987.6343\n",
      "For batch number 258 the model has a loss of 0.25357645750045776\n",
      "\u001b[1m258/689\u001b[0m \u001b[32m━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━\u001b[0m \u001b[1m25s\u001b[0m 58ms/step - auc: 0.9646 - binary_accuracy: 0.9281 - false_negatives: 99.6124 - false_positives: 201.3372 - loss: 0.2485 - precision: 0.9111 - recall: 0.9519 - true_negatives: 1847.8489 - true_positives: 1995.2015\n",
      "For batch number 259 the model has a loss of 0.25292515754699707\n",
      "\u001b[1m259/689\u001b[0m \u001b[32m━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━\u001b[0m \u001b[1m24s\u001b[0m 58ms/step - auc: 0.9645 - binary_accuracy: 0.9281 - false_negatives: 99.9961 - false_positives: 202.1969 - loss: 0.2485 - precision: 0.9110 - recall: 0.9519 - true_negatives: 1855.0232 - true_positives: 2002.7838\n",
      "For batch number 260 the model has a loss of 0.2533719539642334\n",
      "\u001b[1m260/689\u001b[0m \u001b[32m━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━\u001b[0m \u001b[1m24s\u001b[0m 58ms/step - auc: 0.9645 - binary_accuracy: 0.9281 - false_negatives: 100.3808 - false_positives: 203.0615 - loss: 0.2485 - precision: 0.9110 - recall: 0.9519 - true_negatives: 1862.1962 - true_positives: 2010.3616\n",
      "For batch number 261 the model has a loss of 0.2533363401889801\n",
      "\u001b[1m261/689\u001b[0m \u001b[32m━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━\u001b[0m \u001b[1m24s\u001b[0m 58ms/step - auc: 0.9645 - binary_accuracy: 0.9281 - false_negatives: 100.7663 - false_positives: 203.9310 - loss: 0.2485 - precision: 0.9110 - recall: 0.9519 - true_negatives: 1869.3793 - true_positives: 2017.9233\n",
      "For batch number 262 the model has a loss of 0.25287142395973206\n",
      "\u001b[1m262/689\u001b[0m \u001b[32m━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━\u001b[0m \u001b[1m24s\u001b[0m 58ms/step - auc: 0.9645 - binary_accuracy: 0.9281 - false_negatives: 101.1489 - false_positives: 204.8015 - loss: 0.2485 - precision: 0.9109 - recall: 0.9519 - true_negatives: 1876.5610 - true_positives: 2025.4885\n",
      "For batch number 263 the model has a loss of 0.25281795859336853\n",
      "\u001b[1m263/689\u001b[0m \u001b[32m━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━\u001b[0m \u001b[1m24s\u001b[0m 58ms/step - auc: 0.9645 - binary_accuracy: 0.9281 - false_negatives: 101.5285 - false_positives: 205.6768 - loss: 0.2485 - precision: 0.9109 - recall: 0.9519 - true_negatives: 1883.7452 - true_positives: 2033.0494\n",
      "For batch number 264 the model has a loss of 0.2531541883945465\n",
      "\u001b[1m264/689\u001b[0m \u001b[32m━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━\u001b[0m \u001b[1m24s\u001b[0m 58ms/step - auc: 0.9645 - binary_accuracy: 0.9280 - false_negatives: 101.9091 - false_positives: 206.5530 - loss: 0.2486 - precision: 0.9109 - recall: 0.9519 - true_negatives: 1890.9318 - true_positives: 2040.6061\n",
      "For batch number 265 the model has a loss of 0.25430524349212646\n",
      "\u001b[1m265/689\u001b[0m \u001b[32m━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━\u001b[0m \u001b[1m24s\u001b[0m 58ms/step - auc: 0.9645 - binary_accuracy: 0.9280 - false_negatives: 102.2943 - false_positives: 207.4264 - loss: 0.2486 - precision: 0.9108 - recall: 0.9519 - true_negatives: 1898.1132 - true_positives: 2048.1660\n",
      "For batch number 266 the model has a loss of 0.2545795440673828\n",
      "\u001b[1m266/689\u001b[0m \u001b[32m━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━\u001b[0m \u001b[1m24s\u001b[0m 58ms/step - auc: 0.9645 - binary_accuracy: 0.9280 - false_negatives: 102.6805 - false_positives: 208.2932 - loss: 0.2486 - precision: 0.9108 - recall: 0.9519 - true_negatives: 1905.3008 - true_positives: 2055.7256\n",
      "For batch number 267 the model has a loss of 0.25441038608551025\n",
      "\u001b[1m267/689\u001b[0m \u001b[32m━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━\u001b[0m \u001b[1m24s\u001b[0m 58ms/step - auc: 0.9645 - binary_accuracy: 0.9280 - false_negatives: 103.0637 - false_positives: 209.1610 - loss: 0.2486 - precision: 0.9108 - recall: 0.9519 - true_negatives: 1912.4944 - true_positives: 2063.2810\n",
      "For batch number 268 the model has a loss of 0.2558577060699463\n",
      "\u001b[1m268/689\u001b[0m \u001b[32m━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━\u001b[0m \u001b[1m24s\u001b[0m 58ms/step - auc: 0.9645 - binary_accuracy: 0.9280 - false_negatives: 103.4590 - false_positives: 210.0298 - loss: 0.2486 - precision: 0.9107 - recall: 0.9519 - true_negatives: 1919.6865 - true_positives: 2070.8247\n",
      "For batch number 269 the model has a loss of 0.25537022948265076\n",
      "\u001b[1m269/689\u001b[0m \u001b[32m━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━\u001b[0m \u001b[1m24s\u001b[0m 58ms/step - auc: 0.9645 - binary_accuracy: 0.9280 - false_negatives: 103.8513 - false_positives: 210.8959 - loss: 0.2487 - precision: 0.9107 - recall: 0.9519 - true_negatives: 1926.8773 - true_positives: 2078.3755\n",
      "For batch number 270 the model has a loss of 0.2548370063304901\n",
      "\u001b[1m270/689\u001b[0m \u001b[32m━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━\u001b[0m \u001b[1m24s\u001b[0m 58ms/step - auc: 0.9645 - binary_accuracy: 0.9280 - false_negatives: 104.2407 - false_positives: 211.7556 - loss: 0.2487 - precision: 0.9107 - recall: 0.9519 - true_negatives: 1934.0555 - true_positives: 2085.9482\n",
      "For batch number 271 the model has a loss of 0.2544616162776947\n",
      "\u001b[1m271/689\u001b[0m \u001b[32m━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━\u001b[0m \u001b[1m24s\u001b[0m 58ms/step - auc: 0.9645 - binary_accuracy: 0.9280 - false_negatives: 104.6310 - false_positives: 212.6089 - loss: 0.2487 - precision: 0.9106 - recall: 0.9519 - true_negatives: 1941.2362 - true_positives: 2093.5239\n",
      "For batch number 272 the model has a loss of 0.25522714853286743\n",
      "\u001b[1m272/689\u001b[0m \u001b[32m━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━\u001b[0m \u001b[1m24s\u001b[0m 58ms/step - auc: 0.9644 - binary_accuracy: 0.9279 - false_negatives: 105.0257 - false_positives: 213.4559 - loss: 0.2487 - precision: 0.9106 - recall: 0.9519 - true_negatives: 1948.4081 - true_positives: 2101.1104\n",
      "For batch number 273 the model has a loss of 0.25514042377471924\n",
      "\u001b[1m273/689\u001b[0m \u001b[32m━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━\u001b[0m \u001b[1m24s\u001b[0m 58ms/step - auc: 0.9644 - binary_accuracy: 0.9279 - false_negatives: 105.4176 - false_positives: 214.3040 - loss: 0.2488 - precision: 0.9106 - recall: 0.9519 - true_negatives: 1955.5787 - true_positives: 2108.6997\n",
      "For batch number 274 the model has a loss of 0.2547176778316498\n",
      "\u001b[1m274/689\u001b[0m \u001b[32m━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━\u001b[0m \u001b[1m24s\u001b[0m 58ms/step - auc: 0.9644 - binary_accuracy: 0.9279 - false_negatives: 105.8102 - false_positives: 215.1496 - loss: 0.2488 - precision: 0.9106 - recall: 0.9519 - true_negatives: 1962.7592 - true_positives: 2116.2810\n",
      "For batch number 275 the model has a loss of 0.25504788756370544\n",
      "\u001b[1m275/689\u001b[0m \u001b[32m━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━\u001b[0m \u001b[1m24s\u001b[0m 58ms/step - auc: 0.9644 - binary_accuracy: 0.9279 - false_negatives: 106.2073 - false_positives: 215.9964 - loss: 0.2488 - precision: 0.9105 - recall: 0.9519 - true_negatives: 1969.9272 - true_positives: 2123.8691\n",
      "For batch number 276 the model has a loss of 0.25523388385772705\n",
      "\u001b[1m276/689\u001b[0m \u001b[32m━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━\u001b[0m \u001b[1m23s\u001b[0m 58ms/step - auc: 0.9644 - binary_accuracy: 0.9279 - false_negatives: 106.6051 - false_positives: 216.8442 - loss: 0.2488 - precision: 0.9105 - recall: 0.9519 - true_negatives: 1977.1051 - true_positives: 2131.4456\n",
      "For batch number 277 the model has a loss of 0.25488415360450745\n",
      "\u001b[1m277/689\u001b[0m \u001b[32m━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━\u001b[0m \u001b[1m23s\u001b[0m 58ms/step - auc: 0.9644 - binary_accuracy: 0.9279 - false_negatives: 107.0000 - false_positives: 217.6931 - loss: 0.2489 - precision: 0.9105 - recall: 0.9519 - true_negatives: 1984.2852 - true_positives: 2139.0217\n",
      "For batch number 278 the model has a loss of 0.25478601455688477\n",
      "\u001b[1m278/689\u001b[0m \u001b[32m━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━\u001b[0m \u001b[1m23s\u001b[0m 58ms/step - auc: 0.9644 - binary_accuracy: 0.9279 - false_negatives: 107.3921 - false_positives: 218.5468 - loss: 0.2489 - precision: 0.9105 - recall: 0.9519 - true_negatives: 1991.4712 - true_positives: 2146.5898\n",
      "For batch number 279 the model has a loss of 0.2555972933769226\n",
      "\u001b[1m279/689\u001b[0m \u001b[32m━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━\u001b[0m \u001b[1m23s\u001b[0m 58ms/step - auc: 0.9644 - binary_accuracy: 0.9279 - false_negatives: 107.7849 - false_positives: 219.3978 - loss: 0.2489 - precision: 0.9104 - recall: 0.9519 - true_negatives: 1998.6559 - true_positives: 2154.1614\n",
      "For batch number 280 the model has a loss of 0.25541049242019653\n",
      "\u001b[1m280/689\u001b[0m \u001b[32m━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━\u001b[0m \u001b[1m23s\u001b[0m 58ms/step - auc: 0.9644 - binary_accuracy: 0.9278 - false_negatives: 108.1821 - false_positives: 220.2500 - loss: 0.2489 - precision: 0.9104 - recall: 0.9519 - true_negatives: 2005.8464 - true_positives: 2161.7214\n",
      "For batch number 281 the model has a loss of 0.2554467022418976\n",
      "\u001b[1m281/689\u001b[0m \u001b[32m━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━\u001b[0m \u001b[1m23s\u001b[0m 58ms/step - auc: 0.9644 - binary_accuracy: 0.9278 - false_negatives: 108.5801 - false_positives: 221.1068 - loss: 0.2489 - precision: 0.9104 - recall: 0.9518 - true_negatives: 2013.0320 - true_positives: 2169.2812\n",
      "For batch number 282 the model has a loss of 0.2559818923473358\n",
      "\u001b[1m282/689\u001b[0m \u001b[32m━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━\u001b[0m \u001b[1m23s\u001b[0m 58ms/step - auc: 0.9644 - binary_accuracy: 0.9278 - false_negatives: 108.9823 - false_positives: 221.9645 - loss: 0.2490 - precision: 0.9104 - recall: 0.9518 - true_negatives: 2020.2092 - true_positives: 2176.8440\n",
      "For batch number 283 the model has a loss of 0.2556896507740021\n",
      "\u001b[1m283/689\u001b[0m \u001b[32m━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━\u001b[0m \u001b[1m23s\u001b[0m 58ms/step - auc: 0.9644 - binary_accuracy: 0.9278 - false_negatives: 109.3816 - false_positives: 222.8198 - loss: 0.2490 - precision: 0.9103 - recall: 0.9518 - true_negatives: 2027.3958 - true_positives: 2184.4028\n",
      "For batch number 284 the model has a loss of 0.2561586797237396\n",
      "\u001b[1m284/689\u001b[0m \u001b[32m━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━\u001b[0m \u001b[1m23s\u001b[0m 58ms/step - auc: 0.9644 - binary_accuracy: 0.9278 - false_negatives: 109.7887 - false_positives: 223.6761 - loss: 0.2490 - precision: 0.9103 - recall: 0.9518 - true_negatives: 2034.5775 - true_positives: 2191.9578\n",
      "For batch number 285 the model has a loss of 0.2558731436729431\n",
      "\u001b[1m285/689\u001b[0m \u001b[32m━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━\u001b[0m \u001b[1m23s\u001b[0m 58ms/step - auc: 0.9643 - binary_accuracy: 0.9278 - false_negatives: 110.1965 - false_positives: 224.5298 - loss: 0.2490 - precision: 0.9103 - recall: 0.9518 - true_negatives: 2041.7579 - true_positives: 2199.5159\n",
      "For batch number 286 the model has a loss of 0.255851686000824\n",
      "\u001b[1m286/689\u001b[0m \u001b[32m━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━\u001b[0m \u001b[1m23s\u001b[0m 58ms/step - auc: 0.9643 - binary_accuracy: 0.9278 - false_negatives: 110.6084 - false_positives: 225.3881 - loss: 0.2491 - precision: 0.9103 - recall: 0.9518 - true_negatives: 2048.9370 - true_positives: 2207.0664\n",
      "For batch number 287 the model has a loss of 0.2561782896518707\n",
      "\u001b[1m287/689\u001b[0m \u001b[32m━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━\u001b[0m \u001b[1m23s\u001b[0m 58ms/step - auc: 0.9643 - binary_accuracy: 0.9278 - false_negatives: 111.0209 - false_positives: 226.2439 - loss: 0.2491 - precision: 0.9102 - recall: 0.9518 - true_negatives: 2056.1116 - true_positives: 2214.6238\n",
      "For batch number 288 the model has a loss of 0.2562536895275116\n",
      "\u001b[1m288/689\u001b[0m \u001b[32m━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━\u001b[0m \u001b[1m23s\u001b[0m 58ms/step - auc: 0.9643 - binary_accuracy: 0.9277 - false_negatives: 111.4340 - false_positives: 227.1007 - loss: 0.2491 - precision: 0.9102 - recall: 0.9518 - true_negatives: 2063.2847 - true_positives: 2222.1807\n",
      "For batch number 289 the model has a loss of 0.2561231255531311\n",
      "\u001b[1m289/689\u001b[0m \u001b[32m━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━\u001b[0m \u001b[1m23s\u001b[0m 58ms/step - auc: 0.9643 - binary_accuracy: 0.9277 - false_negatives: 111.8477 - false_positives: 227.9585 - loss: 0.2491 - precision: 0.9102 - recall: 0.9518 - true_negatives: 2070.4741 - true_positives: 2229.7197\n",
      "For batch number 290 the model has a loss of 0.2557136118412018\n",
      "\u001b[1m290/689\u001b[0m \u001b[32m━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━\u001b[0m \u001b[1m23s\u001b[0m 58ms/step - auc: 0.9643 - binary_accuracy: 0.9277 - false_negatives: 112.2586 - false_positives: 228.8138 - loss: 0.2492 - precision: 0.9102 - recall: 0.9518 - true_negatives: 2077.6758 - true_positives: 2237.2517\n",
      "For batch number 291 the model has a loss of 0.25535136461257935\n",
      "\u001b[1m291/689\u001b[0m \u001b[32m━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━\u001b[0m \u001b[1m23s\u001b[0m 58ms/step - auc: 0.9643 - binary_accuracy: 0.9277 - false_negatives: 112.6667 - false_positives: 229.6667 - loss: 0.2492 - precision: 0.9101 - recall: 0.9518 - true_negatives: 2084.8831 - true_positives: 2244.7834\n",
      "For batch number 292 the model has a loss of 0.25486698746681213\n",
      "\u001b[1m292/689\u001b[0m \u001b[32m━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━\u001b[0m \u001b[1m22s\u001b[0m 58ms/step - auc: 0.9643 - binary_accuracy: 0.9277 - false_negatives: 113.0719 - false_positives: 230.5171 - loss: 0.2492 - precision: 0.9101 - recall: 0.9518 - true_negatives: 2092.0994 - true_positives: 2252.3115\n",
      "For batch number 293 the model has a loss of 0.2546377182006836\n",
      "\u001b[1m293/689\u001b[0m \u001b[32m━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━\u001b[0m \u001b[1m22s\u001b[0m 58ms/step - auc: 0.9643 - binary_accuracy: 0.9277 - false_negatives: 113.4744 - false_positives: 231.3652 - loss: 0.2492 - precision: 0.9101 - recall: 0.9518 - true_negatives: 2099.3105 - true_positives: 2259.8499\n",
      "For batch number 294 the model has a loss of 0.25535157322883606\n",
      "\u001b[1m294/689\u001b[0m \u001b[32m━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━\u001b[0m \u001b[1m22s\u001b[0m 58ms/step - auc: 0.9643 - binary_accuracy: 0.9277 - false_negatives: 113.8810 - false_positives: 232.2075 - loss: 0.2492 - precision: 0.9101 - recall: 0.9518 - true_negatives: 2106.5068 - true_positives: 2267.4048\n",
      "For batch number 295 the model has a loss of 0.25545766949653625\n",
      "\u001b[1m295/689\u001b[0m \u001b[32m━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━\u001b[0m \u001b[1m22s\u001b[0m 58ms/step - auc: 0.9643 - binary_accuracy: 0.9277 - false_negatives: 114.2847 - false_positives: 233.0576 - loss: 0.2493 - precision: 0.9100 - recall: 0.9518 - true_negatives: 2113.7119 - true_positives: 2274.9458\n",
      "For batch number 296 the model has a loss of 0.25520065426826477\n",
      "\u001b[1m296/689\u001b[0m \u001b[32m━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━\u001b[0m \u001b[1m22s\u001b[0m 58ms/step - auc: 0.9643 - binary_accuracy: 0.9276 - false_negatives: 114.6858 - false_positives: 233.9088 - loss: 0.2493 - precision: 0.9100 - recall: 0.9518 - true_negatives: 2120.9189 - true_positives: 2282.4866\n",
      "For batch number 297 the model has a loss of 0.25535115599632263\n",
      "\u001b[1m297/689\u001b[0m \u001b[32m━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━\u001b[0m \u001b[1m22s\u001b[0m 58ms/step - auc: 0.9643 - binary_accuracy: 0.9276 - false_negatives: 115.0842 - false_positives: 234.7710 - loss: 0.2493 - precision: 0.9100 - recall: 0.9518 - true_negatives: 2128.1245 - true_positives: 2290.0203\n",
      "For batch number 298 the model has a loss of 0.2561914920806885\n",
      "\u001b[1m298/689\u001b[0m \u001b[32m━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━\u001b[0m \u001b[1m22s\u001b[0m 58ms/step - auc: 0.9643 - binary_accuracy: 0.9276 - false_negatives: 115.4866 - false_positives: 235.6376 - loss: 0.2493 - precision: 0.9100 - recall: 0.9518 - true_negatives: 2135.3323 - true_positives: 2297.5437\n",
      "For batch number 299 the model has a loss of 0.25655800104141235\n",
      "\u001b[1m299/689\u001b[0m \u001b[32m━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━\u001b[0m \u001b[1m22s\u001b[0m 58ms/step - auc: 0.9643 - binary_accuracy: 0.9276 - false_negatives: 115.8863 - false_positives: 236.5084 - loss: 0.2494 - precision: 0.9099 - recall: 0.9518 - true_negatives: 2142.5251 - true_positives: 2305.0803\n",
      "For batch number 300 the model has a loss of 0.25617149472236633\n",
      "\u001b[1m300/689\u001b[0m \u001b[32m━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━\u001b[0m \u001b[1m22s\u001b[0m 58ms/step - auc: 0.9642 - binary_accuracy: 0.9276 - false_negatives: 116.2867 - false_positives: 237.3733 - loss: 0.2494 - precision: 0.9099 - recall: 0.9518 - true_negatives: 2149.7134 - true_positives: 2312.6267\n",
      "For batch number 301 the model has a loss of 0.25555798411369324\n",
      "\u001b[1m301/689\u001b[0m \u001b[32m━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━\u001b[0m \u001b[1m22s\u001b[0m 58ms/step - auc: 0.9642 - binary_accuracy: 0.9276 - false_negatives: 116.6844 - false_positives: 238.2326 - loss: 0.2494 - precision: 0.9099 - recall: 0.9518 - true_negatives: 2156.9136 - true_positives: 2320.1694\n",
      "For batch number 302 the model has a loss of 0.25642821192741394\n",
      "\u001b[1m302/689\u001b[0m \u001b[32m━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━\u001b[0m \u001b[1m22s\u001b[0m 58ms/step - auc: 0.9642 - binary_accuracy: 0.9276 - false_negatives: 117.0960 - false_positives: 239.0861 - loss: 0.2494 - precision: 0.9099 - recall: 0.9518 - true_negatives: 2164.0928 - true_positives: 2327.7251\n",
      "For batch number 303 the model has a loss of 0.2559641897678375\n",
      "\u001b[1m303/689\u001b[0m \u001b[32m━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━\u001b[0m \u001b[1m22s\u001b[0m 58ms/step - auc: 0.9642 - binary_accuracy: 0.9276 - false_negatives: 117.5050 - false_positives: 239.9340 - loss: 0.2494 - precision: 0.9098 - recall: 0.9518 - true_negatives: 2171.2639 - true_positives: 2335.2971\n",
      "For batch number 304 the model has a loss of 0.2557241916656494\n",
      "\u001b[1m304/689\u001b[0m \u001b[32m━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━\u001b[0m \u001b[1m22s\u001b[0m 58ms/step - auc: 0.9642 - binary_accuracy: 0.9275 - false_negatives: 117.9145 - false_positives: 240.7829 - loss: 0.2495 - precision: 0.9098 - recall: 0.9518 - true_negatives: 2178.4407 - true_positives: 2342.8618\n",
      "For batch number 305 the model has a loss of 0.2559411823749542\n",
      "\u001b[1m305/689\u001b[0m \u001b[32m━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━\u001b[0m \u001b[1m22s\u001b[0m 58ms/step - auc: 0.9642 - binary_accuracy: 0.9275 - false_negatives: 118.3213 - false_positives: 241.6361 - loss: 0.2495 - precision: 0.9098 - recall: 0.9517 - true_negatives: 2185.6394 - true_positives: 2350.4033\n",
      "For batch number 306 the model has a loss of 0.25579458475112915\n",
      "\u001b[1m306/689\u001b[0m \u001b[32m━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━\u001b[0m \u001b[1m22s\u001b[0m 58ms/step - auc: 0.9642 - binary_accuracy: 0.9275 - false_negatives: 118.7288 - false_positives: 242.4902 - loss: 0.2495 - precision: 0.9098 - recall: 0.9517 - true_negatives: 2192.8496 - true_positives: 2357.9314\n",
      "For batch number 307 the model has a loss of 0.2555294930934906\n",
      "\u001b[1m307/689\u001b[0m \u001b[32m━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━\u001b[0m \u001b[1m22s\u001b[0m 58ms/step - auc: 0.9642 - binary_accuracy: 0.9275 - false_negatives: 119.1336 - false_positives: 243.3420 - loss: 0.2495 - precision: 0.9097 - recall: 0.9517 - true_negatives: 2200.0586 - true_positives: 2365.4658\n",
      "For batch number 308 the model has a loss of 0.25516074895858765\n",
      "\u001b[1m308/689\u001b[0m \u001b[32m━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━\u001b[0m \u001b[1m22s\u001b[0m 58ms/step - auc: 0.9642 - binary_accuracy: 0.9275 - false_negatives: 119.5357 - false_positives: 244.1916 - loss: 0.2495 - precision: 0.9097 - recall: 0.9517 - true_negatives: 2207.2793 - true_positives: 2372.9934\n",
      "For batch number 309 the model has a loss of 0.25503793358802795\n",
      "\u001b[1m309/689\u001b[0m \u001b[32m━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━\u001b[0m \u001b[1m21s\u001b[0m 58ms/step - auc: 0.9642 - binary_accuracy: 0.9275 - false_negatives: 119.9385 - false_positives: 245.0421 - loss: 0.2496 - precision: 0.9097 - recall: 0.9517 - true_negatives: 2214.4888 - true_positives: 2380.5308\n",
      "For batch number 310 the model has a loss of 0.2548682391643524\n",
      "\u001b[1m310/689\u001b[0m \u001b[32m━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━\u001b[0m \u001b[1m21s\u001b[0m 58ms/step - auc: 0.9642 - binary_accuracy: 0.9275 - false_negatives: 120.3419 - false_positives: 245.8903 - loss: 0.2496 - precision: 0.9097 - recall: 0.9517 - true_negatives: 2221.6968 - true_positives: 2388.0710\n",
      "For batch number 311 the model has a loss of 0.2543962895870209\n",
      "\u001b[1m311/689\u001b[0m \u001b[32m━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━\u001b[0m \u001b[1m21s\u001b[0m 58ms/step - auc: 0.9642 - binary_accuracy: 0.9275 - false_negatives: 120.7428 - false_positives: 246.7395 - loss: 0.2496 - precision: 0.9096 - recall: 0.9517 - true_negatives: 2228.9067 - true_positives: 2395.6108\n",
      "For batch number 312 the model has a loss of 0.25432172417640686\n",
      "\u001b[1m312/689\u001b[0m \u001b[32m━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━\u001b[0m \u001b[1m21s\u001b[0m 58ms/step - auc: 0.9642 - binary_accuracy: 0.9275 - false_negatives: 121.1442 - false_positives: 247.5865 - loss: 0.2496 - precision: 0.9096 - recall: 0.9517 - true_negatives: 2236.1121 - true_positives: 2403.1570\n",
      "For batch number 313 the model has a loss of 0.2538723349571228\n",
      "\u001b[1m313/689\u001b[0m \u001b[32m━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━\u001b[0m \u001b[1m21s\u001b[0m 58ms/step - auc: 0.9642 - binary_accuracy: 0.9274 - false_negatives: 121.5431 - false_positives: 248.4313 - loss: 0.2496 - precision: 0.9096 - recall: 0.9517 - true_negatives: 2243.3291 - true_positives: 2410.6965\n",
      "For batch number 314 the model has a loss of 0.2539921998977661\n",
      "\u001b[1m314/689\u001b[0m \u001b[32m━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━\u001b[0m \u001b[1m21s\u001b[0m 58ms/step - auc: 0.9642 - binary_accuracy: 0.9274 - false_negatives: 121.9427 - false_positives: 249.2771 - loss: 0.2496 - precision: 0.9096 - recall: 0.9517 - true_negatives: 2250.5349 - true_positives: 2418.2451\n",
      "For batch number 315 the model has a loss of 0.25389352440834045\n",
      "\u001b[1m315/689\u001b[0m \u001b[32m━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━\u001b[0m \u001b[1m21s\u001b[0m 58ms/step - auc: 0.9642 - binary_accuracy: 0.9274 - false_negatives: 122.3429 - false_positives: 250.1206 - loss: 0.2496 - precision: 0.9096 - recall: 0.9517 - true_negatives: 2257.7302 - true_positives: 2425.8064\n",
      "For batch number 316 the model has a loss of 0.2538260817527771\n",
      "\u001b[1m316/689\u001b[0m \u001b[32m━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━\u001b[0m \u001b[1m21s\u001b[0m 58ms/step - auc: 0.9642 - binary_accuracy: 0.9274 - false_negatives: 122.7405 - false_positives: 250.9652 - loss: 0.2497 - precision: 0.9095 - recall: 0.9517 - true_negatives: 2264.9368 - true_positives: 2433.3577\n",
      "For batch number 317 the model has a loss of 0.2532460689544678\n",
      "\u001b[1m317/689\u001b[0m \u001b[32m━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━\u001b[0m \u001b[1m21s\u001b[0m 58ms/step - auc: 0.9641 - binary_accuracy: 0.9274 - false_negatives: 123.1356 - false_positives: 251.8044 - loss: 0.2497 - precision: 0.9095 - recall: 0.9517 - true_negatives: 2272.1389 - true_positives: 2440.9211\n",
      "For batch number 318 the model has a loss of 0.2535952031612396\n",
      "\u001b[1m318/689\u001b[0m \u001b[32m━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━\u001b[0m \u001b[1m21s\u001b[0m 58ms/step - auc: 0.9641 - binary_accuracy: 0.9274 - false_negatives: 123.5377 - false_positives: 252.6415 - loss: 0.2497 - precision: 0.9095 - recall: 0.9517 - true_negatives: 2279.3459 - true_positives: 2448.4749\n",
      "For batch number 319 the model has a loss of 0.25317418575286865\n",
      "\u001b[1m319/689\u001b[0m \u001b[32m━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━\u001b[0m \u001b[1m21s\u001b[0m 58ms/step - auc: 0.9641 - binary_accuracy: 0.9274 - false_negatives: 123.9404 - false_positives: 253.4734 - loss: 0.2497 - precision: 0.9095 - recall: 0.9517 - true_negatives: 2286.5486 - true_positives: 2456.0376\n",
      "For batch number 320 the model has a loss of 0.2532680630683899\n",
      "\u001b[1m320/689\u001b[0m \u001b[32m━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━\u001b[0m \u001b[1m21s\u001b[0m 58ms/step - auc: 0.9641 - binary_accuracy: 0.9274 - false_negatives: 124.3438 - false_positives: 254.3000 - loss: 0.2497 - precision: 0.9095 - recall: 0.9517 - true_negatives: 2293.7563 - true_positives: 2463.6001\n",
      "For batch number 321 the model has a loss of 0.2543855607509613\n",
      "\u001b[1m321/689\u001b[0m \u001b[32m━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━\u001b[0m \u001b[1m21s\u001b[0m 58ms/step - auc: 0.9641 - binary_accuracy: 0.9274 - false_negatives: 124.7508 - false_positives: 255.1246 - loss: 0.2497 - precision: 0.9095 - recall: 0.9517 - true_negatives: 2300.9658 - true_positives: 2471.1589\n",
      "For batch number 322 the model has a loss of 0.25551527738571167\n",
      "\u001b[1m322/689\u001b[0m \u001b[32m━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━\u001b[0m \u001b[1m21s\u001b[0m 58ms/step - auc: 0.9641 - binary_accuracy: 0.9274 - false_negatives: 125.1584 - false_positives: 255.9472 - loss: 0.2497 - precision: 0.9094 - recall: 0.9517 - true_negatives: 2308.1802 - true_positives: 2478.7144\n",
      "For batch number 323 the model has a loss of 0.2551210820674896\n",
      "\u001b[1m323/689\u001b[0m \u001b[32m━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━\u001b[0m \u001b[1m21s\u001b[0m 58ms/step - auc: 0.9641 - binary_accuracy: 0.9274 - false_negatives: 125.5635 - false_positives: 256.7678 - loss: 0.2498 - precision: 0.9094 - recall: 0.9517 - true_negatives: 2315.3994 - true_positives: 2486.2693\n",
      "For batch number 324 the model has a loss of 0.25621312856674194\n",
      "\u001b[1m324/689\u001b[0m \u001b[32m━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━\u001b[0m \u001b[1m21s\u001b[0m 58ms/step - auc: 0.9641 - binary_accuracy: 0.9273 - false_negatives: 125.9722 - false_positives: 257.5988 - loss: 0.2498 - precision: 0.9094 - recall: 0.9517 - true_negatives: 2322.6296 - true_positives: 2493.7993\n",
      "For batch number 325 the model has a loss of 0.2562035322189331\n",
      "\u001b[1m325/689\u001b[0m \u001b[32m━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━\u001b[0m \u001b[1m21s\u001b[0m 58ms/step - auc: 0.9641 - binary_accuracy: 0.9273 - false_negatives: 126.3877 - false_positives: 258.4246 - loss: 0.2498 - precision: 0.9094 - recall: 0.9517 - true_negatives: 2329.8584 - true_positives: 2501.3293\n",
      "For batch number 326 the model has a loss of 0.25620678067207336\n",
      "\u001b[1m326/689\u001b[0m \u001b[32m━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━\u001b[0m \u001b[1m20s\u001b[0m 58ms/step - auc: 0.9641 - binary_accuracy: 0.9273 - false_negatives: 126.8037 - false_positives: 259.2485 - loss: 0.2498 - precision: 0.9094 - recall: 0.9517 - true_negatives: 2337.0798 - true_positives: 2508.8682\n",
      "For batch number 327 the model has a loss of 0.2559206187725067\n",
      "\u001b[1m327/689\u001b[0m \u001b[32m━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━\u001b[0m \u001b[1m20s\u001b[0m 58ms/step - auc: 0.9641 - binary_accuracy: 0.9273 - false_negatives: 127.2171 - false_positives: 260.0703 - loss: 0.2498 - precision: 0.9094 - recall: 0.9517 - true_negatives: 2344.3088 - true_positives: 2516.4036\n",
      "For batch number 328 the model has a loss of 0.2559845447540283\n",
      "\u001b[1m328/689\u001b[0m \u001b[32m━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━\u001b[0m \u001b[1m20s\u001b[0m 58ms/step - auc: 0.9641 - binary_accuracy: 0.9273 - false_negatives: 127.6311 - false_positives: 260.8933 - loss: 0.2499 - precision: 0.9093 - recall: 0.9517 - true_negatives: 2351.5457 - true_positives: 2523.9299\n",
      "For batch number 329 the model has a loss of 0.2560800313949585\n",
      "\u001b[1m329/689\u001b[0m \u001b[32m━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━\u001b[0m \u001b[1m20s\u001b[0m 58ms/step - auc: 0.9641 - binary_accuracy: 0.9273 - false_negatives: 128.0426 - false_positives: 261.7234 - loss: 0.2499 - precision: 0.9093 - recall: 0.9517 - true_negatives: 2358.7964 - true_positives: 2531.4377\n",
      "For batch number 330 the model has a loss of 0.2558740973472595\n",
      "\u001b[1m330/689\u001b[0m \u001b[32m━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━\u001b[0m \u001b[1m20s\u001b[0m 58ms/step - auc: 0.9641 - binary_accuracy: 0.9273 - false_negatives: 128.4515 - false_positives: 262.5515 - loss: 0.2499 - precision: 0.9093 - recall: 0.9517 - true_negatives: 2366.0576 - true_positives: 2538.9395\n",
      "For batch number 331 the model has a loss of 0.25598835945129395\n",
      "\u001b[1m331/689\u001b[0m \u001b[32m━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━\u001b[0m \u001b[1m20s\u001b[0m 58ms/step - auc: 0.9641 - binary_accuracy: 0.9273 - false_negatives: 128.8610 - false_positives: 263.3807 - loss: 0.2499 - precision: 0.9093 - recall: 0.9517 - true_negatives: 2373.3052 - true_positives: 2546.4531\n",
      "For batch number 332 the model has a loss of 0.2556430697441101\n",
      "\u001b[1m332/689\u001b[0m \u001b[32m━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━\u001b[0m \u001b[1m20s\u001b[0m 58ms/step - auc: 0.9641 - binary_accuracy: 0.9273 - false_negatives: 129.2681 - false_positives: 264.2048 - loss: 0.2499 - precision: 0.9093 - recall: 0.9516 - true_negatives: 2380.5513 - true_positives: 2553.9758\n",
      "For batch number 333 the model has a loss of 0.25555187463760376\n",
      "\u001b[1m333/689\u001b[0m \u001b[32m━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━\u001b[0m \u001b[1m20s\u001b[0m 58ms/step - auc: 0.9641 - binary_accuracy: 0.9273 - false_negatives: 129.6757 - false_positives: 265.0270 - loss: 0.2499 - precision: 0.9092 - recall: 0.9516 - true_negatives: 2387.7959 - true_positives: 2561.5015\n",
      "For batch number 334 the model has a loss of 0.25587543845176697\n",
      "\u001b[1m334/689\u001b[0m \u001b[32m━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━\u001b[0m \u001b[1m20s\u001b[0m 58ms/step - auc: 0.9641 - binary_accuracy: 0.9273 - false_negatives: 130.0868 - false_positives: 265.8503 - loss: 0.2500 - precision: 0.9092 - recall: 0.9516 - true_negatives: 2395.0359 - true_positives: 2569.0269\n",
      "For batch number 335 the model has a loss of 0.2560492753982544\n",
      "\u001b[1m335/689\u001b[0m \u001b[32m━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━\u001b[0m \u001b[1m20s\u001b[0m 58ms/step - auc: 0.9641 - binary_accuracy: 0.9272 - false_negatives: 130.4955 - false_positives: 266.6776 - loss: 0.2500 - precision: 0.9092 - recall: 0.9516 - true_negatives: 2402.2747 - true_positives: 2576.5522\n",
      "For batch number 336 the model has a loss of 0.2557222247123718\n",
      "\u001b[1m336/689\u001b[0m \u001b[32m━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━\u001b[0m \u001b[1m20s\u001b[0m 58ms/step - auc: 0.9641 - binary_accuracy: 0.9272 - false_negatives: 130.9048 - false_positives: 267.5060 - loss: 0.2500 - precision: 0.9092 - recall: 0.9516 - true_negatives: 2409.5120 - true_positives: 2584.0774\n",
      "For batch number 337 the model has a loss of 0.2555947005748749\n",
      "\u001b[1m337/689\u001b[0m \u001b[32m━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━\u001b[0m \u001b[1m20s\u001b[0m 58ms/step - auc: 0.9640 - binary_accuracy: 0.9272 - false_negatives: 131.3145 - false_positives: 268.3323 - loss: 0.2500 - precision: 0.9092 - recall: 0.9516 - true_negatives: 2416.7478 - true_positives: 2591.6052\n",
      "For batch number 338 the model has a loss of 0.25561732053756714\n",
      "\u001b[1m338/689\u001b[0m \u001b[32m━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━\u001b[0m \u001b[1m20s\u001b[0m 58ms/step - auc: 0.9640 - binary_accuracy: 0.9272 - false_negatives: 131.7249 - false_positives: 269.1598 - loss: 0.2500 - precision: 0.9092 - recall: 0.9516 - true_negatives: 2423.9851 - true_positives: 2599.1301\n",
      "For batch number 339 the model has a loss of 0.2554309070110321\n",
      "\u001b[1m339/689\u001b[0m \u001b[32m━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━\u001b[0m \u001b[1m20s\u001b[0m 58ms/step - auc: 0.9640 - binary_accuracy: 0.9272 - false_negatives: 132.1357 - false_positives: 269.9853 - loss: 0.2500 - precision: 0.9091 - recall: 0.9516 - true_negatives: 2431.2124 - true_positives: 2606.6667\n",
      "For batch number 340 the model has a loss of 0.25527212023735046\n",
      "\u001b[1m340/689\u001b[0m \u001b[32m━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━\u001b[0m \u001b[1m20s\u001b[0m 58ms/step - auc: 0.9640 - binary_accuracy: 0.9272 - false_negatives: 132.5441 - false_positives: 270.8147 - loss: 0.2501 - precision: 0.9091 - recall: 0.9516 - true_negatives: 2438.4324 - true_positives: 2614.2087\n",
      "For batch number 341 the model has a loss of 0.25585657358169556\n",
      "\u001b[1m341/689\u001b[0m \u001b[32m━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━\u001b[0m \u001b[1m20s\u001b[0m 58ms/step - auc: 0.9640 - binary_accuracy: 0.9272 - false_negatives: 132.9531 - false_positives: 271.6422 - loss: 0.2501 - precision: 0.9091 - recall: 0.9516 - true_negatives: 2445.6511 - true_positives: 2621.7537\n",
      "For batch number 342 the model has a loss of 0.2557201087474823\n",
      "\u001b[1m342/689\u001b[0m \u001b[32m━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━\u001b[0m \u001b[1m20s\u001b[0m 58ms/step - auc: 0.9640 - binary_accuracy: 0.9272 - false_negatives: 133.3596 - false_positives: 272.4708 - loss: 0.2501 - precision: 0.9091 - recall: 0.9516 - true_negatives: 2452.8772 - true_positives: 2629.2925\n",
      "For batch number 343 the model has a loss of 0.255646675825119\n",
      "\u001b[1m343/689\u001b[0m \u001b[32m━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━\u001b[0m \u001b[1m19s\u001b[0m 58ms/step - auc: 0.9640 - binary_accuracy: 0.9272 - false_negatives: 133.7668 - false_positives: 273.2974 - loss: 0.2501 - precision: 0.9091 - recall: 0.9516 - true_negatives: 2460.1108 - true_positives: 2636.8250\n",
      "For batch number 344 the model has a loss of 0.25560104846954346\n",
      "\u001b[1m344/689\u001b[0m \u001b[32m━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━\u001b[0m \u001b[1m19s\u001b[0m 58ms/step - auc: 0.9640 - binary_accuracy: 0.9272 - false_negatives: 134.1715 - false_positives: 274.1250 - loss: 0.2501 - precision: 0.9091 - recall: 0.9516 - true_negatives: 2467.3430 - true_positives: 2644.3604\n",
      "For batch number 345 the model has a loss of 0.2555945813655853\n",
      "\u001b[1m345/689\u001b[0m \u001b[32m━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━\u001b[0m \u001b[1m19s\u001b[0m 58ms/step - auc: 0.9640 - binary_accuracy: 0.9272 - false_negatives: 134.5739 - false_positives: 274.9565 - loss: 0.2501 - precision: 0.9091 - recall: 0.9516 - true_negatives: 2474.5710 - true_positives: 2651.8984\n",
      "For batch number 346 the model has a loss of 0.25598400831222534\n",
      "\u001b[1m346/689\u001b[0m \u001b[32m━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━\u001b[0m \u001b[1m19s\u001b[0m 58ms/step - auc: 0.9640 - binary_accuracy: 0.9272 - false_negatives: 134.9740 - false_positives: 275.8006 - loss: 0.2502 - precision: 0.9090 - recall: 0.9516 - true_negatives: 2481.8035 - true_positives: 2659.4219\n",
      "For batch number 347 the model has a loss of 0.256288081407547\n",
      "\u001b[1m347/689\u001b[0m \u001b[32m━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━\u001b[0m \u001b[1m19s\u001b[0m 58ms/step - auc: 0.9640 - binary_accuracy: 0.9271 - false_negatives: 135.3775 - false_positives: 276.6484 - loss: 0.2502 - precision: 0.9090 - recall: 0.9516 - true_negatives: 2489.0374 - true_positives: 2666.9365\n",
      "For batch number 348 the model has a loss of 0.25664666295051575\n",
      "\u001b[1m348/689\u001b[0m \u001b[32m━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━\u001b[0m \u001b[1m19s\u001b[0m 58ms/step - auc: 0.9640 - binary_accuracy: 0.9271 - false_negatives: 135.7845 - false_positives: 277.4943 - loss: 0.2502 - precision: 0.9090 - recall: 0.9516 - true_negatives: 2496.2874 - true_positives: 2674.4338\n",
      "For batch number 349 the model has a loss of 0.2569286823272705\n",
      "\u001b[1m349/689\u001b[0m \u001b[32m━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━\u001b[0m \u001b[1m19s\u001b[0m 58ms/step - auc: 0.9640 - binary_accuracy: 0.9271 - false_negatives: 136.1891 - false_positives: 278.3410 - loss: 0.2502 - precision: 0.9090 - recall: 0.9516 - true_negatives: 2503.5271 - true_positives: 2681.9426\n",
      "For batch number 350 the model has a loss of 0.257038950920105\n",
      "\u001b[1m350/689\u001b[0m \u001b[32m━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━\u001b[0m \u001b[1m19s\u001b[0m 58ms/step - auc: 0.9640 - binary_accuracy: 0.9271 - false_negatives: 136.5914 - false_positives: 279.1971 - loss: 0.2502 - precision: 0.9090 - recall: 0.9516 - true_negatives: 2510.7742 - true_positives: 2689.4373\n",
      "For batch number 351 the model has a loss of 0.25701841711997986\n",
      "\u001b[1m351/689\u001b[0m \u001b[32m━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━\u001b[0m \u001b[1m19s\u001b[0m 58ms/step - auc: 0.9640 - binary_accuracy: 0.9271 - false_negatives: 136.9915 - false_positives: 280.0513 - loss: 0.2503 - precision: 0.9089 - recall: 0.9516 - true_negatives: 2518.0227 - true_positives: 2696.9346\n",
      "For batch number 352 the model has a loss of 0.2570091784000397\n",
      "\u001b[1m352/689\u001b[0m \u001b[32m━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━\u001b[0m \u001b[1m19s\u001b[0m 58ms/step - auc: 0.9640 - binary_accuracy: 0.9271 - false_negatives: 137.3892 - false_positives: 280.9034 - loss: 0.2503 - precision: 0.9089 - recall: 0.9516 - true_negatives: 2525.2642 - true_positives: 2704.4431\n",
      "For batch number 353 the model has a loss of 0.25678446888923645\n",
      "\u001b[1m353/689\u001b[0m \u001b[32m━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━\u001b[0m \u001b[1m19s\u001b[0m 58ms/step - auc: 0.9640 - binary_accuracy: 0.9271 - false_negatives: 137.7847 - false_positives: 281.7507 - loss: 0.2503 - precision: 0.9089 - recall: 0.9516 - true_negatives: 2532.5042 - true_positives: 2711.9604\n",
      "For batch number 354 the model has a loss of 0.25650498270988464\n",
      "\u001b[1m354/689\u001b[0m \u001b[32m━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━\u001b[0m \u001b[1m19s\u001b[0m 58ms/step - auc: 0.9640 - binary_accuracy: 0.9271 - false_negatives: 138.1780 - false_positives: 282.5960 - loss: 0.2503 - precision: 0.9089 - recall: 0.9516 - true_negatives: 2539.7458 - true_positives: 2719.4802\n",
      "For batch number 355 the model has a loss of 0.2565416693687439\n",
      "\u001b[1m355/689\u001b[0m \u001b[32m━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━\u001b[0m \u001b[1m19s\u001b[0m 58ms/step - auc: 0.9640 - binary_accuracy: 0.9271 - false_negatives: 138.5690 - false_positives: 283.4451 - loss: 0.2503 - precision: 0.9089 - recall: 0.9516 - true_negatives: 2546.9775 - true_positives: 2727.0085\n",
      "For batch number 356 the model has a loss of 0.2564457356929779\n",
      "\u001b[1m356/689\u001b[0m \u001b[32m━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━\u001b[0m \u001b[1m19s\u001b[0m 58ms/step - auc: 0.9639 - binary_accuracy: 0.9271 - false_negatives: 138.9579 - false_positives: 284.2950 - loss: 0.2503 - precision: 0.9088 - recall: 0.9516 - true_negatives: 2554.2163 - true_positives: 2734.5310\n",
      "For batch number 357 the model has a loss of 0.2564004063606262\n",
      "\u001b[1m357/689\u001b[0m \u001b[32m━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━\u001b[0m \u001b[1m19s\u001b[0m 58ms/step - auc: 0.9639 - binary_accuracy: 0.9271 - false_negatives: 139.3445 - false_positives: 285.1457 - loss: 0.2504 - precision: 0.9088 - recall: 0.9516 - true_negatives: 2561.4539 - true_positives: 2742.0559\n",
      "For batch number 358 the model has a loss of 0.2574367821216583\n",
      "\u001b[1m358/689\u001b[0m \u001b[32m━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━\u001b[0m \u001b[1m19s\u001b[0m 58ms/step - auc: 0.9639 - binary_accuracy: 0.9271 - false_negatives: 139.7402 - false_positives: 285.9916 - loss: 0.2504 - precision: 0.9088 - recall: 0.9516 - true_negatives: 2568.6816 - true_positives: 2749.5867\n",
      "For batch number 359 the model has a loss of 0.2570957541465759\n",
      "\u001b[1m359/689\u001b[0m \u001b[32m━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━\u001b[0m \u001b[1m19s\u001b[0m 58ms/step - auc: 0.9639 - binary_accuracy: 0.9270 - false_negatives: 140.1365 - false_positives: 286.8329 - loss: 0.2504 - precision: 0.9088 - recall: 0.9516 - true_negatives: 2575.9080 - true_positives: 2757.1226\n",
      "For batch number 360 the model has a loss of 0.25739192962646484\n",
      "\u001b[1m360/689\u001b[0m \u001b[32m━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━\u001b[0m \u001b[1m18s\u001b[0m 58ms/step - auc: 0.9639 - binary_accuracy: 0.9270 - false_negatives: 140.5389 - false_positives: 287.6722 - loss: 0.2504 - precision: 0.9088 - recall: 0.9516 - true_negatives: 2583.1194 - true_positives: 2764.6694\n",
      "For batch number 361 the model has a loss of 0.2576698362827301\n",
      "\u001b[1m361/689\u001b[0m \u001b[32m━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━\u001b[0m \u001b[1m18s\u001b[0m 58ms/step - auc: 0.9639 - binary_accuracy: 0.9270 - false_negatives: 140.9391 - false_positives: 288.5125 - loss: 0.2504 - precision: 0.9088 - recall: 0.9516 - true_negatives: 2590.3157 - true_positives: 2772.2327\n",
      "For batch number 362 the model has a loss of 0.2579478323459625\n",
      "\u001b[1m362/689\u001b[0m \u001b[32m━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━\u001b[0m \u001b[1m18s\u001b[0m 58ms/step - auc: 0.9639 - binary_accuracy: 0.9270 - false_negatives: 141.3370 - false_positives: 289.3647 - loss: 0.2505 - precision: 0.9087 - recall: 0.9516 - true_negatives: 2597.5110 - true_positives: 2779.7874\n",
      "For batch number 363 the model has a loss of 0.2578977346420288\n",
      "\u001b[1m363/689\u001b[0m \u001b[32m━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━\u001b[0m \u001b[1m18s\u001b[0m 58ms/step - auc: 0.9639 - binary_accuracy: 0.9270 - false_negatives: 141.7328 - false_positives: 290.2204 - loss: 0.2505 - precision: 0.9087 - recall: 0.9515 - true_negatives: 2604.7107 - true_positives: 2787.3362\n",
      "For batch number 364 the model has a loss of 0.2574705183506012\n",
      "\u001b[1m364/689\u001b[0m \u001b[32m━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━\u001b[0m \u001b[1m18s\u001b[0m 58ms/step - auc: 0.9639 - binary_accuracy: 0.9270 - false_negatives: 142.1264 - false_positives: 291.0769 - loss: 0.2505 - precision: 0.9087 - recall: 0.9515 - true_negatives: 2611.9094 - true_positives: 2794.8875\n",
      "For batch number 365 the model has a loss of 0.25914281606674194\n",
      "\u001b[1m365/689\u001b[0m \u001b[32m━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━\u001b[0m \u001b[1m18s\u001b[0m 58ms/step - auc: 0.9639 - binary_accuracy: 0.9270 - false_negatives: 142.5233 - false_positives: 291.9425 - loss: 0.2505 - precision: 0.9087 - recall: 0.9515 - true_negatives: 2619.1096 - true_positives: 2802.4246\n",
      "For batch number 366 the model has a loss of 0.25906291604042053\n",
      "\u001b[1m366/689\u001b[0m \u001b[32m━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━\u001b[0m \u001b[1m18s\u001b[0m 58ms/step - auc: 0.9639 - binary_accuracy: 0.9270 - false_negatives: 142.9208 - false_positives: 292.8060 - loss: 0.2505 - precision: 0.9087 - recall: 0.9515 - true_negatives: 2626.3115 - true_positives: 2809.9617\n",
      "For batch number 367 the model has a loss of 0.2589123249053955\n",
      "\u001b[1m367/689\u001b[0m \u001b[32m━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━\u001b[0m \u001b[1m18s\u001b[0m 58ms/step - auc: 0.9639 - binary_accuracy: 0.9270 - false_negatives: 143.3161 - false_positives: 293.6758 - loss: 0.2506 - precision: 0.9086 - recall: 0.9515 - true_negatives: 2633.5205 - true_positives: 2817.4878\n",
      "For batch number 368 the model has a loss of 0.25844743847846985\n",
      "\u001b[1m368/689\u001b[0m \u001b[32m━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━\u001b[0m \u001b[1m18s\u001b[0m 58ms/step - auc: 0.9639 - binary_accuracy: 0.9270 - false_negatives: 143.7092 - false_positives: 294.5408 - loss: 0.2506 - precision: 0.9086 - recall: 0.9515 - true_negatives: 2640.7310 - true_positives: 2825.0190\n",
      "For batch number 369 the model has a loss of 0.25796371698379517\n",
      "\u001b[1m369/689\u001b[0m \u001b[32m━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━\u001b[0m \u001b[1m18s\u001b[0m 58ms/step - auc: 0.9639 - binary_accuracy: 0.9270 - false_negatives: 144.1030 - false_positives: 295.4011 - loss: 0.2506 - precision: 0.9086 - recall: 0.9515 - true_negatives: 2647.9431 - true_positives: 2832.5527\n",
      "For batch number 370 the model has a loss of 0.2575191259384155\n",
      "\u001b[1m370/689\u001b[0m \u001b[32m━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━\u001b[0m \u001b[1m18s\u001b[0m 58ms/step - auc: 0.9639 - binary_accuracy: 0.9269 - false_negatives: 144.4973 - false_positives: 296.2595 - loss: 0.2506 - precision: 0.9086 - recall: 0.9515 - true_negatives: 2655.1433 - true_positives: 2840.1001\n",
      "For batch number 371 the model has a loss of 0.2570558488368988\n",
      "\u001b[1m371/689\u001b[0m \u001b[32m━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━\u001b[0m \u001b[1m18s\u001b[0m 58ms/step - auc: 0.9639 - binary_accuracy: 0.9269 - false_negatives: 144.8895 - false_positives: 297.1159 - loss: 0.2506 - precision: 0.9086 - recall: 0.9515 - true_negatives: 2662.3503 - true_positives: 2847.6443\n",
      "For batch number 372 the model has a loss of 0.2584204077720642\n",
      "\u001b[1m372/689\u001b[0m \u001b[32m━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━\u001b[0m \u001b[1m18s\u001b[0m 58ms/step - auc: 0.9638 - binary_accuracy: 0.9269 - false_negatives: 145.2876 - false_positives: 297.9731 - loss: 0.2507 - precision: 0.9085 - recall: 0.9515 - true_negatives: 2669.5537 - true_positives: 2855.1855\n",
      "For batch number 373 the model has a loss of 0.2582654058933258\n",
      "\u001b[1m373/689\u001b[0m \u001b[32m━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━\u001b[0m \u001b[1m18s\u001b[0m 58ms/step - auc: 0.9638 - binary_accuracy: 0.9269 - false_negatives: 145.6836 - false_positives: 298.8338 - loss: 0.2507 - precision: 0.9085 - recall: 0.9515 - true_negatives: 2676.7615 - true_positives: 2862.7212\n",
      "For batch number 374 the model has a loss of 0.2578999996185303\n",
      "\u001b[1m374/689\u001b[0m \u001b[32m━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━\u001b[0m \u001b[1m18s\u001b[0m 58ms/step - auc: 0.9638 - binary_accuracy: 0.9269 - false_negatives: 146.0775 - false_positives: 299.6898 - loss: 0.2507 - precision: 0.9085 - recall: 0.9515 - true_negatives: 2683.9758 - true_positives: 2870.2566\n",
      "For batch number 375 the model has a loss of 0.2581140995025635\n",
      "\u001b[1m375/689\u001b[0m \u001b[32m━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━\u001b[0m \u001b[1m18s\u001b[0m 58ms/step - auc: 0.9638 - binary_accuracy: 0.9269 - false_negatives: 146.4720 - false_positives: 300.5467 - loss: 0.2507 - precision: 0.9085 - recall: 0.9515 - true_negatives: 2691.1895 - true_positives: 2877.7920\n",
      "For batch number 376 the model has a loss of 0.2579130530357361\n",
      "\u001b[1m376/689\u001b[0m \u001b[32m━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━\u001b[0m \u001b[1m18s\u001b[0m 58ms/step - auc: 0.9638 - binary_accuracy: 0.9269 - false_negatives: 146.8644 - false_positives: 301.3989 - loss: 0.2507 - precision: 0.9085 - recall: 0.9515 - true_negatives: 2698.3936 - true_positives: 2885.3430\n",
      "For batch number 377 the model has a loss of 0.2576598823070526\n",
      "\u001b[1m377/689\u001b[0m \u001b[32m━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━\u001b[0m \u001b[1m17s\u001b[0m 58ms/step - auc: 0.9638 - binary_accuracy: 0.9269 - false_negatives: 147.2546 - false_positives: 302.2493 - loss: 0.2508 - precision: 0.9085 - recall: 0.9515 - true_negatives: 2705.6021 - true_positives: 2892.8938\n",
      "For batch number 378 the model has a loss of 0.25759613513946533\n",
      "\u001b[1m378/689\u001b[0m \u001b[32m━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━\u001b[0m \u001b[1m17s\u001b[0m 58ms/step - auc: 0.9638 - binary_accuracy: 0.9269 - false_negatives: 147.6455 - false_positives: 303.1032 - loss: 0.2508 - precision: 0.9084 - recall: 0.9515 - true_negatives: 2712.8015 - true_positives: 2900.4497\n",
      "For batch number 379 the model has a loss of 0.2576432526111603\n",
      "\u001b[1m379/689\u001b[0m \u001b[32m━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━\u001b[0m \u001b[1m17s\u001b[0m 58ms/step - auc: 0.9638 - binary_accuracy: 0.9269 - false_negatives: 148.0369 - false_positives: 303.9578 - loss: 0.2508 - precision: 0.9084 - recall: 0.9515 - true_negatives: 2719.9946 - true_positives: 2908.0105\n",
      "For batch number 380 the model has a loss of 0.25735294818878174\n",
      "\u001b[1m380/689\u001b[0m \u001b[32m━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━\u001b[0m \u001b[1m17s\u001b[0m 58ms/step - auc: 0.9638 - binary_accuracy: 0.9269 - false_negatives: 148.4263 - false_positives: 304.8105 - loss: 0.2508 - precision: 0.9084 - recall: 0.9515 - true_negatives: 2727.1895 - true_positives: 2915.5737\n",
      "For batch number 381 the model has a loss of 0.25720858573913574\n",
      "\u001b[1m381/689\u001b[0m \u001b[32m━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━\u001b[0m \u001b[1m17s\u001b[0m 58ms/step - auc: 0.9638 - binary_accuracy: 0.9269 - false_negatives: 148.8136 - false_positives: 305.6667 - loss: 0.2508 - precision: 0.9084 - recall: 0.9515 - true_negatives: 2734.3938 - true_positives: 2923.1260\n",
      "For batch number 382 the model has a loss of 0.2575299143791199\n",
      "\u001b[1m382/689\u001b[0m \u001b[32m━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━\u001b[0m \u001b[1m17s\u001b[0m 58ms/step - auc: 0.9638 - binary_accuracy: 0.9268 - false_negatives: 149.2016 - false_positives: 306.5236 - loss: 0.2509 - precision: 0.9084 - recall: 0.9515 - true_negatives: 2741.5969 - true_positives: 2930.6780\n",
      "For batch number 383 the model has a loss of 0.2575054168701172\n",
      "\u001b[1m383/689\u001b[0m \u001b[32m━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━\u001b[0m \u001b[1m17s\u001b[0m 58ms/step - auc: 0.9638 - binary_accuracy: 0.9268 - false_negatives: 149.5875 - false_positives: 307.3838 - loss: 0.2509 - precision: 0.9084 - recall: 0.9515 - true_negatives: 2748.7991 - true_positives: 2938.2297\n",
      "For batch number 384 the model has a loss of 0.25720152258872986\n",
      "\u001b[1m384/689\u001b[0m \u001b[32m━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━\u001b[0m \u001b[1m17s\u001b[0m 58ms/step - auc: 0.9638 - binary_accuracy: 0.9268 - false_negatives: 149.9714 - false_positives: 308.2422 - loss: 0.2509 - precision: 0.9083 - recall: 0.9515 - true_negatives: 2756.0105 - true_positives: 2945.7761\n",
      "For batch number 385 the model has a loss of 0.25701475143432617\n",
      "\u001b[1m385/689\u001b[0m \u001b[32m━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━\u001b[0m \u001b[1m17s\u001b[0m 58ms/step - auc: 0.9638 - binary_accuracy: 0.9268 - false_negatives: 150.3584 - false_positives: 309.0987 - loss: 0.2509 - precision: 0.9083 - recall: 0.9515 - true_negatives: 2763.2183 - true_positives: 2953.3247\n",
      "For batch number 386 the model has a loss of 0.2568622827529907\n",
      "\u001b[1m386/689\u001b[0m \u001b[32m━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━\u001b[0m \u001b[1m17s\u001b[0m 58ms/step - auc: 0.9638 - binary_accuracy: 0.9268 - false_negatives: 150.7487 - false_positives: 309.9508 - loss: 0.2509 - precision: 0.9083 - recall: 0.9515 - true_negatives: 2770.4353 - true_positives: 2960.8652\n",
      "For batch number 387 the model has a loss of 0.2567056119441986\n",
      "\u001b[1m387/689\u001b[0m \u001b[32m━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━\u001b[0m \u001b[1m17s\u001b[0m 58ms/step - auc: 0.9638 - binary_accuracy: 0.9268 - false_negatives: 151.1421 - false_positives: 310.8010 - loss: 0.2509 - precision: 0.9083 - recall: 0.9515 - true_negatives: 2777.6538 - true_positives: 2968.4031\n",
      "For batch number 388 the model has a loss of 0.25687167048454285\n",
      "\u001b[1m388/689\u001b[0m \u001b[32m━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━\u001b[0m \u001b[1m17s\u001b[0m 58ms/step - auc: 0.9638 - binary_accuracy: 0.9268 - false_negatives: 151.5361 - false_positives: 311.6495 - loss: 0.2509 - precision: 0.9083 - recall: 0.9515 - true_negatives: 2784.8711 - true_positives: 2975.9434\n",
      "For batch number 389 the model has a loss of 0.2568017244338989\n",
      "\u001b[1m389/689\u001b[0m \u001b[32m━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━\u001b[0m \u001b[1m17s\u001b[0m 58ms/step - auc: 0.9638 - binary_accuracy: 0.9268 - false_negatives: 151.9306 - false_positives: 312.4987 - loss: 0.2510 - precision: 0.9083 - recall: 0.9515 - true_negatives: 2792.0977 - true_positives: 2983.4729\n",
      "For batch number 390 the model has a loss of 0.25686511397361755\n",
      "\u001b[1m390/689\u001b[0m \u001b[32m━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━\u001b[0m \u001b[1m17s\u001b[0m 58ms/step - auc: 0.9637 - binary_accuracy: 0.9268 - false_negatives: 152.3231 - false_positives: 313.3539 - loss: 0.2510 - precision: 0.9082 - recall: 0.9515 - true_negatives: 2799.3308 - true_positives: 2990.9922\n",
      "For batch number 391 the model has a loss of 0.2566065490245819\n",
      "\u001b[1m391/689\u001b[0m \u001b[32m━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━\u001b[0m \u001b[1m17s\u001b[0m 58ms/step - auc: 0.9637 - binary_accuracy: 0.9268 - false_negatives: 152.7161 - false_positives: 314.2046 - loss: 0.2510 - precision: 0.9082 - recall: 0.9515 - true_negatives: 2806.5525 - true_positives: 2998.5269\n",
      "For batch number 392 the model has a loss of 0.2567512094974518\n",
      "\u001b[1m392/689\u001b[0m \u001b[32m━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━\u001b[0m \u001b[1m17s\u001b[0m 58ms/step - auc: 0.9637 - binary_accuracy: 0.9268 - false_negatives: 153.1097 - false_positives: 315.0510 - loss: 0.2510 - precision: 0.9082 - recall: 0.9515 - true_negatives: 2813.7654 - true_positives: 3006.0740\n",
      "For batch number 393 the model has a loss of 0.2565898299217224\n",
      "\u001b[1m393/689\u001b[0m \u001b[32m━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━\u001b[0m \u001b[1m17s\u001b[0m 58ms/step - auc: 0.9637 - binary_accuracy: 0.9268 - false_negatives: 153.5038 - false_positives: 315.8931 - loss: 0.2510 - precision: 0.9082 - recall: 0.9515 - true_negatives: 2820.9771 - true_positives: 3013.6260\n",
      "For batch number 394 the model has a loss of 0.25659042596817017\n",
      "\u001b[1m394/689\u001b[0m \u001b[32m━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━\u001b[0m \u001b[1m16s\u001b[0m 58ms/step - auc: 0.9637 - binary_accuracy: 0.9268 - false_negatives: 153.9010 - false_positives: 316.7335 - loss: 0.2510 - precision: 0.9082 - recall: 0.9515 - true_negatives: 2828.1904 - true_positives: 3021.1750\n",
      "For batch number 395 the model has a loss of 0.2566477060317993\n",
      "\u001b[1m395/689\u001b[0m \u001b[32m━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━\u001b[0m \u001b[1m16s\u001b[0m 58ms/step - auc: 0.9637 - binary_accuracy: 0.9268 - false_negatives: 154.2987 - false_positives: 317.5721 - loss: 0.2510 - precision: 0.9082 - recall: 0.9515 - true_negatives: 2835.4050 - true_positives: 3028.7241\n",
      "For batch number 396 the model has a loss of 0.25625574588775635\n",
      "\u001b[1m396/689\u001b[0m \u001b[32m━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━\u001b[0m \u001b[1m16s\u001b[0m 58ms/step - auc: 0.9637 - binary_accuracy: 0.9267 - false_negatives: 154.6944 - false_positives: 318.4066 - loss: 0.2511 - precision: 0.9081 - recall: 0.9515 - true_negatives: 2842.6289 - true_positives: 3036.2703\n",
      "For batch number 397 the model has a loss of 0.25636669993400574\n",
      "\u001b[1m397/689\u001b[0m \u001b[32m━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━\u001b[0m \u001b[1m16s\u001b[0m 58ms/step - auc: 0.9637 - binary_accuracy: 0.9267 - false_negatives: 155.0982 - false_positives: 319.2393 - loss: 0.2511 - precision: 0.9081 - recall: 0.9515 - true_negatives: 2849.8413 - true_positives: 3043.8210\n",
      "For batch number 398 the model has a loss of 0.2563157081604004\n",
      "\u001b[1m398/689\u001b[0m \u001b[32m━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━\u001b[0m \u001b[1m16s\u001b[0m 58ms/step - auc: 0.9637 - binary_accuracy: 0.9267 - false_negatives: 155.5000 - false_positives: 320.0754 - loss: 0.2511 - precision: 0.9081 - recall: 0.9515 - true_negatives: 2857.0479 - true_positives: 3051.3770\n",
      "For batch number 399 the model has a loss of 0.2560477554798126\n",
      "\u001b[1m399/689\u001b[0m \u001b[32m━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━\u001b[0m \u001b[1m16s\u001b[0m 58ms/step - auc: 0.9637 - binary_accuracy: 0.9267 - false_negatives: 155.8997 - false_positives: 320.9098 - loss: 0.2511 - precision: 0.9081 - recall: 0.9515 - true_negatives: 2864.2480 - true_positives: 3058.9424\n",
      "For batch number 400 the model has a loss of 0.256111741065979\n",
      "\u001b[1m400/689\u001b[0m \u001b[32m━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━\u001b[0m \u001b[1m16s\u001b[0m 58ms/step - auc: 0.9637 - binary_accuracy: 0.9267 - false_negatives: 156.3000 - false_positives: 321.7475 - loss: 0.2511 - precision: 0.9081 - recall: 0.9515 - true_negatives: 2871.4424 - true_positives: 3066.5100\n",
      "For batch number 401 the model has a loss of 0.256058007478714\n",
      "\u001b[1m401/689\u001b[0m \u001b[32m━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━\u001b[0m \u001b[1m16s\u001b[0m 57ms/step - auc: 0.9637 - binary_accuracy: 0.9267 - false_negatives: 156.6983 - false_positives: 322.5860 - loss: 0.2511 - precision: 0.9081 - recall: 0.9515 - true_negatives: 2878.6409 - true_positives: 3074.0747\n",
      "For batch number 402 the model has a loss of 0.25583651661872864\n",
      "\u001b[1m402/689\u001b[0m \u001b[32m━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━\u001b[0m \u001b[1m16s\u001b[0m 57ms/step - auc: 0.9637 - binary_accuracy: 0.9267 - false_negatives: 157.0945 - false_positives: 323.4254 - loss: 0.2511 - precision: 0.9081 - recall: 0.9515 - true_negatives: 2885.8408 - true_positives: 3081.6394\n",
      "For batch number 403 the model has a loss of 0.2561061680316925\n",
      "\u001b[1m403/689\u001b[0m \u001b[32m━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━\u001b[0m \u001b[1m16s\u001b[0m 57ms/step - auc: 0.9637 - binary_accuracy: 0.9267 - false_negatives: 157.4913 - false_positives: 324.2680 - loss: 0.2512 - precision: 0.9080 - recall: 0.9515 - true_negatives: 2893.0447 - true_positives: 3089.1960\n",
      "For batch number 404 the model has a loss of 0.2566002309322357\n",
      "\u001b[1m404/689\u001b[0m \u001b[32m━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━\u001b[0m \u001b[1m16s\u001b[0m 57ms/step - auc: 0.9637 - binary_accuracy: 0.9267 - false_negatives: 157.8886 - false_positives: 325.1114 - loss: 0.2512 - precision: 0.9080 - recall: 0.9515 - true_negatives: 2900.2375 - true_positives: 3096.7625\n",
      "For batch number 405 the model has a loss of 0.25755447149276733\n",
      "\u001b[1m405/689\u001b[0m \u001b[32m━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━\u001b[0m \u001b[1m16s\u001b[0m 57ms/step - auc: 0.9637 - binary_accuracy: 0.9267 - false_negatives: 158.2889 - false_positives: 325.9556 - loss: 0.2512 - precision: 0.9080 - recall: 0.9515 - true_negatives: 2907.4297 - true_positives: 3104.3259\n",
      "For batch number 406 the model has a loss of 0.25759661197662354\n",
      "\u001b[1m406/689\u001b[0m \u001b[32m━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━\u001b[0m \u001b[1m16s\u001b[0m 57ms/step - auc: 0.9637 - binary_accuracy: 0.9267 - false_negatives: 158.6872 - false_positives: 326.8103 - loss: 0.2512 - precision: 0.9080 - recall: 0.9515 - true_negatives: 2914.6230 - true_positives: 3111.8794\n",
      "For batch number 407 the model has a loss of 0.2578764855861664\n",
      "\u001b[1m407/689\u001b[0m \u001b[32m━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━\u001b[0m \u001b[1m16s\u001b[0m 57ms/step - auc: 0.9637 - binary_accuracy: 0.9267 - false_negatives: 159.0885 - false_positives: 327.6683 - loss: 0.2512 - precision: 0.9080 - recall: 0.9515 - true_negatives: 2921.8108 - true_positives: 3119.4324\n",
      "For batch number 408 the model has a loss of 0.2577846348285675\n",
      "\u001b[1m408/689\u001b[0m \u001b[32m━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━\u001b[0m \u001b[1m16s\u001b[0m 57ms/step - auc: 0.9637 - binary_accuracy: 0.9267 - false_negatives: 159.4877 - false_positives: 328.5269 - loss: 0.2512 - precision: 0.9080 - recall: 0.9515 - true_negatives: 2929.0024 - true_positives: 3126.9829\n",
      "For batch number 409 the model has a loss of 0.2581470310688019\n",
      "\u001b[1m409/689\u001b[0m \u001b[32m━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━\u001b[0m \u001b[1m16s\u001b[0m 57ms/step - auc: 0.9637 - binary_accuracy: 0.9267 - false_negatives: 159.8900 - false_positives: 329.3863 - loss: 0.2512 - precision: 0.9080 - recall: 0.9515 - true_negatives: 2936.1956 - true_positives: 3134.5281\n",
      "For batch number 410 the model has a loss of 0.2578512728214264\n",
      "\u001b[1m410/689\u001b[0m \u001b[32m━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━\u001b[0m \u001b[1m16s\u001b[0m 57ms/step - auc: 0.9637 - binary_accuracy: 0.9266 - false_negatives: 160.2902 - false_positives: 330.2439 - loss: 0.2513 - precision: 0.9079 - recall: 0.9515 - true_negatives: 2943.3828 - true_positives: 3142.0830\n",
      "For batch number 411 the model has a loss of 0.25818586349487305\n",
      "\u001b[1m411/689\u001b[0m \u001b[32m━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━\u001b[0m \u001b[1m15s\u001b[0m 57ms/step - auc: 0.9637 - binary_accuracy: 0.9266 - false_negatives: 160.6910 - false_positives: 331.1071 - loss: 0.2513 - precision: 0.9079 - recall: 0.9515 - true_negatives: 2950.5767 - true_positives: 3149.6252\n",
      "For batch number 412 the model has a loss of 0.2577895522117615\n",
      "\u001b[1m412/689\u001b[0m \u001b[32m━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━\u001b[0m \u001b[1m15s\u001b[0m 57ms/step - auc: 0.9636 - binary_accuracy: 0.9266 - false_negatives: 161.0898 - false_positives: 331.9660 - loss: 0.2513 - precision: 0.9079 - recall: 0.9515 - true_negatives: 2957.7695 - true_positives: 3157.1748\n",
      "For batch number 413 the model has a loss of 0.25771304965019226\n",
      "\u001b[1m413/689\u001b[0m \u001b[32m━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━\u001b[0m \u001b[1m15s\u001b[0m 57ms/step - auc: 0.9636 - binary_accuracy: 0.9266 - false_negatives: 161.4891 - false_positives: 332.8232 - loss: 0.2513 - precision: 0.9079 - recall: 0.9515 - true_negatives: 2964.9563 - true_positives: 3164.7312\n",
      "For batch number 414 the model has a loss of 0.25753048062324524\n",
      "\u001b[1m414/689\u001b[0m \u001b[32m━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━\u001b[0m \u001b[1m15s\u001b[0m 57ms/step - auc: 0.9636 - binary_accuracy: 0.9266 - false_negatives: 161.8865 - false_positives: 333.6787 - loss: 0.2513 - precision: 0.9079 - recall: 0.9515 - true_negatives: 2972.1475 - true_positives: 3172.2874\n",
      "For batch number 415 the model has a loss of 0.25733664631843567\n",
      "\u001b[1m415/689\u001b[0m \u001b[32m━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━\u001b[0m \u001b[1m15s\u001b[0m 57ms/step - auc: 0.9636 - binary_accuracy: 0.9266 - false_negatives: 162.2843 - false_positives: 334.5325 - loss: 0.2513 - precision: 0.9079 - recall: 0.9515 - true_negatives: 2979.3398 - true_positives: 3179.8433\n",
      "For batch number 416 the model has a loss of 0.25721409916877747\n",
      "\u001b[1m416/689\u001b[0m \u001b[32m━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━\u001b[0m \u001b[1m15s\u001b[0m 57ms/step - auc: 0.9636 - binary_accuracy: 0.9266 - false_negatives: 162.6803 - false_positives: 335.3870 - loss: 0.2514 - precision: 0.9078 - recall: 0.9515 - true_negatives: 2986.5312 - true_positives: 3187.4014\n",
      "For batch number 417 the model has a loss of 0.2571607530117035\n",
      "\u001b[1m417/689\u001b[0m \u001b[32m━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━\u001b[0m \u001b[1m15s\u001b[0m 57ms/step - auc: 0.9636 - binary_accuracy: 0.9266 - false_negatives: 163.0743 - false_positives: 336.2446 - loss: 0.2514 - precision: 0.9078 - recall: 0.9515 - true_negatives: 2993.7170 - true_positives: 3194.9641\n",
      "For batch number 418 the model has a loss of 0.2574400007724762\n",
      "\u001b[1m418/689\u001b[0m \u001b[32m━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━\u001b[0m \u001b[1m15s\u001b[0m 57ms/step - auc: 0.9636 - binary_accuracy: 0.9266 - false_negatives: 163.4665 - false_positives: 337.1077 - loss: 0.2514 - precision: 0.9078 - recall: 0.9515 - true_negatives: 3000.8972 - true_positives: 3202.5288\n",
      "For batch number 419 the model has a loss of 0.2590453028678894\n",
      "\u001b[1m419/689\u001b[0m \u001b[32m━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━\u001b[0m \u001b[1m15s\u001b[0m 57ms/step - auc: 0.9636 - binary_accuracy: 0.9266 - false_negatives: 163.8663 - false_positives: 337.9738 - loss: 0.2514 - precision: 0.9078 - recall: 0.9515 - true_negatives: 3008.0691 - true_positives: 3210.0906\n",
      "For batch number 420 the model has a loss of 0.2593126595020294\n",
      "\u001b[1m420/689\u001b[0m \u001b[32m━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━\u001b[0m \u001b[1m15s\u001b[0m 57ms/step - auc: 0.9636 - binary_accuracy: 0.9266 - false_negatives: 164.2690 - false_positives: 338.8429 - loss: 0.2514 - precision: 0.9078 - recall: 0.9515 - true_negatives: 3015.2356 - true_positives: 3217.6523\n",
      "For batch number 421 the model has a loss of 0.25912120938301086\n",
      "\u001b[1m421/689\u001b[0m \u001b[32m━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━\u001b[0m \u001b[1m15s\u001b[0m 57ms/step - auc: 0.9636 - binary_accuracy: 0.9266 - false_negatives: 164.6698 - false_positives: 339.7126 - loss: 0.2514 - precision: 0.9078 - recall: 0.9515 - true_negatives: 3022.4062 - true_positives: 3225.2114\n",
      "For batch number 422 the model has a loss of 0.25948086380958557\n",
      "\u001b[1m422/689\u001b[0m \u001b[32m━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━\u001b[0m \u001b[1m15s\u001b[0m 57ms/step - auc: 0.9636 - binary_accuracy: 0.9266 - false_negatives: 165.0687 - false_positives: 340.5877 - loss: 0.2515 - precision: 0.9078 - recall: 0.9515 - true_negatives: 3029.5735 - true_positives: 3232.7703\n",
      "For batch number 423 the model has a loss of 0.2597919702529907\n",
      "\u001b[1m423/689\u001b[0m \u001b[32m━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━\u001b[0m \u001b[1m15s\u001b[0m 57ms/step - auc: 0.9636 - binary_accuracy: 0.9265 - false_negatives: 165.4681 - false_positives: 341.4633 - loss: 0.2515 - precision: 0.9077 - recall: 0.9515 - true_negatives: 3036.7329 - true_positives: 3240.3357\n",
      "For batch number 424 the model has a loss of 0.25984686613082886\n",
      "\u001b[1m424/689\u001b[0m \u001b[32m━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━\u001b[0m \u001b[1m15s\u001b[0m 57ms/step - auc: 0.9636 - binary_accuracy: 0.9265 - false_negatives: 165.8679 - false_positives: 342.3372 - loss: 0.2515 - precision: 0.9077 - recall: 0.9515 - true_negatives: 3043.8916 - true_positives: 3247.9033\n",
      "For batch number 425 the model has a loss of 0.2596086263656616\n",
      "\u001b[1m425/689\u001b[0m \u001b[32m━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━\u001b[0m \u001b[1m15s\u001b[0m 57ms/step - auc: 0.9636 - binary_accuracy: 0.9265 - false_negatives: 166.2659 - false_positives: 343.2094 - loss: 0.2515 - precision: 0.9077 - recall: 0.9514 - true_negatives: 3051.0588 - true_positives: 3255.4658\n",
      "For batch number 426 the model has a loss of 0.2603609561920166\n",
      "\u001b[1m426/689\u001b[0m \u001b[32m━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━\u001b[0m \u001b[1m15s\u001b[0m 57ms/step - auc: 0.9636 - binary_accuracy: 0.9265 - false_negatives: 166.6690 - false_positives: 344.0775 - loss: 0.2515 - precision: 0.9077 - recall: 0.9514 - true_negatives: 3058.2229 - true_positives: 3263.0305\n",
      "For batch number 427 the model has a loss of 0.2611193358898163\n",
      "\u001b[1m427/689\u001b[0m \u001b[32m━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━\u001b[0m \u001b[1m15s\u001b[0m 57ms/step - auc: 0.9636 - binary_accuracy: 0.9265 - false_negatives: 167.0796 - false_positives: 344.9461 - loss: 0.2516 - precision: 0.9077 - recall: 0.9514 - true_negatives: 3065.3865 - true_positives: 3270.5879\n",
      "For batch number 428 the model has a loss of 0.2612851858139038\n",
      "\u001b[1m428/689\u001b[0m \u001b[32m━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━\u001b[0m \u001b[1m14s\u001b[0m 57ms/step - auc: 0.9635 - binary_accuracy: 0.9265 - false_negatives: 167.4907 - false_positives: 345.8177 - loss: 0.2516 - precision: 0.9077 - recall: 0.9514 - true_negatives: 3072.5466 - true_positives: 3278.1448\n",
      "For batch number 429 the model has a loss of 0.2617228925228119\n",
      "\u001b[1m429/689\u001b[0m \u001b[32m━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━\u001b[0m \u001b[1m14s\u001b[0m 57ms/step - auc: 0.9635 - binary_accuracy: 0.9265 - false_negatives: 167.9068 - false_positives: 346.6900 - loss: 0.2516 - precision: 0.9076 - recall: 0.9514 - true_negatives: 3079.6992 - true_positives: 3285.7039\n",
      "For batch number 430 the model has a loss of 0.26162874698638916\n",
      "\u001b[1m430/689\u001b[0m \u001b[32m━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━\u001b[0m \u001b[1m14s\u001b[0m 57ms/step - auc: 0.9635 - binary_accuracy: 0.9265 - false_negatives: 168.3256 - false_positives: 347.5628 - loss: 0.2516 - precision: 0.9076 - recall: 0.9514 - true_negatives: 3086.8418 - true_positives: 3293.2698\n",
      "For batch number 431 the model has a loss of 0.2627100646495819\n",
      "\u001b[1m431/689\u001b[0m \u001b[32m━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━\u001b[0m \u001b[1m14s\u001b[0m 57ms/step - auc: 0.9635 - binary_accuracy: 0.9265 - false_negatives: 168.7541 - false_positives: 348.4362 - loss: 0.2517 - precision: 0.9076 - recall: 0.9514 - true_negatives: 3093.9744 - true_positives: 3300.8352\n",
      "For batch number 432 the model has a loss of 0.262500137090683\n",
      "\u001b[1m432/689\u001b[0m \u001b[32m━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━\u001b[0m \u001b[1m14s\u001b[0m 57ms/step - auc: 0.9635 - binary_accuracy: 0.9265 - false_negatives: 169.1806 - false_positives: 349.3079 - loss: 0.2517 - precision: 0.9076 - recall: 0.9514 - true_negatives: 3101.1135 - true_positives: 3308.3982\n",
      "For batch number 433 the model has a loss of 0.26230356097221375\n",
      "\u001b[1m433/689\u001b[0m \u001b[32m━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━\u001b[0m \u001b[1m14s\u001b[0m 57ms/step - auc: 0.9635 - binary_accuracy: 0.9264 - false_negatives: 169.6051 - false_positives: 350.1824 - loss: 0.2517 - precision: 0.9076 - recall: 0.9514 - true_negatives: 3108.2542 - true_positives: 3315.9585\n",
      "For batch number 434 the model has a loss of 0.26237544417381287\n",
      "\u001b[1m434/689\u001b[0m \u001b[32m━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━\u001b[0m \u001b[1m14s\u001b[0m 57ms/step - auc: 0.9635 - binary_accuracy: 0.9264 - false_negatives: 170.0346 - false_positives: 351.0530 - loss: 0.2517 - precision: 0.9076 - recall: 0.9514 - true_negatives: 3115.3962 - true_positives: 3323.5161\n",
      "For batch number 435 the model has a loss of 0.2620348334312439\n",
      "\u001b[1m435/689\u001b[0m \u001b[32m━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━\u001b[0m \u001b[1m14s\u001b[0m 57ms/step - auc: 0.9635 - binary_accuracy: 0.9264 - false_negatives: 170.4621 - false_positives: 351.9218 - loss: 0.2518 - precision: 0.9075 - recall: 0.9514 - true_negatives: 3122.5425 - true_positives: 3331.0735\n",
      "For batch number 436 the model has a loss of 0.2618326246738434\n",
      "\u001b[1m436/689\u001b[0m \u001b[32m━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━\u001b[0m \u001b[1m14s\u001b[0m 57ms/step - auc: 0.9635 - binary_accuracy: 0.9264 - false_negatives: 170.8876 - false_positives: 352.7913 - loss: 0.2518 - precision: 0.9075 - recall: 0.9514 - true_negatives: 3129.6880 - true_positives: 3338.6331\n",
      "For batch number 437 the model has a loss of 0.2614685893058777\n",
      "\u001b[1m437/689\u001b[0m \u001b[32m━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━\u001b[0m \u001b[1m14s\u001b[0m 57ms/step - auc: 0.9635 - binary_accuracy: 0.9264 - false_negatives: 171.3112 - false_positives: 353.6567 - loss: 0.2518 - precision: 0.9075 - recall: 0.9514 - true_negatives: 3136.8376 - true_positives: 3346.1946\n",
      "For batch number 438 the model has a loss of 0.26153331995010376\n",
      "\u001b[1m438/689\u001b[0m \u001b[32m━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━\u001b[0m \u001b[1m14s\u001b[0m 57ms/step - auc: 0.9635 - binary_accuracy: 0.9264 - false_negatives: 171.7352 - false_positives: 354.5251 - loss: 0.2518 - precision: 0.9075 - recall: 0.9514 - true_negatives: 3143.9841 - true_positives: 3353.7556\n",
      "For batch number 439 the model has a loss of 0.2613377869129181\n",
      "\u001b[1m439/689\u001b[0m \u001b[32m━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━\u001b[0m \u001b[1m14s\u001b[0m 57ms/step - auc: 0.9635 - binary_accuracy: 0.9264 - false_negatives: 172.1595 - false_positives: 355.3895 - loss: 0.2518 - precision: 0.9075 - recall: 0.9514 - true_negatives: 3151.1299 - true_positives: 3361.3213\n",
      "For batch number 440 the model has a loss of 0.2614114284515381\n",
      "\u001b[1m440/689\u001b[0m \u001b[32m━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━\u001b[0m \u001b[1m14s\u001b[0m 57ms/step - auc: 0.9635 - binary_accuracy: 0.9264 - false_negatives: 172.5818 - false_positives: 356.2614 - loss: 0.2519 - precision: 0.9075 - recall: 0.9514 - true_negatives: 3158.2773 - true_positives: 3368.8796\n",
      "For batch number 441 the model has a loss of 0.2615644037723541\n",
      "\u001b[1m441/689\u001b[0m \u001b[32m━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━\u001b[0m \u001b[1m14s\u001b[0m 57ms/step - auc: 0.9634 - binary_accuracy: 0.9264 - false_negatives: 173.0045 - false_positives: 357.1315 - loss: 0.2519 - precision: 0.9074 - recall: 0.9514 - true_negatives: 3165.4285 - true_positives: 3376.4353\n",
      "For batch number 442 the model has a loss of 0.2613994777202606\n",
      "\u001b[1m442/689\u001b[0m \u001b[32m━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━\u001b[0m \u001b[1m14s\u001b[0m 57ms/step - auc: 0.9634 - binary_accuracy: 0.9264 - false_negatives: 173.4253 - false_positives: 357.9977 - loss: 0.2519 - precision: 0.9074 - recall: 0.9514 - true_negatives: 3172.5769 - true_positives: 3384.0000\n",
      "For batch number 443 the model has a loss of 0.26149171590805054\n",
      "\u001b[1m443/689\u001b[0m \u001b[32m━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━\u001b[0m \u001b[1m14s\u001b[0m 57ms/step - auc: 0.9634 - binary_accuracy: 0.9264 - false_negatives: 173.8465 - false_positives: 358.8668 - loss: 0.2519 - precision: 0.9074 - recall: 0.9514 - true_negatives: 3179.7246 - true_positives: 3391.5620\n",
      "For batch number 444 the model has a loss of 0.26156511902809143\n",
      "\u001b[1m444/689\u001b[0m \u001b[32m━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━\u001b[0m \u001b[1m14s\u001b[0m 57ms/step - auc: 0.9634 - binary_accuracy: 0.9263 - false_negatives: 174.2680 - false_positives: 359.7387 - loss: 0.2519 - precision: 0.9074 - recall: 0.9514 - true_negatives: 3186.8738 - true_positives: 3399.1194\n",
      "For batch number 445 the model has a loss of 0.26192933320999146\n",
      "\u001b[1m445/689\u001b[0m \u001b[32m━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━\u001b[0m \u001b[1m13s\u001b[0m 57ms/step - auc: 0.9634 - binary_accuracy: 0.9263 - false_negatives: 174.6944 - false_positives: 360.6090 - loss: 0.2520 - precision: 0.9074 - recall: 0.9514 - true_negatives: 3194.0337 - true_positives: 3406.6628\n",
      "For batch number 446 the model has a loss of 0.2617374062538147\n",
      "\u001b[1m446/689\u001b[0m \u001b[32m━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━\u001b[0m \u001b[1m13s\u001b[0m 57ms/step - auc: 0.9634 - binary_accuracy: 0.9263 - false_negatives: 175.1188 - false_positives: 361.4776 - loss: 0.2520 - precision: 0.9074 - recall: 0.9513 - true_negatives: 3201.1929 - true_positives: 3414.2107\n",
      "For batch number 447 the model has a loss of 0.2620539963245392\n",
      "\u001b[1m447/689\u001b[0m \u001b[32m━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━\u001b[0m \u001b[1m13s\u001b[0m 57ms/step - auc: 0.9634 - binary_accuracy: 0.9263 - false_negatives: 175.5459 - false_positives: 362.3423 - loss: 0.2520 - precision: 0.9074 - recall: 0.9513 - true_negatives: 3208.3445 - true_positives: 3421.7673\n",
      "For batch number 448 the model has a loss of 0.2618294358253479\n",
      "\u001b[1m448/689\u001b[0m \u001b[32m━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━\u001b[0m \u001b[1m13s\u001b[0m 57ms/step - auc: 0.9634 - binary_accuracy: 0.9263 - false_negatives: 175.9710 - false_positives: 363.2031 - loss: 0.2520 - precision: 0.9073 - recall: 0.9513 - true_negatives: 3215.5022 - true_positives: 3429.3237\n",
      "For batch number 449 the model has a loss of 0.26161956787109375\n",
      "\u001b[1m449/689\u001b[0m \u001b[32m━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━\u001b[0m \u001b[1m13s\u001b[0m 57ms/step - auc: 0.9634 - binary_accuracy: 0.9263 - false_negatives: 176.3942 - false_positives: 364.0623 - loss: 0.2521 - precision: 0.9073 - recall: 0.9513 - true_negatives: 3222.6570 - true_positives: 3436.8865\n",
      "For batch number 450 the model has a loss of 0.26176029443740845\n",
      "\u001b[1m450/689\u001b[0m \u001b[32m━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━\u001b[0m \u001b[1m13s\u001b[0m 57ms/step - auc: 0.9634 - binary_accuracy: 0.9263 - false_negatives: 176.8156 - false_positives: 364.9244 - loss: 0.2521 - precision: 0.9073 - recall: 0.9513 - true_negatives: 3229.8201 - true_positives: 3444.4399\n",
      "For batch number 451 the model has a loss of 0.26176708936691284\n",
      "\u001b[1m451/689\u001b[0m \u001b[32m━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━\u001b[0m \u001b[1m13s\u001b[0m 57ms/step - auc: 0.9634 - binary_accuracy: 0.9263 - false_negatives: 177.2417 - false_positives: 365.7827 - loss: 0.2521 - precision: 0.9073 - recall: 0.9513 - true_negatives: 3236.9844 - true_positives: 3451.9912\n",
      "For batch number 452 the model has a loss of 0.2617121934890747\n",
      "\u001b[1m452/689\u001b[0m \u001b[32m━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━\u001b[0m \u001b[1m13s\u001b[0m 57ms/step - auc: 0.9634 - binary_accuracy: 0.9263 - false_negatives: 177.6659 - false_positives: 366.6438 - loss: 0.2521 - precision: 0.9073 - recall: 0.9513 - true_negatives: 3244.1526 - true_positives: 3459.5376\n",
      "For batch number 453 the model has a loss of 0.26141318678855896\n",
      "\u001b[1m453/689\u001b[0m \u001b[32m━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━\u001b[0m \u001b[1m13s\u001b[0m 57ms/step - auc: 0.9634 - binary_accuracy: 0.9263 - false_negatives: 178.0883 - false_positives: 367.5011 - loss: 0.2521 - precision: 0.9073 - recall: 0.9513 - true_negatives: 3251.3245 - true_positives: 3467.0862\n",
      "For batch number 454 the model has a loss of 0.26161566376686096\n",
      "\u001b[1m454/689\u001b[0m \u001b[32m━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━\u001b[0m \u001b[1m13s\u001b[0m 57ms/step - auc: 0.9634 - binary_accuracy: 0.9263 - false_negatives: 178.5154 - false_positives: 368.3546 - loss: 0.2522 - precision: 0.9073 - recall: 0.9513 - true_negatives: 3258.4956 - true_positives: 3474.6343\n",
      "For batch number 455 the model has a loss of 0.2614383101463318\n",
      "\u001b[1m455/689\u001b[0m \u001b[32m━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━\u001b[0m \u001b[1m13s\u001b[0m 57ms/step - auc: 0.9633 - binary_accuracy: 0.9262 - false_negatives: 178.9407 - false_positives: 369.2044 - loss: 0.2522 - precision: 0.9072 - recall: 0.9513 - true_negatives: 3265.6528 - true_positives: 3482.2021\n",
      "For batch number 456 the model has a loss of 0.2615360617637634\n",
      "\u001b[1m456/689\u001b[0m \u001b[32m━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━\u001b[0m \u001b[1m13s\u001b[0m 57ms/step - auc: 0.9633 - binary_accuracy: 0.9262 - false_negatives: 179.3640 - false_positives: 370.0570 - loss: 0.2522 - precision: 0.9072 - recall: 0.9513 - true_negatives: 3272.8071 - true_positives: 3489.7720\n",
      "For batch number 457 the model has a loss of 0.2617303729057312\n",
      "\u001b[1m457/689\u001b[0m \u001b[32m━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━\u001b[0m \u001b[1m13s\u001b[0m 57ms/step - auc: 0.9633 - binary_accuracy: 0.9262 - false_negatives: 179.7899 - false_positives: 370.9125 - loss: 0.2522 - precision: 0.9072 - recall: 0.9513 - true_negatives: 3279.9651 - true_positives: 3497.3325\n",
      "For batch number 458 the model has a loss of 0.26171067357063293\n",
      "\u001b[1m458/689\u001b[0m \u001b[32m━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━\u001b[0m \u001b[1m13s\u001b[0m 57ms/step - auc: 0.9633 - binary_accuracy: 0.9262 - false_negatives: 180.2183 - false_positives: 371.7664 - loss: 0.2522 - precision: 0.9072 - recall: 0.9513 - true_negatives: 3287.1157 - true_positives: 3504.8997\n",
      "For batch number 459 the model has a loss of 0.2616877257823944\n",
      "\u001b[1m459/689\u001b[0m \u001b[32m━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━\u001b[0m \u001b[1m13s\u001b[0m 57ms/step - auc: 0.9633 - binary_accuracy: 0.9262 - false_negatives: 180.6449 - false_positives: 372.6231 - loss: 0.2523 - precision: 0.9072 - recall: 0.9513 - true_negatives: 3294.2744 - true_positives: 3512.4575\n",
      "For batch number 460 the model has a loss of 0.26148614287376404\n",
      "\u001b[1m460/689\u001b[0m \u001b[32m━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━\u001b[0m \u001b[1m13s\u001b[0m 57ms/step - auc: 0.9633 - binary_accuracy: 0.9262 - false_negatives: 181.0696 - false_positives: 373.4804 - loss: 0.2523 - precision: 0.9072 - recall: 0.9513 - true_negatives: 3301.4392 - true_positives: 3520.0110\n",
      "For batch number 461 the model has a loss of 0.2613201141357422\n",
      "\u001b[1m461/689\u001b[0m \u001b[32m━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━\u001b[0m \u001b[1m13s\u001b[0m 57ms/step - auc: 0.9633 - binary_accuracy: 0.9262 - false_negatives: 181.4924 - false_positives: 374.3384 - loss: 0.2523 - precision: 0.9072 - recall: 0.9513 - true_negatives: 3308.5986 - true_positives: 3527.5706\n",
      "For batch number 462 the model has a loss of 0.26105067133903503\n",
      "\u001b[1m462/689\u001b[0m \u001b[32m━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━\u001b[0m \u001b[1m13s\u001b[0m 57ms/step - auc: 0.9633 - binary_accuracy: 0.9262 - false_negatives: 181.9134 - false_positives: 375.1970 - loss: 0.2523 - precision: 0.9072 - recall: 0.9513 - true_negatives: 3315.7642 - true_positives: 3535.1255\n",
      "For batch number 463 the model has a loss of 0.2610800862312317\n",
      "\u001b[1m463/689\u001b[0m \u001b[32m━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━\u001b[0m \u001b[1m12s\u001b[0m 57ms/step - auc: 0.9633 - binary_accuracy: 0.9262 - false_negatives: 182.3326 - false_positives: 376.0562 - loss: 0.2523 - precision: 0.9071 - recall: 0.9513 - true_negatives: 3322.9243 - true_positives: 3542.6868\n",
      "For batch number 464 the model has a loss of 0.26123329997062683\n",
      "\u001b[1m464/689\u001b[0m \u001b[32m━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━\u001b[0m \u001b[1m12s\u001b[0m 57ms/step - auc: 0.9633 - binary_accuracy: 0.9262 - false_negatives: 182.7500 - false_positives: 376.9203 - loss: 0.2524 - precision: 0.9071 - recall: 0.9513 - true_negatives: 3330.0884 - true_positives: 3550.2415\n",
      "For batch number 465 the model has a loss of 0.2614140510559082\n",
      "\u001b[1m465/689\u001b[0m \u001b[32m━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━\u001b[0m \u001b[1m12s\u001b[0m 57ms/step - auc: 0.9633 - binary_accuracy: 0.9262 - false_negatives: 183.1656 - false_positives: 377.7849 - loss: 0.2524 - precision: 0.9071 - recall: 0.9513 - true_negatives: 3337.2517 - true_positives: 3557.7979\n",
      "For batch number 466 the model has a loss of 0.261405885219574\n",
      "\u001b[1m466/689\u001b[0m \u001b[32m━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━\u001b[0m \u001b[1m12s\u001b[0m 57ms/step - auc: 0.9633 - binary_accuracy: 0.9262 - false_negatives: 183.5837 - false_positives: 378.6459 - loss: 0.2524 - precision: 0.9071 - recall: 0.9512 - true_negatives: 3344.4099 - true_positives: 3565.3606\n",
      "For batch number 467 the model has a loss of 0.26129889488220215\n",
      "\u001b[1m467/689\u001b[0m \u001b[32m━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━\u001b[0m \u001b[1m12s\u001b[0m 57ms/step - auc: 0.9633 - binary_accuracy: 0.9261 - false_negatives: 184.0000 - false_positives: 379.5096 - loss: 0.2524 - precision: 0.9071 - recall: 0.9512 - true_negatives: 3351.5696 - true_positives: 3572.9207\n",
      "For batch number 468 the model has a loss of 0.2609410285949707\n",
      "\u001b[1m468/689\u001b[0m \u001b[32m━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━\u001b[0m \u001b[1m12s\u001b[0m 57ms/step - auc: 0.9633 - binary_accuracy: 0.9261 - false_negatives: 184.4145 - false_positives: 380.3718 - loss: 0.2524 - precision: 0.9071 - recall: 0.9512 - true_negatives: 3358.7329 - true_positives: 3580.4807\n",
      "For batch number 469 the model has a loss of 0.26111337542533875\n",
      "\u001b[1m469/689\u001b[0m \u001b[32m━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━\u001b[0m \u001b[1m12s\u001b[0m 57ms/step - auc: 0.9632 - binary_accuracy: 0.9261 - false_negatives: 184.8294 - false_positives: 381.2324 - loss: 0.2525 - precision: 0.9071 - recall: 0.9512 - true_negatives: 3365.8999 - true_positives: 3588.0383\n",
      "For batch number 470 the model has a loss of 0.26095569133758545\n",
      "\u001b[1m470/689\u001b[0m \u001b[32m━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━\u001b[0m \u001b[1m12s\u001b[0m 57ms/step - auc: 0.9632 - binary_accuracy: 0.9261 - false_negatives: 185.2426 - false_positives: 382.0936 - loss: 0.2525 - precision: 0.9070 - recall: 0.9512 - true_negatives: 3373.0659 - true_positives: 3595.5979\n",
      "For batch number 471 the model has a loss of 0.26081159710884094\n",
      "\u001b[1m471/689\u001b[0m \u001b[32m━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━\u001b[0m \u001b[1m12s\u001b[0m 57ms/step - auc: 0.9632 - binary_accuracy: 0.9261 - false_negatives: 185.6539 - false_positives: 382.9533 - loss: 0.2525 - precision: 0.9070 - recall: 0.9512 - true_negatives: 3380.2314 - true_positives: 3603.1614\n",
      "For batch number 472 the model has a loss of 0.26071563363075256\n",
      "\u001b[1m472/689\u001b[0m \u001b[32m━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━\u001b[0m \u001b[1m12s\u001b[0m 57ms/step - auc: 0.9632 - binary_accuracy: 0.9261 - false_negatives: 186.0636 - false_positives: 383.8157 - loss: 0.2525 - precision: 0.9070 - recall: 0.9512 - true_negatives: 3387.3918 - true_positives: 3610.7288\n",
      "For batch number 473 the model has a loss of 0.2605297565460205\n",
      "\u001b[1m473/689\u001b[0m \u001b[32m━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━\u001b[0m \u001b[1m12s\u001b[0m 57ms/step - auc: 0.9632 - binary_accuracy: 0.9261 - false_negatives: 186.4715 - false_positives: 384.6808 - loss: 0.2525 - precision: 0.9070 - recall: 0.9512 - true_negatives: 3394.5518 - true_positives: 3618.2959\n",
      "For batch number 474 the model has a loss of 0.2604007124900818\n",
      "\u001b[1m474/689\u001b[0m \u001b[32m━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━\u001b[0m \u001b[1m12s\u001b[0m 57ms/step - auc: 0.9632 - binary_accuracy: 0.9261 - false_negatives: 186.8776 - false_positives: 385.5443 - loss: 0.2525 - precision: 0.9070 - recall: 0.9512 - true_negatives: 3401.7236 - true_positives: 3625.8545\n",
      "For batch number 475 the model has a loss of 0.26008668541908264\n",
      "\u001b[1m475/689\u001b[0m \u001b[32m━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━\u001b[0m \u001b[1m12s\u001b[0m 57ms/step - auc: 0.9632 - binary_accuracy: 0.9261 - false_negatives: 187.2821 - false_positives: 386.4042 - loss: 0.2526 - precision: 0.9070 - recall: 0.9512 - true_negatives: 3408.8948 - true_positives: 3633.4189\n",
      "For batch number 476 the model has a loss of 0.2597602307796478\n",
      "\u001b[1m476/689\u001b[0m \u001b[32m━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━\u001b[0m \u001b[1m12s\u001b[0m 57ms/step - auc: 0.9632 - binary_accuracy: 0.9261 - false_negatives: 187.6849 - false_positives: 387.2626 - loss: 0.2526 - precision: 0.9070 - recall: 0.9512 - true_negatives: 3416.0671 - true_positives: 3640.9854\n",
      "For batch number 477 the model has a loss of 0.2600955367088318\n",
      "\u001b[1m477/689\u001b[0m \u001b[32m━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━\u001b[0m \u001b[1m12s\u001b[0m 57ms/step - auc: 0.9632 - binary_accuracy: 0.9261 - false_negatives: 188.0901 - false_positives: 388.1195 - loss: 0.2526 - precision: 0.9070 - recall: 0.9512 - true_negatives: 3423.2390 - true_positives: 3648.5513\n",
      "For batch number 478 the model has a loss of 0.2601463794708252\n",
      "\u001b[1m478/689\u001b[0m \u001b[32m━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━\u001b[0m \u001b[1m12s\u001b[0m 57ms/step - auc: 0.9632 - binary_accuracy: 0.9261 - false_negatives: 188.4979 - false_positives: 388.9749 - loss: 0.2526 - precision: 0.9069 - recall: 0.9512 - true_negatives: 3430.4080 - true_positives: 3656.1191\n",
      "For batch number 479 the model has a loss of 0.2599555552005768\n",
      "\u001b[1m479/689\u001b[0m \u001b[32m━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━\u001b[0m \u001b[1m12s\u001b[0m 57ms/step - auc: 0.9632 - binary_accuracy: 0.9261 - false_negatives: 188.9040 - false_positives: 389.8288 - loss: 0.2526 - precision: 0.9069 - recall: 0.9512 - true_negatives: 3437.5930 - true_positives: 3663.6743\n",
      "For batch number 480 the model has a loss of 0.25974923372268677\n",
      "\u001b[1m480/689\u001b[0m \u001b[32m━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━\u001b[0m \u001b[1m11s\u001b[0m 57ms/step - auc: 0.9632 - binary_accuracy: 0.9261 - false_negatives: 189.3104 - false_positives: 390.6792 - loss: 0.2526 - precision: 0.9069 - recall: 0.9512 - true_negatives: 3444.7771 - true_positives: 3671.2334\n",
      "For batch number 481 the model has a loss of 0.2596721649169922\n",
      "\u001b[1m481/689\u001b[0m \u001b[32m━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━\u001b[0m \u001b[1m11s\u001b[0m 57ms/step - auc: 0.9632 - binary_accuracy: 0.9260 - false_negatives: 189.7152 - false_positives: 391.5302 - loss: 0.2527 - precision: 0.9069 - recall: 0.9512 - true_negatives: 3451.9563 - true_positives: 3678.7983\n",
      "For batch number 482 the model has a loss of 0.25939199328422546\n",
      "\u001b[1m482/689\u001b[0m \u001b[32m━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━\u001b[0m \u001b[1m11s\u001b[0m 57ms/step - auc: 0.9632 - binary_accuracy: 0.9260 - false_negatives: 190.1183 - false_positives: 392.3797 - loss: 0.2527 - precision: 0.9069 - recall: 0.9512 - true_negatives: 3459.1389 - true_positives: 3686.3630\n",
      "For batch number 483 the model has a loss of 0.2594083845615387\n",
      "\u001b[1m483/689\u001b[0m \u001b[32m━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━\u001b[0m \u001b[1m11s\u001b[0m 57ms/step - auc: 0.9632 - binary_accuracy: 0.9260 - false_negatives: 190.5197 - false_positives: 393.2298 - loss: 0.2527 - precision: 0.9069 - recall: 0.9512 - true_negatives: 3466.3167 - true_positives: 3693.9338\n",
      "For batch number 484 the model has a loss of 0.2592761218547821\n",
      "\u001b[1m484/689\u001b[0m \u001b[32m━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━\u001b[0m \u001b[1m11s\u001b[0m 57ms/step - auc: 0.9632 - binary_accuracy: 0.9260 - false_negatives: 190.9194 - false_positives: 394.0785 - loss: 0.2527 - precision: 0.9069 - recall: 0.9512 - true_negatives: 3473.5000 - true_positives: 3701.5020\n",
      "For batch number 485 the model has a loss of 0.2589826285839081\n",
      "\u001b[1m485/689\u001b[0m \u001b[32m━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━\u001b[0m \u001b[1m11s\u001b[0m 57ms/step - auc: 0.9631 - binary_accuracy: 0.9260 - false_negatives: 191.3175 - false_positives: 394.9237 - loss: 0.2527 - precision: 0.9069 - recall: 0.9512 - true_negatives: 3480.6887 - true_positives: 3709.0701\n",
      "For batch number 486 the model has a loss of 0.25933846831321716\n",
      "\u001b[1m486/689\u001b[0m \u001b[32m━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━\u001b[0m \u001b[1m11s\u001b[0m 57ms/step - auc: 0.9631 - binary_accuracy: 0.9260 - false_negatives: 191.7181 - false_positives: 395.7654 - loss: 0.2527 - precision: 0.9069 - recall: 0.9512 - true_negatives: 3487.8806 - true_positives: 3716.6357\n",
      "For batch number 487 the model has a loss of 0.2593226730823517\n",
      "\u001b[1m487/689\u001b[0m \u001b[32m━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━\u001b[0m \u001b[1m11s\u001b[0m 57ms/step - auc: 0.9631 - binary_accuracy: 0.9260 - false_negatives: 192.1212 - false_positives: 396.6057 - loss: 0.2527 - precision: 0.9068 - recall: 0.9512 - true_negatives: 3495.0740 - true_positives: 3724.1992\n",
      "For batch number 488 the model has a loss of 0.25958579778671265\n",
      "\u001b[1m488/689\u001b[0m \u001b[32m━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━\u001b[0m \u001b[1m11s\u001b[0m 57ms/step - auc: 0.9631 - binary_accuracy: 0.9260 - false_negatives: 192.5266 - false_positives: 397.4467 - loss: 0.2527 - precision: 0.9068 - recall: 0.9512 - true_negatives: 3502.2622 - true_positives: 3731.7644\n",
      "For batch number 489 the model has a loss of 0.25949856638908386\n",
      "\u001b[1m489/689\u001b[0m \u001b[32m━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━\u001b[0m \u001b[1m11s\u001b[0m 57ms/step - auc: 0.9631 - binary_accuracy: 0.9260 - false_negatives: 192.9346 - false_positives: 398.2842 - loss: 0.2528 - precision: 0.9068 - recall: 0.9512 - true_negatives: 3509.4438 - true_positives: 3739.3374\n",
      "For batch number 490 the model has a loss of 0.26019465923309326\n",
      "\u001b[1m490/689\u001b[0m \u001b[32m━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━\u001b[0m \u001b[1m11s\u001b[0m 57ms/step - auc: 0.9631 - binary_accuracy: 0.9260 - false_negatives: 193.3428 - false_positives: 399.1224 - loss: 0.2528 - precision: 0.9068 - recall: 0.9512 - true_negatives: 3516.6245 - true_positives: 3746.9102\n",
      "For batch number 491 the model has a loss of 0.2600914239883423\n",
      "\u001b[1m491/689\u001b[0m \u001b[32m━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━\u001b[0m \u001b[1m11s\u001b[0m 57ms/step - auc: 0.9631 - binary_accuracy: 0.9260 - false_negatives: 193.7495 - false_positives: 399.9633 - loss: 0.2528 - precision: 0.9068 - recall: 0.9512 - true_negatives: 3523.8003 - true_positives: 3754.4868\n",
      "For batch number 492 the model has a loss of 0.2604236900806427\n",
      "\u001b[1m492/689\u001b[0m \u001b[32m━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━\u001b[0m \u001b[1m11s\u001b[0m 57ms/step - auc: 0.9631 - binary_accuracy: 0.9260 - false_negatives: 194.1565 - false_positives: 400.8049 - loss: 0.2528 - precision: 0.9068 - recall: 0.9512 - true_negatives: 3530.9775 - true_positives: 3762.0610\n",
      "For batch number 493 the model has a loss of 0.26118001341819763\n",
      "\u001b[1m493/689\u001b[0m \u001b[32m━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━\u001b[0m \u001b[1m11s\u001b[0m 57ms/step - auc: 0.9631 - binary_accuracy: 0.9260 - false_negatives: 194.5659 - false_positives: 401.6450 - loss: 0.2528 - precision: 0.9068 - recall: 0.9512 - true_negatives: 3538.1541 - true_positives: 3769.6350\n",
      "For batch number 494 the model has a loss of 0.2620120644569397\n",
      "\u001b[1m494/689\u001b[0m \u001b[32m━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━\u001b[0m \u001b[1m11s\u001b[0m 57ms/step - auc: 0.9631 - binary_accuracy: 0.9260 - false_negatives: 194.9757 - false_positives: 402.4858 - loss: 0.2528 - precision: 0.9068 - recall: 0.9512 - true_negatives: 3545.3340 - true_positives: 3777.2043\n",
      "For batch number 495 the model has a loss of 0.2630867660045624\n",
      "\u001b[1m495/689\u001b[0m \u001b[32m━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━\u001b[0m \u001b[1m11s\u001b[0m 57ms/step - auc: 0.9631 - binary_accuracy: 0.9260 - false_negatives: 195.3879 - false_positives: 403.3313 - loss: 0.2529 - precision: 0.9068 - recall: 0.9512 - true_negatives: 3552.5151 - true_positives: 3784.7656\n",
      "For batch number 496 the model has a loss of 0.2630126476287842\n",
      "\u001b[1m496/689\u001b[0m \u001b[32m━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━\u001b[0m \u001b[1m11s\u001b[0m 57ms/step - auc: 0.9631 - binary_accuracy: 0.9260 - false_negatives: 195.7984 - false_positives: 404.1794 - loss: 0.2529 - precision: 0.9067 - recall: 0.9512 - true_negatives: 3559.6975 - true_positives: 3792.3247\n",
      "For batch number 497 the model has a loss of 0.2626582384109497\n",
      "\u001b[1m497/689\u001b[0m \u001b[32m━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━\u001b[0m \u001b[1m10s\u001b[0m 57ms/step - auc: 0.9631 - binary_accuracy: 0.9260 - false_negatives: 196.2093 - false_positives: 405.0241 - loss: 0.2529 - precision: 0.9067 - recall: 0.9512 - true_negatives: 3566.8853 - true_positives: 3799.8813\n",
      "For batch number 498 the model has a loss of 0.26344531774520874\n",
      "\u001b[1m498/689\u001b[0m \u001b[32m━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━\u001b[0m \u001b[1m10s\u001b[0m 57ms/step - auc: 0.9631 - binary_accuracy: 0.9260 - false_negatives: 196.6245 - false_positives: 405.8675 - loss: 0.2529 - precision: 0.9067 - recall: 0.9512 - true_negatives: 3574.0703 - true_positives: 3807.4377\n",
      "For batch number 499 the model has a loss of 0.26303309202194214\n",
      "\u001b[1m499/689\u001b[0m \u001b[32m━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━\u001b[0m \u001b[1m10s\u001b[0m 57ms/step - auc: 0.9631 - binary_accuracy: 0.9259 - false_negatives: 197.0381 - false_positives: 406.7074 - loss: 0.2529 - precision: 0.9067 - recall: 0.9512 - true_negatives: 3581.2566 - true_positives: 3814.9980\n",
      "For batch number 500 the model has a loss of 0.26278775930404663\n",
      "\u001b[1m500/689\u001b[0m \u001b[32m━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━\u001b[0m \u001b[1m10s\u001b[0m 57ms/step - auc: 0.9631 - binary_accuracy: 0.9259 - false_negatives: 197.4520 - false_positives: 407.5440 - loss: 0.2530 - precision: 0.9067 - recall: 0.9512 - true_negatives: 3588.4460 - true_positives: 3822.5581\n",
      "For batch number 501 the model has a loss of 0.2625950872898102\n",
      "\u001b[1m501/689\u001b[0m \u001b[32m━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━\u001b[0m \u001b[1m10s\u001b[0m 57ms/step - auc: 0.9631 - binary_accuracy: 0.9259 - false_negatives: 197.8663 - false_positives: 408.3773 - loss: 0.2530 - precision: 0.9067 - recall: 0.9512 - true_negatives: 3595.6328 - true_positives: 3830.1238\n",
      "For batch number 502 the model has a loss of 0.2624972462654114\n",
      "\u001b[1m502/689\u001b[0m \u001b[32m━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━\u001b[0m \u001b[1m10s\u001b[0m 57ms/step - auc: 0.9630 - binary_accuracy: 0.9259 - false_negatives: 198.2789 - false_positives: 409.2112 - loss: 0.2530 - precision: 0.9067 - recall: 0.9511 - true_negatives: 3602.8247 - true_positives: 3837.6853\n",
      "For batch number 503 the model has a loss of 0.26243504881858826\n",
      "\u001b[1m503/689\u001b[0m \u001b[32m━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━\u001b[0m \u001b[1m10s\u001b[0m 57ms/step - auc: 0.9630 - binary_accuracy: 0.9259 - false_negatives: 198.6938 - false_positives: 410.0457 - loss: 0.2530 - precision: 0.9067 - recall: 0.9511 - true_negatives: 3610.0139 - true_positives: 3845.2466\n",
      "For batch number 504 the model has a loss of 0.2621676027774811\n",
      "\u001b[1m504/689\u001b[0m \u001b[32m━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━\u001b[0m \u001b[1m10s\u001b[0m 57ms/step - auc: 0.9630 - binary_accuracy: 0.9259 - false_negatives: 199.1071 - false_positives: 410.8790 - loss: 0.2530 - precision: 0.9067 - recall: 0.9511 - true_negatives: 3617.1965 - true_positives: 3852.8174\n",
      "For batch number 505 the model has a loss of 0.26194456219673157\n",
      "\u001b[1m505/689\u001b[0m \u001b[32m━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━\u001b[0m \u001b[1m10s\u001b[0m 57ms/step - auc: 0.9630 - binary_accuracy: 0.9259 - false_negatives: 199.5208 - false_positives: 411.7109 - loss: 0.2531 - precision: 0.9067 - recall: 0.9511 - true_negatives: 3624.3762 - true_positives: 3860.3921\n",
      "For batch number 506 the model has a loss of 0.26186197996139526\n",
      "\u001b[1m506/689\u001b[0m \u001b[32m━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━\u001b[0m \u001b[1m10s\u001b[0m 57ms/step - auc: 0.9630 - binary_accuracy: 0.9259 - false_negatives: 199.9348 - false_positives: 412.5435 - loss: 0.2531 - precision: 0.9067 - recall: 0.9511 - true_negatives: 3631.5593 - true_positives: 3867.9624\n",
      "For batch number 507 the model has a loss of 0.2620709240436554\n",
      "\u001b[1m507/689\u001b[0m \u001b[32m━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━\u001b[0m \u001b[1m10s\u001b[0m 57ms/step - auc: 0.9630 - binary_accuracy: 0.9259 - false_negatives: 200.3531 - false_positives: 413.3748 - loss: 0.2531 - precision: 0.9066 - recall: 0.9511 - true_negatives: 3638.7336 - true_positives: 3875.5386\n",
      "For batch number 508 the model has a loss of 0.2619342505931854\n",
      "\u001b[1m508/689\u001b[0m \u001b[32m━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━\u001b[0m \u001b[1m10s\u001b[0m 57ms/step - auc: 0.9630 - binary_accuracy: 0.9259 - false_negatives: 200.7697 - false_positives: 414.2047 - loss: 0.2531 - precision: 0.9066 - recall: 0.9511 - true_negatives: 3645.9075 - true_positives: 3883.1182\n",
      "For batch number 509 the model has a loss of 0.2618444859981537\n",
      "\u001b[1m509/689\u001b[0m \u001b[32m━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━\u001b[0m \u001b[1m10s\u001b[0m 57ms/step - auc: 0.9630 - binary_accuracy: 0.9259 - false_negatives: 201.1847 - false_positives: 415.0373 - loss: 0.2531 - precision: 0.9066 - recall: 0.9511 - true_negatives: 3653.0845 - true_positives: 3890.6936\n",
      "For batch number 510 the model has a loss of 0.2617971897125244\n",
      "\u001b[1m510/689\u001b[0m \u001b[32m━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━\u001b[0m \u001b[1m10s\u001b[0m 57ms/step - auc: 0.9630 - binary_accuracy: 0.9259 - false_negatives: 201.6000 - false_positives: 415.8745 - loss: 0.2531 - precision: 0.9066 - recall: 0.9511 - true_negatives: 3660.2549 - true_positives: 3898.2705\n",
      "For batch number 511 the model has a loss of 0.26157325506210327\n",
      "\u001b[1m511/689\u001b[0m \u001b[32m━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━\u001b[0m \u001b[1m10s\u001b[0m 57ms/step - auc: 0.9630 - binary_accuracy: 0.9259 - false_negatives: 202.0137 - false_positives: 416.7104 - loss: 0.2532 - precision: 0.9066 - recall: 0.9511 - true_negatives: 3667.4226 - true_positives: 3905.8533\n",
      "For batch number 512 the model has a loss of 0.26142367720603943\n",
      "\u001b[1m512/689\u001b[0m \u001b[32m━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━\u001b[0m \u001b[1m10s\u001b[0m 57ms/step - auc: 0.9630 - binary_accuracy: 0.9259 - false_negatives: 202.4277 - false_positives: 417.5469 - loss: 0.2532 - precision: 0.9066 - recall: 0.9511 - true_negatives: 3674.5938 - true_positives: 3913.4316\n",
      "For batch number 513 the model has a loss of 0.2613659203052521\n",
      "\u001b[1m513/689\u001b[0m \u001b[32m━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━\u001b[0m \u001b[1m10s\u001b[0m 57ms/step - auc: 0.9630 - binary_accuracy: 0.9259 - false_negatives: 202.8401 - false_positives: 418.3879 - loss: 0.2532 - precision: 0.9066 - recall: 0.9511 - true_negatives: 3681.7603 - true_positives: 3921.0117\n",
      "For batch number 514 the model has a loss of 0.26114559173583984\n",
      "\u001b[1m514/689\u001b[0m \u001b[32m━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━\u001b[0m \u001b[1m10s\u001b[0m 57ms/step - auc: 0.9630 - binary_accuracy: 0.9259 - false_negatives: 203.2529 - false_positives: 419.2296 - loss: 0.2532 - precision: 0.9066 - recall: 0.9511 - true_negatives: 3688.9319 - true_positives: 3928.5857\n",
      "For batch number 515 the model has a loss of 0.2617745101451874\n",
      "\u001b[1m515/689\u001b[0m \u001b[32m━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━\u001b[0m \u001b[1m9s\u001b[0m 57ms/step - auc: 0.9630 - binary_accuracy: 0.9259 - false_negatives: 203.6641 - false_positives: 420.0796 - loss: 0.2532 - precision: 0.9066 - recall: 0.9511 - true_negatives: 3696.1106 - true_positives: 3936.1458 \n",
      "For batch number 516 the model has a loss of 0.2617208957672119\n",
      "\u001b[1m516/689\u001b[0m \u001b[32m━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━\u001b[0m \u001b[1m9s\u001b[0m 57ms/step - auc: 0.9630 - binary_accuracy: 0.9259 - false_negatives: 204.0736 - false_positives: 420.9302 - loss: 0.2532 - precision: 0.9066 - recall: 0.9511 - true_negatives: 3703.2983 - true_positives: 3943.6978\n",
      "For batch number 517 the model has a loss of 0.2615589201450348\n",
      "\u001b[1m517/689\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m9s\u001b[0m 57ms/step - auc: 0.9630 - binary_accuracy: 0.9258 - false_negatives: 204.4836 - false_positives: 421.7814 - loss: 0.2533 - precision: 0.9066 - recall: 0.9511 - true_negatives: 3710.4875 - true_positives: 3951.2476\n",
      "For batch number 518 the model has a loss of 0.26134729385375977\n",
      "\u001b[1m518/689\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m9s\u001b[0m 57ms/step - auc: 0.9630 - binary_accuracy: 0.9258 - false_negatives: 204.8919 - false_positives: 422.6332 - loss: 0.2533 - precision: 0.9065 - recall: 0.9511 - true_negatives: 3717.6758 - true_positives: 3958.7993\n",
      "For batch number 519 the model has a loss of 0.2615223228931427\n",
      "\u001b[1m519/689\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m9s\u001b[0m 57ms/step - auc: 0.9630 - binary_accuracy: 0.9258 - false_negatives: 205.2986 - false_positives: 423.4933 - loss: 0.2533 - precision: 0.9065 - recall: 0.9511 - true_negatives: 3724.8633 - true_positives: 3966.3450\n",
      "For batch number 520 the model has a loss of 0.2613717019557953\n",
      "\u001b[1m520/689\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m9s\u001b[0m 57ms/step - auc: 0.9629 - binary_accuracy: 0.9258 - false_negatives: 205.7077 - false_positives: 424.3500 - loss: 0.2533 - precision: 0.9065 - recall: 0.9511 - true_negatives: 3732.0500 - true_positives: 3973.8923\n",
      "For batch number 521 the model has a loss of 0.26108428835868835\n",
      "\u001b[1m521/689\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m9s\u001b[0m 57ms/step - auc: 0.9629 - binary_accuracy: 0.9258 - false_negatives: 206.1152 - false_positives: 425.2035 - loss: 0.2533 - precision: 0.9065 - recall: 0.9511 - true_negatives: 3739.2341 - true_positives: 3981.4473\n",
      "For batch number 522 the model has a loss of 0.2615933418273926\n",
      "\u001b[1m522/689\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m9s\u001b[0m 57ms/step - auc: 0.9629 - binary_accuracy: 0.9258 - false_negatives: 206.5211 - false_positives: 426.0594 - loss: 0.2533 - precision: 0.9065 - recall: 0.9511 - true_negatives: 3746.4177 - true_positives: 3989.0020\n",
      "For batch number 523 the model has a loss of 0.2618466317653656\n",
      "\u001b[1m523/689\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m9s\u001b[0m 57ms/step - auc: 0.9629 - binary_accuracy: 0.9258 - false_negatives: 206.9273 - false_positives: 426.9178 - loss: 0.2534 - precision: 0.9065 - recall: 0.9511 - true_negatives: 3753.6062 - true_positives: 3996.5488\n",
      "For batch number 524 the model has a loss of 0.2616221606731415\n",
      "\u001b[1m524/689\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m9s\u001b[0m 57ms/step - auc: 0.9629 - binary_accuracy: 0.9258 - false_negatives: 207.3321 - false_positives: 427.7748 - loss: 0.2534 - precision: 0.9065 - recall: 0.9511 - true_negatives: 3760.8054 - true_positives: 4004.0879\n",
      "For batch number 525 the model has a loss of 0.26134273409843445\n",
      "\u001b[1m525/689\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m9s\u001b[0m 57ms/step - auc: 0.9629 - binary_accuracy: 0.9258 - false_negatives: 207.7371 - false_positives: 428.6286 - loss: 0.2534 - precision: 0.9065 - recall: 0.9511 - true_negatives: 3768.0056 - true_positives: 4011.6287\n",
      "For batch number 526 the model has a loss of 0.26147669553756714\n",
      "\u001b[1m526/689\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m9s\u001b[0m 57ms/step - auc: 0.9629 - binary_accuracy: 0.9258 - false_negatives: 208.1407 - false_positives: 429.4848 - loss: 0.2534 - precision: 0.9065 - recall: 0.9511 - true_negatives: 3775.2014 - true_positives: 4019.1731\n",
      "For batch number 527 the model has a loss of 0.26157671213150024\n",
      "\u001b[1m527/689\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m9s\u001b[0m 57ms/step - auc: 0.9629 - binary_accuracy: 0.9258 - false_negatives: 208.5446 - false_positives: 430.3434 - loss: 0.2534 - precision: 0.9064 - recall: 0.9511 - true_negatives: 3782.3965 - true_positives: 4026.7153\n",
      "For batch number 528 the model has a loss of 0.26142558455467224\n",
      "\u001b[1m528/689\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m9s\u001b[0m 57ms/step - auc: 0.9629 - binary_accuracy: 0.9258 - false_negatives: 208.9470 - false_positives: 431.2026 - loss: 0.2534 - precision: 0.9064 - recall: 0.9511 - true_negatives: 3789.5891 - true_positives: 4034.2615\n",
      "For batch number 529 the model has a loss of 0.2614555060863495\n",
      "\u001b[1m529/689\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m9s\u001b[0m 57ms/step - auc: 0.9629 - binary_accuracy: 0.9258 - false_negatives: 209.3535 - false_positives: 432.0586 - loss: 0.2534 - precision: 0.9064 - recall: 0.9511 - true_negatives: 3796.7827 - true_positives: 4041.8052\n",
      "For batch number 530 the model has a loss of 0.26156744360923767\n",
      "\u001b[1m530/689\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m9s\u001b[0m 57ms/step - auc: 0.9629 - binary_accuracy: 0.9258 - false_negatives: 209.7585 - false_positives: 432.9170 - loss: 0.2535 - precision: 0.9064 - recall: 0.9511 - true_negatives: 3803.9812 - true_positives: 4049.3435\n",
      "For batch number 531 the model has a loss of 0.26126357913017273\n",
      "\u001b[1m531/689\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m9s\u001b[0m 57ms/step - auc: 0.9629 - binary_accuracy: 0.9258 - false_negatives: 210.1620 - false_positives: 433.7721 - loss: 0.2535 - precision: 0.9064 - recall: 0.9511 - true_negatives: 3811.1770 - true_positives: 4056.8889\n",
      "For batch number 532 the model has a loss of 0.2611708641052246\n",
      "\u001b[1m532/689\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m8s\u001b[0m 57ms/step - auc: 0.9629 - binary_accuracy: 0.9258 - false_negatives: 210.5658 - false_positives: 434.6297 - loss: 0.2535 - precision: 0.9064 - recall: 0.9511 - true_negatives: 3818.3721 - true_positives: 4064.4324\n",
      "For batch number 533 the model has a loss of 0.2610333561897278\n",
      "\u001b[1m533/689\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m8s\u001b[0m 57ms/step - auc: 0.9629 - binary_accuracy: 0.9258 - false_negatives: 210.9719 - false_positives: 435.4859 - loss: 0.2535 - precision: 0.9064 - recall: 0.9511 - true_negatives: 3825.5684 - true_positives: 4071.9736\n",
      "For batch number 534 the model has a loss of 0.2614733576774597\n",
      "\u001b[1m534/689\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m8s\u001b[0m 57ms/step - auc: 0.9629 - binary_accuracy: 0.9257 - false_negatives: 211.3801 - false_positives: 436.3427 - loss: 0.2535 - precision: 0.9064 - recall: 0.9511 - true_negatives: 3832.7585 - true_positives: 4079.5188\n",
      "For batch number 535 the model has a loss of 0.26139864325523376\n",
      "\u001b[1m535/689\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m8s\u001b[0m 57ms/step - auc: 0.9629 - binary_accuracy: 0.9257 - false_negatives: 211.7888 - false_positives: 437.2019 - loss: 0.2535 - precision: 0.9064 - recall: 0.9511 - true_negatives: 3839.9495 - true_positives: 4087.0598\n",
      "For batch number 536 the model has a loss of 0.2612341046333313\n",
      "\u001b[1m536/689\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m8s\u001b[0m 57ms/step - auc: 0.9629 - binary_accuracy: 0.9257 - false_negatives: 212.1959 - false_positives: 438.0616 - loss: 0.2535 - precision: 0.9063 - recall: 0.9511 - true_negatives: 3847.1455 - true_positives: 4094.5969\n",
      "For batch number 537 the model has a loss of 0.2621515095233917\n",
      "\u001b[1m537/689\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m8s\u001b[0m 57ms/step - auc: 0.9629 - binary_accuracy: 0.9257 - false_negatives: 212.6071 - false_positives: 438.9218 - loss: 0.2536 - precision: 0.9063 - recall: 0.9511 - true_negatives: 3854.3425 - true_positives: 4102.1284\n",
      "For batch number 538 the model has a loss of 0.2620666027069092\n",
      "\u001b[1m538/689\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m8s\u001b[0m 57ms/step - auc: 0.9629 - binary_accuracy: 0.9257 - false_negatives: 213.0167 - false_positives: 439.7788 - loss: 0.2536 - precision: 0.9063 - recall: 0.9511 - true_negatives: 3861.5334 - true_positives: 4109.6709\n",
      "For batch number 539 the model has a loss of 0.26182296872138977\n",
      "\u001b[1m539/689\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m8s\u001b[0m 57ms/step - auc: 0.9629 - binary_accuracy: 0.9257 - false_negatives: 213.4267 - false_positives: 440.6327 - loss: 0.2536 - precision: 0.9063 - recall: 0.9510 - true_negatives: 3868.7253 - true_positives: 4117.2153\n",
      "For batch number 540 the model has a loss of 0.2622746229171753\n",
      "\u001b[1m540/689\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m8s\u001b[0m 57ms/step - auc: 0.9628 - binary_accuracy: 0.9257 - false_negatives: 213.8426 - false_positives: 441.4833 - loss: 0.2536 - precision: 0.9063 - recall: 0.9510 - true_negatives: 3875.9128 - true_positives: 4124.7612\n",
      "For batch number 541 the model has a loss of 0.2623398005962372\n",
      "\u001b[1m541/689\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m8s\u001b[0m 57ms/step - auc: 0.9628 - binary_accuracy: 0.9257 - false_negatives: 214.2588 - false_positives: 442.3309 - loss: 0.2536 - precision: 0.9063 - recall: 0.9510 - true_negatives: 3883.1091 - true_positives: 4132.3013\n",
      "For batch number 542 the model has a loss of 0.2621358036994934\n",
      "\u001b[1m542/689\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m8s\u001b[0m 57ms/step - auc: 0.9628 - binary_accuracy: 0.9257 - false_negatives: 214.6734 - false_positives: 443.1771 - loss: 0.2536 - precision: 0.9063 - recall: 0.9510 - true_negatives: 3890.3081 - true_positives: 4139.8413\n",
      "For batch number 543 the model has a loss of 0.2626446485519409\n",
      "\u001b[1m543/689\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m8s\u001b[0m 57ms/step - auc: 0.9628 - binary_accuracy: 0.9257 - false_negatives: 215.0921 - false_positives: 444.0240 - loss: 0.2537 - precision: 0.9063 - recall: 0.9510 - true_negatives: 3897.5027 - true_positives: 4147.3813\n",
      "For batch number 544 the model has a loss of 0.2627880275249481\n",
      "\u001b[1m544/689\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m8s\u001b[0m 57ms/step - auc: 0.9628 - binary_accuracy: 0.9257 - false_negatives: 215.5129 - false_positives: 444.8713 - loss: 0.2537 - precision: 0.9063 - recall: 0.9510 - true_negatives: 3904.6948 - true_positives: 4154.9209\n",
      "For batch number 545 the model has a loss of 0.2631850838661194\n",
      "\u001b[1m545/689\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m8s\u001b[0m 57ms/step - auc: 0.9628 - binary_accuracy: 0.9257 - false_negatives: 215.9358 - false_positives: 445.7211 - loss: 0.2537 - precision: 0.9063 - recall: 0.9510 - true_negatives: 3911.8918 - true_positives: 4162.4512\n",
      "For batch number 546 the model has a loss of 0.26306867599487305\n",
      "\u001b[1m546/689\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m8s\u001b[0m 57ms/step - auc: 0.9628 - binary_accuracy: 0.9257 - false_negatives: 216.3571 - false_positives: 446.5733 - loss: 0.2537 - precision: 0.9062 - recall: 0.9510 - true_negatives: 3919.0825 - true_positives: 4169.9873\n",
      "For batch number 547 the model has a loss of 0.2633107006549835\n",
      "\u001b[1m547/689\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m8s\u001b[0m 57ms/step - auc: 0.9628 - binary_accuracy: 0.9257 - false_negatives: 216.7788 - false_positives: 447.4223 - loss: 0.2537 - precision: 0.9062 - recall: 0.9510 - true_negatives: 3926.2705 - true_positives: 4177.5283\n",
      "For batch number 548 the model has a loss of 0.26308342814445496\n",
      "\u001b[1m548/689\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m8s\u001b[0m 57ms/step - auc: 0.9628 - binary_accuracy: 0.9257 - false_negatives: 217.2007 - false_positives: 448.2682 - loss: 0.2537 - precision: 0.9062 - recall: 0.9510 - true_negatives: 3933.4600 - true_positives: 4185.0713\n",
      "For batch number 549 the model has a loss of 0.2631602883338928\n",
      "\u001b[1m549/689\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m8s\u001b[0m 57ms/step - auc: 0.9628 - binary_accuracy: 0.9257 - false_negatives: 217.6230 - false_positives: 449.1129 - loss: 0.2538 - precision: 0.9062 - recall: 0.9510 - true_negatives: 3940.6411 - true_positives: 4192.6230\n",
      "For batch number 550 the model has a loss of 0.26323169469833374\n",
      "\u001b[1m550/689\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m7s\u001b[0m 57ms/step - auc: 0.9628 - binary_accuracy: 0.9257 - false_negatives: 218.0455 - false_positives: 449.9582 - loss: 0.2538 - precision: 0.9062 - recall: 0.9510 - true_negatives: 3947.8237 - true_positives: 4200.1729\n",
      "For batch number 551 the model has a loss of 0.2632720172405243\n",
      "\u001b[1m551/689\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m7s\u001b[0m 57ms/step - auc: 0.9628 - binary_accuracy: 0.9256 - false_negatives: 218.4700 - false_positives: 450.8004 - loss: 0.2538 - precision: 0.9062 - recall: 0.9510 - true_negatives: 3955.0037 - true_positives: 4207.7261\n",
      "For batch number 552 the model has a loss of 0.263212651014328\n",
      "\u001b[1m552/689\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m7s\u001b[0m 57ms/step - auc: 0.9628 - binary_accuracy: 0.9256 - false_negatives: 218.8949 - false_positives: 451.6413 - loss: 0.2538 - precision: 0.9062 - recall: 0.9510 - true_negatives: 3962.1812 - true_positives: 4215.2827\n",
      "For batch number 553 the model has a loss of 0.26299232244491577\n",
      "\u001b[1m553/689\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m7s\u001b[0m 57ms/step - auc: 0.9628 - binary_accuracy: 0.9256 - false_negatives: 219.3183 - false_positives: 452.4810 - loss: 0.2538 - precision: 0.9062 - recall: 0.9510 - true_negatives: 3969.3599 - true_positives: 4222.8408\n",
      "For batch number 554 the model has a loss of 0.2632914185523987\n",
      "\u001b[1m554/689\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m7s\u001b[0m 57ms/step - auc: 0.9628 - binary_accuracy: 0.9256 - false_negatives: 219.7401 - false_positives: 453.3267 - loss: 0.2538 - precision: 0.9062 - recall: 0.9510 - true_negatives: 3976.5378 - true_positives: 4230.3955\n",
      "For batch number 555 the model has a loss of 0.26330631971359253\n",
      "\u001b[1m555/689\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m7s\u001b[0m 57ms/step - auc: 0.9628 - binary_accuracy: 0.9256 - false_negatives: 220.1604 - false_positives: 454.1730 - loss: 0.2539 - precision: 0.9062 - recall: 0.9510 - true_negatives: 3983.7100 - true_positives: 4237.9565\n",
      "For batch number 556 the model has a loss of 0.2632836401462555\n",
      "\u001b[1m556/689\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m7s\u001b[0m 57ms/step - auc: 0.9628 - binary_accuracy: 0.9256 - false_negatives: 220.5791 - false_positives: 455.0198 - loss: 0.2539 - precision: 0.9062 - recall: 0.9510 - true_negatives: 3990.8850 - true_positives: 4245.5161\n",
      "For batch number 557 the model has a loss of 0.26314955949783325\n",
      "\u001b[1m557/689\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m7s\u001b[0m 57ms/step - auc: 0.9628 - binary_accuracy: 0.9256 - false_negatives: 220.9982 - false_positives: 455.8636 - loss: 0.2539 - precision: 0.9062 - recall: 0.9510 - true_negatives: 3998.0647 - true_positives: 4253.0737\n",
      "For batch number 558 the model has a loss of 0.26305079460144043\n",
      "\u001b[1m558/689\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m7s\u001b[0m 57ms/step - auc: 0.9628 - binary_accuracy: 0.9256 - false_negatives: 221.4176 - false_positives: 456.7061 - loss: 0.2539 - precision: 0.9061 - recall: 0.9510 - true_negatives: 4005.2456 - true_positives: 4260.6309\n",
      "For batch number 559 the model has a loss of 0.2633310556411743\n",
      "\u001b[1m559/689\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m7s\u001b[0m 57ms/step - auc: 0.9627 - binary_accuracy: 0.9256 - false_negatives: 221.8390 - false_positives: 457.5510 - loss: 0.2539 - precision: 0.9061 - recall: 0.9510 - true_negatives: 4012.4275 - true_positives: 4268.1826\n",
      "For batch number 560 the model has a loss of 0.2632153630256653\n",
      "\u001b[1m560/689\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m7s\u001b[0m 57ms/step - auc: 0.9627 - binary_accuracy: 0.9256 - false_negatives: 222.2589 - false_positives: 458.3964 - loss: 0.2539 - precision: 0.9061 - recall: 0.9510 - true_negatives: 4019.6179 - true_positives: 4275.7266\n",
      "For batch number 561 the model has a loss of 0.2632591128349304\n",
      "\u001b[1m561/689\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m7s\u001b[0m 57ms/step - auc: 0.9627 - binary_accuracy: 0.9256 - false_negatives: 222.6774 - false_positives: 459.2442 - loss: 0.2540 - precision: 0.9061 - recall: 0.9510 - true_negatives: 4026.8040 - true_positives: 4283.2744\n",
      "For batch number 562 the model has a loss of 0.2638534903526306\n",
      "\u001b[1m562/689\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m7s\u001b[0m 57ms/step - auc: 0.9627 - binary_accuracy: 0.9256 - false_negatives: 223.0961 - false_positives: 460.0996 - loss: 0.2540 - precision: 0.9061 - recall: 0.9510 - true_negatives: 4033.9893 - true_positives: 4290.8149\n",
      "For batch number 563 the model has a loss of 0.26427197456359863\n",
      "\u001b[1m563/689\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m7s\u001b[0m 57ms/step - auc: 0.9627 - binary_accuracy: 0.9256 - false_negatives: 223.5133 - false_positives: 460.9662 - loss: 0.2540 - precision: 0.9061 - recall: 0.9510 - true_negatives: 4041.1741 - true_positives: 4298.3462\n",
      "For batch number 564 the model has a loss of 0.26447582244873047\n",
      "\u001b[1m564/689\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m7s\u001b[0m 57ms/step - auc: 0.9627 - binary_accuracy: 0.9256 - false_negatives: 223.9308 - false_positives: 461.8333 - loss: 0.2540 - precision: 0.9061 - recall: 0.9510 - true_negatives: 4048.3545 - true_positives: 4305.8813\n",
      "For batch number 565 the model has a loss of 0.26449090242385864\n",
      "\u001b[1m565/689\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m7s\u001b[0m 57ms/step - auc: 0.9627 - binary_accuracy: 0.9256 - false_negatives: 224.3469 - false_positives: 462.6991 - loss: 0.2540 - precision: 0.9061 - recall: 0.9510 - true_negatives: 4055.5398 - true_positives: 4313.4141\n",
      "For batch number 566 the model has a loss of 0.26470255851745605\n",
      "\u001b[1m566/689\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m7s\u001b[0m 57ms/step - auc: 0.9627 - binary_accuracy: 0.9256 - false_negatives: 224.7650 - false_positives: 463.5636 - loss: 0.2541 - precision: 0.9061 - recall: 0.9510 - true_negatives: 4062.7297 - true_positives: 4320.9419\n",
      "For batch number 567 the model has a loss of 0.2650204002857208\n",
      "\u001b[1m567/689\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m6s\u001b[0m 57ms/step - auc: 0.9627 - binary_accuracy: 0.9256 - false_negatives: 225.1852 - false_positives: 464.4250 - loss: 0.2541 - precision: 0.9061 - recall: 0.9509 - true_negatives: 4069.9136 - true_positives: 4328.4761\n",
      "For batch number 568 the model has a loss of 0.26524338126182556\n",
      "\u001b[1m568/689\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m6s\u001b[0m 57ms/step - auc: 0.9627 - binary_accuracy: 0.9256 - false_negatives: 225.6074 - false_positives: 465.2852 - loss: 0.2541 - precision: 0.9060 - recall: 0.9509 - true_negatives: 4077.0969 - true_positives: 4336.0107\n",
      "For batch number 569 the model has a loss of 0.26508718729019165\n",
      "\u001b[1m569/689\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m6s\u001b[0m 57ms/step - auc: 0.9627 - binary_accuracy: 0.9255 - false_negatives: 226.0281 - false_positives: 466.1476 - loss: 0.2541 - precision: 0.9060 - recall: 0.9509 - true_negatives: 4084.2795 - true_positives: 4343.5449\n",
      "For batch number 570 the model has a loss of 0.2649511396884918\n",
      "\u001b[1m570/689\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m6s\u001b[0m 57ms/step - auc: 0.9627 - binary_accuracy: 0.9255 - false_negatives: 226.4474 - false_positives: 467.0088 - loss: 0.2541 - precision: 0.9060 - recall: 0.9509 - true_negatives: 4091.4648 - true_positives: 4351.0791\n",
      "For batch number 571 the model has a loss of 0.26480188965797424\n",
      "\u001b[1m571/689\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m6s\u001b[0m 57ms/step - auc: 0.9627 - binary_accuracy: 0.9255 - false_negatives: 226.8669 - false_positives: 467.8669 - loss: 0.2542 - precision: 0.9060 - recall: 0.9509 - true_negatives: 4098.6460 - true_positives: 4358.6201\n",
      "For batch number 572 the model has a loss of 0.26462799310684204\n",
      "\u001b[1m572/689\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m6s\u001b[0m 57ms/step - auc: 0.9627 - binary_accuracy: 0.9255 - false_negatives: 227.2850 - false_positives: 468.7238 - loss: 0.2542 - precision: 0.9060 - recall: 0.9509 - true_negatives: 4105.8271 - true_positives: 4366.1646\n",
      "For batch number 573 the model has a loss of 0.2643662393093109\n",
      "\u001b[1m573/689\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m6s\u001b[0m 57ms/step - auc: 0.9627 - binary_accuracy: 0.9255 - false_negatives: 227.7033 - false_positives: 469.5777 - loss: 0.2542 - precision: 0.9060 - recall: 0.9509 - true_negatives: 4113.0103 - true_positives: 4373.7085\n",
      "For batch number 574 the model has a loss of 0.2643086910247803\n",
      "\u001b[1m574/689\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m6s\u001b[0m 57ms/step - auc: 0.9627 - binary_accuracy: 0.9255 - false_negatives: 228.1219 - false_positives: 470.4303 - loss: 0.2542 - precision: 0.9060 - recall: 0.9509 - true_negatives: 4120.1968 - true_positives: 4381.2510\n",
      "For batch number 575 the model has a loss of 0.26418477296829224\n",
      "\u001b[1m575/689\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m6s\u001b[0m 57ms/step - auc: 0.9627 - binary_accuracy: 0.9255 - false_negatives: 228.5391 - false_positives: 471.2835 - loss: 0.2542 - precision: 0.9060 - recall: 0.9509 - true_negatives: 4127.3789 - true_positives: 4388.7983\n",
      "For batch number 576 the model has a loss of 0.2642454504966736\n",
      "\u001b[1m576/689\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m6s\u001b[0m 57ms/step - auc: 0.9626 - binary_accuracy: 0.9255 - false_negatives: 228.9549 - false_positives: 472.1389 - loss: 0.2542 - precision: 0.9060 - recall: 0.9509 - true_negatives: 4134.5693 - true_positives: 4396.3369\n",
      "For batch number 577 the model has a loss of 0.2642335295677185\n",
      "\u001b[1m577/689\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m6s\u001b[0m 57ms/step - auc: 0.9626 - binary_accuracy: 0.9255 - false_negatives: 229.3726 - false_positives: 472.9931 - loss: 0.2543 - precision: 0.9060 - recall: 0.9509 - true_negatives: 4141.7573 - true_positives: 4403.8770\n",
      "For batch number 578 the model has a loss of 0.26433220505714417\n",
      "\u001b[1m578/689\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m6s\u001b[0m 57ms/step - auc: 0.9626 - binary_accuracy: 0.9255 - false_negatives: 229.7889 - false_positives: 473.8512 - loss: 0.2543 - precision: 0.9059 - recall: 0.9509 - true_negatives: 4148.9463 - true_positives: 4411.4136\n",
      "For batch number 579 the model has a loss of 0.26443061232566833\n",
      "\u001b[1m579/689\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m6s\u001b[0m 57ms/step - auc: 0.9626 - binary_accuracy: 0.9255 - false_negatives: 230.2055 - false_positives: 474.7081 - loss: 0.2543 - precision: 0.9059 - recall: 0.9509 - true_negatives: 4156.1382 - true_positives: 4418.9482\n",
      "For batch number 580 the model has a loss of 0.26478317379951477\n",
      "\u001b[1m580/689\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m6s\u001b[0m 57ms/step - auc: 0.9626 - binary_accuracy: 0.9255 - false_negatives: 230.6224 - false_positives: 475.5655 - loss: 0.2543 - precision: 0.9059 - recall: 0.9509 - true_negatives: 4163.3208 - true_positives: 4426.4912\n",
      "For batch number 581 the model has a loss of 0.2650732696056366\n",
      "\u001b[1m581/689\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m6s\u001b[0m 57ms/step - auc: 0.9626 - binary_accuracy: 0.9255 - false_negatives: 231.0413 - false_positives: 476.4200 - loss: 0.2543 - precision: 0.9059 - recall: 0.9509 - true_negatives: 4170.5044 - true_positives: 4434.0342\n",
      "For batch number 582 the model has a loss of 0.2652846574783325\n",
      "\u001b[1m582/689\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m6s\u001b[0m 57ms/step - auc: 0.9626 - binary_accuracy: 0.9255 - false_negatives: 231.4605 - false_positives: 477.2715 - loss: 0.2543 - precision: 0.9059 - recall: 0.9509 - true_negatives: 4177.6768 - true_positives: 4441.5913\n",
      "For batch number 583 the model has a loss of 0.26521095633506775\n",
      "\u001b[1m583/689\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m6s\u001b[0m 57ms/step - auc: 0.9626 - binary_accuracy: 0.9255 - false_negatives: 231.8817 - false_positives: 478.1201 - loss: 0.2544 - precision: 0.9059 - recall: 0.9509 - true_negatives: 4184.8540 - true_positives: 4449.1440\n",
      "For batch number 584 the model has a loss of 0.26495829224586487\n",
      "\u001b[1m584/689\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m6s\u001b[0m 57ms/step - auc: 0.9626 - binary_accuracy: 0.9255 - false_negatives: 232.3014 - false_positives: 478.9658 - loss: 0.2544 - precision: 0.9059 - recall: 0.9509 - true_negatives: 4192.0361 - true_positives: 4456.6968\n",
      "For batch number 585 the model has a loss of 0.26478469371795654\n",
      "\u001b[1m585/689\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m5s\u001b[0m 57ms/step - auc: 0.9626 - binary_accuracy: 0.9255 - false_negatives: 232.7214 - false_positives: 479.8102 - loss: 0.2544 - precision: 0.9059 - recall: 0.9509 - true_negatives: 4199.2139 - true_positives: 4464.2549\n",
      "For batch number 586 the model has a loss of 0.26467329263687134\n",
      "\u001b[1m586/689\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m5s\u001b[0m 57ms/step - auc: 0.9626 - binary_accuracy: 0.9254 - false_negatives: 233.1399 - false_positives: 480.6587 - loss: 0.2544 - precision: 0.9059 - recall: 0.9509 - true_negatives: 4206.3906 - true_positives: 4471.8105\n",
      "For batch number 587 the model has a loss of 0.2646469175815582\n",
      "\u001b[1m587/689\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m5s\u001b[0m 57ms/step - auc: 0.9626 - binary_accuracy: 0.9254 - false_negatives: 233.5588 - false_positives: 481.5077 - loss: 0.2544 - precision: 0.9059 - recall: 0.9509 - true_negatives: 4213.5640 - true_positives: 4479.3696\n",
      "For batch number 588 the model has a loss of 0.26465731859207153\n",
      "\u001b[1m588/689\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m5s\u001b[0m 57ms/step - auc: 0.9626 - binary_accuracy: 0.9254 - false_negatives: 233.9779 - false_positives: 482.3571 - loss: 0.2545 - precision: 0.9059 - recall: 0.9509 - true_negatives: 4220.7329 - true_positives: 4486.9321\n",
      "For batch number 589 the model has a loss of 0.26510462164878845\n",
      "\u001b[1m589/689\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m5s\u001b[0m 57ms/step - auc: 0.9626 - binary_accuracy: 0.9254 - false_negatives: 234.3990 - false_positives: 483.2088 - loss: 0.2545 - precision: 0.9059 - recall: 0.9509 - true_negatives: 4227.9014 - true_positives: 4494.4907\n",
      "For batch number 590 the model has a loss of 0.26493075489997864\n",
      "\u001b[1m590/689\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m5s\u001b[0m 57ms/step - auc: 0.9626 - binary_accuracy: 0.9254 - false_negatives: 234.8203 - false_positives: 484.0576 - loss: 0.2545 - precision: 0.9058 - recall: 0.9509 - true_negatives: 4235.0679 - true_positives: 4502.0542\n",
      "For batch number 591 the model has a loss of 0.2649141252040863\n",
      "\u001b[1m591/689\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m5s\u001b[0m 57ms/step - auc: 0.9626 - binary_accuracy: 0.9254 - false_negatives: 235.2420 - false_positives: 484.9052 - loss: 0.2545 - precision: 0.9058 - recall: 0.9509 - true_negatives: 4242.2388 - true_positives: 4509.6143\n",
      "For batch number 592 the model has a loss of 0.2647508978843689\n",
      "\u001b[1m592/689\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m5s\u001b[0m 57ms/step - auc: 0.9626 - binary_accuracy: 0.9254 - false_negatives: 235.6638 - false_positives: 485.7517 - loss: 0.2545 - precision: 0.9058 - recall: 0.9509 - true_negatives: 4249.4072 - true_positives: 4517.1772\n",
      "For batch number 593 the model has a loss of 0.2644955813884735\n",
      "\u001b[1m593/689\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m5s\u001b[0m 57ms/step - auc: 0.9626 - binary_accuracy: 0.9254 - false_negatives: 236.0843 - false_positives: 486.5953 - loss: 0.2545 - precision: 0.9058 - recall: 0.9509 - true_negatives: 4256.5786 - true_positives: 4524.7422\n",
      "For batch number 594 the model has a loss of 0.26436054706573486\n",
      "\u001b[1m594/689\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m5s\u001b[0m 57ms/step - auc: 0.9626 - binary_accuracy: 0.9254 - false_negatives: 236.5034 - false_positives: 487.4377 - loss: 0.2546 - precision: 0.9058 - recall: 0.9509 - true_negatives: 4263.7490 - true_positives: 4532.3096\n",
      "For batch number 595 the model has a loss of 0.2644408643245697\n",
      "\u001b[1m595/689\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m5s\u001b[0m 57ms/step - auc: 0.9625 - binary_accuracy: 0.9254 - false_negatives: 236.9210 - false_positives: 488.2823 - loss: 0.2546 - precision: 0.9058 - recall: 0.9509 - true_negatives: 4270.9209 - true_positives: 4539.8755\n",
      "For batch number 596 the model has a loss of 0.26440897583961487\n",
      "\u001b[1m596/689\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m5s\u001b[0m 57ms/step - auc: 0.9625 - binary_accuracy: 0.9254 - false_negatives: 237.3372 - false_positives: 489.1325 - loss: 0.2546 - precision: 0.9058 - recall: 0.9508 - true_negatives: 4278.0938 - true_positives: 4547.4360\n",
      "For batch number 597 the model has a loss of 0.26443833112716675\n",
      "\u001b[1m597/689\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m5s\u001b[0m 57ms/step - auc: 0.9625 - binary_accuracy: 0.9254 - false_negatives: 237.7554 - false_positives: 489.9799 - loss: 0.2546 - precision: 0.9058 - recall: 0.9508 - true_negatives: 4285.2681 - true_positives: 4554.9966\n",
      "For batch number 598 the model has a loss of 0.2646488845348358\n",
      "\u001b[1m598/689\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m5s\u001b[0m 57ms/step - auc: 0.9625 - binary_accuracy: 0.9254 - false_negatives: 238.1722 - false_positives: 490.8328 - loss: 0.2546 - precision: 0.9058 - recall: 0.9508 - true_negatives: 4292.4399 - true_positives: 4562.5552\n",
      "For batch number 599 the model has a loss of 0.2645145058631897\n",
      "\u001b[1m599/689\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m5s\u001b[0m 57ms/step - auc: 0.9625 - binary_accuracy: 0.9254 - false_negatives: 238.5910 - false_positives: 491.6845 - loss: 0.2546 - precision: 0.9058 - recall: 0.9508 - true_negatives: 4299.6128 - true_positives: 4570.1118\n",
      "For batch number 600 the model has a loss of 0.26455047726631165\n",
      "\u001b[1m600/689\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m5s\u001b[0m 57ms/step - auc: 0.9625 - binary_accuracy: 0.9254 - false_negatives: 239.0083 - false_positives: 492.5417 - loss: 0.2547 - precision: 0.9058 - recall: 0.9508 - true_negatives: 4306.7866 - true_positives: 4577.6636\n",
      "For batch number 601 the model has a loss of 0.2649599015712738\n",
      "\u001b[1m601/689\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m5s\u001b[0m 57ms/step - auc: 0.9625 - binary_accuracy: 0.9254 - false_negatives: 239.4243 - false_positives: 493.4060 - loss: 0.2547 - precision: 0.9058 - recall: 0.9508 - true_negatives: 4313.9648 - true_positives: 4585.2046\n",
      "For batch number 602 the model has a loss of 0.2649298906326294\n",
      "\u001b[1m602/689\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m4s\u001b[0m 57ms/step - auc: 0.9625 - binary_accuracy: 0.9254 - false_negatives: 239.8389 - false_positives: 494.2708 - loss: 0.2547 - precision: 0.9057 - recall: 0.9508 - true_negatives: 4321.1362 - true_positives: 4592.7544\n",
      "For batch number 603 the model has a loss of 0.2646750211715698\n",
      "\u001b[1m603/689\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m4s\u001b[0m 57ms/step - auc: 0.9625 - binary_accuracy: 0.9254 - false_negatives: 240.2521 - false_positives: 495.1327 - loss: 0.2547 - precision: 0.9057 - recall: 0.9508 - true_negatives: 4328.3037 - true_positives: 4600.3120\n",
      "For batch number 604 the model has a loss of 0.2645230293273926\n",
      "\u001b[1m604/689\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m4s\u001b[0m 57ms/step - auc: 0.9625 - binary_accuracy: 0.9254 - false_negatives: 240.6639 - false_positives: 495.9917 - loss: 0.2547 - precision: 0.9057 - recall: 0.9508 - true_negatives: 4335.4668 - true_positives: 4607.8774\n",
      "For batch number 605 the model has a loss of 0.26435548067092896\n",
      "\u001b[1m605/689\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m4s\u001b[0m 57ms/step - auc: 0.9625 - binary_accuracy: 0.9254 - false_negatives: 241.0760 - false_positives: 496.8496 - loss: 0.2547 - precision: 0.9057 - recall: 0.9508 - true_negatives: 4342.6299 - true_positives: 4615.4448\n",
      "For batch number 606 the model has a loss of 0.2642035186290741\n",
      "\u001b[1m606/689\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m4s\u001b[0m 57ms/step - auc: 0.9625 - binary_accuracy: 0.9253 - false_negatives: 241.4868 - false_positives: 497.7079 - loss: 0.2548 - precision: 0.9057 - recall: 0.9508 - true_negatives: 4349.7886 - true_positives: 4623.0166\n",
      "For batch number 607 the model has a loss of 0.2646096646785736\n",
      "\u001b[1m607/689\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m4s\u001b[0m 57ms/step - auc: 0.9625 - binary_accuracy: 0.9253 - false_negatives: 241.8995 - false_positives: 498.5651 - loss: 0.2548 - precision: 0.9057 - recall: 0.9508 - true_negatives: 4356.9409 - true_positives: 4630.5947\n",
      "For batch number 608 the model has a loss of 0.26467621326446533\n",
      "\u001b[1m608/689\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m4s\u001b[0m 57ms/step - auc: 0.9625 - binary_accuracy: 0.9253 - false_negatives: 242.3125 - false_positives: 499.4243 - loss: 0.2548 - precision: 0.9057 - recall: 0.9508 - true_negatives: 4364.0952 - true_positives: 4638.1680\n",
      "For batch number 609 the model has a loss of 0.26478666067123413\n",
      "\u001b[1m609/689\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m4s\u001b[0m 57ms/step - auc: 0.9625 - binary_accuracy: 0.9253 - false_negatives: 242.7258 - false_positives: 500.2841 - loss: 0.2548 - precision: 0.9057 - recall: 0.9508 - true_negatives: 4371.2432 - true_positives: 4645.7471\n",
      "For batch number 610 the model has a loss of 0.26455429196357727\n",
      "\u001b[1m610/689\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m4s\u001b[0m 57ms/step - auc: 0.9625 - binary_accuracy: 0.9253 - false_negatives: 243.1377 - false_positives: 501.1426 - loss: 0.2548 - precision: 0.9057 - recall: 0.9508 - true_negatives: 4378.3950 - true_positives: 4653.3247\n",
      "For batch number 611 the model has a loss of 0.2647151052951813\n",
      "\u001b[1m611/689\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m4s\u001b[0m 57ms/step - auc: 0.9625 - binary_accuracy: 0.9253 - false_negatives: 243.5499 - false_positives: 502.0016 - loss: 0.2548 - precision: 0.9057 - recall: 0.9508 - true_negatives: 4385.5483 - true_positives: 4660.9004\n",
      "For batch number 612 the model has a loss of 0.2646186351776123\n",
      "\u001b[1m612/689\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m4s\u001b[0m 57ms/step - auc: 0.9625 - binary_accuracy: 0.9253 - false_negatives: 243.9608 - false_positives: 502.8611 - loss: 0.2549 - precision: 0.9057 - recall: 0.9508 - true_negatives: 4392.7061 - true_positives: 4668.4722\n",
      "For batch number 613 the model has a loss of 0.2651157081127167\n",
      "\u001b[1m613/689\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m4s\u001b[0m 57ms/step - auc: 0.9625 - binary_accuracy: 0.9253 - false_negatives: 244.3703 - false_positives: 503.7227 - loss: 0.2549 - precision: 0.9057 - recall: 0.9508 - true_negatives: 4399.8564 - true_positives: 4676.0508\n",
      "For batch number 614 the model has a loss of 0.2659788429737091\n",
      "\u001b[1m614/689\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m4s\u001b[0m 57ms/step - auc: 0.9624 - binary_accuracy: 0.9253 - false_negatives: 244.7818 - false_positives: 504.5896 - loss: 0.2549 - precision: 0.9056 - recall: 0.9508 - true_negatives: 4407.0034 - true_positives: 4683.6255\n",
      "For batch number 615 the model has a loss of 0.26590684056282043\n",
      "\u001b[1m615/689\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m4s\u001b[0m 57ms/step - auc: 0.9624 - binary_accuracy: 0.9253 - false_negatives: 245.1935 - false_positives: 505.4553 - loss: 0.2549 - precision: 0.9056 - recall: 0.9508 - true_negatives: 4414.1479 - true_positives: 4691.2031\n",
      "For batch number 616 the model has a loss of 0.26577290892601013\n",
      "\u001b[1m616/689\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m4s\u001b[0m 57ms/step - auc: 0.9624 - binary_accuracy: 0.9253 - false_negatives: 245.6039 - false_positives: 506.3198 - loss: 0.2549 - precision: 0.9056 - recall: 0.9508 - true_negatives: 4421.2920 - true_positives: 4698.7842\n",
      "For batch number 617 the model has a loss of 0.26629507541656494\n",
      "\u001b[1m617/689\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m4s\u001b[0m 57ms/step - auc: 0.9624 - binary_accuracy: 0.9253 - false_negatives: 246.0146 - false_positives: 507.1848 - loss: 0.2549 - precision: 0.9056 - recall: 0.9508 - true_negatives: 4428.4326 - true_positives: 4706.3677\n",
      "For batch number 618 the model has a loss of 0.2672187387943268\n",
      "\u001b[1m618/689\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m4s\u001b[0m 57ms/step - auc: 0.9624 - binary_accuracy: 0.9253 - false_negatives: 246.4515 - false_positives: 508.0502 - loss: 0.2550 - precision: 0.9056 - recall: 0.9508 - true_negatives: 4435.5649 - true_positives: 4713.9336\n",
      "For batch number 619 the model has a loss of 0.2675093114376068\n",
      "\u001b[1m619/689\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m4s\u001b[0m 57ms/step - auc: 0.9624 - binary_accuracy: 0.9253 - false_negatives: 246.8885 - false_positives: 508.9192 - loss: 0.2550 - precision: 0.9056 - recall: 0.9508 - true_negatives: 4442.6978 - true_positives: 4721.4941\n",
      "For batch number 620 the model has a loss of 0.2674144208431244\n",
      "\u001b[1m620/689\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m3s\u001b[0m 57ms/step - auc: 0.9624 - binary_accuracy: 0.9253 - false_negatives: 247.3242 - false_positives: 509.7903 - loss: 0.2550 - precision: 0.9056 - recall: 0.9508 - true_negatives: 4449.8242 - true_positives: 4729.0615\n",
      "For batch number 621 the model has a loss of 0.267522931098938\n",
      "\u001b[1m621/689\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m3s\u001b[0m 57ms/step - auc: 0.9624 - binary_accuracy: 0.9253 - false_negatives: 247.7585 - false_positives: 510.6635 - loss: 0.2550 - precision: 0.9056 - recall: 0.9508 - true_negatives: 4456.9468 - true_positives: 4736.6313\n",
      "For batch number 622 the model has a loss of 0.2673663794994354\n",
      "\u001b[1m622/689\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m3s\u001b[0m 57ms/step - auc: 0.9624 - binary_accuracy: 0.9253 - false_negatives: 248.1913 - false_positives: 511.5370 - loss: 0.2550 - precision: 0.9056 - recall: 0.9508 - true_negatives: 4464.0708 - true_positives: 4744.2012\n",
      "For batch number 623 the model has a loss of 0.26726630330085754\n",
      "\u001b[1m623/689\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m3s\u001b[0m 57ms/step - auc: 0.9624 - binary_accuracy: 0.9252 - false_negatives: 248.6228 - false_positives: 512.4093 - loss: 0.2551 - precision: 0.9056 - recall: 0.9508 - true_negatives: 4471.1958 - true_positives: 4751.7720\n",
      "For batch number 624 the model has a loss of 0.26755455136299133\n",
      "\u001b[1m624/689\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m3s\u001b[0m 57ms/step - auc: 0.9624 - binary_accuracy: 0.9252 - false_negatives: 249.0545 - false_positives: 513.2805 - loss: 0.2551 - precision: 0.9055 - recall: 0.9508 - true_negatives: 4478.3174 - true_positives: 4759.3477\n",
      "For batch number 625 the model has a loss of 0.2678997814655304\n",
      "\u001b[1m625/689\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m3s\u001b[0m 57ms/step - auc: 0.9624 - binary_accuracy: 0.9252 - false_negatives: 249.4864 - false_positives: 514.1536 - loss: 0.2551 - precision: 0.9055 - recall: 0.9508 - true_negatives: 4485.4302 - true_positives: 4766.9297\n",
      "For batch number 626 the model has a loss of 0.2684689462184906\n",
      "\u001b[1m626/689\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m3s\u001b[0m 57ms/step - auc: 0.9624 - binary_accuracy: 0.9252 - false_negatives: 249.9185 - false_positives: 515.0319 - loss: 0.2551 - precision: 0.9055 - recall: 0.9508 - true_negatives: 4492.5479 - true_positives: 4774.5015\n",
      "For batch number 627 the model has a loss of 0.2682955861091614\n",
      "\u001b[1m627/689\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m3s\u001b[0m 57ms/step - auc: 0.9624 - binary_accuracy: 0.9252 - false_negatives: 250.3493 - false_positives: 515.9107 - loss: 0.2551 - precision: 0.9055 - recall: 0.9508 - true_negatives: 4499.6572 - true_positives: 4782.0830\n",
      "For batch number 628 the model has a loss of 0.2685733139514923\n",
      "\u001b[1m628/689\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m3s\u001b[0m 57ms/step - auc: 0.9624 - binary_accuracy: 0.9252 - false_negatives: 250.7834 - false_positives: 516.7898 - loss: 0.2552 - precision: 0.9055 - recall: 0.9507 - true_negatives: 4506.7612 - true_positives: 4789.6655\n",
      "For batch number 629 the model has a loss of 0.2685488164424896\n",
      "\u001b[1m629/689\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m3s\u001b[0m 57ms/step - auc: 0.9624 - binary_accuracy: 0.9252 - false_negatives: 251.2194 - false_positives: 517.6661 - loss: 0.2552 - precision: 0.9055 - recall: 0.9507 - true_negatives: 4513.8647 - true_positives: 4797.2495\n",
      "For batch number 630 the model has a loss of 0.26880156993865967\n",
      "\u001b[1m630/689\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m3s\u001b[0m 57ms/step - auc: 0.9624 - binary_accuracy: 0.9252 - false_negatives: 251.6572 - false_positives: 518.5428 - loss: 0.2552 - precision: 0.9055 - recall: 0.9507 - true_negatives: 4520.9648 - true_positives: 4804.8350\n",
      "For batch number 631 the model has a loss of 0.26873740553855896\n",
      "\u001b[1m631/689\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m3s\u001b[0m 57ms/step - auc: 0.9623 - binary_accuracy: 0.9252 - false_negatives: 252.0935 - false_positives: 519.4200 - loss: 0.2552 - precision: 0.9055 - recall: 0.9507 - true_negatives: 4528.0649 - true_positives: 4812.4214\n",
      "For batch number 632 the model has a loss of 0.26840636134147644\n",
      "\u001b[1m632/689\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m3s\u001b[0m 57ms/step - auc: 0.9623 - binary_accuracy: 0.9252 - false_negatives: 252.5285 - false_positives: 520.2943 - loss: 0.2553 - precision: 0.9055 - recall: 0.9507 - true_negatives: 4535.1631 - true_positives: 4820.0142\n",
      "For batch number 633 the model has a loss of 0.26822641491889954\n",
      "\u001b[1m633/689\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m3s\u001b[0m 57ms/step - auc: 0.9623 - binary_accuracy: 0.9252 - false_negatives: 252.9637 - false_positives: 521.1659 - loss: 0.2553 - precision: 0.9055 - recall: 0.9507 - true_negatives: 4542.2573 - true_positives: 4827.6128\n",
      "For batch number 634 the model has a loss of 0.2687625586986542\n",
      "\u001b[1m634/689\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m3s\u001b[0m 57ms/step - auc: 0.9623 - binary_accuracy: 0.9252 - false_negatives: 253.4006 - false_positives: 522.0378 - loss: 0.2553 - precision: 0.9055 - recall: 0.9507 - true_negatives: 4549.3472 - true_positives: 4835.2144\n",
      "For batch number 635 the model has a loss of 0.2694609761238098\n",
      "\u001b[1m635/689\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m3s\u001b[0m 57ms/step - auc: 0.9623 - binary_accuracy: 0.9252 - false_negatives: 253.8378 - false_positives: 522.9181 - loss: 0.2553 - precision: 0.9054 - recall: 0.9507 - true_negatives: 4556.4346 - true_positives: 4842.8096\n",
      "For batch number 636 the model has a loss of 0.26938915252685547\n",
      "\u001b[1m636/689\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m3s\u001b[0m 57ms/step - auc: 0.9623 - binary_accuracy: 0.9252 - false_negatives: 254.2736 - false_positives: 523.7988 - loss: 0.2553 - precision: 0.9054 - recall: 0.9507 - true_negatives: 4563.5190 - true_positives: 4850.4087\n",
      "For batch number 637 the model has a loss of 0.2692580819129944\n",
      "\u001b[1m637/689\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m2s\u001b[0m 57ms/step - auc: 0.9623 - binary_accuracy: 0.9252 - false_negatives: 254.7080 - false_positives: 524.6797 - loss: 0.2554 - precision: 0.9054 - recall: 0.9507 - true_negatives: 4570.5952 - true_positives: 4858.0171\n",
      "For batch number 638 the model has a loss of 0.2695160210132599\n",
      "\u001b[1m638/689\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m2s\u001b[0m 57ms/step - auc: 0.9623 - binary_accuracy: 0.9251 - false_negatives: 255.1426 - false_positives: 525.5643 - loss: 0.2554 - precision: 0.9054 - recall: 0.9507 - true_negatives: 4577.6694 - true_positives: 4865.6240\n",
      "For batch number 639 the model has a loss of 0.2694273591041565\n",
      "\u001b[1m639/689\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m2s\u001b[0m 57ms/step - auc: 0.9623 - binary_accuracy: 0.9251 - false_negatives: 255.5775 - false_positives: 526.4476 - loss: 0.2554 - precision: 0.9054 - recall: 0.9507 - true_negatives: 4584.7480 - true_positives: 4873.2271\n",
      "For batch number 640 the model has a loss of 0.2696998715400696\n",
      "\u001b[1m640/689\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m2s\u001b[0m 57ms/step - auc: 0.9623 - binary_accuracy: 0.9251 - false_negatives: 256.0125 - false_positives: 527.3344 - loss: 0.2554 - precision: 0.9054 - recall: 0.9507 - true_negatives: 4591.8267 - true_positives: 4880.8267\n",
      "For batch number 641 the model has a loss of 0.26967740058898926\n",
      "\u001b[1m641/689\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m2s\u001b[0m 57ms/step - auc: 0.9623 - binary_accuracy: 0.9251 - false_negatives: 256.4462 - false_positives: 528.2231 - loss: 0.2554 - precision: 0.9054 - recall: 0.9507 - true_negatives: 4598.9019 - true_positives: 4888.4292\n",
      "For batch number 642 the model has a loss of 0.2694316804409027\n",
      "\u001b[1m642/689\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m2s\u001b[0m 57ms/step - auc: 0.9623 - binary_accuracy: 0.9251 - false_negatives: 256.8785 - false_positives: 529.1106 - loss: 0.2555 - precision: 0.9054 - recall: 0.9507 - true_negatives: 4605.9829 - true_positives: 4896.0278\n",
      "For batch number 643 the model has a loss of 0.269436240196228\n",
      "\u001b[1m643/689\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m2s\u001b[0m 57ms/step - auc: 0.9623 - binary_accuracy: 0.9251 - false_negatives: 257.3126 - false_positives: 529.9969 - loss: 0.2555 - precision: 0.9054 - recall: 0.9507 - true_negatives: 4613.0591 - true_positives: 4903.6313\n",
      "For batch number 644 the model has a loss of 0.26941680908203125\n",
      "\u001b[1m644/689\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m2s\u001b[0m 57ms/step - auc: 0.9623 - binary_accuracy: 0.9251 - false_negatives: 257.7453 - false_positives: 530.8851 - loss: 0.2555 - precision: 0.9054 - recall: 0.9507 - true_negatives: 4620.1367 - true_positives: 4911.2329\n",
      "For batch number 645 the model has a loss of 0.26919808983802795\n",
      "\u001b[1m645/689\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m2s\u001b[0m 57ms/step - auc: 0.9623 - binary_accuracy: 0.9251 - false_negatives: 258.1768 - false_positives: 531.7736 - loss: 0.2555 - precision: 0.9053 - recall: 0.9507 - true_negatives: 4627.2153 - true_positives: 4918.8340\n",
      "For batch number 646 the model has a loss of 0.2690470218658447\n",
      "\u001b[1m646/689\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m2s\u001b[0m 57ms/step - auc: 0.9623 - binary_accuracy: 0.9251 - false_negatives: 258.6068 - false_positives: 532.6610 - loss: 0.2556 - precision: 0.9053 - recall: 0.9507 - true_negatives: 4634.2939 - true_positives: 4926.4380\n",
      "For batch number 647 the model has a loss of 0.26886433362960815\n",
      "\u001b[1m647/689\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m2s\u001b[0m 57ms/step - auc: 0.9622 - binary_accuracy: 0.9251 - false_negatives: 259.0356 - false_positives: 533.5471 - loss: 0.2556 - precision: 0.9053 - recall: 0.9507 - true_negatives: 4641.3770 - true_positives: 4934.0400\n",
      "For batch number 648 the model has a loss of 0.26874059438705444\n",
      "\u001b[1m648/689\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m2s\u001b[0m 57ms/step - auc: 0.9622 - binary_accuracy: 0.9251 - false_negatives: 259.4630 - false_positives: 534.4337 - loss: 0.2556 - precision: 0.9053 - recall: 0.9507 - true_negatives: 4648.4614 - true_positives: 4941.6421\n",
      "For batch number 649 the model has a loss of 0.2687699794769287\n",
      "\u001b[1m649/689\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m2s\u001b[0m 57ms/step - auc: 0.9622 - binary_accuracy: 0.9251 - false_negatives: 259.8891 - false_positives: 535.3220 - loss: 0.2556 - precision: 0.9053 - recall: 0.9507 - true_negatives: 4655.5425 - true_positives: 4949.2466\n",
      "For batch number 650 the model has a loss of 0.26872923970222473\n",
      "\u001b[1m650/689\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m2s\u001b[0m 57ms/step - auc: 0.9622 - binary_accuracy: 0.9251 - false_negatives: 260.3154 - false_positives: 536.2108 - loss: 0.2556 - precision: 0.9053 - recall: 0.9507 - true_negatives: 4662.6245 - true_positives: 4956.8491\n",
      "For batch number 651 the model has a loss of 0.26870569586753845\n",
      "\u001b[1m651/689\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m2s\u001b[0m 57ms/step - auc: 0.9622 - binary_accuracy: 0.9251 - false_negatives: 260.7404 - false_positives: 537.1029 - loss: 0.2557 - precision: 0.9053 - recall: 0.9507 - true_negatives: 4669.7095 - true_positives: 4964.4468\n",
      "For batch number 652 the model has a loss of 0.26865020394325256\n",
      "\u001b[1m652/689\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m2s\u001b[0m 57ms/step - auc: 0.9622 - binary_accuracy: 0.9250 - false_negatives: 261.1687 - false_positives: 537.9923 - loss: 0.2557 - precision: 0.9053 - recall: 0.9507 - true_negatives: 4676.7915 - true_positives: 4972.0474\n",
      "For batch number 653 the model has a loss of 0.2685171663761139\n",
      "\u001b[1m653/689\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m2s\u001b[0m 57ms/step - auc: 0.9622 - binary_accuracy: 0.9250 - false_negatives: 261.5957 - false_positives: 538.8806 - loss: 0.2557 - precision: 0.9053 - recall: 0.9506 - true_negatives: 4683.8760 - true_positives: 4979.6479\n",
      "For batch number 654 the model has a loss of 0.26859572529792786\n",
      "\u001b[1m654/689\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m2s\u001b[0m 57ms/step - auc: 0.9622 - binary_accuracy: 0.9250 - false_negatives: 262.0229 - false_positives: 539.7722 - loss: 0.2557 - precision: 0.9053 - recall: 0.9506 - true_negatives: 4690.9604 - true_positives: 4987.2446\n",
      "For batch number 655 the model has a loss of 0.26868700981140137\n",
      "\u001b[1m655/689\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m1s\u001b[0m 57ms/step - auc: 0.9622 - binary_accuracy: 0.9250 - false_negatives: 262.4489 - false_positives: 540.6656 - loss: 0.2557 - precision: 0.9052 - recall: 0.9506 - true_negatives: 4698.0459 - true_positives: 4994.8398\n",
      "For batch number 656 the model has a loss of 0.26866891980171204\n",
      "\u001b[1m656/689\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m1s\u001b[0m 57ms/step - auc: 0.9622 - binary_accuracy: 0.9250 - false_negatives: 262.8735 - false_positives: 541.5594 - loss: 0.2558 - precision: 0.9052 - recall: 0.9506 - true_negatives: 4705.1309 - true_positives: 5002.4360\n",
      "For batch number 657 the model has a loss of 0.26877692341804504\n",
      "\u001b[1m657/689\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m1s\u001b[0m 57ms/step - auc: 0.9622 - binary_accuracy: 0.9250 - false_negatives: 263.2998 - false_positives: 542.4551 - loss: 0.2558 - precision: 0.9052 - recall: 0.9506 - true_negatives: 4712.2148 - true_positives: 5010.0303\n",
      "For batch number 658 the model has a loss of 0.26850780844688416\n",
      "\u001b[1m658/689\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m1s\u001b[0m 57ms/step - auc: 0.9622 - binary_accuracy: 0.9250 - false_negatives: 263.7249 - false_positives: 543.3480 - loss: 0.2558 - precision: 0.9052 - recall: 0.9506 - true_negatives: 4719.3008 - true_positives: 5017.6260\n",
      "For batch number 659 the model has a loss of 0.2683594822883606\n",
      "\u001b[1m659/689\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m1s\u001b[0m 57ms/step - auc: 0.9622 - binary_accuracy: 0.9250 - false_negatives: 264.1487 - false_positives: 544.2397 - loss: 0.2558 - precision: 0.9052 - recall: 0.9506 - true_negatives: 4726.3901 - true_positives: 5025.2217\n",
      "For batch number 660 the model has a loss of 0.26836955547332764\n",
      "\u001b[1m660/689\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m1s\u001b[0m 57ms/step - auc: 0.9622 - binary_accuracy: 0.9250 - false_negatives: 264.5712 - false_positives: 545.1334 - loss: 0.2558 - precision: 0.9052 - recall: 0.9506 - true_negatives: 4733.4819 - true_positives: 5032.8135\n",
      "For batch number 661 the model has a loss of 0.2681463658809662\n",
      "\u001b[1m661/689\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m1s\u001b[0m 57ms/step - auc: 0.9622 - binary_accuracy: 0.9250 - false_negatives: 264.9924 - false_positives: 546.0257 - loss: 0.2559 - precision: 0.9052 - recall: 0.9506 - true_negatives: 4740.5796 - true_positives: 5040.4023\n",
      "For batch number 662 the model has a loss of 0.26842331886291504\n",
      "\u001b[1m662/689\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m1s\u001b[0m 57ms/step - auc: 0.9622 - binary_accuracy: 0.9250 - false_negatives: 265.4139 - false_positives: 546.9185 - loss: 0.2559 - precision: 0.9052 - recall: 0.9506 - true_negatives: 4747.6841 - true_positives: 5047.9834\n",
      "For batch number 663 the model has a loss of 0.26813703775405884\n",
      "\u001b[1m663/689\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m1s\u001b[0m 57ms/step - auc: 0.9622 - binary_accuracy: 0.9250 - false_negatives: 265.8341 - false_positives: 547.8099 - loss: 0.2559 - precision: 0.9052 - recall: 0.9506 - true_negatives: 4754.7920 - true_positives: 5055.5640\n",
      "For batch number 664 the model has a loss of 0.26803743839263916\n",
      "\u001b[1m664/689\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m1s\u001b[0m 57ms/step - auc: 0.9622 - binary_accuracy: 0.9250 - false_negatives: 266.2530 - false_positives: 548.7018 - loss: 0.2559 - precision: 0.9052 - recall: 0.9506 - true_negatives: 4761.9004 - true_positives: 5063.1445\n",
      "For batch number 665 the model has a loss of 0.26789993047714233\n",
      "\u001b[1m665/689\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m1s\u001b[0m 57ms/step - auc: 0.9621 - binary_accuracy: 0.9250 - false_negatives: 266.6707 - false_positives: 549.5925 - loss: 0.2559 - precision: 0.9051 - recall: 0.9506 - true_negatives: 4769.0137 - true_positives: 5070.7231\n",
      "For batch number 666 the model has a loss of 0.26815086603164673\n",
      "\u001b[1m666/689\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m1s\u001b[0m 57ms/step - auc: 0.9621 - binary_accuracy: 0.9250 - false_negatives: 267.0901 - false_positives: 550.4805 - loss: 0.2559 - precision: 0.9051 - recall: 0.9506 - true_negatives: 4776.1201 - true_positives: 5078.3091\n",
      "For batch number 667 the model has a loss of 0.26800471544265747\n",
      "\u001b[1m667/689\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m1s\u001b[0m 57ms/step - auc: 0.9621 - binary_accuracy: 0.9249 - false_negatives: 267.5082 - false_positives: 551.3673 - loss: 0.2560 - precision: 0.9051 - recall: 0.9506 - true_negatives: 4783.2295 - true_positives: 5085.8950\n",
      "For batch number 668 the model has a loss of 0.26780980825424194\n",
      "\u001b[1m668/689\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m1s\u001b[0m 57ms/step - auc: 0.9621 - binary_accuracy: 0.9249 - false_negatives: 267.9251 - false_positives: 552.2545 - loss: 0.2560 - precision: 0.9051 - recall: 0.9506 - true_negatives: 4790.3428 - true_positives: 5093.4775\n",
      "For batch number 669 the model has a loss of 0.2679304778575897\n",
      "\u001b[1m669/689\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m1s\u001b[0m 57ms/step - auc: 0.9621 - binary_accuracy: 0.9249 - false_negatives: 268.3423 - false_positives: 553.1405 - loss: 0.2560 - precision: 0.9051 - recall: 0.9506 - true_negatives: 4797.4575 - true_positives: 5101.0596\n",
      "For batch number 670 the model has a loss of 0.2677645683288574\n",
      "\u001b[1m670/689\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m1s\u001b[0m 57ms/step - auc: 0.9621 - binary_accuracy: 0.9249 - false_negatives: 268.7582 - false_positives: 554.0239 - loss: 0.2560 - precision: 0.9051 - recall: 0.9506 - true_negatives: 4804.5762 - true_positives: 5108.6416\n",
      "For batch number 671 the model has a loss of 0.26760274171829224\n",
      "\u001b[1m671/689\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m1s\u001b[0m 57ms/step - auc: 0.9621 - binary_accuracy: 0.9249 - false_negatives: 269.1729 - false_positives: 554.9046 - loss: 0.2560 - precision: 0.9051 - recall: 0.9506 - true_negatives: 4811.6973 - true_positives: 5116.2251\n",
      "For batch number 672 the model has a loss of 0.26743263006210327\n",
      "\u001b[1m672/689\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 57ms/step - auc: 0.9621 - binary_accuracy: 0.9249 - false_negatives: 269.5863 - false_positives: 555.7827 - loss: 0.2560 - precision: 0.9051 - recall: 0.9506 - true_negatives: 4818.8242 - true_positives: 5123.8066\n",
      "For batch number 673 the model has a loss of 0.26738670468330383\n",
      "\u001b[1m673/689\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 57ms/step - auc: 0.9621 - binary_accuracy: 0.9249 - false_negatives: 270.0015 - false_positives: 556.6597 - loss: 0.2561 - precision: 0.9051 - recall: 0.9506 - true_negatives: 4825.9541 - true_positives: 5131.3848\n",
      "For batch number 674 the model has a loss of 0.26723384857177734\n",
      "\u001b[1m674/689\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 57ms/step - auc: 0.9621 - binary_accuracy: 0.9249 - false_negatives: 270.4154 - false_positives: 557.5371 - loss: 0.2561 - precision: 0.9051 - recall: 0.9506 - true_negatives: 4833.0874 - true_positives: 5138.9600\n",
      "For batch number 675 the model has a loss of 0.2671160399913788\n",
      "\u001b[1m675/689\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 57ms/step - auc: 0.9621 - binary_accuracy: 0.9249 - false_negatives: 270.8282 - false_positives: 558.4148 - loss: 0.2561 - precision: 0.9051 - recall: 0.9506 - true_negatives: 4840.2207 - true_positives: 5146.5361\n",
      "For batch number 676 the model has a loss of 0.2670435905456543\n",
      "\u001b[1m676/689\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 57ms/step - auc: 0.9621 - binary_accuracy: 0.9249 - false_negatives: 271.2411 - false_positives: 559.2914 - loss: 0.2561 - precision: 0.9050 - recall: 0.9506 - true_negatives: 4847.3477 - true_positives: 5154.1196\n",
      "For batch number 677 the model has a loss of 0.26703959703445435\n",
      "\u001b[1m677/689\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 57ms/step - auc: 0.9621 - binary_accuracy: 0.9249 - false_negatives: 271.6544 - false_positives: 560.1669 - loss: 0.2561 - precision: 0.9050 - recall: 0.9506 - true_negatives: 4854.4741 - true_positives: 5161.7046\n",
      "For batch number 678 the model has a loss of 0.266943097114563\n",
      "\u001b[1m678/689\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 57ms/step - auc: 0.9621 - binary_accuracy: 0.9249 - false_negatives: 272.0678 - false_positives: 561.0413 - loss: 0.2561 - precision: 0.9050 - recall: 0.9506 - true_negatives: 4861.6060 - true_positives: 5169.2847\n",
      "For batch number 679 the model has a loss of 0.2668381929397583\n",
      "\u001b[1m679/689\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 57ms/step - auc: 0.9621 - binary_accuracy: 0.9249 - false_negatives: 272.4801 - false_positives: 561.9161 - loss: 0.2562 - precision: 0.9050 - recall: 0.9506 - true_negatives: 4868.7363 - true_positives: 5176.8677\n",
      "For batch number 680 the model has a loss of 0.26677781343460083\n",
      "\u001b[1m680/689\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 57ms/step - auc: 0.9621 - binary_accuracy: 0.9249 - false_negatives: 272.8941 - false_positives: 562.7897 - loss: 0.2562 - precision: 0.9050 - recall: 0.9506 - true_negatives: 4875.8662 - true_positives: 5184.4502\n",
      "For batch number 681 the model has a loss of 0.2665255069732666\n",
      "\u001b[1m681/689\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 57ms/step - auc: 0.9621 - binary_accuracy: 0.9249 - false_negatives: 273.3069 - false_positives: 563.6608 - loss: 0.2562 - precision: 0.9050 - recall: 0.9506 - true_negatives: 4882.9971 - true_positives: 5192.0352\n",
      "For batch number 682 the model has a loss of 0.26649579405784607\n",
      "\u001b[1m682/689\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 57ms/step - auc: 0.9621 - binary_accuracy: 0.9249 - false_negatives: 273.7199 - false_positives: 564.5323 - loss: 0.2562 - precision: 0.9050 - recall: 0.9506 - true_negatives: 4890.1348 - true_positives: 5199.6128\n",
      "For batch number 683 the model has a loss of 0.26648765802383423\n",
      "\u001b[1m683/689\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 57ms/step - auc: 0.9621 - binary_accuracy: 0.9249 - false_negatives: 274.1318 - false_positives: 565.4056 - loss: 0.2562 - precision: 0.9050 - recall: 0.9506 - true_negatives: 4897.2695 - true_positives: 5207.1934\n",
      "For batch number 684 the model has a loss of 0.26648327708244324\n",
      "\u001b[1m684/689\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 57ms/step - auc: 0.9620 - binary_accuracy: 0.9249 - false_negatives: 274.5468 - false_positives: 566.2763 - loss: 0.2562 - precision: 0.9050 - recall: 0.9506 - true_negatives: 4904.4004 - true_positives: 5214.7764\n",
      "For batch number 685 the model has a loss of 0.2665649950504303\n",
      "\u001b[1m685/689\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 57ms/step - auc: 0.9620 - binary_accuracy: 0.9249 - false_negatives: 274.9606 - false_positives: 567.1489 - loss: 0.2563 - precision: 0.9050 - recall: 0.9506 - true_negatives: 4911.5327 - true_positives: 5222.3574\n",
      "For batch number 686 the model has a loss of 0.2665315270423889\n",
      "\u001b[1m686/689\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 57ms/step - auc: 0.9620 - binary_accuracy: 0.9248 - false_negatives: 275.3746 - false_positives: 568.0204 - loss: 0.2563 - precision: 0.9050 - recall: 0.9506 - true_negatives: 4918.6660 - true_positives: 5229.9390\n",
      "For batch number 687 the model has a loss of 0.266670823097229\n",
      "\u001b[1m687/689\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 57ms/step - auc: 0.9620 - binary_accuracy: 0.9248 - false_negatives: 275.7875 - false_positives: 568.8981 - loss: 0.2563 - precision: 0.9050 - recall: 0.9506 - true_negatives: 4925.8022 - true_positives: 5237.5122\n",
      "For batch number 688 the model has a loss of 0.26648086309432983\n",
      "\u001b[1m688/689\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 57ms/step - auc: 0.9620 - binary_accuracy: 0.9248 - false_negatives: 276.1991 - false_positives: 569.7747 - loss: 0.2563 - precision: 0.9049 - recall: 0.9506 - true_negatives: 4932.9375 - true_positives: 5245.0889\n",
      "For batch number 689 the model has a loss of 0.2666347026824951\n",
      "\u001b[1m689/689\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 57ms/step - auc: 0.9620 - binary_accuracy: 0.9248 - false_negatives: 276.6096 - false_positives: 570.6531 - loss: 0.2563 - precision: 0.9049 - recall: 0.9506 - true_negatives: 4940.0713 - true_positives: 5252.6631\n",
      "For epoch number 4 the model has a loss of 0.2666347026824951\n",
      "\u001b[1m689/689\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 60ms/step - auc: 0.9620 - binary_accuracy: 0.9248 - false_negatives: 277.0188 - false_positives: 571.5290 - loss: 0.2563 - precision: 0.9049 - recall: 0.9506 - true_negatives: 4947.1841 - true_positives: 5260.2158 - val_auc: 0.9666 - val_binary_accuracy: 0.9365 - val_false_negatives: 96.0000 - val_false_positives: 79.0000 - val_loss: 0.2631 - val_precision: 0.9424 - val_recall: 0.9309 - val_true_negatives: 1287.0000 - val_true_positives: 1293.0000\n",
      "Epoch 5/5\n",
      "\n",
      "For batch number 1 the model has a loss of 0.12969514727592468\n",
      "\u001b[1m  1/689\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m1:02\u001b[0m 90ms/step - auc: 1.0000 - binary_accuracy: 0.9688 - false_negatives: 0.0000e+00 - false_positives: 1.0000 - loss: 0.1297 - precision: 0.9333 - recall: 1.0000 - true_negatives: 17.0000 - true_positives: 14.0000\n",
      "For batch number 2 the model has a loss of 0.11724719405174255\n",
      "\u001b[1m  2/689\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m37s\u001b[0m 54ms/step - auc: 1.0000 - binary_accuracy: 0.9766 - false_negatives: 0.0000e+00 - false_positives: 1.0000 - loss: 0.1235 - precision: 0.9510 - recall: 1.0000 - true_negatives: 24.5000 - true_positives: 22.5000 \n",
      "For batch number 3 the model has a loss of 0.17540539801120758\n",
      "\u001b[1m  3/689\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m37s\u001b[0m 55ms/step - auc: 0.9971 - binary_accuracy: 0.9740 - false_negatives: 0.6667 - false_positives: 1.0000 - loss: 0.1408 - precision: 0.9610 - recall: 0.9874 - true_negatives: 30.3333 - true_positives: 32.0000    \n",
      "For batch number 4 the model has a loss of 0.1847115457057953\n",
      "\u001b[1m  4/689\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m38s\u001b[0m 57ms/step - auc: 0.9930 - binary_accuracy: 0.9688 - false_negatives: 1.0000 - false_positives: 1.7500 - loss: 0.1518 - precision: 0.9562 - recall: 0.9831 - true_negatives: 37.0000 - true_positives: 40.2500\n",
      "For batch number 5 the model has a loss of 0.19209988415241241\n",
      "\u001b[1m  5/689\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m39s\u001b[0m 57ms/step - auc: 0.9904 - binary_accuracy: 0.9638 - false_negatives: 1.4000 - false_positives: 2.6000 - loss: 0.1598 - precision: 0.9515 - recall: 0.9795 - true_negatives: 43.2000 - true_positives: 48.8000\n",
      "For batch number 6 the model has a loss of 0.16846372187137604\n",
      "\u001b[1m  6/689\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m38s\u001b[0m 57ms/step - auc: 0.9895 - binary_accuracy: 0.9620 - false_negatives: 1.6667 - false_positives: 3.1667 - loss: 0.1613 - precision: 0.9501 - recall: 0.9781 - true_negatives: 49.8333 - true_positives: 57.3333\n",
      "For batch number 7 the model has a loss of 0.20473715662956238\n",
      "\u001b[1m  7/689\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m38s\u001b[0m 57ms/step - auc: 0.9878 - binary_accuracy: 0.9591 - false_negatives: 2.0000 - false_positives: 4.0000 - loss: 0.1675 - precision: 0.9471 - recall: 0.9765 - true_negatives: 56.1429 - true_positives: 65.8571\n",
      "For batch number 8 the model has a loss of 0.20347224175930023\n",
      "\u001b[1m  8/689\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m38s\u001b[0m 57ms/step - auc: 0.9866 - binary_accuracy: 0.9569 - false_negatives: 2.2500 - false_positives: 4.8750 - loss: 0.1720 - precision: 0.9441 - recall: 0.9758 - true_negatives: 62.6250 - true_positives: 74.2500\n",
      "For batch number 9 the model has a loss of 0.20810550451278687\n",
      "\u001b[1m  9/689\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m38s\u001b[0m 57ms/step - auc: 0.9853 - binary_accuracy: 0.9551 - false_negatives: 2.5556 - false_positives: 5.6667 - loss: 0.1760 - precision: 0.9419 - recall: 0.9748 - true_negatives: 69.5556 - true_positives: 82.2222\n",
      "For batch number 10 the model has a loss of 0.2100277692079544\n",
      "\u001b[1m 10/689\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m38s\u001b[0m 57ms/step - auc: 0.9843 - binary_accuracy: 0.9537 - false_negatives: 2.9000 - false_positives: 6.4000 - loss: 0.1794 - precision: 0.9402 - recall: 0.9737 - true_negatives: 76.6000 - true_positives: 90.1000\n",
      "For batch number 11 the model has a loss of 0.2181112915277481\n",
      "\u001b[1m 11/689\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m38s\u001b[0m 57ms/step - auc: 0.9833 - binary_accuracy: 0.9522 - false_negatives: 3.3636 - false_positives: 7.0909 - loss: 0.1829 - precision: 0.9389 - recall: 0.9721 - true_negatives: 83.8182 - true_positives: 97.7273\n",
      "For batch number 12 the model has a loss of 0.2296389788389206\n",
      "\u001b[1m 12/689\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m38s\u001b[0m 57ms/step - auc: 0.9823 - binary_accuracy: 0.9512 - false_negatives: 3.8333 - false_positives: 7.6667 - loss: 0.1868 - precision: 0.9384 - recall: 0.9707 - true_negatives: 90.7500 - true_positives: 105.7500\n",
      "For batch number 13 the model has a loss of 0.22955557703971863\n",
      "\u001b[1m 13/689\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m38s\u001b[0m 57ms/step - auc: 0.9815 - binary_accuracy: 0.9501 - false_negatives: 4.3846 - false_positives: 8.2308 - loss: 0.1901 - precision: 0.9380 - recall: 0.9692 - true_negatives: 97.6154 - true_positives: 113.7692\n",
      "For batch number 14 the model has a loss of 0.22957444190979004\n",
      "\u001b[1m 14/689\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m38s\u001b[0m 57ms/step - auc: 0.9808 - binary_accuracy: 0.9492 - false_negatives: 4.9286 - false_positives: 8.7857 - loss: 0.1929 - precision: 0.9378 - recall: 0.9678 - true_negatives: 104.2857 - true_positives: 122.0000\n",
      "For batch number 15 the model has a loss of 0.22641870379447937\n",
      "\u001b[1m 15/689\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m38s\u001b[0m 57ms/step - auc: 0.9803 - binary_accuracy: 0.9483 - false_negatives: 5.4000 - false_positives: 9.4667 - loss: 0.1952 - precision: 0.9371 - recall: 0.9668 - true_negatives: 111.0667 - true_positives: 130.0667\n",
      "For batch number 16 the model has a loss of 0.22036708891391754\n",
      "\u001b[1m 16/689\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m38s\u001b[0m 57ms/step - auc: 0.9799 - binary_accuracy: 0.9476 - false_negatives: 5.8125 - false_positives: 10.1250 - loss: 0.1967 - precision: 0.9365 - recall: 0.9661 - true_negatives: 118.0000 - true_positives: 138.0625\n",
      "For batch number 17 the model has a loss of 0.2258121222257614\n",
      "\u001b[1m 17/689\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m38s\u001b[0m 57ms/step - auc: 0.9794 - binary_accuracy: 0.9469 - false_negatives: 6.2353 - false_positives: 10.8235 - loss: 0.1984 - precision: 0.9358 - recall: 0.9654 - true_negatives: 125.1765 - true_positives: 145.7647\n",
      "For batch number 18 the model has a loss of 0.22405388951301575\n",
      "\u001b[1m 18/689\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m38s\u001b[0m 57ms/step - auc: 0.9791 - binary_accuracy: 0.9462 - false_negatives: 6.6667 - false_positives: 11.5556 - loss: 0.1999 - precision: 0.9350 - recall: 0.9647 - true_negatives: 132.3889 - true_positives: 153.3889\n",
      "For batch number 19 the model has a loss of 0.22279584407806396\n",
      "\u001b[1m 19/689\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m38s\u001b[0m 57ms/step - auc: 0.9787 - binary_accuracy: 0.9456 - false_negatives: 7.0526 - false_positives: 12.3158 - loss: 0.2011 - precision: 0.9342 - recall: 0.9642 - true_negatives: 139.6842 - true_positives: 160.9474\n",
      "For batch number 20 the model has a loss of 0.2189997136592865\n",
      "\u001b[1m 20/689\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m37s\u001b[0m 57ms/step - auc: 0.9785 - binary_accuracy: 0.9451 - false_negatives: 7.4500 - false_positives: 13.0000 - loss: 0.2020 - precision: 0.9336 - recall: 0.9637 - true_negatives: 147.0500 - true_positives: 168.5000\n",
      "For batch number 21 the model has a loss of 0.21204246580600739\n",
      "\u001b[1m 21/689\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m37s\u001b[0m 57ms/step - auc: 0.9784 - binary_accuracy: 0.9448 - false_negatives: 7.8095 - false_positives: 13.6190 - loss: 0.2024 - precision: 0.9333 - recall: 0.9633 - true_negatives: 154.4286 - true_positives: 176.1429\n",
      "For batch number 22 the model has a loss of 0.21413137018680573\n",
      "\u001b[1m 22/689\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m37s\u001b[0m 57ms/step - auc: 0.9782 - binary_accuracy: 0.9445 - false_negatives: 8.1364 - false_positives: 14.3182 - loss: 0.2030 - precision: 0.9328 - recall: 0.9631 - true_negatives: 161.7727 - true_positives: 183.7727\n",
      "For batch number 23 the model has a loss of 0.2148701697587967\n",
      "\u001b[1m 23/689\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m37s\u001b[0m 57ms/step - auc: 0.9780 - binary_accuracy: 0.9441 - false_negatives: 8.5217 - false_positives: 15.0000 - loss: 0.2035 - precision: 0.9324 - recall: 0.9627 - true_negatives: 169.0870 - true_positives: 191.3913\n",
      "For batch number 24 the model has a loss of 0.2145347148180008\n",
      "\u001b[1m 24/689\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m37s\u001b[0m 56ms/step - auc: 0.9779 - binary_accuracy: 0.9439 - false_negatives: 8.8750 - false_positives: 15.6250 - loss: 0.2040 - precision: 0.9321 - recall: 0.9625 - true_negatives: 176.3333 - true_positives: 199.1667\n",
      "For batch number 25 the model has a loss of 0.21574729681015015\n",
      "\u001b[1m 25/689\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m37s\u001b[0m 56ms/step - auc: 0.9778 - binary_accuracy: 0.9436 - false_negatives: 9.2000 - false_positives: 16.3600 - loss: 0.2044 - precision: 0.9316 - recall: 0.9623 - true_negatives: 183.6400 - true_positives: 206.8000\n",
      "For batch number 26 the model has a loss of 0.21663421392440796\n",
      "\u001b[1m 26/689\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m37s\u001b[0m 56ms/step - auc: 0.9776 - binary_accuracy: 0.9433 - false_negatives: 9.5769 - false_positives: 17.0385 - loss: 0.2049 - precision: 0.9313 - recall: 0.9620 - true_negatives: 191.0000 - true_positives: 214.3846\n",
      "For batch number 27 the model has a loss of 0.2148134559392929\n",
      "\u001b[1m 27/689\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m37s\u001b[0m 56ms/step - auc: 0.9775 - binary_accuracy: 0.9431 - false_negatives: 9.9259 - false_positives: 17.7037 - loss: 0.2053 - precision: 0.9310 - recall: 0.9618 - true_negatives: 198.4074 - true_positives: 221.9630\n",
      "For batch number 28 the model has a loss of 0.2182578593492508\n",
      "\u001b[1m 28/689\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m37s\u001b[0m 56ms/step - auc: 0.9774 - binary_accuracy: 0.9428 - false_negatives: 10.3214 - false_positives: 18.3929 - loss: 0.2057 - precision: 0.9306 - recall: 0.9616 - true_negatives: 205.8214 - true_positives: 229.4643\n",
      "For batch number 29 the model has a loss of 0.21944989264011383\n",
      "\u001b[1m 29/689\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m37s\u001b[0m 57ms/step - auc: 0.9772 - binary_accuracy: 0.9425 - false_negatives: 10.7241 - false_positives: 19.1034 - loss: 0.2062 - precision: 0.9303 - recall: 0.9613 - true_negatives: 213.1724 - true_positives: 237.0000\n",
      "For batch number 30 the model has a loss of 0.22675687074661255\n",
      "\u001b[1m 30/689\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m37s\u001b[0m 57ms/step - auc: 0.9771 - binary_accuracy: 0.9423 - false_negatives: 11.1333 - false_positives: 19.7667 - loss: 0.2069 - precision: 0.9300 - recall: 0.9610 - true_negatives: 220.5000 - true_positives: 244.6000\n",
      "For batch number 31 the model has a loss of 0.22335879504680634\n",
      "\u001b[1m 31/689\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m37s\u001b[0m 57ms/step - auc: 0.9769 - binary_accuracy: 0.9421 - false_negatives: 11.5161 - false_positives: 20.3871 - loss: 0.2074 - precision: 0.9299 - recall: 0.9608 - true_negatives: 227.7742 - true_positives: 252.3226\n",
      "For batch number 32 the model has a loss of 0.22467784583568573\n",
      "\u001b[1m 32/689\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m37s\u001b[0m 57ms/step - auc: 0.9768 - binary_accuracy: 0.9419 - false_negatives: 11.9062 - false_positives: 21.0625 - loss: 0.2080 - precision: 0.9296 - recall: 0.9606 - true_negatives: 234.9062 - true_positives: 260.1250\n",
      "For batch number 33 the model has a loss of 0.22999504208564758\n",
      "\u001b[1m 33/689\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m37s\u001b[0m 57ms/step - auc: 0.9767 - binary_accuracy: 0.9416 - false_negatives: 12.2727 - false_positives: 21.8788 - loss: 0.2086 - precision: 0.9292 - recall: 0.9604 - true_negatives: 241.9697 - true_positives: 267.8788\n",
      "For batch number 34 the model has a loss of 0.23112046718597412\n",
      "\u001b[1m 34/689\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m37s\u001b[0m 57ms/step - auc: 0.9765 - binary_accuracy: 0.9413 - false_negatives: 12.6471 - false_positives: 22.7647 - loss: 0.2093 - precision: 0.9286 - recall: 0.9603 - true_negatives: 249.0000 - true_positives: 275.5882\n",
      "For batch number 35 the model has a loss of 0.2297067642211914\n",
      "\u001b[1m 35/689\u001b[0m \u001b[32m━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m36s\u001b[0m 57ms/step - auc: 0.9764 - binary_accuracy: 0.9409 - false_negatives: 13.0571 - false_positives: 23.6286 - loss: 0.2099 - precision: 0.9282 - recall: 0.9601 - true_negatives: 256.0000 - true_positives: 283.3143\n",
      "For batch number 36 the model has a loss of 0.22697599232196808\n",
      "\u001b[1m 36/689\u001b[0m \u001b[32m━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m36s\u001b[0m 57ms/step - auc: 0.9763 - binary_accuracy: 0.9406 - false_negatives: 13.4444 - false_positives: 24.4722 - loss: 0.2103 - precision: 0.9277 - recall: 0.9599 - true_negatives: 262.8889 - true_positives: 291.1945\n",
      "For batch number 37 the model has a loss of 0.22478941082954407\n",
      "\u001b[1m 37/689\u001b[0m \u001b[32m━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m36s\u001b[0m 57ms/step - auc: 0.9762 - binary_accuracy: 0.9403 - false_negatives: 13.8108 - false_positives: 25.2703 - loss: 0.2107 - precision: 0.9274 - recall: 0.9598 - true_negatives: 269.8108 - true_positives: 299.1081\n",
      "For batch number 38 the model has a loss of 0.22452901303768158\n",
      "\u001b[1m 38/689\u001b[0m \u001b[32m━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m36s\u001b[0m 57ms/step - auc: 0.9762 - binary_accuracy: 0.9401 - false_negatives: 14.1842 - false_positives: 26.0263 - loss: 0.2111 - precision: 0.9272 - recall: 0.9597 - true_negatives: 276.6053 - true_positives: 307.1842\n",
      "For batch number 39 the model has a loss of 0.2312445044517517\n",
      "\u001b[1m 39/689\u001b[0m \u001b[32m━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m36s\u001b[0m 57ms/step - auc: 0.9761 - binary_accuracy: 0.9399 - false_negatives: 14.6154 - false_positives: 26.7949 - loss: 0.2116 - precision: 0.9269 - recall: 0.9595 - true_negatives: 283.3333 - true_positives: 315.2564\n",
      "For batch number 40 the model has a loss of 0.22849316895008087\n",
      "\u001b[1m 40/689\u001b[0m \u001b[32m━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m37s\u001b[0m 57ms/step - auc: 0.9760 - binary_accuracy: 0.9397 - false_negatives: 15.0250 - false_positives: 27.5500 - loss: 0.2120 - precision: 0.9267 - recall: 0.9594 - true_negatives: 290.0500 - true_positives: 323.3750\n",
      "For batch number 41 the model has a loss of 0.22832240164279938\n",
      "\u001b[1m 41/689\u001b[0m \u001b[32m━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m37s\u001b[0m 57ms/step - auc: 0.9759 - binary_accuracy: 0.9395 - false_negatives: 15.4390 - false_positives: 28.2927 - loss: 0.2124 - precision: 0.9265 - recall: 0.9592 - true_negatives: 296.8781 - true_positives: 331.3903\n",
      "For batch number 42 the model has a loss of 0.2242908775806427\n",
      "\u001b[1m 42/689\u001b[0m \u001b[32m━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m37s\u001b[0m 57ms/step - auc: 0.9758 - binary_accuracy: 0.9393 - false_negatives: 15.8333 - false_positives: 29.0000 - loss: 0.2127 - precision: 0.9263 - recall: 0.9591 - true_negatives: 303.6905 - true_positives: 339.4762\n",
      "For batch number 43 the model has a loss of 0.221273273229599\n",
      "\u001b[1m 43/689\u001b[0m \u001b[32m━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m37s\u001b[0m 57ms/step - auc: 0.9757 - binary_accuracy: 0.9392 - false_negatives: 16.2093 - false_positives: 29.6744 - loss: 0.2129 - precision: 0.9263 - recall: 0.9590 - true_negatives: 310.4884 - true_positives: 347.6279\n",
      "For batch number 44 the model has a loss of 0.21916192770004272\n",
      "\u001b[1m 44/689\u001b[0m \u001b[32m━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m37s\u001b[0m 57ms/step - auc: 0.9757 - binary_accuracy: 0.9391 - false_negatives: 16.5682 - false_positives: 30.3409 - loss: 0.2131 - precision: 0.9262 - recall: 0.9590 - true_negatives: 317.2954 - true_positives: 355.7954\n",
      "For batch number 45 the model has a loss of 0.2196349948644638\n",
      "\u001b[1m 45/689\u001b[0m \u001b[32m━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m36s\u001b[0m 57ms/step - auc: 0.9757 - binary_accuracy: 0.9390 - false_negatives: 16.9111 - false_positives: 31.0222 - loss: 0.2132 - precision: 0.9261 - recall: 0.9589 - true_negatives: 324.1778 - true_positives: 363.8889\n",
      "For batch number 46 the model has a loss of 0.2225404679775238\n",
      "\u001b[1m 46/689\u001b[0m \u001b[32m━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m36s\u001b[0m 57ms/step - auc: 0.9756 - binary_accuracy: 0.9389 - false_negatives: 17.2826 - false_positives: 31.6957 - loss: 0.2134 - precision: 0.9260 - recall: 0.9589 - true_negatives: 331.0000 - true_positives: 372.0217\n",
      "For batch number 47 the model has a loss of 0.22302761673927307\n",
      "\u001b[1m 47/689\u001b[0m \u001b[32m━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m36s\u001b[0m 57ms/step - auc: 0.9755 - binary_accuracy: 0.9389 - false_negatives: 17.6596 - false_positives: 32.3617 - loss: 0.2136 - precision: 0.9259 - recall: 0.9588 - true_negatives: 337.7872 - true_positives: 380.1915\n",
      "For batch number 48 the model has a loss of 0.22492855787277222\n",
      "\u001b[1m 48/689\u001b[0m \u001b[32m━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m36s\u001b[0m 57ms/step - auc: 0.9755 - binary_accuracy: 0.9387 - false_negatives: 18.0208 - false_positives: 33.0833 - loss: 0.2138 - precision: 0.9258 - recall: 0.9588 - true_negatives: 344.6667 - true_positives: 388.2292\n",
      "For batch number 49 the model has a loss of 0.22243043780326843\n",
      "\u001b[1m 49/689\u001b[0m \u001b[32m━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m36s\u001b[0m 57ms/step - auc: 0.9754 - binary_accuracy: 0.9387 - false_negatives: 18.3673 - false_positives: 33.7959 - loss: 0.2140 - precision: 0.9257 - recall: 0.9587 - true_negatives: 351.6122 - true_positives: 396.2245\n",
      "For batch number 50 the model has a loss of 0.2207040637731552\n",
      "\u001b[1m 50/689\u001b[0m \u001b[32m━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m36s\u001b[0m 57ms/step - auc: 0.9754 - binary_accuracy: 0.9386 - false_negatives: 18.7200 - false_positives: 34.4800 - loss: 0.2142 - precision: 0.9256 - recall: 0.9587 - true_negatives: 358.5600 - true_positives: 404.2400\n",
      "For batch number 51 the model has a loss of 0.2190491408109665\n",
      "\u001b[1m 51/689\u001b[0m \u001b[32m━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m36s\u001b[0m 57ms/step - auc: 0.9753 - binary_accuracy: 0.9385 - false_negatives: 19.0784 - false_positives: 35.1373 - loss: 0.2143 - precision: 0.9256 - recall: 0.9586 - true_negatives: 365.3922 - true_positives: 412.3922\n",
      "For batch number 52 the model has a loss of 0.2203422337770462\n",
      "\u001b[1m 52/689\u001b[0m \u001b[32m━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m36s\u001b[0m 57ms/step - auc: 0.9753 - binary_accuracy: 0.9384 - false_negatives: 19.4423 - false_positives: 35.8269 - loss: 0.2144 - precision: 0.9255 - recall: 0.9586 - true_negatives: 372.1923 - true_positives: 420.5385\n",
      "For batch number 53 the model has a loss of 0.22358402609825134\n",
      "\u001b[1m 53/689\u001b[0m \u001b[32m━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m36s\u001b[0m 57ms/step - auc: 0.9752 - binary_accuracy: 0.9384 - false_negatives: 19.8113 - false_positives: 36.5283 - loss: 0.2145 - precision: 0.9254 - recall: 0.9586 - true_negatives: 379.0755 - true_positives: 428.5849\n",
      "For batch number 54 the model has a loss of 0.22152079641819\n",
      "\u001b[1m 54/689\u001b[0m \u001b[32m━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m36s\u001b[0m 57ms/step - auc: 0.9751 - binary_accuracy: 0.9383 - false_negatives: 20.1667 - false_positives: 37.2037 - loss: 0.2147 - precision: 0.9253 - recall: 0.9585 - true_negatives: 385.9445 - true_positives: 436.6852\n",
      "For batch number 55 the model has a loss of 0.22141820192337036\n",
      "\u001b[1m 55/689\u001b[0m \u001b[32m━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m36s\u001b[0m 57ms/step - auc: 0.9751 - binary_accuracy: 0.9382 - false_negatives: 20.5091 - false_positives: 37.9091 - loss: 0.2148 - precision: 0.9252 - recall: 0.9585 - true_negatives: 392.8546 - true_positives: 444.7273\n",
      "For batch number 56 the model has a loss of 0.22018857300281525\n",
      "\u001b[1m 56/689\u001b[0m \u001b[32m━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m36s\u001b[0m 57ms/step - auc: 0.9751 - binary_accuracy: 0.9382 - false_negatives: 20.8393 - false_positives: 38.6250 - loss: 0.2149 - precision: 0.9251 - recall: 0.9585 - true_negatives: 399.7500 - true_positives: 452.7857\n",
      "For batch number 57 the model has a loss of 0.22440779209136963\n",
      "\u001b[1m 57/689\u001b[0m \u001b[32m━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m36s\u001b[0m 57ms/step - auc: 0.9750 - binary_accuracy: 0.9381 - false_negatives: 21.1754 - false_positives: 39.3158 - loss: 0.2151 - precision: 0.9251 - recall: 0.9585 - true_negatives: 406.6316 - true_positives: 460.8772\n",
      "For batch number 58 the model has a loss of 0.23046784102916718\n",
      "\u001b[1m 58/689\u001b[0m \u001b[32m━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m36s\u001b[0m 57ms/step - auc: 0.9749 - binary_accuracy: 0.9380 - false_negatives: 21.5000 - false_positives: 40.1552 - loss: 0.2153 - precision: 0.9249 - recall: 0.9585 - true_negatives: 413.5345 - true_positives: 468.8103\n",
      "For batch number 59 the model has a loss of 0.22987937927246094\n",
      "\u001b[1m 59/689\u001b[0m \u001b[32m━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m36s\u001b[0m 57ms/step - auc: 0.9748 - binary_accuracy: 0.9379 - false_negatives: 21.8305 - false_positives: 41.0000 - loss: 0.2156 - precision: 0.9247 - recall: 0.9585 - true_negatives: 420.4237 - true_positives: 476.7458\n",
      "For batch number 60 the model has a loss of 0.22839003801345825\n",
      "\u001b[1m 60/689\u001b[0m \u001b[32m━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m35s\u001b[0m 57ms/step - auc: 0.9748 - binary_accuracy: 0.9378 - false_negatives: 22.1500 - false_positives: 41.8167 - loss: 0.2158 - precision: 0.9245 - recall: 0.9585 - true_negatives: 427.3667 - true_positives: 484.6667\n",
      "For batch number 61 the model has a loss of 0.2324831187725067\n",
      "\u001b[1m 61/689\u001b[0m \u001b[32m━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m35s\u001b[0m 57ms/step - auc: 0.9747 - binary_accuracy: 0.9376 - false_negatives: 22.4918 - false_positives: 42.7049 - loss: 0.2161 - precision: 0.9242 - recall: 0.9585 - true_negatives: 434.2787 - true_positives: 492.5246\n",
      "For batch number 62 the model has a loss of 0.23129481077194214\n",
      "\u001b[1m 62/689\u001b[0m \u001b[32m━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m35s\u001b[0m 57ms/step - auc: 0.9746 - binary_accuracy: 0.9375 - false_negatives: 22.8226 - false_positives: 43.5806 - loss: 0.2163 - precision: 0.9240 - recall: 0.9585 - true_negatives: 441.1452 - true_positives: 500.4516\n",
      "For batch number 63 the model has a loss of 0.23372644186019897\n",
      "\u001b[1m 63/689\u001b[0m \u001b[32m━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m35s\u001b[0m 57ms/step - auc: 0.9745 - binary_accuracy: 0.9373 - false_negatives: 23.1905 - false_positives: 44.4921 - loss: 0.2166 - precision: 0.9237 - recall: 0.9584 - true_negatives: 448.0000 - true_positives: 508.3175\n",
      "For batch number 64 the model has a loss of 0.23801305890083313\n",
      "\u001b[1m 64/689\u001b[0m \u001b[32m━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m35s\u001b[0m 57ms/step - auc: 0.9744 - binary_accuracy: 0.9371 - false_negatives: 23.5938 - false_positives: 45.4062 - loss: 0.2169 - precision: 0.9235 - recall: 0.9583 - true_negatives: 454.8281 - true_positives: 516.1719\n",
      "For batch number 65 the model has a loss of 0.236210897564888\n",
      "\u001b[1m 65/689\u001b[0m \u001b[32m━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m35s\u001b[0m 57ms/step - auc: 0.9743 - binary_accuracy: 0.9370 - false_negatives: 23.9846 - false_positives: 46.2923 - loss: 0.2172 - precision: 0.9233 - recall: 0.9583 - true_negatives: 461.6461 - true_positives: 524.0769\n",
      "For batch number 66 the model has a loss of 0.24065829813480377\n",
      "\u001b[1m 66/689\u001b[0m \u001b[32m━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m35s\u001b[0m 57ms/step - auc: 0.9742 - binary_accuracy: 0.9368 - false_negatives: 24.3788 - false_positives: 47.1970 - loss: 0.2176 - precision: 0.9230 - recall: 0.9582 - true_negatives: 468.4091 - true_positives: 532.0151\n",
      "For batch number 67 the model has a loss of 0.2391248345375061\n",
      "\u001b[1m 67/689\u001b[0m \u001b[32m━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m35s\u001b[0m 57ms/step - auc: 0.9740 - binary_accuracy: 0.9367 - false_negatives: 24.7761 - false_positives: 48.0896 - loss: 0.2179 - precision: 0.9228 - recall: 0.9582 - true_negatives: 475.1642 - true_positives: 539.9702\n",
      "For batch number 68 the model has a loss of 0.2400597333908081\n",
      "\u001b[1m 68/689\u001b[0m \u001b[32m━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m35s\u001b[0m 57ms/step - auc: 0.9739 - binary_accuracy: 0.9365 - false_negatives: 25.1765 - false_positives: 49.0147 - loss: 0.2182 - precision: 0.9226 - recall: 0.9581 - true_negatives: 481.9559 - true_positives: 547.8530\n",
      "For batch number 69 the model has a loss of 0.23991817235946655\n",
      "\u001b[1m 69/689\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m35s\u001b[0m 57ms/step - auc: 0.9738 - binary_accuracy: 0.9363 - false_negatives: 25.5652 - false_positives: 49.9275 - loss: 0.2185 - precision: 0.9224 - recall: 0.9581 - true_negatives: 488.7246 - true_positives: 555.7826\n",
      "For batch number 70 the model has a loss of 0.24104933440685272\n",
      "\u001b[1m 70/689\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m35s\u001b[0m 57ms/step - auc: 0.9737 - binary_accuracy: 0.9362 - false_negatives: 25.9714 - false_positives: 50.8143 - loss: 0.2189 - precision: 0.9222 - recall: 0.9580 - true_negatives: 495.4857 - true_positives: 563.7286\n",
      "For batch number 71 the model has a loss of 0.23921053111553192\n",
      "\u001b[1m 71/689\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m35s\u001b[0m 57ms/step - auc: 0.9736 - binary_accuracy: 0.9361 - false_negatives: 26.3662 - false_positives: 51.6761 - loss: 0.2191 - precision: 0.9220 - recall: 0.9579 - true_negatives: 502.3239 - true_positives: 571.6338\n",
      "For batch number 72 the model has a loss of 0.241084024310112\n",
      "\u001b[1m 72/689\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m35s\u001b[0m 57ms/step - auc: 0.9735 - binary_accuracy: 0.9359 - false_negatives: 26.7639 - false_positives: 52.5139 - loss: 0.2194 - precision: 0.9218 - recall: 0.9579 - true_negatives: 509.1389 - true_positives: 579.5833\n",
      "For batch number 73 the model has a loss of 0.24567104876041412\n",
      "\u001b[1m 73/689\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m35s\u001b[0m 57ms/step - auc: 0.9734 - binary_accuracy: 0.9358 - false_negatives: 27.1644 - false_positives: 53.3699 - loss: 0.2198 - precision: 0.9217 - recall: 0.9578 - true_negatives: 515.9863 - true_positives: 587.4794\n",
      "For batch number 74 the model has a loss of 0.2496437132358551\n",
      "\u001b[1m 74/689\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m34s\u001b[0m 57ms/step - auc: 0.9733 - binary_accuracy: 0.9357 - false_negatives: 27.5946 - false_positives: 54.2027 - loss: 0.2202 - precision: 0.9215 - recall: 0.9578 - true_negatives: 522.8108 - true_positives: 595.3919\n",
      "For batch number 75 the model has a loss of 0.2504982650279999\n",
      "\u001b[1m 75/689\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m34s\u001b[0m 57ms/step - auc: 0.9732 - binary_accuracy: 0.9356 - false_negatives: 28.0267 - false_positives: 55.0267 - loss: 0.2206 - precision: 0.9214 - recall: 0.9577 - true_negatives: 529.6533 - true_positives: 603.2933\n",
      "For batch number 76 the model has a loss of 0.25104865431785583\n",
      "\u001b[1m 76/689\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m34s\u001b[0m 57ms/step - auc: 0.9731 - binary_accuracy: 0.9354 - false_negatives: 28.4605 - false_positives: 55.8684 - loss: 0.2210 - precision: 0.9212 - recall: 0.9576 - true_negatives: 536.4868 - true_positives: 611.1842\n",
      "For batch number 77 the model has a loss of 0.24935363233089447\n",
      "\u001b[1m 77/689\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m34s\u001b[0m 57ms/step - auc: 0.9730 - binary_accuracy: 0.9353 - false_negatives: 28.8961 - false_positives: 56.6883 - loss: 0.2214 - precision: 0.9211 - recall: 0.9575 - true_negatives: 543.3246 - true_positives: 619.0909\n",
      "For batch number 78 the model has a loss of 0.24946922063827515\n",
      "\u001b[1m 78/689\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m34s\u001b[0m 57ms/step - auc: 0.9729 - binary_accuracy: 0.9352 - false_negatives: 29.3333 - false_positives: 57.5256 - loss: 0.2217 - precision: 0.9210 - recall: 0.9574 - true_negatives: 550.1667 - true_positives: 626.9744\n",
      "For batch number 79 the model has a loss of 0.24892479181289673\n",
      "\u001b[1m 79/689\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m34s\u001b[0m 57ms/step - auc: 0.9728 - binary_accuracy: 0.9351 - false_negatives: 29.7595 - false_positives: 58.3544 - loss: 0.2221 - precision: 0.9208 - recall: 0.9574 - true_negatives: 556.9620 - true_positives: 634.9241\n",
      "For batch number 80 the model has a loss of 0.24804718792438507\n",
      "\u001b[1m 80/689\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m34s\u001b[0m 57ms/step - auc: 0.9727 - binary_accuracy: 0.9350 - false_negatives: 30.1750 - false_positives: 59.1750 - loss: 0.2224 - precision: 0.9207 - recall: 0.9573 - true_negatives: 563.7625 - true_positives: 642.8875\n",
      "For batch number 81 the model has a loss of 0.2483702003955841\n",
      "\u001b[1m 81/689\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m34s\u001b[0m 57ms/step - auc: 0.9726 - binary_accuracy: 0.9349 - false_negatives: 30.5802 - false_positives: 60.0123 - loss: 0.2227 - precision: 0.9206 - recall: 0.9573 - true_negatives: 570.5679 - true_positives: 650.8395\n",
      "For batch number 82 the model has a loss of 0.2480040043592453\n",
      "\u001b[1m 82/689\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m34s\u001b[0m 57ms/step - auc: 0.9726 - binary_accuracy: 0.9348 - false_negatives: 30.9756 - false_positives: 60.8780 - loss: 0.2230 - precision: 0.9204 - recall: 0.9572 - true_negatives: 577.3658 - true_positives: 658.7805\n",
      "For batch number 83 the model has a loss of 0.24612578749656677\n",
      "\u001b[1m 83/689\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m34s\u001b[0m 57ms/step - auc: 0.9725 - binary_accuracy: 0.9347 - false_negatives: 31.3614 - false_positives: 61.7349 - loss: 0.2233 - precision: 0.9203 - recall: 0.9572 - true_negatives: 584.1927 - true_positives: 666.7108\n",
      "For batch number 84 the model has a loss of 0.24568162858486176\n",
      "\u001b[1m 84/689\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m34s\u001b[0m 57ms/step - auc: 0.9724 - binary_accuracy: 0.9346 - false_negatives: 31.7500 - false_positives: 62.6190 - loss: 0.2236 - precision: 0.9201 - recall: 0.9572 - true_negatives: 590.9762 - true_positives: 674.6548\n",
      "For batch number 85 the model has a loss of 0.24419443309307098\n",
      "\u001b[1m 85/689\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m34s\u001b[0m 57ms/step - auc: 0.9723 - binary_accuracy: 0.9345 - false_negatives: 32.1294 - false_positives: 63.4824 - loss: 0.2238 - precision: 0.9200 - recall: 0.9571 - true_negatives: 597.8118 - true_positives: 682.5765\n",
      "For batch number 86 the model has a loss of 0.24376101791858673\n",
      "\u001b[1m 86/689\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m34s\u001b[0m 57ms/step - auc: 0.9723 - binary_accuracy: 0.9344 - false_negatives: 32.5000 - false_positives: 64.3488 - loss: 0.2241 - precision: 0.9199 - recall: 0.9571 - true_negatives: 604.6860 - true_positives: 690.4651\n",
      "For batch number 87 the model has a loss of 0.24276867508888245\n",
      "\u001b[1m 87/689\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m34s\u001b[0m 57ms/step - auc: 0.9722 - binary_accuracy: 0.9343 - false_negatives: 32.8621 - false_positives: 65.2184 - loss: 0.2243 - precision: 0.9197 - recall: 0.9571 - true_negatives: 611.5517 - true_positives: 698.3678\n",
      "For batch number 88 the model has a loss of 0.24250997602939606\n",
      "\u001b[1m 88/689\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m34s\u001b[0m 57ms/step - auc: 0.9722 - binary_accuracy: 0.9342 - false_negatives: 33.2273 - false_positives: 66.0909 - loss: 0.2245 - precision: 0.9196 - recall: 0.9571 - true_negatives: 618.3977 - true_positives: 706.2841\n",
      "For batch number 89 the model has a loss of 0.24216195940971375\n",
      "\u001b[1m 89/689\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m34s\u001b[0m 57ms/step - auc: 0.9721 - binary_accuracy: 0.9341 - false_negatives: 33.5843 - false_positives: 66.9663 - loss: 0.2247 - precision: 0.9194 - recall: 0.9571 - true_negatives: 625.3033 - true_positives: 714.1461\n",
      "For batch number 90 the model has a loss of 0.24265062808990479\n",
      "\u001b[1m 90/689\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m34s\u001b[0m 57ms/step - auc: 0.9721 - binary_accuracy: 0.9340 - false_negatives: 33.9556 - false_positives: 67.8333 - loss: 0.2249 - precision: 0.9193 - recall: 0.9570 - true_negatives: 632.2222 - true_positives: 721.9889\n",
      "For batch number 91 the model has a loss of 0.24162665009498596\n",
      "\u001b[1m 91/689\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m33s\u001b[0m 57ms/step - auc: 0.9720 - binary_accuracy: 0.9340 - false_negatives: 34.3187 - false_positives: 68.7033 - loss: 0.2251 - precision: 0.9192 - recall: 0.9570 - true_negatives: 639.1429 - true_positives: 729.8351\n",
      "For batch number 92 the model has a loss of 0.24205158650875092\n",
      "\u001b[1m 92/689\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m33s\u001b[0m 57ms/step - auc: 0.9720 - binary_accuracy: 0.9339 - false_negatives: 34.6739 - false_positives: 69.5870 - loss: 0.2252 - precision: 0.9190 - recall: 0.9570 - true_negatives: 646.1087 - true_positives: 737.6304\n",
      "For batch number 93 the model has a loss of 0.2417135089635849\n",
      "\u001b[1m 93/689\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m33s\u001b[0m 57ms/step - auc: 0.9719 - binary_accuracy: 0.9338 - false_negatives: 35.0323 - false_positives: 70.4624 - loss: 0.2254 - precision: 0.9189 - recall: 0.9570 - true_negatives: 653.0860 - true_positives: 745.4194\n",
      "For batch number 94 the model has a loss of 0.24224358797073364\n",
      "\u001b[1m 94/689\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m33s\u001b[0m 57ms/step - auc: 0.9719 - binary_accuracy: 0.9337 - false_negatives: 35.4043 - false_positives: 71.3298 - loss: 0.2256 - precision: 0.9188 - recall: 0.9570 - true_negatives: 660.0638 - true_positives: 753.2021\n",
      "For batch number 95 the model has a loss of 0.2430827021598816\n",
      "\u001b[1m 95/689\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m33s\u001b[0m 57ms/step - auc: 0.9718 - binary_accuracy: 0.9336 - false_negatives: 35.7789 - false_positives: 72.2211 - loss: 0.2258 - precision: 0.9186 - recall: 0.9569 - true_negatives: 667.0211 - true_positives: 760.9789\n",
      "For batch number 96 the model has a loss of 0.24319277703762054\n",
      "\u001b[1m 96/689\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m33s\u001b[0m 57ms/step - auc: 0.9718 - binary_accuracy: 0.9335 - false_negatives: 36.1562 - false_positives: 73.1146 - loss: 0.2260 - precision: 0.9185 - recall: 0.9569 - true_negatives: 673.9583 - true_positives: 768.7708\n",
      "For batch number 97 the model has a loss of 0.24394607543945312\n",
      "\u001b[1m 97/689\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m33s\u001b[0m 57ms/step - auc: 0.9717 - binary_accuracy: 0.9334 - false_negatives: 36.5258 - false_positives: 74.0412 - loss: 0.2262 - precision: 0.9183 - recall: 0.9569 - true_negatives: 680.9382 - true_positives: 776.4949\n",
      "For batch number 98 the model has a loss of 0.2438018023967743\n",
      "\u001b[1m 98/689\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m33s\u001b[0m 57ms/step - auc: 0.9717 - binary_accuracy: 0.9334 - false_negatives: 36.8980 - false_positives: 74.9694 - loss: 0.2263 - precision: 0.9182 - recall: 0.9569 - true_negatives: 687.9490 - true_positives: 784.1837\n",
      "For batch number 99 the model has a loss of 0.24217860400676727\n",
      "\u001b[1m 99/689\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m33s\u001b[0m 57ms/step - auc: 0.9717 - binary_accuracy: 0.9333 - false_negatives: 37.2626 - false_positives: 75.8889 - loss: 0.2265 - precision: 0.9180 - recall: 0.9568 - true_negatives: 694.9798 - true_positives: 791.8687\n",
      "For batch number 100 the model has a loss of 0.24495507776737213\n",
      "\u001b[1m100/689\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m33s\u001b[0m 57ms/step - auc: 0.9716 - binary_accuracy: 0.9332 - false_negatives: 37.6400 - false_positives: 76.7900 - loss: 0.2267 - precision: 0.9179 - recall: 0.9568 - true_negatives: 701.9900 - true_positives: 799.5800\n",
      "For batch number 101 the model has a loss of 0.245194673538208\n",
      "\u001b[1m101/689\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m33s\u001b[0m 57ms/step - auc: 0.9716 - binary_accuracy: 0.9331 - false_negatives: 38.0198 - false_positives: 77.6832 - loss: 0.2269 - precision: 0.9177 - recall: 0.9568 - true_negatives: 709.0099 - true_positives: 807.2871\n",
      "For batch number 102 the model has a loss of 0.24556489288806915\n",
      "\u001b[1m102/689\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m33s\u001b[0m 57ms/step - auc: 0.9715 - binary_accuracy: 0.9330 - false_negatives: 38.3922 - false_positives: 78.5980 - loss: 0.2270 - precision: 0.9176 - recall: 0.9568 - true_negatives: 716.0490 - true_positives: 814.9608\n",
      "For batch number 103 the model has a loss of 0.24639806151390076\n",
      "\u001b[1m103/689\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m33s\u001b[0m 57ms/step - auc: 0.9715 - binary_accuracy: 0.9329 - false_negatives: 38.7670 - false_positives: 79.5243 - loss: 0.2272 - precision: 0.9174 - recall: 0.9567 - true_negatives: 723.0777 - true_positives: 822.6310\n",
      "For batch number 104 the model has a loss of 0.24637724459171295\n",
      "\u001b[1m104/689\u001b[0m \u001b[32m━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m33s\u001b[0m 57ms/step - auc: 0.9714 - binary_accuracy: 0.9328 - false_negatives: 39.1346 - false_positives: 80.4423 - loss: 0.2274 - precision: 0.9173 - recall: 0.9567 - true_negatives: 730.1442 - true_positives: 830.2789\n",
      "For batch number 105 the model has a loss of 0.2486218810081482\n",
      "\u001b[1m105/689\u001b[0m \u001b[32m━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m33s\u001b[0m 57ms/step - auc: 0.9714 - binary_accuracy: 0.9327 - false_negatives: 39.5238 - false_positives: 81.3714 - loss: 0.2276 - precision: 0.9171 - recall: 0.9567 - true_negatives: 737.1905 - true_positives: 837.9143\n",
      "For batch number 106 the model has a loss of 0.24854755401611328\n",
      "\u001b[1m106/689\u001b[0m \u001b[32m━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m33s\u001b[0m 57ms/step - auc: 0.9713 - binary_accuracy: 0.9327 - false_negatives: 39.9057 - false_positives: 82.3019 - loss: 0.2278 - precision: 0.9170 - recall: 0.9567 - true_negatives: 744.2547 - true_positives: 845.5377\n",
      "For batch number 107 the model has a loss of 0.24771590530872345\n",
      "\u001b[1m107/689\u001b[0m \u001b[32m━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m33s\u001b[0m 57ms/step - auc: 0.9713 - binary_accuracy: 0.9326 - false_negatives: 40.2804 - false_positives: 83.2150 - loss: 0.2280 - precision: 0.9169 - recall: 0.9566 - true_negatives: 751.3271 - true_positives: 853.1776\n",
      "For batch number 108 the model has a loss of 0.24765130877494812\n",
      "\u001b[1m108/689\u001b[0m \u001b[32m━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m33s\u001b[0m 57ms/step - auc: 0.9712 - binary_accuracy: 0.9325 - false_negatives: 40.6481 - false_positives: 84.1296 - loss: 0.2282 - precision: 0.9167 - recall: 0.9566 - true_negatives: 758.4074 - true_positives: 860.8148\n",
      "For batch number 109 the model has a loss of 0.24753597378730774\n",
      "\u001b[1m109/689\u001b[0m \u001b[32m━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m32s\u001b[0m 57ms/step - auc: 0.9712 - binary_accuracy: 0.9324 - false_negatives: 41.0275 - false_positives: 85.0275 - loss: 0.2284 - precision: 0.9166 - recall: 0.9566 - true_negatives: 765.4771 - true_positives: 868.4679\n",
      "For batch number 110 the model has a loss of 0.24726331233978271\n",
      "\u001b[1m110/689\u001b[0m \u001b[32m━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m32s\u001b[0m 57ms/step - auc: 0.9711 - binary_accuracy: 0.9323 - false_negatives: 41.4182 - false_positives: 85.9273 - loss: 0.2285 - precision: 0.9165 - recall: 0.9566 - true_negatives: 772.5546 - true_positives: 876.1000\n",
      "For batch number 111 the model has a loss of 0.24905215203762054\n",
      "\u001b[1m111/689\u001b[0m \u001b[32m━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m32s\u001b[0m 57ms/step - auc: 0.9711 - binary_accuracy: 0.9323 - false_negatives: 41.8288 - false_positives: 86.8288 - loss: 0.2287 - precision: 0.9163 - recall: 0.9565 - true_negatives: 779.6757 - true_positives: 883.6667\n",
      "For batch number 112 the model has a loss of 0.24861766397953033\n",
      "\u001b[1m112/689\u001b[0m \u001b[32m━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m32s\u001b[0m 57ms/step - auc: 0.9710 - binary_accuracy: 0.9322 - false_negatives: 42.2321 - false_positives: 87.7321 - loss: 0.2289 - precision: 0.9162 - recall: 0.9565 - true_negatives: 786.8036 - true_positives: 891.2321\n",
      "For batch number 113 the model has a loss of 0.2474587857723236\n",
      "\u001b[1m113/689\u001b[0m \u001b[32m━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m32s\u001b[0m 57ms/step - auc: 0.9710 - binary_accuracy: 0.9321 - false_negatives: 42.6372 - false_positives: 88.6195 - loss: 0.2291 - precision: 0.9161 - recall: 0.9564 - true_negatives: 793.9469 - true_positives: 898.7964\n",
      "For batch number 114 the model has a loss of 0.24620993435382843\n",
      "\u001b[1m114/689\u001b[0m \u001b[32m━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m32s\u001b[0m 57ms/step - auc: 0.9709 - binary_accuracy: 0.9320 - false_negatives: 43.0351 - false_positives: 89.5000 - loss: 0.2292 - precision: 0.9160 - recall: 0.9564 - true_negatives: 801.0965 - true_positives: 906.3684\n",
      "For batch number 115 the model has a loss of 0.2488550990819931\n",
      "\u001b[1m115/689\u001b[0m \u001b[32m━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m32s\u001b[0m 57ms/step - auc: 0.9709 - binary_accuracy: 0.9320 - false_negatives: 43.4522 - false_positives: 90.3826 - loss: 0.2294 - precision: 0.9159 - recall: 0.9564 - true_negatives: 808.2261 - true_positives: 913.9391\n",
      "For batch number 116 the model has a loss of 0.248687282204628\n",
      "\u001b[1m116/689\u001b[0m \u001b[32m━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m32s\u001b[0m 57ms/step - auc: 0.9709 - binary_accuracy: 0.9319 - false_negatives: 43.8621 - false_positives: 91.2759 - loss: 0.2295 - precision: 0.9158 - recall: 0.9563 - true_negatives: 815.3448 - true_positives: 921.5172\n",
      "For batch number 117 the model has a loss of 0.24956676363945007\n",
      "\u001b[1m117/689\u001b[0m \u001b[32m━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m32s\u001b[0m 57ms/step - auc: 0.9708 - binary_accuracy: 0.9318 - false_negatives: 44.2735 - false_positives: 92.1709 - loss: 0.2297 - precision: 0.9156 - recall: 0.9563 - true_negatives: 822.4786 - true_positives: 929.0769\n",
      "For batch number 118 the model has a loss of 0.24887913465499878\n",
      "\u001b[1m118/689\u001b[0m \u001b[32m━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m32s\u001b[0m 57ms/step - auc: 0.9708 - binary_accuracy: 0.9317 - false_negatives: 44.6864 - false_positives: 93.0508 - loss: 0.2299 - precision: 0.9155 - recall: 0.9562 - true_negatives: 829.5932 - true_positives: 936.6695\n",
      "For batch number 119 the model has a loss of 0.2500520944595337\n",
      "\u001b[1m119/689\u001b[0m \u001b[32m━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m32s\u001b[0m 57ms/step - auc: 0.9707 - binary_accuracy: 0.9317 - false_negatives: 45.1008 - false_positives: 93.9496 - loss: 0.2300 - precision: 0.9154 - recall: 0.9562 - true_negatives: 836.7227 - true_positives: 944.2269\n",
      "For batch number 120 the model has a loss of 0.25051021575927734\n",
      "\u001b[1m120/689\u001b[0m \u001b[32m━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m32s\u001b[0m 57ms/step - auc: 0.9706 - binary_accuracy: 0.9316 - false_negatives: 45.5083 - false_positives: 94.8583 - loss: 0.2302 - precision: 0.9153 - recall: 0.9562 - true_negatives: 843.8666 - true_positives: 951.7667\n",
      "For batch number 121 the model has a loss of 0.25203609466552734\n",
      "\u001b[1m121/689\u001b[0m \u001b[32m━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m32s\u001b[0m 57ms/step - auc: 0.9706 - binary_accuracy: 0.9315 - false_negatives: 45.9256 - false_positives: 95.7769 - loss: 0.2304 - precision: 0.9152 - recall: 0.9561 - true_negatives: 851.0000 - true_positives: 959.2975\n",
      "For batch number 122 the model has a loss of 0.25199148058891296\n",
      "\u001b[1m122/689\u001b[0m \u001b[32m━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m32s\u001b[0m 57ms/step - auc: 0.9705 - binary_accuracy: 0.9314 - false_negatives: 46.3525 - false_positives: 96.6885 - loss: 0.2306 - precision: 0.9151 - recall: 0.9561 - true_negatives: 858.1393 - true_positives: 966.8197\n",
      "For batch number 123 the model has a loss of 0.25252607464790344\n",
      "\u001b[1m123/689\u001b[0m \u001b[32m━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m32s\u001b[0m 57ms/step - auc: 0.9705 - binary_accuracy: 0.9313 - false_negatives: 46.7805 - false_positives: 97.6179 - loss: 0.2308 - precision: 0.9149 - recall: 0.9560 - true_negatives: 865.3090 - true_positives: 974.2927\n",
      "For batch number 124 the model has a loss of 0.25124117732048035\n",
      "\u001b[1m124/689\u001b[0m \u001b[32m━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m32s\u001b[0m 57ms/step - auc: 0.9704 - binary_accuracy: 0.9313 - false_negatives: 47.2016 - false_positives: 98.5323 - loss: 0.2309 - precision: 0.9148 - recall: 0.9560 - true_negatives: 872.4839 - true_positives: 981.7823\n",
      "For batch number 125 the model has a loss of 0.25103452801704407\n",
      "\u001b[1m125/689\u001b[0m \u001b[32m━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m31s\u001b[0m 57ms/step - auc: 0.9704 - binary_accuracy: 0.9312 - false_negatives: 47.6160 - false_positives: 99.4480 - loss: 0.2311 - precision: 0.9147 - recall: 0.9559 - true_negatives: 879.6960 - true_positives: 989.2400\n",
      "For batch number 126 the model has a loss of 0.2503840923309326\n",
      "\u001b[1m126/689\u001b[0m \u001b[32m━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m31s\u001b[0m 57ms/step - auc: 0.9703 - binary_accuracy: 0.9311 - false_negatives: 48.0238 - false_positives: 100.3571 - loss: 0.2312 - precision: 0.9146 - recall: 0.9559 - true_negatives: 886.8969 - true_positives: 996.7222\n",
      "For batch number 127 the model has a loss of 0.2503974437713623\n",
      "\u001b[1m127/689\u001b[0m \u001b[32m━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m31s\u001b[0m 57ms/step - auc: 0.9703 - binary_accuracy: 0.9311 - false_negatives: 48.4252 - false_positives: 101.2677 - loss: 0.2314 - precision: 0.9145 - recall: 0.9559 - true_negatives: 894.1260 - true_positives: 1004.1811\n",
      "For batch number 128 the model has a loss of 0.24986687302589417\n",
      "\u001b[1m128/689\u001b[0m \u001b[32m━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m31s\u001b[0m 57ms/step - auc: 0.9703 - binary_accuracy: 0.9310 - false_negatives: 48.8359 - false_positives: 102.1719 - loss: 0.2315 - precision: 0.9144 - recall: 0.9558 - true_negatives: 901.3594 - true_positives: 1011.6328\n",
      "For batch number 129 the model has a loss of 0.25021496415138245\n",
      "\u001b[1m129/689\u001b[0m \u001b[32m━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m31s\u001b[0m 57ms/step - auc: 0.9702 - binary_accuracy: 0.9309 - false_negatives: 49.2481 - false_positives: 103.0698 - loss: 0.2317 - precision: 0.9142 - recall: 0.9558 - true_negatives: 908.5969 - true_positives: 1019.0853\n",
      "For batch number 130 the model has a loss of 0.2496899515390396\n",
      "\u001b[1m130/689\u001b[0m \u001b[32m━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m31s\u001b[0m 57ms/step - auc: 0.9702 - binary_accuracy: 0.9309 - false_negatives: 49.6615 - false_positives: 103.9538 - loss: 0.2318 - precision: 0.9141 - recall: 0.9557 - true_negatives: 915.8231 - true_positives: 1026.5615\n",
      "For batch number 131 the model has a loss of 0.2486398071050644\n",
      "\u001b[1m131/689\u001b[0m \u001b[32m━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m31s\u001b[0m 57ms/step - auc: 0.9701 - binary_accuracy: 0.9308 - false_negatives: 50.0687 - false_positives: 104.8244 - loss: 0.2319 - precision: 0.9141 - recall: 0.9557 - true_negatives: 923.0764 - true_positives: 1034.0305\n",
      "For batch number 132 the model has a loss of 0.2485741674900055\n",
      "\u001b[1m132/689\u001b[0m \u001b[32m━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m31s\u001b[0m 57ms/step - auc: 0.9701 - binary_accuracy: 0.9308 - false_negatives: 50.4697 - false_positives: 105.6894 - loss: 0.2321 - precision: 0.9140 - recall: 0.9557 - true_negatives: 930.3333 - true_positives: 1041.5076\n",
      "For batch number 133 the model has a loss of 0.24847939610481262\n",
      "\u001b[1m133/689\u001b[0m \u001b[32m━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m31s\u001b[0m 57ms/step - auc: 0.9701 - binary_accuracy: 0.9307 - false_negatives: 50.8722 - false_positives: 106.5639 - loss: 0.2322 - precision: 0.9139 - recall: 0.9556 - true_negatives: 937.5940 - true_positives: 1048.9700\n",
      "For batch number 134 the model has a loss of 0.24782952666282654\n",
      "\u001b[1m134/689\u001b[0m \u001b[32m━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m31s\u001b[0m 57ms/step - auc: 0.9700 - binary_accuracy: 0.9306 - false_negatives: 51.2910 - false_positives: 107.4254 - loss: 0.2323 - precision: 0.9138 - recall: 0.9556 - true_negatives: 944.8508 - true_positives: 1056.4329\n",
      "For batch number 135 the model has a loss of 0.24710914492607117\n",
      "\u001b[1m135/689\u001b[0m \u001b[32m━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m31s\u001b[0m 57ms/step - auc: 0.9700 - binary_accuracy: 0.9306 - false_negatives: 51.7037 - false_positives: 108.2741 - loss: 0.2324 - precision: 0.9137 - recall: 0.9556 - true_negatives: 952.1185 - true_positives: 1063.9037\n",
      "For batch number 136 the model has a loss of 0.2492680549621582\n",
      "\u001b[1m136/689\u001b[0m \u001b[32m━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m31s\u001b[0m 57ms/step - auc: 0.9699 - binary_accuracy: 0.9305 - false_negatives: 52.1176 - false_positives: 109.1176 - loss: 0.2325 - precision: 0.9136 - recall: 0.9555 - true_negatives: 959.3677 - true_positives: 1071.3971\n",
      "For batch number 137 the model has a loss of 0.2502054274082184\n",
      "\u001b[1m137/689\u001b[0m \u001b[32m━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m31s\u001b[0m 57ms/step - auc: 0.9699 - binary_accuracy: 0.9305 - false_negatives: 52.5328 - false_positives: 109.9562 - loss: 0.2327 - precision: 0.9136 - recall: 0.9555 - true_negatives: 966.5912 - true_positives: 1078.9197\n",
      "For batch number 138 the model has a loss of 0.24973440170288086\n",
      "\u001b[1m138/689\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m31s\u001b[0m 57ms/step - auc: 0.9699 - binary_accuracy: 0.9305 - false_negatives: 52.9493 - false_positives: 110.7826 - loss: 0.2328 - precision: 0.9135 - recall: 0.9555 - true_negatives: 973.8116 - true_positives: 1086.4565\n",
      "For batch number 139 the model has a loss of 0.2504729628562927\n",
      "\u001b[1m139/689\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m31s\u001b[0m 57ms/step - auc: 0.9698 - binary_accuracy: 0.9304 - false_negatives: 53.3669 - false_positives: 111.6187 - loss: 0.2329 - precision: 0.9134 - recall: 0.9554 - true_negatives: 981.0504 - true_positives: 1093.9640\n",
      "For batch number 140 the model has a loss of 0.25115254521369934\n",
      "\u001b[1m140/689\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m31s\u001b[0m 57ms/step - auc: 0.9698 - binary_accuracy: 0.9304 - false_negatives: 53.7857 - false_positives: 112.4643 - loss: 0.2331 - precision: 0.9133 - recall: 0.9554 - true_negatives: 988.2928 - true_positives: 1101.4572\n",
      "For batch number 141 the model has a loss of 0.2534372806549072\n",
      "\u001b[1m141/689\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m31s\u001b[0m 57ms/step - auc: 0.9698 - binary_accuracy: 0.9303 - false_negatives: 54.2128 - false_positives: 113.3050 - loss: 0.2332 - precision: 0.9133 - recall: 0.9554 - true_negatives: 995.5461 - true_positives: 1108.9362\n",
      "For batch number 142 the model has a loss of 0.2527983486652374\n",
      "\u001b[1m142/689\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m30s\u001b[0m 57ms/step - auc: 0.9697 - binary_accuracy: 0.9303 - false_negatives: 54.6338 - false_positives: 114.1338 - loss: 0.2333 - precision: 0.9132 - recall: 0.9553 - true_negatives: 1002.8028 - true_positives: 1116.4296\n",
      "For batch number 143 the model has a loss of 0.25214195251464844\n",
      "\u001b[1m143/689\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m30s\u001b[0m 57ms/step - auc: 0.9697 - binary_accuracy: 0.9302 - false_negatives: 55.0490 - false_positives: 114.9580 - loss: 0.2335 - precision: 0.9131 - recall: 0.9553 - true_negatives: 1010.0560 - true_positives: 1123.9370\n",
      "For batch number 144 the model has a loss of 0.25270694494247437\n",
      "\u001b[1m144/689\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m30s\u001b[0m 57ms/step - auc: 0.9696 - binary_accuracy: 0.9302 - false_negatives: 55.4583 - false_positives: 115.8056 - loss: 0.2336 - precision: 0.9131 - recall: 0.9553 - true_negatives: 1017.2847 - true_positives: 1131.4514\n",
      "For batch number 145 the model has a loss of 0.25279784202575684\n",
      "\u001b[1m145/689\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m30s\u001b[0m 57ms/step - auc: 0.9696 - binary_accuracy: 0.9301 - false_negatives: 55.8621 - false_positives: 116.6621 - loss: 0.2337 - precision: 0.9130 - recall: 0.9552 - true_negatives: 1024.5034 - true_positives: 1138.9724\n",
      "For batch number 146 the model has a loss of 0.252005934715271\n",
      "\u001b[1m146/689\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m30s\u001b[0m 57ms/step - auc: 0.9696 - binary_accuracy: 0.9301 - false_negatives: 56.2603 - false_positives: 117.5137 - loss: 0.2339 - precision: 0.9129 - recall: 0.9552 - true_negatives: 1031.7466 - true_positives: 1146.4795\n",
      "For batch number 147 the model has a loss of 0.2520088851451874\n",
      "\u001b[1m147/689\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m30s\u001b[0m 57ms/step - auc: 0.9695 - binary_accuracy: 0.9301 - false_negatives: 56.6531 - false_positives: 118.3673 - loss: 0.2340 - precision: 0.9129 - recall: 0.9552 - true_negatives: 1038.9524 - true_positives: 1154.0272\n",
      "For batch number 148 the model has a loss of 0.251991331577301\n",
      "\u001b[1m148/689\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m30s\u001b[0m 57ms/step - auc: 0.9695 - binary_accuracy: 0.9300 - false_negatives: 57.0473 - false_positives: 119.2230 - loss: 0.2341 - precision: 0.9128 - recall: 0.9552 - true_negatives: 1046.1487 - true_positives: 1161.5811\n",
      "For batch number 149 the model has a loss of 0.2519546449184418\n",
      "\u001b[1m149/689\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m30s\u001b[0m 57ms/step - auc: 0.9695 - binary_accuracy: 0.9300 - false_negatives: 57.4430 - false_positives: 120.0872 - loss: 0.2342 - precision: 0.9127 - recall: 0.9551 - true_negatives: 1053.3490 - true_positives: 1169.1208\n",
      "For batch number 150 the model has a loss of 0.2517052888870239\n",
      "\u001b[1m150/689\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m30s\u001b[0m 57ms/step - auc: 0.9694 - binary_accuracy: 0.9299 - false_negatives: 57.8400 - false_positives: 120.9600 - loss: 0.2343 - precision: 0.9126 - recall: 0.9551 - true_negatives: 1060.5333 - true_positives: 1176.6666\n",
      "For batch number 151 the model has a loss of 0.250916987657547\n",
      "\u001b[1m151/689\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m30s\u001b[0m 57ms/step - auc: 0.9694 - binary_accuracy: 0.9299 - false_negatives: 58.2318 - false_positives: 121.8212 - loss: 0.2344 - precision: 0.9126 - recall: 0.9551 - true_negatives: 1067.7086 - true_positives: 1184.2384\n",
      "For batch number 152 the model has a loss of 0.25039100646972656\n",
      "\u001b[1m152/689\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m30s\u001b[0m 57ms/step - auc: 0.9694 - binary_accuracy: 0.9298 - false_negatives: 58.6184 - false_positives: 122.6711 - loss: 0.2346 - precision: 0.9125 - recall: 0.9551 - true_negatives: 1074.8882 - true_positives: 1191.8224\n",
      "For batch number 153 the model has a loss of 0.250381201505661\n",
      "\u001b[1m153/689\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m30s\u001b[0m 57ms/step - auc: 0.9693 - binary_accuracy: 0.9298 - false_negatives: 59.0000 - false_positives: 123.5294 - loss: 0.2347 - precision: 0.9124 - recall: 0.9551 - true_negatives: 1082.0785 - true_positives: 1199.3922\n",
      "For batch number 154 the model has a loss of 0.24985261261463165\n",
      "\u001b[1m154/689\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m30s\u001b[0m 57ms/step - auc: 0.9693 - binary_accuracy: 0.9298 - false_negatives: 59.3766 - false_positives: 124.3896 - loss: 0.2348 - precision: 0.9124 - recall: 0.9550 - true_negatives: 1089.2662 - true_positives: 1206.9675\n",
      "For batch number 155 the model has a loss of 0.2501164972782135\n",
      "\u001b[1m155/689\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m30s\u001b[0m 57ms/step - auc: 0.9693 - binary_accuracy: 0.9297 - false_negatives: 59.7548 - false_positives: 125.2516 - loss: 0.2349 - precision: 0.9123 - recall: 0.9550 - true_negatives: 1096.4580 - true_positives: 1214.5355\n",
      "For batch number 156 the model has a loss of 0.24999062716960907\n",
      "\u001b[1m156/689\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m30s\u001b[0m 57ms/step - auc: 0.9692 - binary_accuracy: 0.9297 - false_negatives: 60.1282 - false_positives: 126.1154 - loss: 0.2350 - precision: 0.9122 - recall: 0.9550 - true_negatives: 1103.6475 - true_positives: 1222.1090\n",
      "For batch number 157 the model has a loss of 0.24954065680503845\n",
      "\u001b[1m157/689\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m30s\u001b[0m 57ms/step - auc: 0.9692 - binary_accuracy: 0.9297 - false_negatives: 60.4968 - false_positives: 126.9682 - loss: 0.2350 - precision: 0.9122 - recall: 0.9550 - true_negatives: 1110.8408 - true_positives: 1229.6942\n",
      "For batch number 158 the model has a loss of 0.24900966882705688\n",
      "\u001b[1m158/689\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m30s\u001b[0m 57ms/step - auc: 0.9692 - binary_accuracy: 0.9296 - false_negatives: 60.8608 - false_positives: 127.8228 - loss: 0.2351 - precision: 0.9121 - recall: 0.9550 - true_negatives: 1118.0380 - true_positives: 1237.2784\n",
      "For batch number 159 the model has a loss of 0.24882742762565613\n",
      "\u001b[1m159/689\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m30s\u001b[0m 57ms/step - auc: 0.9692 - binary_accuracy: 0.9296 - false_negatives: 61.2201 - false_positives: 128.6855 - loss: 0.2352 - precision: 0.9121 - recall: 0.9550 - true_negatives: 1125.2264 - true_positives: 1244.8679\n",
      "For batch number 160 the model has a loss of 0.24814558029174805\n",
      "\u001b[1m160/689\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m29s\u001b[0m 57ms/step - auc: 0.9691 - binary_accuracy: 0.9296 - false_negatives: 61.5750 - false_positives: 129.5375 - loss: 0.2353 - precision: 0.9120 - recall: 0.9550 - true_negatives: 1132.4125 - true_positives: 1252.4750\n",
      "For batch number 161 the model has a loss of 0.24753397703170776\n",
      "\u001b[1m161/689\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m29s\u001b[0m 57ms/step - auc: 0.9691 - binary_accuracy: 0.9296 - false_negatives: 61.9255 - false_positives: 130.3851 - loss: 0.2354 - precision: 0.9120 - recall: 0.9550 - true_negatives: 1139.6211 - true_positives: 1260.0684\n",
      "For batch number 162 the model has a loss of 0.24723368883132935\n",
      "\u001b[1m162/689\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m29s\u001b[0m 57ms/step - auc: 0.9691 - binary_accuracy: 0.9295 - false_negatives: 62.2716 - false_positives: 131.2407 - loss: 0.2354 - precision: 0.9119 - recall: 0.9550 - true_negatives: 1146.8210 - true_positives: 1267.6666\n",
      "For batch number 163 the model has a loss of 0.24865776300430298\n",
      "\u001b[1m163/689\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m29s\u001b[0m 57ms/step - auc: 0.9691 - binary_accuracy: 0.9295 - false_negatives: 62.6196 - false_positives: 132.0859 - loss: 0.2355 - precision: 0.9118 - recall: 0.9550 - true_negatives: 1154.0000 - true_positives: 1275.2944\n",
      "For batch number 164 the model has a loss of 0.24897566437721252\n",
      "\u001b[1m164/689\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m29s\u001b[0m 57ms/step - auc: 0.9690 - binary_accuracy: 0.9295 - false_negatives: 62.9756 - false_positives: 132.9268 - loss: 0.2356 - precision: 0.9118 - recall: 0.9550 - true_negatives: 1161.1464 - true_positives: 1282.9512\n",
      "For batch number 165 the model has a loss of 0.24893994629383087\n",
      "\u001b[1m165/689\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m29s\u001b[0m 57ms/step - auc: 0.9690 - binary_accuracy: 0.9295 - false_negatives: 63.3273 - false_positives: 133.7818 - loss: 0.2357 - precision: 0.9117 - recall: 0.9550 - true_negatives: 1168.3030 - true_positives: 1290.5879\n",
      "For batch number 166 the model has a loss of 0.2490759938955307\n",
      "\u001b[1m166/689\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m29s\u001b[0m 57ms/step - auc: 0.9690 - binary_accuracy: 0.9294 - false_negatives: 63.6747 - false_positives: 134.6506 - loss: 0.2358 - precision: 0.9117 - recall: 0.9550 - true_negatives: 1175.4698 - true_positives: 1298.2048\n",
      "For batch number 167 the model has a loss of 0.2504834830760956\n",
      "\u001b[1m167/689\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m29s\u001b[0m 57ms/step - auc: 0.9690 - binary_accuracy: 0.9294 - false_negatives: 64.0479 - false_positives: 135.5210 - loss: 0.2359 - precision: 0.9116 - recall: 0.9550 - true_negatives: 1182.6348 - true_positives: 1305.7964\n",
      "For batch number 168 the model has a loss of 0.2499646693468094\n",
      "\u001b[1m168/689\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m29s\u001b[0m 57ms/step - auc: 0.9689 - binary_accuracy: 0.9294 - false_negatives: 64.4167 - false_positives: 136.3929 - loss: 0.2359 - precision: 0.9116 - recall: 0.9549 - true_negatives: 1189.8036 - true_positives: 1313.3870\n",
      "For batch number 169 the model has a loss of 0.24910952150821686\n",
      "\u001b[1m169/689\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m29s\u001b[0m 57ms/step - auc: 0.9689 - binary_accuracy: 0.9293 - false_negatives: 64.7811 - false_positives: 137.2544 - loss: 0.2360 - precision: 0.9115 - recall: 0.9549 - true_negatives: 1196.9882 - true_positives: 1320.9763\n",
      "For batch number 170 the model has a loss of 0.24867115914821625\n",
      "\u001b[1m170/689\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m29s\u001b[0m 57ms/step - auc: 0.9689 - binary_accuracy: 0.9293 - false_negatives: 65.1412 - false_positives: 138.1118 - loss: 0.2361 - precision: 0.9114 - recall: 0.9549 - true_negatives: 1204.1588 - true_positives: 1328.5883\n",
      "For batch number 171 the model has a loss of 0.24851231276988983\n",
      "\u001b[1m171/689\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m29s\u001b[0m 57ms/step - auc: 0.9689 - binary_accuracy: 0.9293 - false_negatives: 65.4971 - false_positives: 138.9708 - loss: 0.2362 - precision: 0.9114 - recall: 0.9549 - true_negatives: 1211.3392 - true_positives: 1336.1930\n",
      "For batch number 172 the model has a loss of 0.2486049234867096\n",
      "\u001b[1m172/689\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m29s\u001b[0m 57ms/step - auc: 0.9689 - binary_accuracy: 0.9293 - false_negatives: 65.8605 - false_positives: 139.8198 - loss: 0.2362 - precision: 0.9113 - recall: 0.9549 - true_negatives: 1218.5059 - true_positives: 1343.8140\n",
      "For batch number 173 the model has a loss of 0.24907918274402618\n",
      "\u001b[1m173/689\u001b[0m \u001b[32m━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━\u001b[0m \u001b[1m29s\u001b[0m 57ms/step - auc: 0.9688 - binary_accuracy: 0.9292 - false_negatives: 66.2254 - false_positives: 140.6647 - loss: 0.2363 - precision: 0.9113 - recall: 0.9549 - true_negatives: 1225.6821 - true_positives: 1351.4277\n",
      "For batch number 174 the model has a loss of 0.24920117855072021\n",
      "\u001b[1m174/689\u001b[0m \u001b[32m━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━\u001b[0m \u001b[1m29s\u001b[0m 57ms/step - auc: 0.9688 - binary_accuracy: 0.9292 - false_negatives: 66.5920 - false_positives: 141.5172 - loss: 0.2364 - precision: 0.9112 - recall: 0.9549 - true_negatives: 1232.8793 - true_positives: 1359.0115\n",
      "For batch number 175 the model has a loss of 0.25277137756347656\n",
      "\u001b[1m175/689\u001b[0m \u001b[32m━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━\u001b[0m \u001b[1m29s\u001b[0m 57ms/step - auc: 0.9688 - binary_accuracy: 0.9292 - false_negatives: 67.0057 - false_positives: 142.3829 - loss: 0.2365 - precision: 0.9112 - recall: 0.9549 - true_negatives: 1240.0571 - true_positives: 1366.5543\n",
      "For batch number 176 the model has a loss of 0.2523730993270874\n",
      "\u001b[1m176/689\u001b[0m \u001b[32m━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━\u001b[0m \u001b[1m28s\u001b[0m 57ms/step - auc: 0.9688 - binary_accuracy: 0.9291 - false_negatives: 67.4148 - false_positives: 143.2500 - loss: 0.2366 - precision: 0.9111 - recall: 0.9549 - true_negatives: 1247.2443 - true_positives: 1374.0909\n",
      "For batch number 177 the model has a loss of 0.2539898157119751\n",
      "\u001b[1m177/689\u001b[0m \u001b[32m━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━\u001b[0m \u001b[1m28s\u001b[0m 57ms/step - auc: 0.9687 - binary_accuracy: 0.9291 - false_negatives: 67.8305 - false_positives: 144.1243 - loss: 0.2367 - precision: 0.9111 - recall: 0.9548 - true_negatives: 1254.4237 - true_positives: 1381.6215\n",
      "For batch number 178 the model has a loss of 0.25394099950790405\n",
      "\u001b[1m178/689\u001b[0m \u001b[32m━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━\u001b[0m \u001b[1m28s\u001b[0m 57ms/step - auc: 0.9687 - binary_accuracy: 0.9291 - false_negatives: 68.2472 - false_positives: 145.0056 - loss: 0.2368 - precision: 0.9110 - recall: 0.9548 - true_negatives: 1261.6067 - true_positives: 1389.1405\n",
      "For batch number 179 the model has a loss of 0.25420090556144714\n",
      "\u001b[1m179/689\u001b[0m \u001b[32m━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━\u001b[0m \u001b[1m28s\u001b[0m 57ms/step - auc: 0.9687 - binary_accuracy: 0.9290 - false_negatives: 68.6648 - false_positives: 145.8939 - loss: 0.2369 - precision: 0.9109 - recall: 0.9548 - true_negatives: 1268.7877 - true_positives: 1396.6537\n",
      "For batch number 180 the model has a loss of 0.2536250352859497\n",
      "\u001b[1m180/689\u001b[0m \u001b[32m━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━\u001b[0m \u001b[1m28s\u001b[0m 56ms/step - auc: 0.9686 - binary_accuracy: 0.9290 - false_negatives: 69.0778 - false_positives: 146.7722 - loss: 0.2370 - precision: 0.9109 - recall: 0.9548 - true_negatives: 1275.9667 - true_positives: 1404.1833\n",
      "For batch number 181 the model has a loss of 0.2532331943511963\n",
      "\u001b[1m181/689\u001b[0m \u001b[32m━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━\u001b[0m \u001b[1m28s\u001b[0m 56ms/step - auc: 0.9686 - binary_accuracy: 0.9289 - false_negatives: 69.4917 - false_positives: 147.6464 - loss: 0.2370 - precision: 0.9108 - recall: 0.9547 - true_negatives: 1283.1492 - true_positives: 1411.7126\n",
      "For batch number 182 the model has a loss of 0.2532830834388733\n",
      "\u001b[1m182/689\u001b[0m \u001b[32m━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━\u001b[0m \u001b[1m28s\u001b[0m 56ms/step - auc: 0.9686 - binary_accuracy: 0.9289 - false_negatives: 69.9011 - false_positives: 148.5275 - loss: 0.2371 - precision: 0.9108 - recall: 0.9547 - true_negatives: 1290.3352 - true_positives: 1419.2362\n",
      "For batch number 183 the model has a loss of 0.2526888847351074\n",
      "\u001b[1m183/689\u001b[0m \u001b[32m━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━\u001b[0m \u001b[1m28s\u001b[0m 57ms/step - auc: 0.9686 - binary_accuracy: 0.9289 - false_negatives: 70.3060 - false_positives: 149.4044 - loss: 0.2372 - precision: 0.9107 - recall: 0.9547 - true_negatives: 1297.5245 - true_positives: 1426.7650\n",
      "For batch number 184 the model has a loss of 0.2530154287815094\n",
      "\u001b[1m184/689\u001b[0m \u001b[32m━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━\u001b[0m \u001b[1m28s\u001b[0m 57ms/step - auc: 0.9686 - binary_accuracy: 0.9288 - false_negatives: 70.7174 - false_positives: 150.2772 - loss: 0.2373 - precision: 0.9107 - recall: 0.9547 - true_negatives: 1304.7065 - true_positives: 1434.2990\n",
      "For batch number 185 the model has a loss of 0.25524115562438965\n",
      "\u001b[1m185/689\u001b[0m \u001b[32m━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━\u001b[0m \u001b[1m28s\u001b[0m 56ms/step - auc: 0.9685 - binary_accuracy: 0.9288 - false_negatives: 71.1297 - false_positives: 151.1730 - loss: 0.2374 - precision: 0.9106 - recall: 0.9547 - true_negatives: 1311.8865 - true_positives: 1441.8108\n",
      "For batch number 186 the model has a loss of 0.25467777252197266\n",
      "\u001b[1m186/689\u001b[0m \u001b[32m━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━\u001b[0m \u001b[1m28s\u001b[0m 56ms/step - auc: 0.9685 - binary_accuracy: 0.9288 - false_negatives: 71.5376 - false_positives: 152.0699 - loss: 0.2375 - precision: 0.9105 - recall: 0.9546 - true_negatives: 1319.0646 - true_positives: 1449.3280\n",
      "For batch number 187 the model has a loss of 0.2551470994949341\n",
      "\u001b[1m187/689\u001b[0m \u001b[32m━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━\u001b[0m \u001b[1m28s\u001b[0m 56ms/step - auc: 0.9685 - binary_accuracy: 0.9287 - false_negatives: 71.9465 - false_positives: 152.9733 - loss: 0.2376 - precision: 0.9105 - recall: 0.9546 - true_negatives: 1326.2139 - true_positives: 1456.8663\n",
      "For batch number 188 the model has a loss of 0.254128634929657\n",
      "\u001b[1m188/689\u001b[0m \u001b[32m━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━\u001b[0m \u001b[1m28s\u001b[0m 56ms/step - auc: 0.9685 - binary_accuracy: 0.9287 - false_negatives: 72.3511 - false_positives: 153.8670 - loss: 0.2377 - precision: 0.9104 - recall: 0.9546 - true_negatives: 1333.3617 - true_positives: 1464.4202\n",
      "For batch number 189 the model has a loss of 0.2546997666358948\n",
      "\u001b[1m189/689\u001b[0m \u001b[32m━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━\u001b[0m \u001b[1m28s\u001b[0m 56ms/step - auc: 0.9684 - binary_accuracy: 0.9287 - false_negatives: 72.7566 - false_positives: 154.7513 - loss: 0.2378 - precision: 0.9104 - recall: 0.9546 - true_negatives: 1340.5027 - true_positives: 1471.9894\n",
      "For batch number 190 the model has a loss of 0.253883957862854\n",
      "\u001b[1m190/689\u001b[0m \u001b[32m━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━\u001b[0m \u001b[1m28s\u001b[0m 56ms/step - auc: 0.9684 - binary_accuracy: 0.9286 - false_negatives: 73.1579 - false_positives: 155.6316 - loss: 0.2379 - precision: 0.9103 - recall: 0.9546 - true_negatives: 1347.6316 - true_positives: 1479.5790\n",
      "For batch number 191 the model has a loss of 0.2535393536090851\n",
      "\u001b[1m191/689\u001b[0m \u001b[32m━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━\u001b[0m \u001b[1m28s\u001b[0m 56ms/step - auc: 0.9684 - binary_accuracy: 0.9286 - false_negatives: 73.5602 - false_positives: 156.5079 - loss: 0.2379 - precision: 0.9103 - recall: 0.9545 - true_negatives: 1354.7697 - true_positives: 1487.1624\n",
      "For batch number 192 the model has a loss of 0.2538534104824066\n",
      "\u001b[1m192/689\u001b[0m \u001b[32m━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━\u001b[0m \u001b[1m28s\u001b[0m 56ms/step - auc: 0.9684 - binary_accuracy: 0.9286 - false_negatives: 73.9635 - false_positives: 157.3750 - loss: 0.2380 - precision: 0.9102 - recall: 0.9545 - true_negatives: 1361.9062 - true_positives: 1494.7552\n",
      "For batch number 193 the model has a loss of 0.25370466709136963\n",
      "\u001b[1m193/689\u001b[0m \u001b[32m━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━\u001b[0m \u001b[1m28s\u001b[0m 56ms/step - auc: 0.9683 - binary_accuracy: 0.9285 - false_negatives: 74.3679 - false_positives: 158.2383 - loss: 0.2381 - precision: 0.9102 - recall: 0.9545 - true_negatives: 1369.0311 - true_positives: 1502.3627\n",
      "For batch number 194 the model has a loss of 0.25370264053344727\n",
      "\u001b[1m194/689\u001b[0m \u001b[32m━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━\u001b[0m \u001b[1m27s\u001b[0m 57ms/step - auc: 0.9683 - binary_accuracy: 0.9285 - false_negatives: 74.7732 - false_positives: 159.1031 - loss: 0.2382 - precision: 0.9101 - recall: 0.9545 - true_negatives: 1376.1649 - true_positives: 1509.9587\n",
      "For batch number 195 the model has a loss of 0.25460925698280334\n",
      "\u001b[1m195/689\u001b[0m \u001b[32m━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━\u001b[0m \u001b[1m27s\u001b[0m 57ms/step - auc: 0.9683 - binary_accuracy: 0.9285 - false_negatives: 75.1795 - false_positives: 159.9744 - loss: 0.2383 - precision: 0.9101 - recall: 0.9545 - true_negatives: 1383.2924 - true_positives: 1517.5538\n",
      "For batch number 196 the model has a loss of 0.25353747606277466\n",
      "\u001b[1m196/689\u001b[0m \u001b[32m━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━\u001b[0m \u001b[1m27s\u001b[0m 57ms/step - auc: 0.9683 - binary_accuracy: 0.9285 - false_negatives: 75.5816 - false_positives: 160.8367 - loss: 0.2383 - precision: 0.9100 - recall: 0.9545 - true_negatives: 1390.4286 - true_positives: 1525.1531\n",
      "For batch number 197 the model has a loss of 0.25410738587379456\n",
      "\u001b[1m197/689\u001b[0m \u001b[32m━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━\u001b[0m \u001b[1m27s\u001b[0m 57ms/step - auc: 0.9683 - binary_accuracy: 0.9284 - false_negatives: 75.9949 - false_positives: 161.7005 - loss: 0.2384 - precision: 0.9100 - recall: 0.9544 - true_negatives: 1397.5482 - true_positives: 1532.7563\n",
      "For batch number 198 the model has a loss of 0.2544087767601013\n",
      "\u001b[1m198/689\u001b[0m \u001b[32m━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━\u001b[0m \u001b[1m27s\u001b[0m 57ms/step - auc: 0.9682 - binary_accuracy: 0.9284 - false_negatives: 76.4091 - false_positives: 162.5707 - loss: 0.2385 - precision: 0.9099 - recall: 0.9544 - true_negatives: 1404.6666 - true_positives: 1540.3535\n",
      "For batch number 199 the model has a loss of 0.25511205196380615\n",
      "\u001b[1m199/689\u001b[0m \u001b[32m━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━\u001b[0m \u001b[1m27s\u001b[0m 57ms/step - auc: 0.9682 - binary_accuracy: 0.9284 - false_negatives: 76.8291 - false_positives: 163.4422 - loss: 0.2386 - precision: 0.9099 - recall: 0.9544 - true_negatives: 1411.7789 - true_positives: 1547.9497\n",
      "For batch number 200 the model has a loss of 0.2543025314807892\n",
      "\u001b[1m200/689\u001b[0m \u001b[32m━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━\u001b[0m \u001b[1m27s\u001b[0m 57ms/step - auc: 0.9682 - binary_accuracy: 0.9283 - false_negatives: 77.2450 - false_positives: 164.3050 - loss: 0.2387 - precision: 0.9099 - recall: 0.9544 - true_negatives: 1418.9050 - true_positives: 1555.5450\n",
      "For batch number 201 the model has a loss of 0.2536070644855499\n",
      "\u001b[1m201/689\u001b[0m \u001b[32m━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━\u001b[0m \u001b[1m27s\u001b[0m 56ms/step - auc: 0.9682 - binary_accuracy: 0.9283 - false_negatives: 77.6567 - false_positives: 165.1642 - loss: 0.2387 - precision: 0.9098 - recall: 0.9544 - true_negatives: 1426.0448 - true_positives: 1563.1343\n",
      "For batch number 202 the model has a loss of 0.25288963317871094\n",
      "\u001b[1m202/689\u001b[0m \u001b[32m━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━\u001b[0m \u001b[1m27s\u001b[0m 56ms/step - auc: 0.9682 - binary_accuracy: 0.9283 - false_negatives: 78.0644 - false_positives: 166.0148 - loss: 0.2388 - precision: 0.9098 - recall: 0.9543 - true_negatives: 1433.1732 - true_positives: 1570.7476\n",
      "For batch number 203 the model has a loss of 0.25220218300819397\n",
      "\u001b[1m203/689\u001b[0m \u001b[32m━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━\u001b[0m \u001b[1m27s\u001b[0m 56ms/step - auc: 0.9681 - binary_accuracy: 0.9283 - false_negatives: 78.4729 - false_positives: 166.8571 - loss: 0.2389 - precision: 0.9097 - recall: 0.9543 - true_negatives: 1440.3054 - true_positives: 1578.3645\n",
      "For batch number 204 the model has a loss of 0.25166186690330505\n",
      "\u001b[1m204/689\u001b[0m \u001b[32m━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━\u001b[0m \u001b[1m27s\u001b[0m 56ms/step - auc: 0.9681 - binary_accuracy: 0.9282 - false_negatives: 78.8774 - false_positives: 167.7010 - loss: 0.2389 - precision: 0.9097 - recall: 0.9543 - true_negatives: 1447.4314 - true_positives: 1585.9902\n",
      "For batch number 205 the model has a loss of 0.2512313425540924\n",
      "\u001b[1m205/689\u001b[0m \u001b[32m━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━\u001b[0m \u001b[1m27s\u001b[0m 56ms/step - auc: 0.9681 - binary_accuracy: 0.9282 - false_negatives: 79.2780 - false_positives: 168.5415 - loss: 0.2390 - precision: 0.9097 - recall: 0.9543 - true_negatives: 1454.5415 - true_positives: 1593.6390\n",
      "For batch number 206 the model has a loss of 0.2509031295776367\n",
      "\u001b[1m206/689\u001b[0m \u001b[32m━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━\u001b[0m \u001b[1m27s\u001b[0m 56ms/step - auc: 0.9681 - binary_accuracy: 0.9282 - false_negatives: 79.6845 - false_positives: 169.3786 - loss: 0.2391 - precision: 0.9096 - recall: 0.9543 - true_negatives: 1461.6699 - true_positives: 1601.2670\n",
      "For batch number 207 the model has a loss of 0.25066402554512024\n",
      "\u001b[1m207/689\u001b[0m \u001b[32m━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━\u001b[0m \u001b[1m27s\u001b[0m 56ms/step - auc: 0.9681 - binary_accuracy: 0.9282 - false_negatives: 80.0870 - false_positives: 170.2126 - loss: 0.2391 - precision: 0.9096 - recall: 0.9543 - true_negatives: 1468.8164 - true_positives: 1608.8840\n",
      "For batch number 208 the model has a loss of 0.25138556957244873\n",
      "\u001b[1m208/689\u001b[0m \u001b[32m━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━\u001b[0m \u001b[1m27s\u001b[0m 57ms/step - auc: 0.9681 - binary_accuracy: 0.9281 - false_negatives: 80.4856 - false_positives: 171.0625 - loss: 0.2392 - precision: 0.9096 - recall: 0.9543 - true_negatives: 1475.9519 - true_positives: 1616.5000\n",
      "For batch number 209 the model has a loss of 0.2508460581302643\n",
      "\u001b[1m209/689\u001b[0m \u001b[32m━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━\u001b[0m \u001b[1m27s\u001b[0m 56ms/step - auc: 0.9680 - binary_accuracy: 0.9281 - false_negatives: 80.8804 - false_positives: 171.9043 - loss: 0.2392 - precision: 0.9095 - recall: 0.9542 - true_negatives: 1483.1101 - true_positives: 1624.1052\n",
      "For batch number 210 the model has a loss of 0.25082099437713623\n",
      "\u001b[1m210/689\u001b[0m \u001b[32m━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━\u001b[0m \u001b[1m27s\u001b[0m 56ms/step - auc: 0.9680 - binary_accuracy: 0.9281 - false_negatives: 81.2762 - false_positives: 172.7476 - loss: 0.2393 - precision: 0.9095 - recall: 0.9542 - true_negatives: 1490.2715 - true_positives: 1631.7047\n",
      "For batch number 211 the model has a loss of 0.25133994221687317\n",
      "\u001b[1m211/689\u001b[0m \u001b[32m━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━\u001b[0m \u001b[1m26s\u001b[0m 56ms/step - auc: 0.9680 - binary_accuracy: 0.9281 - false_negatives: 81.6730 - false_positives: 173.6019 - loss: 0.2393 - precision: 0.9095 - recall: 0.9542 - true_negatives: 1497.4265 - true_positives: 1639.2986\n",
      "For batch number 212 the model has a loss of 0.25150883197784424\n",
      "\u001b[1m212/689\u001b[0m \u001b[32m━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━\u001b[0m \u001b[1m26s\u001b[0m 56ms/step - auc: 0.9680 - binary_accuracy: 0.9280 - false_negatives: 82.0708 - false_positives: 174.4623 - loss: 0.2394 - precision: 0.9094 - recall: 0.9542 - true_negatives: 1504.5896 - true_positives: 1646.8773\n",
      "For batch number 213 the model has a loss of 0.25149041414260864\n",
      "\u001b[1m213/689\u001b[0m \u001b[32m━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━\u001b[0m \u001b[1m26s\u001b[0m 56ms/step - auc: 0.9680 - binary_accuracy: 0.9280 - false_negatives: 82.4695 - false_positives: 175.3192 - loss: 0.2395 - precision: 0.9094 - recall: 0.9542 - true_negatives: 1511.7699 - true_positives: 1654.4413\n",
      "For batch number 214 the model has a loss of 0.2525738477706909\n",
      "\u001b[1m214/689\u001b[0m \u001b[32m━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━\u001b[0m \u001b[1m26s\u001b[0m 56ms/step - auc: 0.9680 - binary_accuracy: 0.9280 - false_negatives: 82.8692 - false_positives: 176.2009 - loss: 0.2395 - precision: 0.9093 - recall: 0.9542 - true_negatives: 1518.9486 - true_positives: 1661.9813\n",
      "For batch number 215 the model has a loss of 0.2520122528076172\n",
      "\u001b[1m215/689\u001b[0m \u001b[32m━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━\u001b[0m \u001b[1m26s\u001b[0m 56ms/step - auc: 0.9680 - binary_accuracy: 0.9280 - false_negatives: 83.2651 - false_positives: 177.0791 - loss: 0.2396 - precision: 0.9093 - recall: 0.9542 - true_negatives: 1526.1256 - true_positives: 1669.5303\n",
      "For batch number 216 the model has a loss of 0.2518717050552368\n",
      "\u001b[1m216/689\u001b[0m \u001b[32m━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━\u001b[0m \u001b[1m26s\u001b[0m 56ms/step - auc: 0.9679 - binary_accuracy: 0.9279 - false_negatives: 83.6574 - false_positives: 177.9630 - loss: 0.2396 - precision: 0.9093 - recall: 0.9541 - true_negatives: 1533.3055 - true_positives: 1677.0741\n",
      "For batch number 217 the model has a loss of 0.2516290545463562\n",
      "\u001b[1m217/689\u001b[0m \u001b[32m━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━\u001b[0m \u001b[1m26s\u001b[0m 56ms/step - auc: 0.9679 - binary_accuracy: 0.9279 - false_negatives: 84.0461 - false_positives: 178.8479 - loss: 0.2397 - precision: 0.9092 - recall: 0.9541 - true_negatives: 1540.4977 - true_positives: 1684.6083\n",
      "For batch number 218 the model has a loss of 0.25121933221817017\n",
      "\u001b[1m218/689\u001b[0m \u001b[32m━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━\u001b[0m \u001b[1m26s\u001b[0m 56ms/step - auc: 0.9679 - binary_accuracy: 0.9279 - false_negatives: 84.4312 - false_positives: 179.7294 - loss: 0.2397 - precision: 0.9092 - recall: 0.9541 - true_negatives: 1547.6790 - true_positives: 1692.1605\n",
      "For batch number 219 the model has a loss of 0.2508891820907593\n",
      "\u001b[1m219/689\u001b[0m \u001b[32m━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━\u001b[0m \u001b[1m26s\u001b[0m 56ms/step - auc: 0.9679 - binary_accuracy: 0.9279 - false_negatives: 84.8128 - false_positives: 180.6073 - loss: 0.2398 - precision: 0.9091 - recall: 0.9541 - true_negatives: 1554.8676 - true_positives: 1699.7123\n",
      "For batch number 220 the model has a loss of 0.2507707178592682\n",
      "\u001b[1m220/689\u001b[0m \u001b[32m━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━\u001b[0m \u001b[1m26s\u001b[0m 56ms/step - auc: 0.9679 - binary_accuracy: 0.9279 - false_negatives: 85.1909 - false_positives: 181.4818 - loss: 0.2398 - precision: 0.9091 - recall: 0.9541 - true_negatives: 1562.0454 - true_positives: 1707.2819\n",
      "For batch number 221 the model has a loss of 0.2507432997226715\n",
      "\u001b[1m221/689\u001b[0m \u001b[32m━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━\u001b[0m \u001b[1m26s\u001b[0m 56ms/step - auc: 0.9679 - binary_accuracy: 0.9278 - false_negatives: 85.5701 - false_positives: 182.3575 - loss: 0.2399 - precision: 0.9090 - recall: 0.9541 - true_negatives: 1569.2081 - true_positives: 1714.8643\n",
      "For batch number 222 the model has a loss of 0.25036120414733887\n",
      "\u001b[1m222/689\u001b[0m \u001b[32m━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━\u001b[0m \u001b[1m26s\u001b[0m 56ms/step - auc: 0.9679 - binary_accuracy: 0.9278 - false_negatives: 85.9504 - false_positives: 183.2252 - loss: 0.2399 - precision: 0.9090 - recall: 0.9541 - true_negatives: 1576.3739 - true_positives: 1722.4504\n",
      "For batch number 223 the model has a loss of 0.25203177332878113\n",
      "\u001b[1m223/689\u001b[0m \u001b[32m━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━\u001b[0m \u001b[1m26s\u001b[0m 56ms/step - auc: 0.9678 - binary_accuracy: 0.9278 - false_negatives: 86.3498 - false_positives: 184.0942 - loss: 0.2400 - precision: 0.9090 - recall: 0.9541 - true_negatives: 1583.5247 - true_positives: 1730.0314\n",
      "For batch number 224 the model has a loss of 0.2521613538265228\n",
      "\u001b[1m224/689\u001b[0m \u001b[32m━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━\u001b[0m \u001b[1m26s\u001b[0m 56ms/step - auc: 0.9678 - binary_accuracy: 0.9278 - false_negatives: 86.7500 - false_positives: 184.9643 - loss: 0.2400 - precision: 0.9089 - recall: 0.9541 - true_negatives: 1590.6741 - true_positives: 1737.6116\n",
      "For batch number 225 the model has a loss of 0.25367501378059387\n",
      "\u001b[1m225/689\u001b[0m \u001b[32m━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━\u001b[0m \u001b[1m26s\u001b[0m 56ms/step - auc: 0.9678 - binary_accuracy: 0.9277 - false_negatives: 87.1556 - false_positives: 185.8356 - loss: 0.2401 - precision: 0.9089 - recall: 0.9541 - true_negatives: 1597.8177 - true_positives: 1745.1912\n",
      "For batch number 226 the model has a loss of 0.2534271478652954\n",
      "\u001b[1m226/689\u001b[0m \u001b[32m━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━\u001b[0m \u001b[1m26s\u001b[0m 56ms/step - auc: 0.9678 - binary_accuracy: 0.9277 - false_negatives: 87.5620 - false_positives: 186.7080 - loss: 0.2402 - precision: 0.9089 - recall: 0.9540 - true_negatives: 1604.9602 - true_positives: 1752.7699\n",
      "For batch number 227 the model has a loss of 0.2533629238605499\n",
      "\u001b[1m227/689\u001b[0m \u001b[32m━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━\u001b[0m \u001b[1m26s\u001b[0m 56ms/step - auc: 0.9678 - binary_accuracy: 0.9277 - false_negatives: 87.9648 - false_positives: 187.5859 - loss: 0.2402 - precision: 0.9088 - recall: 0.9540 - true_negatives: 1612.1101 - true_positives: 1760.3392\n",
      "For batch number 228 the model has a loss of 0.2535155117511749\n",
      "\u001b[1m228/689\u001b[0m \u001b[32m━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━\u001b[0m \u001b[1m26s\u001b[0m 56ms/step - auc: 0.9677 - binary_accuracy: 0.9277 - false_negatives: 88.3684 - false_positives: 188.4649 - loss: 0.2403 - precision: 0.9088 - recall: 0.9540 - true_negatives: 1619.2764 - true_positives: 1767.8904\n",
      "For batch number 229 the model has a loss of 0.25451260805130005\n",
      "\u001b[1m229/689\u001b[0m \u001b[32m━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━\u001b[0m \u001b[1m25s\u001b[0m 56ms/step - auc: 0.9677 - binary_accuracy: 0.9276 - false_negatives: 88.7729 - false_positives: 189.3581 - loss: 0.2403 - precision: 0.9087 - recall: 0.9540 - true_negatives: 1626.4541 - true_positives: 1775.4148\n",
      "For batch number 230 the model has a loss of 0.25516989827156067\n",
      "\u001b[1m230/689\u001b[0m \u001b[32m━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━\u001b[0m \u001b[1m25s\u001b[0m 56ms/step - auc: 0.9677 - binary_accuracy: 0.9276 - false_negatives: 89.1783 - false_positives: 190.2478 - loss: 0.2404 - precision: 0.9087 - recall: 0.9540 - true_negatives: 1633.6217 - true_positives: 1782.9521\n",
      "For batch number 231 the model has a loss of 0.25537919998168945\n",
      "\u001b[1m231/689\u001b[0m \u001b[32m━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━\u001b[0m \u001b[1m25s\u001b[0m 56ms/step - auc: 0.9677 - binary_accuracy: 0.9276 - false_negatives: 89.5844 - false_positives: 191.1429 - loss: 0.2405 - precision: 0.9086 - recall: 0.9540 - true_negatives: 1640.7965 - true_positives: 1790.4762\n",
      "For batch number 232 the model has a loss of 0.2545405328273773\n",
      "\u001b[1m232/689\u001b[0m \u001b[32m━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━\u001b[0m \u001b[1m25s\u001b[0m 56ms/step - auc: 0.9677 - binary_accuracy: 0.9276 - false_negatives: 89.9914 - false_positives: 192.0302 - loss: 0.2405 - precision: 0.9086 - recall: 0.9540 - true_negatives: 1647.9741 - true_positives: 1798.0043\n",
      "For batch number 233 the model has a loss of 0.25426778197288513\n",
      "\u001b[1m233/689\u001b[0m \u001b[32m━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━\u001b[0m \u001b[1m25s\u001b[0m 56ms/step - auc: 0.9676 - binary_accuracy: 0.9275 - false_negatives: 90.3991 - false_positives: 192.9142 - loss: 0.2406 - precision: 0.9086 - recall: 0.9539 - true_negatives: 1655.1503 - true_positives: 1805.5365\n",
      "For batch number 234 the model has a loss of 0.254922091960907\n",
      "\u001b[1m234/689\u001b[0m \u001b[32m━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━\u001b[0m \u001b[1m25s\u001b[0m 56ms/step - auc: 0.9676 - binary_accuracy: 0.9275 - false_negatives: 90.8077 - false_positives: 193.8034 - loss: 0.2407 - precision: 0.9085 - recall: 0.9539 - true_negatives: 1662.3291 - true_positives: 1813.0598\n",
      "For batch number 235 the model has a loss of 0.25465869903564453\n",
      "\u001b[1m235/689\u001b[0m \u001b[32m━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━\u001b[0m \u001b[1m25s\u001b[0m 56ms/step - auc: 0.9676 - binary_accuracy: 0.9275 - false_negatives: 91.2170 - false_positives: 194.6851 - loss: 0.2407 - precision: 0.9085 - recall: 0.9539 - true_negatives: 1669.5106 - true_positives: 1820.5873\n",
      "For batch number 236 the model has a loss of 0.255270779132843\n",
      "\u001b[1m236/689\u001b[0m \u001b[32m━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━\u001b[0m \u001b[1m25s\u001b[0m 56ms/step - auc: 0.9676 - binary_accuracy: 0.9275 - false_negatives: 91.6314 - false_positives: 195.5636 - loss: 0.2408 - precision: 0.9084 - recall: 0.9539 - true_negatives: 1676.6823 - true_positives: 1828.1229\n",
      "For batch number 237 the model has a loss of 0.25583019852638245\n",
      "\u001b[1m237/689\u001b[0m \u001b[32m━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━\u001b[0m \u001b[1m25s\u001b[0m 56ms/step - auc: 0.9676 - binary_accuracy: 0.9274 - false_negatives: 92.0464 - false_positives: 196.4430 - loss: 0.2408 - precision: 0.9084 - recall: 0.9539 - true_negatives: 1683.8523 - true_positives: 1835.6582\n",
      "For batch number 238 the model has a loss of 0.25584128499031067\n",
      "\u001b[1m238/689\u001b[0m \u001b[32m━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━\u001b[0m \u001b[1m25s\u001b[0m 56ms/step - auc: 0.9675 - binary_accuracy: 0.9274 - false_negatives: 92.4622 - false_positives: 197.3235 - loss: 0.2409 - precision: 0.9084 - recall: 0.9539 - true_negatives: 1691.0210 - true_positives: 1843.1932\n",
      "For batch number 239 the model has a loss of 0.2553752362728119\n",
      "\u001b[1m239/689\u001b[0m \u001b[32m━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━\u001b[0m \u001b[1m25s\u001b[0m 56ms/step - auc: 0.9675 - binary_accuracy: 0.9274 - false_negatives: 92.8745 - false_positives: 198.2050 - loss: 0.2410 - precision: 0.9083 - recall: 0.9538 - true_negatives: 1698.1882 - true_positives: 1850.7322\n",
      "For batch number 240 the model has a loss of 0.2549666464328766\n",
      "\u001b[1m240/689\u001b[0m \u001b[32m━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━\u001b[0m \u001b[1m25s\u001b[0m 56ms/step - auc: 0.9675 - binary_accuracy: 0.9274 - false_negatives: 93.2833 - false_positives: 199.0875 - loss: 0.2410 - precision: 0.9083 - recall: 0.9538 - true_negatives: 1705.3541 - true_positives: 1858.2750\n",
      "For batch number 241 the model has a loss of 0.25487029552459717\n",
      "\u001b[1m241/689\u001b[0m \u001b[32m━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━\u001b[0m \u001b[1m25s\u001b[0m 56ms/step - auc: 0.9675 - binary_accuracy: 0.9273 - false_negatives: 93.6971 - false_positives: 199.9709 - loss: 0.2411 - precision: 0.9083 - recall: 0.9538 - true_negatives: 1712.5104 - true_positives: 1865.8215\n",
      "For batch number 242 the model has a loss of 0.25497978925704956\n",
      "\u001b[1m242/689\u001b[0m \u001b[32m━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━\u001b[0m \u001b[1m25s\u001b[0m 56ms/step - auc: 0.9675 - binary_accuracy: 0.9273 - false_negatives: 94.1074 - false_positives: 200.8636 - loss: 0.2411 - precision: 0.9082 - recall: 0.9538 - true_negatives: 1719.6777 - true_positives: 1873.3512\n",
      "For batch number 243 the model has a loss of 0.2547832131385803\n",
      "\u001b[1m243/689\u001b[0m \u001b[32m━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━\u001b[0m \u001b[1m25s\u001b[0m 56ms/step - auc: 0.9674 - binary_accuracy: 0.9273 - false_negatives: 94.5144 - false_positives: 201.7572 - loss: 0.2412 - precision: 0.9082 - recall: 0.9538 - true_negatives: 1726.8354 - true_positives: 1880.8929\n",
      "For batch number 244 the model has a loss of 0.2555570900440216\n",
      "\u001b[1m244/689\u001b[0m \u001b[32m━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━\u001b[0m \u001b[1m25s\u001b[0m 56ms/step - auc: 0.9674 - binary_accuracy: 0.9273 - false_negatives: 94.9221 - false_positives: 202.6475 - loss: 0.2413 - precision: 0.9081 - recall: 0.9538 - true_negatives: 1733.9877 - true_positives: 1888.4426\n",
      "For batch number 245 the model has a loss of 0.25512054562568665\n",
      "\u001b[1m245/689\u001b[0m \u001b[32m━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━\u001b[0m \u001b[1m25s\u001b[0m 56ms/step - auc: 0.9674 - binary_accuracy: 0.9273 - false_negatives: 95.3265 - false_positives: 203.5388 - loss: 0.2413 - precision: 0.9081 - recall: 0.9538 - true_negatives: 1741.1346 - true_positives: 1896.0000\n",
      "For batch number 246 the model has a loss of 0.25603798031806946\n",
      "\u001b[1m246/689\u001b[0m \u001b[32m━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━\u001b[0m \u001b[1m25s\u001b[0m 56ms/step - auc: 0.9674 - binary_accuracy: 0.9272 - false_negatives: 95.7317 - false_positives: 204.4228 - loss: 0.2414 - precision: 0.9081 - recall: 0.9537 - true_negatives: 1748.2886 - true_positives: 1903.5569\n",
      "For batch number 247 the model has a loss of 0.25615787506103516\n",
      "\u001b[1m247/689\u001b[0m \u001b[32m━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━\u001b[0m \u001b[1m24s\u001b[0m 56ms/step - auc: 0.9674 - binary_accuracy: 0.9272 - false_negatives: 96.1376 - false_positives: 205.2996 - loss: 0.2414 - precision: 0.9080 - recall: 0.9537 - true_negatives: 1755.4292 - true_positives: 1911.1335\n",
      "For batch number 248 the model has a loss of 0.25586679577827454\n",
      "\u001b[1m248/689\u001b[0m \u001b[32m━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━\u001b[0m \u001b[1m24s\u001b[0m 56ms/step - auc: 0.9673 - binary_accuracy: 0.9272 - false_negatives: 96.5403 - false_positives: 206.1734 - loss: 0.2415 - precision: 0.9080 - recall: 0.9537 - true_negatives: 1762.6049 - true_positives: 1918.6814\n",
      "For batch number 249 the model has a loss of 0.257000207901001\n",
      "\u001b[1m249/689\u001b[0m \u001b[32m━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━\u001b[0m \u001b[1m24s\u001b[0m 56ms/step - auc: 0.9673 - binary_accuracy: 0.9272 - false_negatives: 96.9478 - false_positives: 207.0442 - loss: 0.2415 - precision: 0.9080 - recall: 0.9537 - true_negatives: 1769.7751 - true_positives: 1926.2329\n",
      "For batch number 250 the model has a loss of 0.2574165165424347\n",
      "\u001b[1m250/689\u001b[0m \u001b[32m━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━\u001b[0m \u001b[1m24s\u001b[0m 56ms/step - auc: 0.9673 - binary_accuracy: 0.9271 - false_negatives: 97.3600 - false_positives: 207.9080 - loss: 0.2416 - precision: 0.9079 - recall: 0.9537 - true_negatives: 1776.9399 - true_positives: 1933.7920\n",
      "For batch number 251 the model has a loss of 0.25701984763145447\n",
      "\u001b[1m251/689\u001b[0m \u001b[32m━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━\u001b[0m \u001b[1m24s\u001b[0m 56ms/step - auc: 0.9673 - binary_accuracy: 0.9271 - false_negatives: 97.7689 - false_positives: 208.7689 - loss: 0.2417 - precision: 0.9079 - recall: 0.9537 - true_negatives: 1784.1116 - true_positives: 1941.3506\n",
      "For batch number 252 the model has a loss of 0.2567571997642517\n",
      "\u001b[1m252/689\u001b[0m \u001b[32m━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━\u001b[0m \u001b[1m24s\u001b[0m 56ms/step - auc: 0.9673 - binary_accuracy: 0.9271 - false_negatives: 98.1746 - false_positives: 209.6270 - loss: 0.2417 - precision: 0.9079 - recall: 0.9537 - true_negatives: 1791.2778 - true_positives: 1948.9207\n",
      "For batch number 253 the model has a loss of 0.2577192187309265\n",
      "\u001b[1m253/689\u001b[0m \u001b[32m━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━\u001b[0m \u001b[1m24s\u001b[0m 56ms/step - auc: 0.9672 - binary_accuracy: 0.9271 - false_negatives: 98.5850 - false_positives: 210.5059 - loss: 0.2418 - precision: 0.9078 - recall: 0.9537 - true_negatives: 1798.4506 - true_positives: 1956.4585\n",
      "For batch number 254 the model has a loss of 0.2581250071525574\n",
      "\u001b[1m254/689\u001b[0m \u001b[32m━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━\u001b[0m \u001b[1m24s\u001b[0m 56ms/step - auc: 0.9672 - binary_accuracy: 0.9271 - false_negatives: 99.0000 - false_positives: 211.3858 - loss: 0.2419 - precision: 0.9078 - recall: 0.9536 - true_negatives: 1805.6182 - true_positives: 1963.9961\n",
      "For batch number 255 the model has a loss of 0.25800415873527527\n",
      "\u001b[1m255/689\u001b[0m \u001b[32m━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━\u001b[0m \u001b[1m24s\u001b[0m 56ms/step - auc: 0.9672 - binary_accuracy: 0.9270 - false_negatives: 99.4157 - false_positives: 212.2667 - loss: 0.2419 - precision: 0.9078 - recall: 0.9536 - true_negatives: 1812.7843 - true_positives: 1971.5333\n",
      "For batch number 256 the model has a loss of 0.2577304542064667\n",
      "\u001b[1m256/689\u001b[0m \u001b[32m━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━\u001b[0m \u001b[1m24s\u001b[0m 56ms/step - auc: 0.9672 - binary_accuracy: 0.9270 - false_negatives: 99.8320 - false_positives: 213.1484 - loss: 0.2420 - precision: 0.9077 - recall: 0.9536 - true_negatives: 1819.9492 - true_positives: 1979.0703\n",
      "For batch number 257 the model has a loss of 0.2577921152114868\n",
      "\u001b[1m257/689\u001b[0m \u001b[32m━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━\u001b[0m \u001b[1m24s\u001b[0m 56ms/step - auc: 0.9672 - binary_accuracy: 0.9270 - false_negatives: 100.2490 - false_positives: 214.0350 - loss: 0.2420 - precision: 0.9077 - recall: 0.9536 - true_negatives: 1827.1245 - true_positives: 1986.5914\n",
      "For batch number 258 the model has a loss of 0.25757914781570435\n",
      "\u001b[1m258/689\u001b[0m \u001b[32m━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━\u001b[0m \u001b[1m24s\u001b[0m 56ms/step - auc: 0.9672 - binary_accuracy: 0.9270 - false_negatives: 100.6628 - false_positives: 214.9186 - loss: 0.2421 - precision: 0.9077 - recall: 0.9536 - true_negatives: 1834.2985 - true_positives: 1994.1201\n",
      "For batch number 259 the model has a loss of 0.25743111968040466\n",
      "\u001b[1m259/689\u001b[0m \u001b[32m━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━\u001b[0m \u001b[1m24s\u001b[0m 56ms/step - auc: 0.9671 - binary_accuracy: 0.9270 - false_negatives: 101.0811 - false_positives: 215.7954 - loss: 0.2422 - precision: 0.9076 - recall: 0.9536 - true_negatives: 1841.4749 - true_positives: 2001.6487\n",
      "For batch number 260 the model has a loss of 0.25699669122695923\n",
      "\u001b[1m260/689\u001b[0m \u001b[32m━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━\u001b[0m \u001b[1m24s\u001b[0m 56ms/step - auc: 0.9671 - binary_accuracy: 0.9269 - false_negatives: 101.4962 - false_positives: 216.6692 - loss: 0.2422 - precision: 0.9076 - recall: 0.9536 - true_negatives: 1848.6500 - true_positives: 2009.1846\n",
      "For batch number 261 the model has a loss of 0.25678515434265137\n",
      "\u001b[1m261/689\u001b[0m \u001b[32m━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━\u001b[0m \u001b[1m24s\u001b[0m 56ms/step - auc: 0.9671 - binary_accuracy: 0.9269 - false_negatives: 101.9080 - false_positives: 217.5402 - loss: 0.2423 - precision: 0.9076 - recall: 0.9535 - true_negatives: 1855.8276 - true_positives: 2016.7241\n",
      "For batch number 262 the model has a loss of 0.25645333528518677\n",
      "\u001b[1m262/689\u001b[0m \u001b[32m━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━\u001b[0m \u001b[1m24s\u001b[0m 56ms/step - auc: 0.9671 - binary_accuracy: 0.9269 - false_negatives: 102.3168 - false_positives: 218.4160 - loss: 0.2423 - precision: 0.9075 - recall: 0.9535 - true_negatives: 1863.0115 - true_positives: 2024.2557\n",
      "For batch number 263 the model has a loss of 0.2570817172527313\n",
      "\u001b[1m263/689\u001b[0m \u001b[32m━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━\u001b[0m \u001b[1m24s\u001b[0m 56ms/step - auc: 0.9671 - binary_accuracy: 0.9269 - false_negatives: 102.7300 - false_positives: 219.2890 - loss: 0.2424 - precision: 0.9075 - recall: 0.9535 - true_negatives: 1870.2015 - true_positives: 2031.7794\n",
      "For batch number 264 the model has a loss of 0.2566275894641876\n",
      "\u001b[1m264/689\u001b[0m \u001b[32m━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━\u001b[0m \u001b[1m23s\u001b[0m 56ms/step - auc: 0.9671 - binary_accuracy: 0.9269 - false_negatives: 103.1439 - false_positives: 220.1553 - loss: 0.2424 - precision: 0.9075 - recall: 0.9535 - true_negatives: 1877.4015 - true_positives: 2039.2992\n",
      "For batch number 265 the model has a loss of 0.2573404610157013\n",
      "\u001b[1m265/689\u001b[0m \u001b[32m━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━\u001b[0m \u001b[1m23s\u001b[0m 56ms/step - auc: 0.9670 - binary_accuracy: 0.9268 - false_negatives: 103.5660 - false_positives: 221.0189 - loss: 0.2425 - precision: 0.9074 - recall: 0.9535 - true_negatives: 1884.6000 - true_positives: 2046.8151\n",
      "For batch number 266 the model has a loss of 0.2577408254146576\n",
      "\u001b[1m266/689\u001b[0m \u001b[32m━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━\u001b[0m \u001b[1m23s\u001b[0m 56ms/step - auc: 0.9670 - binary_accuracy: 0.9268 - false_negatives: 103.9925 - false_positives: 221.8797 - loss: 0.2426 - precision: 0.9074 - recall: 0.9535 - true_negatives: 1891.7858 - true_positives: 2054.3420\n",
      "For batch number 267 the model has a loss of 0.25734320282936096\n",
      "\u001b[1m267/689\u001b[0m \u001b[32m━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━\u001b[0m \u001b[1m23s\u001b[0m 56ms/step - auc: 0.9670 - binary_accuracy: 0.9268 - false_negatives: 104.4157 - false_positives: 222.7416 - loss: 0.2426 - precision: 0.9074 - recall: 0.9535 - true_negatives: 1898.9701 - true_positives: 2061.8726\n",
      "For batch number 268 the model has a loss of 0.25754767656326294\n",
      "\u001b[1m268/689\u001b[0m \u001b[32m━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━\u001b[0m \u001b[1m23s\u001b[0m 56ms/step - auc: 0.9670 - binary_accuracy: 0.9268 - false_negatives: 104.8433 - false_positives: 223.6082 - loss: 0.2427 - precision: 0.9074 - recall: 0.9534 - true_negatives: 1906.1604 - true_positives: 2069.3879\n",
      "For batch number 269 the model has a loss of 0.2576488256454468\n",
      "\u001b[1m269/689\u001b[0m \u001b[32m━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━\u001b[0m \u001b[1m23s\u001b[0m 56ms/step - auc: 0.9670 - binary_accuracy: 0.9268 - false_negatives: 105.2751 - false_positives: 224.4758 - loss: 0.2427 - precision: 0.9073 - recall: 0.9534 - true_negatives: 1913.3531 - true_positives: 2076.8960\n",
      "For batch number 270 the model has a loss of 0.2571386694908142\n",
      "\u001b[1m270/689\u001b[0m \u001b[32m━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━\u001b[0m \u001b[1m23s\u001b[0m 56ms/step - auc: 0.9670 - binary_accuracy: 0.9267 - false_negatives: 105.7037 - false_positives: 225.3370 - loss: 0.2428 - precision: 0.9073 - recall: 0.9534 - true_negatives: 1920.5370 - true_positives: 2084.4221\n",
      "For batch number 271 the model has a loss of 0.2568957805633545\n",
      "\u001b[1m271/689\u001b[0m \u001b[32m━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━\u001b[0m \u001b[1m23s\u001b[0m 56ms/step - auc: 0.9669 - binary_accuracy: 0.9267 - false_negatives: 106.1292 - false_positives: 226.1919 - loss: 0.2428 - precision: 0.9073 - recall: 0.9534 - true_negatives: 1927.7233 - true_positives: 2091.9558\n",
      "For batch number 272 the model has a loss of 0.2581444978713989\n",
      "\u001b[1m272/689\u001b[0m \u001b[32m━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━\u001b[0m \u001b[1m23s\u001b[0m 56ms/step - auc: 0.9669 - binary_accuracy: 0.9267 - false_negatives: 106.5625 - false_positives: 227.0441 - loss: 0.2429 - precision: 0.9073 - recall: 0.9534 - true_negatives: 1934.8896 - true_positives: 2099.5037\n",
      "For batch number 273 the model has a loss of 0.25819581747055054\n",
      "\u001b[1m273/689\u001b[0m \u001b[32m━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━\u001b[0m \u001b[1m23s\u001b[0m 56ms/step - auc: 0.9669 - binary_accuracy: 0.9267 - false_negatives: 106.9927 - false_positives: 227.9048 - loss: 0.2429 - precision: 0.9072 - recall: 0.9534 - true_negatives: 1942.0513 - true_positives: 2107.0513\n",
      "For batch number 274 the model has a loss of 0.25814351439476013\n",
      "\u001b[1m274/689\u001b[0m \u001b[32m━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━\u001b[0m \u001b[1m23s\u001b[0m 56ms/step - auc: 0.9669 - binary_accuracy: 0.9267 - false_negatives: 107.4197 - false_positives: 228.7701 - loss: 0.2430 - precision: 0.9072 - recall: 0.9533 - true_negatives: 1949.2153 - true_positives: 2114.5950\n",
      "For batch number 275 the model has a loss of 0.2588329613208771\n",
      "\u001b[1m275/689\u001b[0m \u001b[32m━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━\u001b[0m \u001b[1m23s\u001b[0m 56ms/step - auc: 0.9669 - binary_accuracy: 0.9266 - false_negatives: 107.8473 - false_positives: 229.6436 - loss: 0.2431 - precision: 0.9072 - recall: 0.9533 - true_negatives: 1956.3818 - true_positives: 2122.1272\n",
      "For batch number 276 the model has a loss of 0.25949564576148987\n",
      "\u001b[1m276/689\u001b[0m \u001b[32m━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━\u001b[0m \u001b[1m23s\u001b[0m 56ms/step - auc: 0.9669 - binary_accuracy: 0.9266 - false_negatives: 108.2826 - false_positives: 230.5145 - loss: 0.2431 - precision: 0.9071 - recall: 0.9533 - true_negatives: 1963.5326 - true_positives: 2129.6704\n",
      "For batch number 277 the model has a loss of 0.25952059030532837\n",
      "\u001b[1m277/689\u001b[0m \u001b[32m━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━\u001b[0m \u001b[1m23s\u001b[0m 57ms/step - auc: 0.9668 - binary_accuracy: 0.9266 - false_negatives: 108.7220 - false_positives: 231.3899 - loss: 0.2432 - precision: 0.9071 - recall: 0.9533 - true_negatives: 1970.6823 - true_positives: 2137.2058\n",
      "For batch number 278 the model has a loss of 0.25997257232666016\n",
      "\u001b[1m278/689\u001b[0m \u001b[32m━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━\u001b[0m \u001b[1m23s\u001b[0m 57ms/step - auc: 0.9668 - binary_accuracy: 0.9266 - false_negatives: 109.1583 - false_positives: 232.2698 - loss: 0.2432 - precision: 0.9071 - recall: 0.9533 - true_negatives: 1977.8381 - true_positives: 2144.7339\n",
      "For batch number 279 the model has a loss of 0.25994643568992615\n",
      "\u001b[1m279/689\u001b[0m \u001b[32m━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━\u001b[0m \u001b[1m23s\u001b[0m 57ms/step - auc: 0.9668 - binary_accuracy: 0.9266 - false_negatives: 109.5914 - false_positives: 233.1577 - loss: 0.2433 - precision: 0.9071 - recall: 0.9533 - true_negatives: 1985.0072 - true_positives: 2152.2437\n",
      "For batch number 280 the model has a loss of 0.2604602873325348\n",
      "\u001b[1m280/689\u001b[0m \u001b[32m━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━\u001b[0m \u001b[1m23s\u001b[0m 57ms/step - auc: 0.9668 - binary_accuracy: 0.9265 - false_negatives: 110.0286 - false_positives: 234.0393 - loss: 0.2434 - precision: 0.9070 - recall: 0.9532 - true_negatives: 1992.1714 - true_positives: 2159.7607\n",
      "For batch number 281 the model has a loss of 0.26059991121292114\n",
      "\u001b[1m281/689\u001b[0m \u001b[32m━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━\u001b[0m \u001b[1m23s\u001b[0m 57ms/step - auc: 0.9668 - binary_accuracy: 0.9265 - false_negatives: 110.4697 - false_positives: 234.9182 - loss: 0.2434 - precision: 0.9070 - recall: 0.9532 - true_negatives: 1999.3381 - true_positives: 2167.2739\n",
      "For batch number 282 the model has a loss of 0.2604885697364807\n",
      "\u001b[1m282/689\u001b[0m \u001b[32m━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━\u001b[0m \u001b[1m23s\u001b[0m 57ms/step - auc: 0.9667 - binary_accuracy: 0.9265 - false_negatives: 110.9149 - false_positives: 235.7943 - loss: 0.2435 - precision: 0.9070 - recall: 0.9532 - true_negatives: 2006.4965 - true_positives: 2174.7944\n",
      "For batch number 283 the model has a loss of 0.26069778203964233\n",
      "\u001b[1m283/689\u001b[0m \u001b[32m━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━\u001b[0m \u001b[1m22s\u001b[0m 57ms/step - auc: 0.9667 - binary_accuracy: 0.9265 - false_negatives: 111.3640 - false_positives: 236.6678 - loss: 0.2435 - precision: 0.9069 - recall: 0.9532 - true_negatives: 2013.6501 - true_positives: 2182.3181\n",
      "For batch number 284 the model has a loss of 0.26081016659736633\n",
      "\u001b[1m284/689\u001b[0m \u001b[32m━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━\u001b[0m \u001b[1m22s\u001b[0m 57ms/step - auc: 0.9667 - binary_accuracy: 0.9264 - false_negatives: 111.8134 - false_positives: 237.5458 - loss: 0.2436 - precision: 0.9069 - recall: 0.9532 - true_negatives: 2020.8064 - true_positives: 2189.8345\n",
      "For batch number 285 the model has a loss of 0.26059311628341675\n",
      "\u001b[1m285/689\u001b[0m \u001b[32m━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━\u001b[0m \u001b[1m22s\u001b[0m 57ms/step - auc: 0.9667 - binary_accuracy: 0.9264 - false_negatives: 112.2597 - false_positives: 238.4211 - loss: 0.2437 - precision: 0.9069 - recall: 0.9531 - true_negatives: 2027.9719 - true_positives: 2197.3474\n",
      "For batch number 286 the model has a loss of 0.2602687180042267\n",
      "\u001b[1m286/689\u001b[0m \u001b[32m━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━\u001b[0m \u001b[1m22s\u001b[0m 57ms/step - auc: 0.9667 - binary_accuracy: 0.9264 - false_negatives: 112.7063 - false_positives: 239.2902 - loss: 0.2437 - precision: 0.9069 - recall: 0.9531 - true_negatives: 2035.1294 - true_positives: 2204.8740\n",
      "For batch number 287 the model has a loss of 0.2604003846645355\n",
      "\u001b[1m287/689\u001b[0m \u001b[32m━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━\u001b[0m \u001b[1m22s\u001b[0m 56ms/step - auc: 0.9666 - binary_accuracy: 0.9264 - false_negatives: 113.1533 - false_positives: 240.1568 - loss: 0.2438 - precision: 0.9068 - recall: 0.9531 - true_negatives: 2042.2892 - true_positives: 2212.4006\n",
      "For batch number 288 the model has a loss of 0.2608766257762909\n",
      "\u001b[1m288/689\u001b[0m \u001b[32m━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━\u001b[0m \u001b[1m22s\u001b[0m 56ms/step - auc: 0.9666 - binary_accuracy: 0.9264 - false_negatives: 113.6007 - false_positives: 241.0278 - loss: 0.2438 - precision: 0.9068 - recall: 0.9531 - true_negatives: 2049.4480 - true_positives: 2219.9236\n",
      "For batch number 289 the model has a loss of 0.26085489988327026\n",
      "\u001b[1m289/689\u001b[0m \u001b[32m━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━\u001b[0m \u001b[1m22s\u001b[0m 56ms/step - auc: 0.9666 - binary_accuracy: 0.9263 - false_negatives: 114.0484 - false_positives: 241.8997 - loss: 0.2439 - precision: 0.9068 - recall: 0.9531 - true_negatives: 2056.6089 - true_positives: 2227.4429\n",
      "For batch number 290 the model has a loss of 0.2610134482383728\n",
      "\u001b[1m290/689\u001b[0m \u001b[32m━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━\u001b[0m \u001b[1m22s\u001b[0m 57ms/step - auc: 0.9666 - binary_accuracy: 0.9263 - false_negatives: 114.4931 - false_positives: 242.7828 - loss: 0.2440 - precision: 0.9067 - recall: 0.9530 - true_negatives: 2063.7896 - true_positives: 2234.9346\n",
      "For batch number 291 the model has a loss of 0.26056239008903503\n",
      "\u001b[1m291/689\u001b[0m \u001b[32m━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━\u001b[0m \u001b[1m22s\u001b[0m 57ms/step - auc: 0.9666 - binary_accuracy: 0.9263 - false_negatives: 114.9347 - false_positives: 243.6632 - loss: 0.2440 - precision: 0.9067 - recall: 0.9530 - true_negatives: 2070.9724 - true_positives: 2242.4294\n",
      "For batch number 292 the model has a loss of 0.2600996196269989\n",
      "\u001b[1m292/689\u001b[0m \u001b[32m━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━\u001b[0m \u001b[1m22s\u001b[0m 57ms/step - auc: 0.9665 - binary_accuracy: 0.9263 - false_negatives: 115.3733 - false_positives: 244.5411 - loss: 0.2441 - precision: 0.9067 - recall: 0.9530 - true_negatives: 2078.1575 - true_positives: 2249.9280\n",
      "For batch number 293 the model has a loss of 0.26021602749824524\n",
      "\u001b[1m293/689\u001b[0m \u001b[32m━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━\u001b[0m \u001b[1m22s\u001b[0m 57ms/step - auc: 0.9665 - binary_accuracy: 0.9263 - false_negatives: 115.8123 - false_positives: 245.4164 - loss: 0.2441 - precision: 0.9067 - recall: 0.9530 - true_negatives: 2085.3413 - true_positives: 2257.4299\n",
      "For batch number 294 the model has a loss of 0.2605559825897217\n",
      "\u001b[1m294/689\u001b[0m \u001b[32m━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━\u001b[0m \u001b[1m22s\u001b[0m 57ms/step - auc: 0.9665 - binary_accuracy: 0.9262 - false_negatives: 116.2551 - false_positives: 246.2891 - loss: 0.2442 - precision: 0.9066 - recall: 0.9530 - true_negatives: 2092.5171 - true_positives: 2264.9387\n",
      "For batch number 295 the model has a loss of 0.26043349504470825\n",
      "\u001b[1m295/689\u001b[0m \u001b[32m━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━\u001b[0m \u001b[1m22s\u001b[0m 57ms/step - auc: 0.9665 - binary_accuracy: 0.9262 - false_negatives: 116.6983 - false_positives: 247.1593 - loss: 0.2442 - precision: 0.9066 - recall: 0.9530 - true_negatives: 2099.6948 - true_positives: 2272.4475\n",
      "For batch number 296 the model has a loss of 0.26023492217063904\n",
      "\u001b[1m296/689\u001b[0m \u001b[32m━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━\u001b[0m \u001b[1m22s\u001b[0m 57ms/step - auc: 0.9665 - binary_accuracy: 0.9262 - false_negatives: 117.1385 - false_positives: 248.0304 - loss: 0.2443 - precision: 0.9066 - recall: 0.9529 - true_negatives: 2106.8818 - true_positives: 2279.9492\n",
      "For batch number 297 the model has a loss of 0.260210782289505\n",
      "\u001b[1m297/689\u001b[0m \u001b[32m━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━\u001b[0m \u001b[1m22s\u001b[0m 57ms/step - auc: 0.9665 - binary_accuracy: 0.9262 - false_negatives: 117.5791 - false_positives: 248.9024 - loss: 0.2443 - precision: 0.9066 - recall: 0.9529 - true_negatives: 2114.0674 - true_positives: 2287.4512\n",
      "For batch number 298 the model has a loss of 0.26110148429870605\n",
      "\u001b[1m298/689\u001b[0m \u001b[32m━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━\u001b[0m \u001b[1m22s\u001b[0m 57ms/step - auc: 0.9664 - binary_accuracy: 0.9262 - false_negatives: 118.0235 - false_positives: 249.7852 - loss: 0.2444 - precision: 0.9065 - recall: 0.9529 - true_negatives: 2121.2517 - true_positives: 2294.9397\n",
      "For batch number 299 the model has a loss of 0.2608024775981903\n",
      "\u001b[1m299/689\u001b[0m \u001b[32m━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━\u001b[0m \u001b[1m22s\u001b[0m 57ms/step - auc: 0.9664 - binary_accuracy: 0.9261 - false_negatives: 118.4649 - false_positives: 250.6622 - loss: 0.2444 - precision: 0.9065 - recall: 0.9529 - true_negatives: 2128.4248 - true_positives: 2302.4482\n",
      "For batch number 300 the model has a loss of 0.2608151435852051\n",
      "\u001b[1m300/689\u001b[0m \u001b[32m━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━\u001b[0m \u001b[1m21s\u001b[0m 57ms/step - auc: 0.9664 - binary_accuracy: 0.9261 - false_negatives: 118.9067 - false_positives: 251.5400 - loss: 0.2445 - precision: 0.9065 - recall: 0.9529 - true_negatives: 2135.5933 - true_positives: 2309.9600\n",
      "For batch number 301 the model has a loss of 0.2604389488697052\n",
      "\u001b[1m301/689\u001b[0m \u001b[32m━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━\u001b[0m \u001b[1m21s\u001b[0m 57ms/step - auc: 0.9664 - binary_accuracy: 0.9261 - false_negatives: 119.3455 - false_positives: 252.4186 - loss: 0.2446 - precision: 0.9065 - recall: 0.9529 - true_negatives: 2142.7642 - true_positives: 2317.4717\n",
      "For batch number 302 the model has a loss of 0.2600046992301941\n",
      "\u001b[1m302/689\u001b[0m \u001b[32m━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━\u001b[0m \u001b[1m21s\u001b[0m 57ms/step - auc: 0.9664 - binary_accuracy: 0.9261 - false_negatives: 119.7848 - false_positives: 253.2947 - loss: 0.2446 - precision: 0.9064 - recall: 0.9528 - true_negatives: 2149.9272 - true_positives: 2324.9934\n",
      "For batch number 303 the model has a loss of 0.2594890296459198\n",
      "\u001b[1m303/689\u001b[0m \u001b[32m━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━\u001b[0m \u001b[1m21s\u001b[0m 57ms/step - auc: 0.9664 - binary_accuracy: 0.9261 - false_negatives: 120.2211 - false_positives: 254.1650 - loss: 0.2447 - precision: 0.9064 - recall: 0.9528 - true_negatives: 2157.0925 - true_positives: 2332.5215\n",
      "For batch number 304 the model has a loss of 0.25970926880836487\n",
      "\u001b[1m304/689\u001b[0m \u001b[32m━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━\u001b[0m \u001b[1m21s\u001b[0m 57ms/step - auc: 0.9663 - binary_accuracy: 0.9261 - false_negatives: 120.6645 - false_positives: 255.0296 - loss: 0.2447 - precision: 0.9064 - recall: 0.9528 - true_negatives: 2164.2500 - true_positives: 2340.0559\n",
      "For batch number 305 the model has a loss of 0.26031437516212463\n",
      "\u001b[1m305/689\u001b[0m \u001b[32m━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━\u001b[0m \u001b[1m21s\u001b[0m 57ms/step - auc: 0.9663 - binary_accuracy: 0.9260 - false_negatives: 121.1049 - false_positives: 255.9115 - loss: 0.2448 - precision: 0.9064 - recall: 0.9528 - true_negatives: 2171.4197 - true_positives: 2347.5640\n",
      "For batch number 306 the model has a loss of 0.25985634326934814\n",
      "\u001b[1m306/689\u001b[0m \u001b[32m━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━\u001b[0m \u001b[1m21s\u001b[0m 57ms/step - auc: 0.9663 - binary_accuracy: 0.9260 - false_negatives: 121.5425 - false_positives: 256.7909 - loss: 0.2448 - precision: 0.9063 - recall: 0.9528 - true_negatives: 2178.6045 - true_positives: 2355.0620\n",
      "For batch number 307 the model has a loss of 0.2596506178379059\n",
      "\u001b[1m307/689\u001b[0m \u001b[32m━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━\u001b[0m \u001b[1m21s\u001b[0m 57ms/step - auc: 0.9663 - binary_accuracy: 0.9260 - false_negatives: 121.9772 - false_positives: 257.6710 - loss: 0.2449 - precision: 0.9063 - recall: 0.9528 - true_negatives: 2185.7947 - true_positives: 2362.5569\n",
      "For batch number 308 the model has a loss of 0.25950634479522705\n",
      "\u001b[1m308/689\u001b[0m \u001b[32m━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━\u001b[0m \u001b[1m21s\u001b[0m 57ms/step - auc: 0.9663 - binary_accuracy: 0.9260 - false_negatives: 122.4091 - false_positives: 258.5519 - loss: 0.2449 - precision: 0.9063 - recall: 0.9527 - true_negatives: 2192.9839 - true_positives: 2370.0552\n",
      "For batch number 309 the model has a loss of 0.2599198818206787\n",
      "\u001b[1m309/689\u001b[0m \u001b[32m━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━\u001b[0m \u001b[1m21s\u001b[0m 57ms/step - auc: 0.9663 - binary_accuracy: 0.9260 - false_negatives: 122.8414 - false_positives: 259.4369 - loss: 0.2450 - precision: 0.9063 - recall: 0.9527 - true_negatives: 2200.1812 - true_positives: 2377.5405\n",
      "For batch number 310 the model has a loss of 0.259676069021225\n",
      "\u001b[1m310/689\u001b[0m \u001b[32m━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━\u001b[0m \u001b[1m21s\u001b[0m 57ms/step - auc: 0.9662 - binary_accuracy: 0.9259 - false_negatives: 123.2710 - false_positives: 260.3194 - loss: 0.2450 - precision: 0.9062 - recall: 0.9527 - true_negatives: 2207.3613 - true_positives: 2385.0483\n",
      "For batch number 311 the model has a loss of 0.25948259234428406\n",
      "\u001b[1m311/689\u001b[0m \u001b[32m━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━\u001b[0m \u001b[1m21s\u001b[0m 57ms/step - auc: 0.9662 - binary_accuracy: 0.9259 - false_negatives: 123.6977 - false_positives: 261.2026 - loss: 0.2450 - precision: 0.9062 - recall: 0.9527 - true_negatives: 2214.5337 - true_positives: 2392.5659\n",
      "For batch number 312 the model has a loss of 0.25917357206344604\n",
      "\u001b[1m312/689\u001b[0m \u001b[32m━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━\u001b[0m \u001b[1m21s\u001b[0m 57ms/step - auc: 0.9662 - binary_accuracy: 0.9259 - false_negatives: 124.1218 - false_positives: 262.0865 - loss: 0.2451 - precision: 0.9062 - recall: 0.9527 - true_negatives: 2221.7019 - true_positives: 2400.0898\n",
      "For batch number 313 the model has a loss of 0.25859785079956055\n",
      "\u001b[1m313/689\u001b[0m \u001b[32m━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━\u001b[0m \u001b[1m21s\u001b[0m 57ms/step - auc: 0.9662 - binary_accuracy: 0.9259 - false_negatives: 124.5431 - false_positives: 262.9648 - loss: 0.2451 - precision: 0.9061 - recall: 0.9527 - true_negatives: 2228.8850 - true_positives: 2407.6069\n",
      "For batch number 314 the model has a loss of 0.2581913471221924\n",
      "\u001b[1m314/689\u001b[0m \u001b[32m━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━\u001b[0m \u001b[1m21s\u001b[0m 57ms/step - auc: 0.9662 - binary_accuracy: 0.9259 - false_negatives: 124.9618 - false_positives: 263.8408 - loss: 0.2452 - precision: 0.9061 - recall: 0.9527 - true_negatives: 2236.0542 - true_positives: 2415.1433\n",
      "For batch number 315 the model has a loss of 0.25799500942230225\n",
      "\u001b[1m315/689\u001b[0m \u001b[32m━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━\u001b[0m \u001b[1m21s\u001b[0m 57ms/step - auc: 0.9662 - binary_accuracy: 0.9259 - false_negatives: 125.3778 - false_positives: 264.7175 - loss: 0.2452 - precision: 0.9061 - recall: 0.9527 - true_negatives: 2243.2285 - true_positives: 2422.6763\n",
      "For batch number 316 the model has a loss of 0.25788819789886475\n",
      "\u001b[1m316/689\u001b[0m \u001b[32m━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━\u001b[0m \u001b[1m21s\u001b[0m 57ms/step - auc: 0.9662 - binary_accuracy: 0.9258 - false_negatives: 125.7911 - false_positives: 265.5981 - loss: 0.2453 - precision: 0.9061 - recall: 0.9526 - true_negatives: 2250.3955 - true_positives: 2430.2151\n",
      "For batch number 317 the model has a loss of 0.25727972388267517\n",
      "\u001b[1m317/689\u001b[0m \u001b[32m━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━\u001b[0m \u001b[1m21s\u001b[0m 57ms/step - auc: 0.9661 - binary_accuracy: 0.9258 - false_negatives: 126.2019 - false_positives: 266.4732 - loss: 0.2453 - precision: 0.9061 - recall: 0.9526 - true_negatives: 2257.5647 - true_positives: 2437.7603\n",
      "For batch number 318 the model has a loss of 0.2566830515861511\n",
      "\u001b[1m318/689\u001b[0m \u001b[32m━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━\u001b[0m \u001b[1m20s\u001b[0m 57ms/step - auc: 0.9661 - binary_accuracy: 0.9258 - false_negatives: 126.6101 - false_positives: 267.3428 - loss: 0.2453 - precision: 0.9060 - recall: 0.9526 - true_negatives: 2264.7422 - true_positives: 2445.3049\n",
      "For batch number 319 the model has a loss of 0.2561797499656677\n",
      "\u001b[1m319/689\u001b[0m \u001b[32m━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━\u001b[0m \u001b[1m20s\u001b[0m 57ms/step - auc: 0.9661 - binary_accuracy: 0.9258 - false_negatives: 127.0157 - false_positives: 268.2100 - loss: 0.2454 - precision: 0.9060 - recall: 0.9526 - true_negatives: 2271.9153 - true_positives: 2452.8589\n",
      "For batch number 320 the model has a loss of 0.2558444142341614\n",
      "\u001b[1m320/689\u001b[0m \u001b[32m━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━\u001b[0m \u001b[1m20s\u001b[0m 57ms/step - auc: 0.9661 - binary_accuracy: 0.9258 - false_negatives: 127.4250 - false_positives: 269.0750 - loss: 0.2454 - precision: 0.9060 - recall: 0.9526 - true_negatives: 2279.0845 - true_positives: 2460.4155\n",
      "For batch number 321 the model has a loss of 0.2576487362384796\n",
      "\u001b[1m321/689\u001b[0m \u001b[32m━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━\u001b[0m \u001b[1m20s\u001b[0m 57ms/step - auc: 0.9661 - binary_accuracy: 0.9258 - false_negatives: 127.8349 - false_positives: 269.9408 - loss: 0.2454 - precision: 0.9060 - recall: 0.9526 - true_negatives: 2286.2554 - true_positives: 2467.9688\n",
      "For batch number 322 the model has a loss of 0.25745588541030884\n",
      "\u001b[1m322/689\u001b[0m \u001b[32m━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━\u001b[0m \u001b[1m20s\u001b[0m 57ms/step - auc: 0.9661 - binary_accuracy: 0.9258 - false_negatives: 128.2422 - false_positives: 270.8075 - loss: 0.2455 - precision: 0.9059 - recall: 0.9526 - true_negatives: 2293.4285 - true_positives: 2475.5217\n",
      "For batch number 323 the model has a loss of 0.257230281829834\n",
      "\u001b[1m323/689\u001b[0m \u001b[32m━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━\u001b[0m \u001b[1m20s\u001b[0m 57ms/step - auc: 0.9661 - binary_accuracy: 0.9258 - false_negatives: 128.6502 - false_positives: 271.6718 - loss: 0.2455 - precision: 0.9059 - recall: 0.9526 - true_negatives: 2300.5945 - true_positives: 2483.0835\n",
      "For batch number 324 the model has a loss of 0.2572537064552307\n",
      "\u001b[1m324/689\u001b[0m \u001b[32m━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━\u001b[0m \u001b[1m20s\u001b[0m 57ms/step - auc: 0.9661 - binary_accuracy: 0.9257 - false_negatives: 129.0586 - false_positives: 272.5370 - loss: 0.2455 - precision: 0.9059 - recall: 0.9526 - true_negatives: 2307.7686 - true_positives: 2490.6357\n",
      "For batch number 325 the model has a loss of 0.25766587257385254\n",
      "\u001b[1m325/689\u001b[0m \u001b[32m━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━\u001b[0m \u001b[1m20s\u001b[0m 57ms/step - auc: 0.9660 - binary_accuracy: 0.9257 - false_negatives: 129.4708 - false_positives: 273.4000 - loss: 0.2456 - precision: 0.9059 - recall: 0.9526 - true_negatives: 2314.9417 - true_positives: 2498.1877\n",
      "For batch number 326 the model has a loss of 0.257242351770401\n",
      "\u001b[1m326/689\u001b[0m \u001b[32m━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━\u001b[0m \u001b[1m20s\u001b[0m 57ms/step - auc: 0.9660 - binary_accuracy: 0.9257 - false_negatives: 129.8804 - false_positives: 274.2607 - loss: 0.2456 - precision: 0.9059 - recall: 0.9526 - true_negatives: 2322.1228 - true_positives: 2505.7361\n",
      "For batch number 327 the model has a loss of 0.2571372985839844\n",
      "\u001b[1m327/689\u001b[0m \u001b[32m━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━\u001b[0m \u001b[1m20s\u001b[0m 57ms/step - auc: 0.9660 - binary_accuracy: 0.9257 - false_negatives: 130.2905 - false_positives: 275.1193 - loss: 0.2457 - precision: 0.9058 - recall: 0.9525 - true_negatives: 2329.3088 - true_positives: 2513.2812\n",
      "For batch number 328 the model has a loss of 0.2573571801185608\n",
      "\u001b[1m328/689\u001b[0m \u001b[32m━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━\u001b[0m \u001b[1m20s\u001b[0m 57ms/step - auc: 0.9660 - binary_accuracy: 0.9257 - false_negatives: 130.7043 - false_positives: 275.9817 - loss: 0.2457 - precision: 0.9058 - recall: 0.9525 - true_negatives: 2336.5090 - true_positives: 2520.8049\n",
      "For batch number 329 the model has a loss of 0.25709861516952515\n",
      "\u001b[1m329/689\u001b[0m \u001b[32m━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━\u001b[0m \u001b[1m20s\u001b[0m 57ms/step - auc: 0.9660 - binary_accuracy: 0.9257 - false_negatives: 131.1155 - false_positives: 276.8419 - loss: 0.2457 - precision: 0.9058 - recall: 0.9525 - true_negatives: 2343.7295 - true_positives: 2528.3130\n",
      "For batch number 330 the model has a loss of 0.257185161113739\n",
      "\u001b[1m330/689\u001b[0m \u001b[32m━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━\u001b[0m \u001b[1m20s\u001b[0m 57ms/step - auc: 0.9660 - binary_accuracy: 0.9257 - false_negatives: 131.5242 - false_positives: 277.7091 - loss: 0.2458 - precision: 0.9058 - recall: 0.9525 - true_negatives: 2350.9546 - true_positives: 2535.8120\n",
      "For batch number 331 the model has a loss of 0.2568970322608948\n",
      "\u001b[1m331/689\u001b[0m \u001b[32m━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━\u001b[0m \u001b[1m20s\u001b[0m 57ms/step - auc: 0.9660 - binary_accuracy: 0.9257 - false_negatives: 131.9335 - false_positives: 278.5740 - loss: 0.2458 - precision: 0.9058 - recall: 0.9525 - true_negatives: 2358.1782 - true_positives: 2543.3142\n",
      "For batch number 332 the model has a loss of 0.2573668658733368\n",
      "\u001b[1m332/689\u001b[0m \u001b[32m━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━\u001b[0m \u001b[1m20s\u001b[0m 57ms/step - auc: 0.9660 - binary_accuracy: 0.9256 - false_negatives: 132.3434 - false_positives: 279.4398 - loss: 0.2458 - precision: 0.9057 - recall: 0.9525 - true_negatives: 2365.3916 - true_positives: 2550.8252\n",
      "For batch number 333 the model has a loss of 0.25791728496551514\n",
      "\u001b[1m333/689\u001b[0m \u001b[32m━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━\u001b[0m \u001b[1m20s\u001b[0m 57ms/step - auc: 0.9660 - binary_accuracy: 0.9256 - false_negatives: 132.7598 - false_positives: 280.3003 - loss: 0.2459 - precision: 0.9057 - recall: 0.9525 - true_negatives: 2372.6006 - true_positives: 2558.3394\n",
      "For batch number 334 the model has a loss of 0.25774672627449036\n",
      "\u001b[1m334/689\u001b[0m \u001b[32m━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━\u001b[0m \u001b[1m20s\u001b[0m 57ms/step - auc: 0.9659 - binary_accuracy: 0.9256 - false_negatives: 133.1767 - false_positives: 281.1587 - loss: 0.2459 - precision: 0.9057 - recall: 0.9525 - true_negatives: 2379.8083 - true_positives: 2565.8562\n",
      "For batch number 335 the model has a loss of 0.25792738795280457\n",
      "\u001b[1m335/689\u001b[0m \u001b[32m━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━\u001b[0m \u001b[1m20s\u001b[0m 57ms/step - auc: 0.9659 - binary_accuracy: 0.9256 - false_negatives: 133.5940 - false_positives: 282.0239 - loss: 0.2459 - precision: 0.9057 - recall: 0.9525 - true_negatives: 2387.0120 - true_positives: 2573.3701\n",
      "For batch number 336 the model has a loss of 0.2583019733428955\n",
      "\u001b[1m336/689\u001b[0m \u001b[32m━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━\u001b[0m \u001b[1m20s\u001b[0m 57ms/step - auc: 0.9659 - binary_accuracy: 0.9256 - false_negatives: 134.0179 - false_positives: 282.8839 - loss: 0.2460 - precision: 0.9057 - recall: 0.9525 - true_negatives: 2394.2053 - true_positives: 2580.8928\n",
      "For batch number 337 the model has a loss of 0.2583267390727997\n",
      "\u001b[1m337/689\u001b[0m \u001b[32m━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━\u001b[0m \u001b[1m19s\u001b[0m 57ms/step - auc: 0.9659 - binary_accuracy: 0.9256 - false_negatives: 134.4421 - false_positives: 283.7448 - loss: 0.2460 - precision: 0.9057 - recall: 0.9524 - true_negatives: 2401.4065 - true_positives: 2588.4065\n",
      "For batch number 338 the model has a loss of 0.25793513655662537\n",
      "\u001b[1m338/689\u001b[0m \u001b[32m━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━\u001b[0m \u001b[1m19s\u001b[0m 57ms/step - auc: 0.9659 - binary_accuracy: 0.9256 - false_negatives: 134.8669 - false_positives: 284.6035 - loss: 0.2460 - precision: 0.9056 - recall: 0.9524 - true_negatives: 2408.6094 - true_positives: 2595.9202\n",
      "For batch number 339 the model has a loss of 0.25813692808151245\n",
      "\u001b[1m339/689\u001b[0m \u001b[32m━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━\u001b[0m \u001b[1m19s\u001b[0m 57ms/step - auc: 0.9659 - binary_accuracy: 0.9256 - false_negatives: 135.2950 - false_positives: 285.4631 - loss: 0.2461 - precision: 0.9056 - recall: 0.9524 - true_negatives: 2415.8142 - true_positives: 2603.4277\n",
      "For batch number 340 the model has a loss of 0.2578710615634918\n",
      "\u001b[1m340/689\u001b[0m \u001b[32m━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━\u001b[0m \u001b[1m19s\u001b[0m 57ms/step - auc: 0.9659 - binary_accuracy: 0.9255 - false_negatives: 135.7206 - false_positives: 286.3235 - loss: 0.2461 - precision: 0.9056 - recall: 0.9524 - true_negatives: 2423.0088 - true_positives: 2610.9470\n",
      "For batch number 341 the model has a loss of 0.25735849142074585\n",
      "\u001b[1m341/689\u001b[0m \u001b[32m━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━\u001b[0m \u001b[1m19s\u001b[0m 57ms/step - auc: 0.9659 - binary_accuracy: 0.9255 - false_negatives: 136.1437 - false_positives: 287.1789 - loss: 0.2461 - precision: 0.9056 - recall: 0.9524 - true_negatives: 2430.1965 - true_positives: 2618.4810\n",
      "For batch number 342 the model has a loss of 0.2579016387462616\n",
      "\u001b[1m342/689\u001b[0m \u001b[32m━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━\u001b[0m \u001b[1m19s\u001b[0m 57ms/step - auc: 0.9658 - binary_accuracy: 0.9255 - false_negatives: 136.5672 - false_positives: 288.0409 - loss: 0.2462 - precision: 0.9056 - recall: 0.9524 - true_negatives: 2437.3860 - true_positives: 2626.0059\n",
      "For batch number 343 the model has a loss of 0.2576709985733032\n",
      "\u001b[1m343/689\u001b[0m \u001b[32m━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━\u001b[0m \u001b[1m19s\u001b[0m 57ms/step - auc: 0.9658 - binary_accuracy: 0.9255 - false_negatives: 136.9913 - false_positives: 288.9009 - loss: 0.2462 - precision: 0.9055 - recall: 0.9524 - true_negatives: 2444.5801 - true_positives: 2633.5276\n",
      "For batch number 344 the model has a loss of 0.2575971186161041\n",
      "\u001b[1m344/689\u001b[0m \u001b[32m━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━\u001b[0m \u001b[1m19s\u001b[0m 57ms/step - auc: 0.9658 - binary_accuracy: 0.9255 - false_negatives: 137.4157 - false_positives: 289.7616 - loss: 0.2462 - precision: 0.9055 - recall: 0.9524 - true_negatives: 2451.7791 - true_positives: 2641.0437\n",
      "For batch number 345 the model has a loss of 0.2578418552875519\n",
      "\u001b[1m345/689\u001b[0m \u001b[32m━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━\u001b[0m \u001b[1m19s\u001b[0m 57ms/step - auc: 0.9658 - binary_accuracy: 0.9255 - false_negatives: 137.8377 - false_positives: 290.6261 - loss: 0.2463 - precision: 0.9055 - recall: 0.9524 - true_negatives: 2458.9797 - true_positives: 2648.5566\n",
      "For batch number 346 the model has a loss of 0.25806862115859985\n",
      "\u001b[1m346/689\u001b[0m \u001b[32m━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━\u001b[0m \u001b[1m19s\u001b[0m 57ms/step - auc: 0.9658 - binary_accuracy: 0.9255 - false_negatives: 138.2572 - false_positives: 291.4971 - loss: 0.2463 - precision: 0.9055 - recall: 0.9523 - true_negatives: 2466.1851 - true_positives: 2656.0608\n",
      "For batch number 347 the model has a loss of 0.25787216424942017\n",
      "\u001b[1m347/689\u001b[0m \u001b[32m━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━\u001b[0m \u001b[1m19s\u001b[0m 57ms/step - auc: 0.9658 - binary_accuracy: 0.9254 - false_negatives: 138.6772 - false_positives: 292.3689 - loss: 0.2463 - precision: 0.9055 - recall: 0.9523 - true_negatives: 2473.3948 - true_positives: 2663.5591\n",
      "For batch number 348 the model has a loss of 0.2584743797779083\n",
      "\u001b[1m348/689\u001b[0m \u001b[32m━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━\u001b[0m \u001b[1m19s\u001b[0m 57ms/step - auc: 0.9658 - binary_accuracy: 0.9254 - false_negatives: 139.1035 - false_positives: 293.2414 - loss: 0.2464 - precision: 0.9054 - recall: 0.9523 - true_negatives: 2480.6150 - true_positives: 2671.0403\n",
      "For batch number 349 the model has a loss of 0.25807636976242065\n",
      "\u001b[1m349/689\u001b[0m \u001b[32m━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━\u001b[0m \u001b[1m19s\u001b[0m 57ms/step - auc: 0.9658 - binary_accuracy: 0.9254 - false_negatives: 139.5272 - false_positives: 294.1118 - loss: 0.2464 - precision: 0.9054 - recall: 0.9523 - true_negatives: 2487.8367 - true_positives: 2678.5244\n",
      "For batch number 350 the model has a loss of 0.2579079866409302\n",
      "\u001b[1m350/689\u001b[0m \u001b[32m━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━\u001b[0m \u001b[1m19s\u001b[0m 57ms/step - auc: 0.9658 - binary_accuracy: 0.9254 - false_negatives: 139.9486 - false_positives: 294.9828 - loss: 0.2464 - precision: 0.9054 - recall: 0.9523 - true_negatives: 2495.0627 - true_positives: 2686.0056\n",
      "For batch number 351 the model has a loss of 0.2586163878440857\n",
      "\u001b[1m351/689\u001b[0m \u001b[32m━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━\u001b[0m \u001b[1m19s\u001b[0m 57ms/step - auc: 0.9657 - binary_accuracy: 0.9254 - false_negatives: 140.3675 - false_positives: 295.8547 - loss: 0.2465 - precision: 0.9054 - recall: 0.9523 - true_negatives: 2502.2764 - true_positives: 2693.5015\n",
      "For batch number 352 the model has a loss of 0.2585125267505646\n",
      "\u001b[1m352/689\u001b[0m \u001b[32m━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━\u001b[0m \u001b[1m19s\u001b[0m 57ms/step - auc: 0.9657 - binary_accuracy: 0.9254 - false_negatives: 140.7841 - false_positives: 296.7273 - loss: 0.2465 - precision: 0.9054 - recall: 0.9523 - true_negatives: 2509.4971 - true_positives: 2700.9915\n",
      "For batch number 353 the model has a loss of 0.25839996337890625\n",
      "\u001b[1m353/689\u001b[0m \u001b[32m━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━\u001b[0m \u001b[1m19s\u001b[0m 57ms/step - auc: 0.9657 - binary_accuracy: 0.9254 - false_negatives: 141.2011 - false_positives: 297.5949 - loss: 0.2466 - precision: 0.9053 - recall: 0.9523 - true_negatives: 2516.7139 - true_positives: 2708.4900\n",
      "For batch number 354 the model has a loss of 0.25809335708618164\n",
      "\u001b[1m354/689\u001b[0m \u001b[32m━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━\u001b[0m \u001b[1m19s\u001b[0m 57ms/step - auc: 0.9657 - binary_accuracy: 0.9254 - false_negatives: 141.6158 - false_positives: 298.4633 - loss: 0.2466 - precision: 0.9053 - recall: 0.9523 - true_negatives: 2523.9294 - true_positives: 2715.9915\n",
      "For batch number 355 the model has a loss of 0.25827863812446594\n",
      "\u001b[1m355/689\u001b[0m \u001b[32m━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━\u001b[0m \u001b[1m19s\u001b[0m 57ms/step - auc: 0.9657 - binary_accuracy: 0.9253 - false_negatives: 142.0310 - false_positives: 299.3324 - loss: 0.2466 - precision: 0.9053 - recall: 0.9523 - true_negatives: 2531.1465 - true_positives: 2723.4902\n",
      "For batch number 356 the model has a loss of 0.2590406537055969\n",
      "\u001b[1m356/689\u001b[0m \u001b[32m━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━\u001b[0m \u001b[1m18s\u001b[0m 57ms/step - auc: 0.9657 - binary_accuracy: 0.9253 - false_negatives: 142.4494 - false_positives: 300.1994 - loss: 0.2467 - precision: 0.9053 - recall: 0.9522 - true_negatives: 2538.3511 - true_positives: 2731.0000\n",
      "For batch number 357 the model has a loss of 0.2586946189403534\n",
      "\u001b[1m357/689\u001b[0m \u001b[32m━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━\u001b[0m \u001b[1m18s\u001b[0m 57ms/step - auc: 0.9657 - binary_accuracy: 0.9253 - false_negatives: 142.8655 - false_positives: 301.0672 - loss: 0.2467 - precision: 0.9053 - recall: 0.9522 - true_negatives: 2545.5547 - true_positives: 2738.5127\n",
      "For batch number 358 the model has a loss of 0.2589457035064697\n",
      "\u001b[1m358/689\u001b[0m \u001b[32m━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━\u001b[0m \u001b[1m18s\u001b[0m 57ms/step - auc: 0.9657 - binary_accuracy: 0.9253 - false_negatives: 143.2849 - false_positives: 301.9358 - loss: 0.2467 - precision: 0.9053 - recall: 0.9522 - true_negatives: 2552.7571 - true_positives: 2746.0225\n",
      "For batch number 359 the model has a loss of 0.2589769959449768\n",
      "\u001b[1m359/689\u001b[0m \u001b[32m━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━\u001b[0m \u001b[1m18s\u001b[0m 57ms/step - auc: 0.9656 - binary_accuracy: 0.9253 - false_negatives: 143.7020 - false_positives: 302.8022 - loss: 0.2468 - precision: 0.9052 - recall: 0.9522 - true_negatives: 2559.9500 - true_positives: 2753.5459\n",
      "For batch number 360 the model has a loss of 0.25921058654785156\n",
      "\u001b[1m360/689\u001b[0m \u001b[32m━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━\u001b[0m \u001b[1m18s\u001b[0m 57ms/step - auc: 0.9656 - binary_accuracy: 0.9253 - false_negatives: 144.1194 - false_positives: 303.6694 - loss: 0.2468 - precision: 0.9052 - recall: 0.9522 - true_negatives: 2567.1389 - true_positives: 2761.0723\n",
      "For batch number 361 the model has a loss of 0.25886639952659607\n",
      "\u001b[1m361/689\u001b[0m \u001b[32m━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━\u001b[0m \u001b[1m18s\u001b[0m 57ms/step - auc: 0.9656 - binary_accuracy: 0.9253 - false_negatives: 144.5346 - false_positives: 304.5319 - loss: 0.2468 - precision: 0.9052 - recall: 0.9522 - true_negatives: 2574.3269 - true_positives: 2768.6067\n",
      "For batch number 362 the model has a loss of 0.2586685121059418\n",
      "\u001b[1m362/689\u001b[0m \u001b[32m━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━\u001b[0m \u001b[1m18s\u001b[0m 57ms/step - auc: 0.9656 - binary_accuracy: 0.9253 - false_negatives: 144.9475 - false_positives: 305.3950 - loss: 0.2469 - precision: 0.9052 - recall: 0.9522 - true_negatives: 2581.5056 - true_positives: 2776.1519\n",
      "For batch number 363 the model has a loss of 0.258711040019989\n",
      "\u001b[1m363/689\u001b[0m \u001b[32m━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━\u001b[0m \u001b[1m18s\u001b[0m 57ms/step - auc: 0.9656 - binary_accuracy: 0.9253 - false_negatives: 145.3609 - false_positives: 306.2617 - loss: 0.2469 - precision: 0.9052 - recall: 0.9522 - true_negatives: 2588.6887 - true_positives: 2783.6887\n",
      "For batch number 364 the model has a loss of 0.2585342228412628\n",
      "\u001b[1m364/689\u001b[0m \u001b[32m━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━\u001b[0m \u001b[1m18s\u001b[0m 57ms/step - auc: 0.9656 - binary_accuracy: 0.9252 - false_negatives: 145.7720 - false_positives: 307.1291 - loss: 0.2469 - precision: 0.9051 - recall: 0.9522 - true_negatives: 2595.8792 - true_positives: 2791.2197\n",
      "For batch number 365 the model has a loss of 0.2582278251647949\n",
      "\u001b[1m365/689\u001b[0m \u001b[32m━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━\u001b[0m \u001b[1m18s\u001b[0m 57ms/step - auc: 0.9656 - binary_accuracy: 0.9252 - false_negatives: 146.1808 - false_positives: 307.9945 - loss: 0.2470 - precision: 0.9051 - recall: 0.9522 - true_negatives: 2603.0740 - true_positives: 2798.7507\n",
      "For batch number 366 the model has a loss of 0.2590296268463135\n",
      "\u001b[1m366/689\u001b[0m \u001b[32m━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━\u001b[0m \u001b[1m18s\u001b[0m 57ms/step - auc: 0.9656 - binary_accuracy: 0.9252 - false_negatives: 146.5929 - false_positives: 308.8634 - loss: 0.2470 - precision: 0.9051 - recall: 0.9522 - true_negatives: 2610.2759 - true_positives: 2806.2678\n",
      "For batch number 367 the model has a loss of 0.25885871052742004\n",
      "\u001b[1m367/689\u001b[0m \u001b[32m━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━\u001b[0m \u001b[1m18s\u001b[0m 57ms/step - auc: 0.9656 - binary_accuracy: 0.9252 - false_negatives: 147.0054 - false_positives: 309.7303 - loss: 0.2470 - precision: 0.9051 - recall: 0.9521 - true_negatives: 2617.4795 - true_positives: 2813.7847\n",
      "For batch number 368 the model has a loss of 0.2584170401096344\n",
      "\u001b[1m368/689\u001b[0m \u001b[32m━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━\u001b[0m \u001b[1m18s\u001b[0m 57ms/step - auc: 0.9655 - binary_accuracy: 0.9252 - false_negatives: 147.4158 - false_positives: 310.5924 - loss: 0.2470 - precision: 0.9051 - recall: 0.9521 - true_negatives: 2624.6929 - true_positives: 2821.2988\n",
      "For batch number 369 the model has a loss of 0.2582685649394989\n",
      "\u001b[1m369/689\u001b[0m \u001b[32m━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━\u001b[0m \u001b[1m18s\u001b[0m 57ms/step - auc: 0.9655 - binary_accuracy: 0.9252 - false_negatives: 147.8266 - false_positives: 311.4499 - loss: 0.2471 - precision: 0.9051 - recall: 0.9521 - true_negatives: 2631.9080 - true_positives: 2828.8157\n",
      "For batch number 370 the model has a loss of 0.2578720152378082\n",
      "\u001b[1m370/689\u001b[0m \u001b[32m━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━\u001b[0m \u001b[1m18s\u001b[0m 57ms/step - auc: 0.9655 - binary_accuracy: 0.9252 - false_negatives: 148.2351 - false_positives: 312.3027 - loss: 0.2471 - precision: 0.9050 - recall: 0.9521 - true_negatives: 2639.1135 - true_positives: 2836.3486\n",
      "For batch number 371 the model has a loss of 0.2576969563961029\n",
      "\u001b[1m371/689\u001b[0m \u001b[32m━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━\u001b[0m \u001b[1m18s\u001b[0m 57ms/step - auc: 0.9655 - binary_accuracy: 0.9252 - false_negatives: 148.6415 - false_positives: 313.1563 - loss: 0.2471 - precision: 0.9050 - recall: 0.9521 - true_negatives: 2646.3262 - true_positives: 2843.8760\n",
      "For batch number 372 the model has a loss of 0.258942186832428\n",
      "\u001b[1m372/689\u001b[0m \u001b[32m━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━\u001b[0m \u001b[1m18s\u001b[0m 57ms/step - auc: 0.9655 - binary_accuracy: 0.9252 - false_negatives: 149.0511 - false_positives: 314.0134 - loss: 0.2472 - precision: 0.9050 - recall: 0.9521 - true_negatives: 2653.5349 - true_positives: 2851.4006\n",
      "For batch number 373 the model has a loss of 0.25890615582466125\n",
      "\u001b[1m373/689\u001b[0m \u001b[32m━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━\u001b[0m \u001b[1m18s\u001b[0m 57ms/step - auc: 0.9655 - binary_accuracy: 0.9252 - false_negatives: 149.4585 - false_positives: 314.8713 - loss: 0.2472 - precision: 0.9050 - recall: 0.9521 - true_negatives: 2660.7319 - true_positives: 2858.9382\n",
      "For batch number 374 the model has a loss of 0.25880053639411926\n",
      "\u001b[1m374/689\u001b[0m \u001b[32m━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━\u001b[0m \u001b[1m17s\u001b[0m 57ms/step - auc: 0.9655 - binary_accuracy: 0.9251 - false_negatives: 149.8636 - false_positives: 315.7299 - loss: 0.2472 - precision: 0.9050 - recall: 0.9521 - true_negatives: 2667.9385 - true_positives: 2866.4680\n",
      "For batch number 375 the model has a loss of 0.25856220722198486\n",
      "\u001b[1m375/689\u001b[0m \u001b[32m━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━\u001b[0m \u001b[1m17s\u001b[0m 57ms/step - auc: 0.9655 - binary_accuracy: 0.9251 - false_negatives: 150.2693 - false_positives: 316.5840 - loss: 0.2473 - precision: 0.9050 - recall: 0.9521 - true_negatives: 2675.1414 - true_positives: 2874.0054\n",
      "For batch number 376 the model has a loss of 0.2584027051925659\n",
      "\u001b[1m376/689\u001b[0m \u001b[32m━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━\u001b[0m \u001b[1m17s\u001b[0m 57ms/step - auc: 0.9655 - binary_accuracy: 0.9251 - false_negatives: 150.6729 - false_positives: 317.4362 - loss: 0.2473 - precision: 0.9050 - recall: 0.9521 - true_negatives: 2682.3430 - true_positives: 2881.5479\n",
      "For batch number 377 the model has a loss of 0.2582850456237793\n",
      "\u001b[1m377/689\u001b[0m \u001b[32m━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━\u001b[0m \u001b[1m17s\u001b[0m 57ms/step - auc: 0.9654 - binary_accuracy: 0.9251 - false_negatives: 151.0743 - false_positives: 318.2891 - loss: 0.2473 - precision: 0.9049 - recall: 0.9521 - true_negatives: 2689.5518 - true_positives: 2889.0850\n",
      "For batch number 378 the model has a loss of 0.2582172155380249\n",
      "\u001b[1m378/689\u001b[0m \u001b[32m━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━\u001b[0m \u001b[1m17s\u001b[0m 57ms/step - auc: 0.9654 - binary_accuracy: 0.9251 - false_negatives: 151.4762 - false_positives: 319.1376 - loss: 0.2473 - precision: 0.9049 - recall: 0.9521 - true_negatives: 2696.7593 - true_positives: 2896.6270\n",
      "For batch number 379 the model has a loss of 0.25790923833847046\n",
      "\u001b[1m379/689\u001b[0m \u001b[32m━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━\u001b[0m \u001b[1m17s\u001b[0m 57ms/step - auc: 0.9654 - binary_accuracy: 0.9251 - false_negatives: 151.8760 - false_positives: 319.9868 - loss: 0.2474 - precision: 0.9049 - recall: 0.9521 - true_negatives: 2703.9683 - true_positives: 2904.1689\n",
      "For batch number 380 the model has a loss of 0.25797608494758606\n",
      "\u001b[1m380/689\u001b[0m \u001b[32m━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━\u001b[0m \u001b[1m17s\u001b[0m 57ms/step - auc: 0.9654 - binary_accuracy: 0.9251 - false_negatives: 152.2789 - false_positives: 320.8369 - loss: 0.2474 - precision: 0.9049 - recall: 0.9521 - true_negatives: 2711.1711 - true_positives: 2911.7131\n",
      "For batch number 381 the model has a loss of 0.2574444115161896\n",
      "\u001b[1m381/689\u001b[0m \u001b[32m━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━\u001b[0m \u001b[1m17s\u001b[0m 57ms/step - auc: 0.9654 - binary_accuracy: 0.9251 - false_negatives: 152.6798 - false_positives: 321.6824 - loss: 0.2474 - precision: 0.9049 - recall: 0.9521 - true_negatives: 2718.3806 - true_positives: 2919.2573\n",
      "For batch number 382 the model has a loss of 0.25768622756004333\n",
      "\u001b[1m382/689\u001b[0m \u001b[32m━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━\u001b[0m \u001b[1m17s\u001b[0m 57ms/step - auc: 0.9654 - binary_accuracy: 0.9251 - false_negatives: 153.0811 - false_positives: 322.5314 - loss: 0.2475 - precision: 0.9049 - recall: 0.9521 - true_negatives: 2725.5942 - true_positives: 2926.7932\n",
      "For batch number 383 the model has a loss of 0.25757065415382385\n",
      "\u001b[1m383/689\u001b[0m \u001b[32m━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━\u001b[0m \u001b[1m17s\u001b[0m 57ms/step - auc: 0.9654 - binary_accuracy: 0.9251 - false_negatives: 153.4804 - false_positives: 323.3838 - loss: 0.2475 - precision: 0.9049 - recall: 0.9520 - true_negatives: 2732.7991 - true_positives: 2934.3369\n",
      "For batch number 384 the model has a loss of 0.2575209438800812\n",
      "\u001b[1m384/689\u001b[0m \u001b[32m━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━\u001b[0m \u001b[1m17s\u001b[0m 57ms/step - auc: 0.9654 - binary_accuracy: 0.9251 - false_negatives: 153.8776 - false_positives: 324.2422 - loss: 0.2475 - precision: 0.9048 - recall: 0.9520 - true_negatives: 2740.0105 - true_positives: 2941.8699\n",
      "For batch number 385 the model has a loss of 0.2573646903038025\n",
      "\u001b[1m385/689\u001b[0m \u001b[32m━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━\u001b[0m \u001b[1m17s\u001b[0m 57ms/step - auc: 0.9654 - binary_accuracy: 0.9251 - false_negatives: 154.2727 - false_positives: 325.0987 - loss: 0.2475 - precision: 0.9048 - recall: 0.9520 - true_negatives: 2747.2183 - true_positives: 2949.4104\n",
      "For batch number 386 the model has a loss of 0.2570383548736572\n",
      "\u001b[1m386/689\u001b[0m \u001b[32m━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━\u001b[0m \u001b[1m17s\u001b[0m 57ms/step - auc: 0.9654 - binary_accuracy: 0.9251 - false_negatives: 154.6684 - false_positives: 325.9534 - loss: 0.2476 - precision: 0.9048 - recall: 0.9520 - true_negatives: 2754.4275 - true_positives: 2956.9507\n",
      "For batch number 387 the model has a loss of 0.2567588686943054\n",
      "\u001b[1m387/689\u001b[0m \u001b[32m━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━\u001b[0m \u001b[1m17s\u001b[0m 57ms/step - auc: 0.9653 - binary_accuracy: 0.9250 - false_negatives: 155.0620 - false_positives: 326.8088 - loss: 0.2476 - precision: 0.9048 - recall: 0.9520 - true_negatives: 2761.6382 - true_positives: 2964.4910\n",
      "For batch number 388 the model has a loss of 0.25669535994529724\n",
      "\u001b[1m388/689\u001b[0m \u001b[32m━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━\u001b[0m \u001b[1m17s\u001b[0m 57ms/step - auc: 0.9653 - binary_accuracy: 0.9250 - false_negatives: 155.4562 - false_positives: 327.6649 - loss: 0.2476 - precision: 0.9048 - recall: 0.9520 - true_negatives: 2768.8506 - true_positives: 2972.0283\n",
      "For batch number 389 the model has a loss of 0.2565664052963257\n",
      "\u001b[1m389/689\u001b[0m \u001b[32m━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━\u001b[0m \u001b[1m17s\u001b[0m 57ms/step - auc: 0.9653 - binary_accuracy: 0.9250 - false_negatives: 155.8483 - false_positives: 328.5244 - loss: 0.2476 - precision: 0.9048 - recall: 0.9520 - true_negatives: 2776.0669 - true_positives: 2979.5603\n",
      "For batch number 390 the model has a loss of 0.25684788823127747\n",
      "\u001b[1m390/689\u001b[0m \u001b[32m━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━\u001b[0m \u001b[1m17s\u001b[0m 57ms/step - auc: 0.9653 - binary_accuracy: 0.9250 - false_negatives: 156.2410 - false_positives: 329.3897 - loss: 0.2477 - precision: 0.9048 - recall: 0.9520 - true_negatives: 2783.2871 - true_positives: 2987.0820\n",
      "For batch number 391 the model has a loss of 0.25707459449768066\n",
      "\u001b[1m391/689\u001b[0m \u001b[32m━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━\u001b[0m \u001b[1m17s\u001b[0m 57ms/step - auc: 0.9653 - binary_accuracy: 0.9250 - false_negatives: 156.6368 - false_positives: 330.2532 - loss: 0.2477 - precision: 0.9047 - recall: 0.9520 - true_negatives: 2790.4961 - true_positives: 2994.6138\n",
      "For batch number 392 the model has a loss of 0.256691575050354\n",
      "\u001b[1m392/689\u001b[0m \u001b[32m━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━\u001b[0m \u001b[1m16s\u001b[0m 57ms/step - auc: 0.9653 - binary_accuracy: 0.9250 - false_negatives: 157.0306 - false_positives: 331.1122 - loss: 0.2477 - precision: 0.9047 - recall: 0.9520 - true_negatives: 2797.7041 - true_positives: 3002.1531\n",
      "For batch number 393 the model has a loss of 0.2569946348667145\n",
      "\u001b[1m393/689\u001b[0m \u001b[32m━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━\u001b[0m \u001b[1m16s\u001b[0m 57ms/step - auc: 0.9653 - binary_accuracy: 0.9250 - false_negatives: 157.4351 - false_positives: 331.9669 - loss: 0.2477 - precision: 0.9047 - recall: 0.9520 - true_negatives: 2804.8958 - true_positives: 3009.7024\n",
      "For batch number 394 the model has a loss of 0.2569252848625183\n",
      "\u001b[1m394/689\u001b[0m \u001b[32m━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━\u001b[0m \u001b[1m16s\u001b[0m 57ms/step - auc: 0.9653 - binary_accuracy: 0.9250 - false_negatives: 157.8426 - false_positives: 332.8223 - loss: 0.2477 - precision: 0.9047 - recall: 0.9520 - true_negatives: 2812.0913 - true_positives: 3017.2437\n",
      "For batch number 395 the model has a loss of 0.25706344842910767\n",
      "\u001b[1m395/689\u001b[0m \u001b[32m━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━\u001b[0m \u001b[1m16s\u001b[0m 57ms/step - auc: 0.9653 - binary_accuracy: 0.9250 - false_negatives: 158.2506 - false_positives: 333.6760 - loss: 0.2478 - precision: 0.9047 - recall: 0.9520 - true_negatives: 2819.2961 - true_positives: 3024.7771\n",
      "For batch number 396 the model has a loss of 0.2568151652812958\n",
      "\u001b[1m396/689\u001b[0m \u001b[32m━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━\u001b[0m \u001b[1m16s\u001b[0m 57ms/step - auc: 0.9653 - binary_accuracy: 0.9250 - false_negatives: 158.6591 - false_positives: 334.5252 - loss: 0.2478 - precision: 0.9047 - recall: 0.9520 - true_negatives: 2826.5051 - true_positives: 3032.3105\n",
      "For batch number 397 the model has a loss of 0.2565254867076874\n",
      "\u001b[1m397/689\u001b[0m \u001b[32m━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━\u001b[0m \u001b[1m16s\u001b[0m 57ms/step - auc: 0.9653 - binary_accuracy: 0.9250 - false_negatives: 159.0655 - false_positives: 335.3703 - loss: 0.2478 - precision: 0.9047 - recall: 0.9520 - true_negatives: 2833.7053 - true_positives: 3039.8589\n",
      "For batch number 398 the model has a loss of 0.2567334473133087\n",
      "\u001b[1m398/689\u001b[0m \u001b[32m━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━\u001b[0m \u001b[1m16s\u001b[0m 57ms/step - auc: 0.9653 - binary_accuracy: 0.9250 - false_negatives: 159.4749 - false_positives: 336.2186 - loss: 0.2478 - precision: 0.9047 - recall: 0.9520 - true_negatives: 2840.8970 - true_positives: 3047.4097\n",
      "For batch number 399 the model has a loss of 0.25702401995658875\n",
      "\u001b[1m399/689\u001b[0m \u001b[32m━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━\u001b[0m \u001b[1m16s\u001b[0m 57ms/step - auc: 0.9652 - binary_accuracy: 0.9249 - false_negatives: 159.8822 - false_positives: 337.0652 - loss: 0.2479 - precision: 0.9046 - recall: 0.9520 - true_negatives: 2848.0828 - true_positives: 3054.9700\n",
      "For batch number 400 the model has a loss of 0.25728875398635864\n",
      "\u001b[1m400/689\u001b[0m \u001b[32m━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━\u001b[0m \u001b[1m16s\u001b[0m 57ms/step - auc: 0.9652 - binary_accuracy: 0.9249 - false_negatives: 160.2900 - false_positives: 337.9150 - loss: 0.2479 - precision: 0.9046 - recall: 0.9520 - true_negatives: 2855.2649 - true_positives: 3062.5300\n",
      "For batch number 401 the model has a loss of 0.257161021232605\n",
      "\u001b[1m401/689\u001b[0m \u001b[32m━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━\u001b[0m \u001b[1m16s\u001b[0m 57ms/step - auc: 0.9652 - binary_accuracy: 0.9249 - false_negatives: 160.6958 - false_positives: 338.7681 - loss: 0.2479 - precision: 0.9046 - recall: 0.9520 - true_negatives: 2862.4563 - true_positives: 3070.0798\n",
      "For batch number 402 the model has a loss of 0.2570834755897522\n",
      "\u001b[1m402/689\u001b[0m \u001b[32m━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━\u001b[0m \u001b[1m16s\u001b[0m 57ms/step - auc: 0.9652 - binary_accuracy: 0.9249 - false_negatives: 161.1020 - false_positives: 339.6194 - loss: 0.2479 - precision: 0.9046 - recall: 0.9519 - true_negatives: 2869.6467 - true_positives: 3077.6318\n",
      "For batch number 403 the model has a loss of 0.2580477297306061\n",
      "\u001b[1m403/689\u001b[0m \u001b[32m━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━\u001b[0m \u001b[1m16s\u001b[0m 57ms/step - auc: 0.9652 - binary_accuracy: 0.9249 - false_negatives: 161.5087 - false_positives: 340.4789 - loss: 0.2480 - precision: 0.9046 - recall: 0.9519 - true_negatives: 2876.8337 - true_positives: 3085.1787\n",
      "For batch number 404 the model has a loss of 0.25799793004989624\n",
      "\u001b[1m404/689\u001b[0m \u001b[32m━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━\u001b[0m \u001b[1m16s\u001b[0m 57ms/step - auc: 0.9652 - binary_accuracy: 0.9249 - false_negatives: 161.9158 - false_positives: 341.3366 - loss: 0.2480 - precision: 0.9046 - recall: 0.9519 - true_negatives: 2884.0173 - true_positives: 3092.7302\n",
      "For batch number 405 the model has a loss of 0.2589878439903259\n",
      "\u001b[1m405/689\u001b[0m \u001b[32m━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━\u001b[0m \u001b[1m16s\u001b[0m 57ms/step - auc: 0.9652 - binary_accuracy: 0.9249 - false_negatives: 162.3259 - false_positives: 342.2000 - loss: 0.2480 - precision: 0.9046 - recall: 0.9519 - true_negatives: 2891.2048 - true_positives: 3100.2690\n",
      "For batch number 406 the model has a loss of 0.2597242295742035\n",
      "\u001b[1m406/689\u001b[0m \u001b[32m━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━\u001b[0m \u001b[1m16s\u001b[0m 57ms/step - auc: 0.9652 - binary_accuracy: 0.9249 - false_negatives: 162.7389 - false_positives: 343.0616 - loss: 0.2480 - precision: 0.9046 - recall: 0.9519 - true_negatives: 2898.3916 - true_positives: 3107.8079\n",
      "For batch number 407 the model has a loss of 0.25954073667526245\n",
      "\u001b[1m407/689\u001b[0m \u001b[32m━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━\u001b[0m \u001b[1m16s\u001b[0m 57ms/step - auc: 0.9652 - binary_accuracy: 0.9249 - false_negatives: 163.1499 - false_positives: 343.9214 - loss: 0.2481 - precision: 0.9045 - recall: 0.9519 - true_negatives: 2905.5725 - true_positives: 3115.3562\n",
      "For batch number 408 the model has a loss of 0.2598487138748169\n",
      "\u001b[1m408/689\u001b[0m \u001b[32m━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━\u001b[0m \u001b[1m16s\u001b[0m 57ms/step - auc: 0.9652 - binary_accuracy: 0.9249 - false_negatives: 163.5613 - false_positives: 344.7868 - loss: 0.2481 - precision: 0.9045 - recall: 0.9519 - true_negatives: 2912.7500 - true_positives: 3122.9019\n",
      "For batch number 409 the model has a loss of 0.25982314348220825\n",
      "\u001b[1m409/689\u001b[0m \u001b[32m━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━\u001b[0m \u001b[1m16s\u001b[0m 57ms/step - auc: 0.9652 - binary_accuracy: 0.9249 - false_negatives: 163.9756 - false_positives: 345.6528 - loss: 0.2481 - precision: 0.9045 - recall: 0.9519 - true_negatives: 2919.9219 - true_positives: 3130.4500\n",
      "For batch number 410 the model has a loss of 0.2597726285457611\n",
      "\u001b[1m410/689\u001b[0m \u001b[32m━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━\u001b[0m \u001b[1m15s\u001b[0m 57ms/step - auc: 0.9651 - binary_accuracy: 0.9249 - false_negatives: 164.3878 - false_positives: 346.5219 - loss: 0.2482 - precision: 0.9045 - recall: 0.9519 - true_negatives: 2927.0977 - true_positives: 3137.9927\n",
      "For batch number 411 the model has a loss of 0.2596893608570099\n",
      "\u001b[1m411/689\u001b[0m \u001b[32m━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━\u001b[0m \u001b[1m15s\u001b[0m 57ms/step - auc: 0.9651 - binary_accuracy: 0.9248 - false_negatives: 164.7980 - false_positives: 347.3917 - loss: 0.2482 - precision: 0.9045 - recall: 0.9519 - true_negatives: 2934.2700 - true_positives: 3145.5400\n",
      "For batch number 412 the model has a loss of 0.2595455050468445\n",
      "\u001b[1m412/689\u001b[0m \u001b[32m━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━\u001b[0m \u001b[1m15s\u001b[0m 57ms/step - auc: 0.9651 - binary_accuracy: 0.9248 - false_negatives: 165.2063 - false_positives: 348.2597 - loss: 0.2482 - precision: 0.9045 - recall: 0.9519 - true_negatives: 2941.4514 - true_positives: 3153.0825\n",
      "For batch number 413 the model has a loss of 0.2592757046222687\n",
      "\u001b[1m413/689\u001b[0m \u001b[32m━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━\u001b[0m \u001b[1m15s\u001b[0m 57ms/step - auc: 0.9651 - binary_accuracy: 0.9248 - false_negatives: 165.6174 - false_positives: 349.1235 - loss: 0.2482 - precision: 0.9045 - recall: 0.9519 - true_negatives: 2948.6343 - true_positives: 3160.6248\n",
      "For batch number 414 the model has a loss of 0.2592813968658447\n",
      "\u001b[1m414/689\u001b[0m \u001b[32m━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━\u001b[0m \u001b[1m15s\u001b[0m 57ms/step - auc: 0.9651 - binary_accuracy: 0.9248 - false_negatives: 166.0266 - false_positives: 349.9903 - loss: 0.2483 - precision: 0.9044 - recall: 0.9519 - true_negatives: 2955.8188 - true_positives: 3168.1643\n",
      "For batch number 415 the model has a loss of 0.25901421904563904\n",
      "\u001b[1m415/689\u001b[0m \u001b[32m━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━\u001b[0m \u001b[1m15s\u001b[0m 57ms/step - auc: 0.9651 - binary_accuracy: 0.9248 - false_negatives: 166.4361 - false_positives: 350.8554 - loss: 0.2483 - precision: 0.9044 - recall: 0.9519 - true_negatives: 2963.0049 - true_positives: 3175.7036\n",
      "For batch number 416 the model has a loss of 0.25893497467041016\n",
      "\u001b[1m416/689\u001b[0m \u001b[32m━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━\u001b[0m \u001b[1m15s\u001b[0m 57ms/step - auc: 0.9651 - binary_accuracy: 0.9248 - false_negatives: 166.8438 - false_positives: 351.7212 - loss: 0.2483 - precision: 0.9044 - recall: 0.9519 - true_negatives: 2970.1875 - true_positives: 3183.2476\n",
      "For batch number 417 the model has a loss of 0.25992658734321594\n",
      "\u001b[1m417/689\u001b[0m \u001b[32m━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━\u001b[0m \u001b[1m15s\u001b[0m 57ms/step - auc: 0.9651 - binary_accuracy: 0.9248 - false_negatives: 167.2518 - false_positives: 352.5995 - loss: 0.2483 - precision: 0.9044 - recall: 0.9519 - true_negatives: 2977.3694 - true_positives: 3190.7793\n",
      "For batch number 418 the model has a loss of 0.2596812844276428\n",
      "\u001b[1m418/689\u001b[0m \u001b[32m━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━\u001b[0m \u001b[1m15s\u001b[0m 57ms/step - auc: 0.9651 - binary_accuracy: 0.9248 - false_negatives: 167.6579 - false_positives: 353.4737 - loss: 0.2484 - precision: 0.9044 - recall: 0.9518 - true_negatives: 2984.5430 - true_positives: 3198.3254\n",
      "For batch number 419 the model has a loss of 0.2623903453350067\n",
      "\u001b[1m419/689\u001b[0m \u001b[32m━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━\u001b[0m \u001b[1m15s\u001b[0m 57ms/step - auc: 0.9651 - binary_accuracy: 0.9248 - false_negatives: 168.0955 - false_positives: 354.3484 - loss: 0.2484 - precision: 0.9044 - recall: 0.9518 - true_negatives: 2991.6968 - true_positives: 3205.8591\n",
      "For batch number 420 the model has a loss of 0.26268598437309265\n",
      "\u001b[1m420/689\u001b[0m \u001b[32m━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━\u001b[0m \u001b[1m15s\u001b[0m 57ms/step - auc: 0.9651 - binary_accuracy: 0.9248 - false_negatives: 168.5309 - false_positives: 355.2310 - loss: 0.2484 - precision: 0.9044 - recall: 0.9518 - true_negatives: 2998.8477 - true_positives: 3213.3904\n",
      "For batch number 421 the model has a loss of 0.26292505860328674\n",
      "\u001b[1m421/689\u001b[0m \u001b[32m━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━\u001b[0m \u001b[1m15s\u001b[0m 57ms/step - auc: 0.9650 - binary_accuracy: 0.9248 - false_negatives: 168.9644 - false_positives: 356.1188 - loss: 0.2485 - precision: 0.9043 - recall: 0.9518 - true_negatives: 3006.0000 - true_positives: 3220.9167\n",
      "For batch number 422 the model has a loss of 0.2626933455467224\n",
      "\u001b[1m422/689\u001b[0m \u001b[32m━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━\u001b[0m \u001b[1m15s\u001b[0m 57ms/step - auc: 0.9650 - binary_accuracy: 0.9247 - false_negatives: 169.3957 - false_positives: 357.0047 - loss: 0.2485 - precision: 0.9043 - recall: 0.9518 - true_negatives: 3013.1587 - true_positives: 3228.4407\n",
      "For batch number 423 the model has a loss of 0.26263582706451416\n",
      "\u001b[1m423/689\u001b[0m \u001b[32m━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━\u001b[0m \u001b[1m15s\u001b[0m 57ms/step - auc: 0.9650 - binary_accuracy: 0.9247 - false_negatives: 169.8251 - false_positives: 357.8889 - loss: 0.2485 - precision: 0.9043 - recall: 0.9518 - true_negatives: 3020.3120 - true_positives: 3235.9739\n",
      "For batch number 424 the model has a loss of 0.262841135263443\n",
      "\u001b[1m424/689\u001b[0m \u001b[32m━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━\u001b[0m \u001b[1m15s\u001b[0m 57ms/step - auc: 0.9650 - binary_accuracy: 0.9247 - false_negatives: 170.2571 - false_positives: 358.7712 - loss: 0.2486 - precision: 0.9043 - recall: 0.9518 - true_negatives: 3027.4575 - true_positives: 3243.5142\n",
      "For batch number 425 the model has a loss of 0.26251456141471863\n",
      "\u001b[1m425/689\u001b[0m \u001b[32m━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━\u001b[0m \u001b[1m15s\u001b[0m 57ms/step - auc: 0.9650 - binary_accuracy: 0.9247 - false_negatives: 170.6871 - false_positives: 359.6518 - loss: 0.2486 - precision: 0.9043 - recall: 0.9518 - true_negatives: 3034.6047 - true_positives: 3251.0564\n",
      "For batch number 426 the model has a loss of 0.262699693441391\n",
      "\u001b[1m426/689\u001b[0m \u001b[32m━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━\u001b[0m \u001b[1m15s\u001b[0m 57ms/step - auc: 0.9650 - binary_accuracy: 0.9247 - false_negatives: 171.1174 - false_positives: 360.5329 - loss: 0.2486 - precision: 0.9043 - recall: 0.9518 - true_negatives: 3041.7534 - true_positives: 3258.5962\n",
      "For batch number 427 the model has a loss of 0.2628857493400574\n",
      "\u001b[1m427/689\u001b[0m \u001b[32m━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━\u001b[0m \u001b[1m15s\u001b[0m 57ms/step - auc: 0.9650 - binary_accuracy: 0.9247 - false_negatives: 171.5550 - false_positives: 361.4099 - loss: 0.2487 - precision: 0.9042 - recall: 0.9518 - true_negatives: 3048.8994 - true_positives: 3266.1357\n",
      "For batch number 428 the model has a loss of 0.26348990201950073\n",
      "\u001b[1m428/689\u001b[0m \u001b[32m━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━\u001b[0m \u001b[1m14s\u001b[0m 57ms/step - auc: 0.9650 - binary_accuracy: 0.9247 - false_negatives: 171.9953 - false_positives: 362.2897 - loss: 0.2487 - precision: 0.9042 - recall: 0.9518 - true_negatives: 3056.0444 - true_positives: 3273.6707\n",
      "For batch number 429 the model has a loss of 0.2632901966571808\n",
      "\u001b[1m429/689\u001b[0m \u001b[32m━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━\u001b[0m \u001b[1m14s\u001b[0m 57ms/step - auc: 0.9650 - binary_accuracy: 0.9247 - false_negatives: 172.4336 - false_positives: 363.1702 - loss: 0.2487 - precision: 0.9042 - recall: 0.9517 - true_negatives: 3063.1887 - true_positives: 3281.2075\n",
      "For batch number 430 the model has a loss of 0.26341307163238525\n",
      "\u001b[1m430/689\u001b[0m \u001b[32m━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━\u001b[0m \u001b[1m14s\u001b[0m 57ms/step - auc: 0.9649 - binary_accuracy: 0.9247 - false_negatives: 172.8698 - false_positives: 364.0558 - loss: 0.2488 - precision: 0.9042 - recall: 0.9517 - true_negatives: 3070.3279 - true_positives: 3288.7466\n",
      "For batch number 431 the model has a loss of 0.2630423605442047\n",
      "\u001b[1m431/689\u001b[0m \u001b[32m━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━\u001b[0m \u001b[1m14s\u001b[0m 57ms/step - auc: 0.9649 - binary_accuracy: 0.9246 - false_negatives: 173.3063 - false_positives: 364.9373 - loss: 0.2488 - precision: 0.9042 - recall: 0.9517 - true_negatives: 3077.4688 - true_positives: 3296.2876\n",
      "For batch number 432 the model has a loss of 0.26392805576324463\n",
      "\u001b[1m432/689\u001b[0m \u001b[32m━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━\u001b[0m \u001b[1m14s\u001b[0m 57ms/step - auc: 0.9649 - binary_accuracy: 0.9246 - false_negatives: 173.7500 - false_positives: 365.8195 - loss: 0.2488 - precision: 0.9042 - recall: 0.9517 - true_negatives: 3084.6018 - true_positives: 3303.8286\n",
      "For batch number 433 the model has a loss of 0.26356667280197144\n",
      "\u001b[1m433/689\u001b[0m \u001b[32m━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━\u001b[0m \u001b[1m14s\u001b[0m 57ms/step - auc: 0.9649 - binary_accuracy: 0.9246 - false_negatives: 174.1917 - false_positives: 366.6974 - loss: 0.2489 - precision: 0.9042 - recall: 0.9517 - true_negatives: 3091.7344 - true_positives: 3311.3765\n",
      "For batch number 434 the model has a loss of 0.2639657258987427\n",
      "\u001b[1m434/689\u001b[0m \u001b[32m━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━\u001b[0m \u001b[1m14s\u001b[0m 57ms/step - auc: 0.9649 - binary_accuracy: 0.9246 - false_negatives: 174.6359 - false_positives: 367.5783 - loss: 0.2489 - precision: 0.9041 - recall: 0.9517 - true_negatives: 3098.8708 - true_positives: 3318.9148\n",
      "For batch number 435 the model has a loss of 0.26433318853378296\n",
      "\u001b[1m435/689\u001b[0m \u001b[32m━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━\u001b[0m \u001b[1m14s\u001b[0m 57ms/step - auc: 0.9649 - binary_accuracy: 0.9246 - false_negatives: 175.0805 - false_positives: 368.4575 - loss: 0.2489 - precision: 0.9041 - recall: 0.9517 - true_negatives: 3106.0068 - true_positives: 3326.4551\n",
      "For batch number 436 the model has a loss of 0.2639698088169098\n",
      "\u001b[1m436/689\u001b[0m \u001b[32m━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━\u001b[0m \u001b[1m14s\u001b[0m 57ms/step - auc: 0.9649 - binary_accuracy: 0.9246 - false_negatives: 175.5252 - false_positives: 369.3326 - loss: 0.2490 - precision: 0.9041 - recall: 0.9517 - true_negatives: 3113.1445 - true_positives: 3333.9978\n",
      "For batch number 437 the model has a loss of 0.26398301124572754\n",
      "\u001b[1m437/689\u001b[0m \u001b[32m━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━\u001b[0m \u001b[1m14s\u001b[0m 57ms/step - auc: 0.9649 - binary_accuracy: 0.9246 - false_negatives: 175.9702 - false_positives: 370.2151 - loss: 0.2490 - precision: 0.9041 - recall: 0.9517 - true_negatives: 3120.2837 - true_positives: 3341.5310\n",
      "For batch number 438 the model has a loss of 0.2636079788208008\n",
      "\u001b[1m438/689\u001b[0m \u001b[32m━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━\u001b[0m \u001b[1m14s\u001b[0m 58ms/step - auc: 0.9649 - binary_accuracy: 0.9246 - false_negatives: 176.4132 - false_positives: 371.0936 - loss: 0.2490 - precision: 0.9041 - recall: 0.9517 - true_negatives: 3127.4270 - true_positives: 3349.0662\n",
      "For batch number 439 the model has a loss of 0.26318031549453735\n",
      "\u001b[1m439/689\u001b[0m \u001b[32m━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━\u001b[0m \u001b[1m14s\u001b[0m 58ms/step - auc: 0.9648 - binary_accuracy: 0.9246 - false_negatives: 176.8542 - false_positives: 371.9681 - loss: 0.2491 - precision: 0.9041 - recall: 0.9516 - true_negatives: 3134.5696 - true_positives: 3356.6082\n",
      "For batch number 440 the model has a loss of 0.2632952332496643\n",
      "\u001b[1m440/689\u001b[0m \u001b[32m━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━\u001b[0m \u001b[1m14s\u001b[0m 58ms/step - auc: 0.9648 - binary_accuracy: 0.9245 - false_negatives: 177.2977 - false_positives: 372.8432 - loss: 0.2491 - precision: 0.9041 - recall: 0.9516 - true_negatives: 3141.7136 - true_positives: 3364.1455\n",
      "For batch number 441 the model has a loss of 0.26309117674827576\n",
      "\u001b[1m441/689\u001b[0m \u001b[32m━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━\u001b[0m \u001b[1m14s\u001b[0m 58ms/step - auc: 0.9648 - binary_accuracy: 0.9245 - false_negatives: 177.7392 - false_positives: 373.7188 - loss: 0.2491 - precision: 0.9040 - recall: 0.9516 - true_negatives: 3148.8616 - true_positives: 3371.6802\n",
      "For batch number 442 the model has a loss of 0.2631709575653076\n",
      "\u001b[1m442/689\u001b[0m \u001b[32m━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━\u001b[0m \u001b[1m14s\u001b[0m 58ms/step - auc: 0.9648 - binary_accuracy: 0.9245 - false_negatives: 178.1833 - false_positives: 374.5928 - loss: 0.2492 - precision: 0.9040 - recall: 0.9516 - true_negatives: 3156.0046 - true_positives: 3379.2195\n",
      "For batch number 443 the model has a loss of 0.2633197605609894\n",
      "\u001b[1m443/689\u001b[0m \u001b[32m━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━\u001b[0m \u001b[1m14s\u001b[0m 58ms/step - auc: 0.9648 - binary_accuracy: 0.9245 - false_negatives: 178.6253 - false_positives: 375.4740 - loss: 0.2492 - precision: 0.9040 - recall: 0.9516 - true_negatives: 3163.1511 - true_positives: 3386.7495\n",
      "For batch number 444 the model has a loss of 0.26311519742012024\n",
      "\u001b[1m444/689\u001b[0m \u001b[32m━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━\u001b[0m \u001b[1m14s\u001b[0m 58ms/step - auc: 0.9648 - binary_accuracy: 0.9245 - false_negatives: 179.0653 - false_positives: 376.3559 - loss: 0.2492 - precision: 0.9040 - recall: 0.9516 - true_negatives: 3170.2952 - true_positives: 3394.2837\n",
      "For batch number 445 the model has a loss of 0.2633959650993347\n",
      "\u001b[1m445/689\u001b[0m \u001b[32m━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━\u001b[0m \u001b[1m14s\u001b[0m 58ms/step - auc: 0.9648 - binary_accuracy: 0.9245 - false_negatives: 179.5101 - false_positives: 377.2360 - loss: 0.2493 - precision: 0.9040 - recall: 0.9516 - true_negatives: 3177.4382 - true_positives: 3401.8157\n",
      "For batch number 446 the model has a loss of 0.2634463608264923\n",
      "\u001b[1m446/689\u001b[0m \u001b[32m━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━\u001b[0m \u001b[1m14s\u001b[0m 58ms/step - auc: 0.9648 - binary_accuracy: 0.9245 - false_negatives: 179.9552 - false_positives: 378.1188 - loss: 0.2493 - precision: 0.9040 - recall: 0.9516 - true_negatives: 3184.5920 - true_positives: 3409.3340\n",
      "For batch number 447 the model has a loss of 0.26348093152046204\n",
      "\u001b[1m447/689\u001b[0m \u001b[32m━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━\u001b[0m \u001b[1m13s\u001b[0m 58ms/step - auc: 0.9648 - binary_accuracy: 0.9245 - false_negatives: 180.4027 - false_positives: 378.9978 - loss: 0.2493 - precision: 0.9040 - recall: 0.9516 - true_negatives: 3191.7361 - true_positives: 3416.8635\n",
      "For batch number 448 the model has a loss of 0.26354360580444336\n",
      "\u001b[1m448/689\u001b[0m \u001b[32m━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━\u001b[0m \u001b[1m13s\u001b[0m 58ms/step - auc: 0.9648 - binary_accuracy: 0.9245 - false_negatives: 180.8504 - false_positives: 379.8772 - loss: 0.2494 - precision: 0.9039 - recall: 0.9516 - true_negatives: 3198.8794 - true_positives: 3424.3928\n",
      "For batch number 449 the model has a loss of 0.2633060812950134\n",
      "\u001b[1m449/689\u001b[0m \u001b[32m━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━\u001b[0m \u001b[1m13s\u001b[0m 58ms/step - auc: 0.9647 - binary_accuracy: 0.9244 - false_negatives: 181.2962 - false_positives: 380.7550 - loss: 0.2494 - precision: 0.9039 - recall: 0.9515 - true_negatives: 3206.0291 - true_positives: 3431.9199\n",
      "For batch number 450 the model has a loss of 0.2631629705429077\n",
      "\u001b[1m450/689\u001b[0m \u001b[32m━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━\u001b[0m \u001b[1m13s\u001b[0m 58ms/step - auc: 0.9647 - binary_accuracy: 0.9244 - false_negatives: 181.7422 - false_positives: 381.6311 - loss: 0.2494 - precision: 0.9039 - recall: 0.9515 - true_negatives: 3213.1733 - true_positives: 3439.4534\n",
      "For batch number 451 the model has a loss of 0.2631622850894928\n",
      "\u001b[1m451/689\u001b[0m \u001b[32m━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━\u001b[0m \u001b[1m13s\u001b[0m 58ms/step - auc: 0.9647 - binary_accuracy: 0.9244 - false_negatives: 182.1862 - false_positives: 382.5100 - loss: 0.2495 - precision: 0.9039 - recall: 0.9515 - true_negatives: 3220.3259 - true_positives: 3446.9778\n",
      "For batch number 452 the model has a loss of 0.2629103660583496\n",
      "\u001b[1m452/689\u001b[0m \u001b[32m━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━\u001b[0m \u001b[1m13s\u001b[0m 58ms/step - auc: 0.9647 - binary_accuracy: 0.9244 - false_negatives: 182.6305 - false_positives: 383.3849 - loss: 0.2495 - precision: 0.9039 - recall: 0.9515 - true_negatives: 3227.4846 - true_positives: 3454.5000\n",
      "For batch number 453 the model has a loss of 0.26314976811408997\n",
      "\u001b[1m453/689\u001b[0m \u001b[32m━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━\u001b[0m \u001b[1m13s\u001b[0m 58ms/step - auc: 0.9647 - binary_accuracy: 0.9244 - false_negatives: 183.0795 - false_positives: 384.2561 - loss: 0.2495 - precision: 0.9039 - recall: 0.9515 - true_negatives: 3234.6379 - true_positives: 3462.0266\n",
      "For batch number 454 the model has a loss of 0.2630261182785034\n",
      "\u001b[1m454/689\u001b[0m \u001b[32m━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━\u001b[0m \u001b[1m13s\u001b[0m 58ms/step - auc: 0.9647 - binary_accuracy: 0.9244 - false_negatives: 183.5286 - false_positives: 385.1234 - loss: 0.2495 - precision: 0.9039 - recall: 0.9515 - true_negatives: 3241.7996 - true_positives: 3469.5483\n",
      "For batch number 455 the model has a loss of 0.2626529335975647\n",
      "\u001b[1m455/689\u001b[0m \u001b[32m━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━\u001b[0m \u001b[1m13s\u001b[0m 58ms/step - auc: 0.9647 - binary_accuracy: 0.9244 - false_negatives: 183.9758 - false_positives: 385.9868 - loss: 0.2496 - precision: 0.9039 - recall: 0.9515 - true_negatives: 3248.9583 - true_positives: 3477.0791\n",
      "For batch number 456 the model has a loss of 0.26274800300598145\n",
      "\u001b[1m456/689\u001b[0m \u001b[32m━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━\u001b[0m \u001b[1m13s\u001b[0m 58ms/step - auc: 0.9647 - binary_accuracy: 0.9244 - false_negatives: 184.4232 - false_positives: 386.8487 - loss: 0.2496 - precision: 0.9038 - recall: 0.9515 - true_negatives: 3256.1030 - true_positives: 3484.6250\n",
      "For batch number 457 the model has a loss of 0.2626587152481079\n",
      "\u001b[1m457/689\u001b[0m \u001b[32m━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━\u001b[0m \u001b[1m13s\u001b[0m 58ms/step - auc: 0.9647 - binary_accuracy: 0.9244 - false_negatives: 184.8709 - false_positives: 387.7090 - loss: 0.2496 - precision: 0.9038 - recall: 0.9515 - true_negatives: 3263.2495 - true_positives: 3492.1707\n",
      "For batch number 458 the model has a loss of 0.26279714703559875\n",
      "\u001b[1m458/689\u001b[0m \u001b[32m━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━\u001b[0m \u001b[1m13s\u001b[0m 58ms/step - auc: 0.9647 - binary_accuracy: 0.9244 - false_negatives: 185.3188 - false_positives: 388.5699 - loss: 0.2497 - precision: 0.9038 - recall: 0.9514 - true_negatives: 3270.3997 - true_positives: 3499.7117\n",
      "For batch number 459 the model has a loss of 0.2626273036003113\n",
      "\u001b[1m459/689\u001b[0m \u001b[32m━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━\u001b[0m \u001b[1m13s\u001b[0m 58ms/step - auc: 0.9646 - binary_accuracy: 0.9243 - false_negatives: 185.7647 - false_positives: 389.4314 - loss: 0.2497 - precision: 0.9038 - recall: 0.9514 - true_negatives: 3277.5491 - true_positives: 3507.2549\n",
      "For batch number 460 the model has a loss of 0.26252734661102295\n",
      "\u001b[1m460/689\u001b[0m \u001b[32m━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━\u001b[0m \u001b[1m13s\u001b[0m 58ms/step - auc: 0.9646 - binary_accuracy: 0.9243 - false_negatives: 186.2087 - false_positives: 390.2935 - loss: 0.2497 - precision: 0.9038 - recall: 0.9514 - true_negatives: 3284.7087 - true_positives: 3514.7891\n",
      "For batch number 461 the model has a loss of 0.2624381184577942\n",
      "\u001b[1m461/689\u001b[0m \u001b[32m━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━\u001b[0m \u001b[1m13s\u001b[0m 58ms/step - auc: 0.9646 - binary_accuracy: 0.9243 - false_negatives: 186.6529 - false_positives: 391.1562 - loss: 0.2497 - precision: 0.9038 - recall: 0.9514 - true_negatives: 3291.8721 - true_positives: 3522.3188\n",
      "For batch number 462 the model has a loss of 0.26204416155815125\n",
      "\u001b[1m462/689\u001b[0m \u001b[32m━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━\u001b[0m \u001b[1m13s\u001b[0m 58ms/step - auc: 0.9646 - binary_accuracy: 0.9243 - false_negatives: 187.0952 - false_positives: 392.0151 - loss: 0.2498 - precision: 0.9038 - recall: 0.9514 - true_negatives: 3299.0410 - true_positives: 3529.8484\n",
      "For batch number 463 the model has a loss of 0.2622847557067871\n",
      "\u001b[1m463/689\u001b[0m \u001b[32m━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━\u001b[0m \u001b[1m13s\u001b[0m 58ms/step - auc: 0.9646 - binary_accuracy: 0.9243 - false_negatives: 187.5356 - false_positives: 392.8769 - loss: 0.2498 - precision: 0.9038 - recall: 0.9514 - true_negatives: 3306.2051 - true_positives: 3537.3823\n",
      "For batch number 464 the model has a loss of 0.26223939657211304\n",
      "\u001b[1m464/689\u001b[0m \u001b[32m━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━\u001b[0m \u001b[1m13s\u001b[0m 58ms/step - auc: 0.9646 - binary_accuracy: 0.9243 - false_negatives: 187.9763 - false_positives: 393.7371 - loss: 0.2498 - precision: 0.9038 - recall: 0.9514 - true_negatives: 3313.3643 - true_positives: 3544.9224\n",
      "For batch number 465 the model has a loss of 0.26211023330688477\n",
      "\u001b[1m465/689\u001b[0m \u001b[32m━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━\u001b[0m \u001b[1m12s\u001b[0m 58ms/step - auc: 0.9646 - binary_accuracy: 0.9243 - false_negatives: 188.4172 - false_positives: 394.5957 - loss: 0.2499 - precision: 0.9037 - recall: 0.9514 - true_negatives: 3320.5247 - true_positives: 3552.4624\n",
      "For batch number 466 the model has a loss of 0.261982798576355\n",
      "\u001b[1m466/689\u001b[0m \u001b[32m━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━\u001b[0m \u001b[1m12s\u001b[0m 58ms/step - auc: 0.9646 - binary_accuracy: 0.9243 - false_negatives: 188.8562 - false_positives: 395.4571 - loss: 0.2499 - precision: 0.9037 - recall: 0.9514 - true_negatives: 3327.6824 - true_positives: 3560.0044\n",
      "For batch number 467 the model has a loss of 0.26232653856277466\n",
      "\u001b[1m467/689\u001b[0m \u001b[32m━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━\u001b[0m \u001b[1m12s\u001b[0m 58ms/step - auc: 0.9646 - binary_accuracy: 0.9243 - false_negatives: 189.2934 - false_positives: 396.3276 - loss: 0.2499 - precision: 0.9037 - recall: 0.9514 - true_negatives: 3334.8352 - true_positives: 3567.5439\n",
      "For batch number 468 the model has a loss of 0.26205331087112427\n",
      "\u001b[1m468/689\u001b[0m \u001b[32m━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━\u001b[0m \u001b[1m12s\u001b[0m 58ms/step - auc: 0.9646 - binary_accuracy: 0.9243 - false_negatives: 189.7286 - false_positives: 397.1966 - loss: 0.2499 - precision: 0.9037 - recall: 0.9514 - true_negatives: 3341.9937 - true_positives: 3575.0813\n",
      "For batch number 469 the model has a loss of 0.26230505108833313\n",
      "\u001b[1m469/689\u001b[0m \u001b[32m━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━\u001b[0m \u001b[1m12s\u001b[0m 58ms/step - auc: 0.9646 - binary_accuracy: 0.9243 - false_negatives: 190.1642 - false_positives: 398.0661 - loss: 0.2500 - precision: 0.9037 - recall: 0.9514 - true_negatives: 3349.1514 - true_positives: 3582.6184\n",
      "For batch number 470 the model has a loss of 0.26216980814933777\n",
      "\u001b[1m470/689\u001b[0m \u001b[32m━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━\u001b[0m \u001b[1m12s\u001b[0m 58ms/step - auc: 0.9646 - binary_accuracy: 0.9242 - false_negatives: 190.5979 - false_positives: 398.9362 - loss: 0.2500 - precision: 0.9037 - recall: 0.9513 - true_negatives: 3356.3105 - true_positives: 3590.1553\n",
      "For batch number 471 the model has a loss of 0.26188838481903076\n",
      "\u001b[1m471/689\u001b[0m \u001b[32m━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━\u001b[0m \u001b[1m12s\u001b[0m 58ms/step - auc: 0.9645 - binary_accuracy: 0.9242 - false_negatives: 191.0318 - false_positives: 399.8026 - loss: 0.2500 - precision: 0.9037 - recall: 0.9513 - true_negatives: 3363.4692 - true_positives: 3597.6963\n",
      "For batch number 472 the model has a loss of 0.2616596817970276\n",
      "\u001b[1m472/689\u001b[0m \u001b[32m━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━\u001b[0m \u001b[1m12s\u001b[0m 58ms/step - auc: 0.9645 - binary_accuracy: 0.9242 - false_negatives: 191.4640 - false_positives: 400.6695 - loss: 0.2500 - precision: 0.9037 - recall: 0.9513 - true_negatives: 3370.6272 - true_positives: 3605.2395\n",
      "For batch number 473 the model has a loss of 0.2614479660987854\n",
      "\u001b[1m473/689\u001b[0m \u001b[32m━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━\u001b[0m \u001b[1m12s\u001b[0m 58ms/step - auc: 0.9645 - binary_accuracy: 0.9242 - false_negatives: 191.8943 - false_positives: 401.5349 - loss: 0.2501 - precision: 0.9037 - recall: 0.9513 - true_negatives: 3377.7864 - true_positives: 3612.7844\n",
      "For batch number 474 the model has a loss of 0.26126232743263245\n",
      "\u001b[1m474/689\u001b[0m \u001b[32m━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━\u001b[0m \u001b[1m12s\u001b[0m 58ms/step - auc: 0.9645 - binary_accuracy: 0.9242 - false_negatives: 192.3228 - false_positives: 402.3987 - loss: 0.2501 - precision: 0.9036 - recall: 0.9513 - true_negatives: 3384.9514 - true_positives: 3620.3269\n",
      "For batch number 475 the model has a loss of 0.26094111800193787\n",
      "\u001b[1m475/689\u001b[0m \u001b[32m━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━\u001b[0m \u001b[1m12s\u001b[0m 58ms/step - auc: 0.9645 - binary_accuracy: 0.9242 - false_negatives: 192.7495 - false_positives: 403.2589 - loss: 0.2501 - precision: 0.9036 - recall: 0.9513 - true_negatives: 3392.1179 - true_positives: 3627.8738\n",
      "For batch number 476 the model has a loss of 0.2606743574142456\n",
      "\u001b[1m476/689\u001b[0m \u001b[32m━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━\u001b[0m \u001b[1m12s\u001b[0m 58ms/step - auc: 0.9645 - binary_accuracy: 0.9242 - false_negatives: 193.1765 - false_positives: 404.1155 - loss: 0.2501 - precision: 0.9036 - recall: 0.9513 - true_negatives: 3399.2878 - true_positives: 3635.4202\n",
      "For batch number 477 the model has a loss of 0.2608984112739563\n",
      "\u001b[1m477/689\u001b[0m \u001b[32m━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━\u001b[0m \u001b[1m12s\u001b[0m 58ms/step - auc: 0.9645 - binary_accuracy: 0.9242 - false_negatives: 193.6059 - false_positives: 404.9727 - loss: 0.2501 - precision: 0.9036 - recall: 0.9513 - true_negatives: 3406.4592 - true_positives: 3642.9622\n",
      "For batch number 478 the model has a loss of 0.2606443166732788\n",
      "\u001b[1m478/689\u001b[0m \u001b[32m━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━\u001b[0m \u001b[1m12s\u001b[0m 58ms/step - auc: 0.9645 - binary_accuracy: 0.9242 - false_negatives: 194.0335 - false_positives: 405.8264 - loss: 0.2502 - precision: 0.9036 - recall: 0.9513 - true_negatives: 3413.6255 - true_positives: 3650.5146\n",
      "For batch number 479 the model has a loss of 0.26068729162216187\n",
      "\u001b[1m479/689\u001b[0m \u001b[32m━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━\u001b[0m \u001b[1m12s\u001b[0m 58ms/step - auc: 0.9645 - binary_accuracy: 0.9242 - false_negatives: 194.4614 - false_positives: 406.6827 - loss: 0.2502 - precision: 0.9036 - recall: 0.9513 - true_negatives: 3420.8059 - true_positives: 3658.0500\n",
      "For batch number 480 the model has a loss of 0.2602900564670563\n",
      "\u001b[1m480/689\u001b[0m \u001b[32m━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━\u001b[0m \u001b[1m12s\u001b[0m 58ms/step - auc: 0.9645 - binary_accuracy: 0.9242 - false_negatives: 194.8875 - false_positives: 407.5354 - loss: 0.2502 - precision: 0.9036 - recall: 0.9513 - true_negatives: 3427.9895 - true_positives: 3665.5874\n",
      "For batch number 481 the model has a loss of 0.2601167559623718\n",
      "\u001b[1m481/689\u001b[0m \u001b[32m━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━\u001b[0m \u001b[1m12s\u001b[0m 58ms/step - auc: 0.9645 - binary_accuracy: 0.9242 - false_negatives: 195.3139 - false_positives: 408.3846 - loss: 0.2502 - precision: 0.9036 - recall: 0.9513 - true_negatives: 3435.1726 - true_positives: 3673.1289\n",
      "For batch number 482 the model has a loss of 0.2599743902683258\n",
      "\u001b[1m482/689\u001b[0m \u001b[32m━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━\u001b[0m \u001b[1m12s\u001b[0m 58ms/step - auc: 0.9645 - binary_accuracy: 0.9242 - false_negatives: 195.7386 - false_positives: 409.2324 - loss: 0.2503 - precision: 0.9036 - recall: 0.9513 - true_negatives: 3442.3569 - true_positives: 3680.6721\n",
      "For batch number 483 the model has a loss of 0.26011377573013306\n",
      "\u001b[1m483/689\u001b[0m \u001b[32m━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━\u001b[0m \u001b[1m11s\u001b[0m 58ms/step - auc: 0.9645 - binary_accuracy: 0.9242 - false_negatives: 196.1636 - false_positives: 410.0787 - loss: 0.2503 - precision: 0.9036 - recall: 0.9513 - true_negatives: 3449.5320 - true_positives: 3688.2256\n",
      "For batch number 484 the model has a loss of 0.2598879337310791\n",
      "\u001b[1m484/689\u001b[0m \u001b[32m━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━\u001b[0m \u001b[1m11s\u001b[0m 58ms/step - auc: 0.9644 - binary_accuracy: 0.9241 - false_negatives: 196.5868 - false_positives: 410.9256 - loss: 0.2503 - precision: 0.9036 - recall: 0.9512 - true_negatives: 3456.7148 - true_positives: 3695.7727\n",
      "For batch number 485 the model has a loss of 0.2599334120750427\n",
      "\u001b[1m485/689\u001b[0m \u001b[32m━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━\u001b[0m \u001b[1m11s\u001b[0m 58ms/step - auc: 0.9644 - binary_accuracy: 0.9241 - false_negatives: 197.0103 - false_positives: 411.7732 - loss: 0.2503 - precision: 0.9035 - recall: 0.9512 - true_negatives: 3463.8970 - true_positives: 3703.3196\n",
      "For batch number 486 the model has a loss of 0.25975656509399414\n",
      "\u001b[1m486/689\u001b[0m \u001b[32m━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━\u001b[0m \u001b[1m11s\u001b[0m 58ms/step - auc: 0.9644 - binary_accuracy: 0.9241 - false_negatives: 197.4342 - false_positives: 412.6173 - loss: 0.2503 - precision: 0.9035 - recall: 0.9512 - true_negatives: 3471.0781 - true_positives: 3710.8704\n",
      "For batch number 487 the model has a loss of 0.2603282034397125\n",
      "\u001b[1m487/689\u001b[0m \u001b[32m━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━\u001b[0m \u001b[1m11s\u001b[0m 58ms/step - auc: 0.9644 - binary_accuracy: 0.9241 - false_negatives: 197.8604 - false_positives: 413.4620 - loss: 0.2504 - precision: 0.9035 - recall: 0.9512 - true_negatives: 3478.2690 - true_positives: 3718.4087\n",
      "For batch number 488 the model has a loss of 0.2602105736732483\n",
      "\u001b[1m488/689\u001b[0m \u001b[32m━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━\u001b[0m \u001b[1m11s\u001b[0m 58ms/step - auc: 0.9644 - binary_accuracy: 0.9241 - false_negatives: 198.2848 - false_positives: 414.3053 - loss: 0.2504 - precision: 0.9035 - recall: 0.9512 - true_negatives: 3485.4529 - true_positives: 3725.9570\n",
      "For batch number 489 the model has a loss of 0.2607705295085907\n",
      "\u001b[1m489/689\u001b[0m \u001b[32m━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━\u001b[0m \u001b[1m11s\u001b[0m 58ms/step - auc: 0.9644 - binary_accuracy: 0.9241 - false_negatives: 198.7157 - false_positives: 415.1452 - loss: 0.2504 - precision: 0.9035 - recall: 0.9512 - true_negatives: 3492.6299 - true_positives: 3733.5093\n",
      "For batch number 490 the model has a loss of 0.26061850786209106\n",
      "\u001b[1m490/689\u001b[0m \u001b[32m━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━\u001b[0m \u001b[1m11s\u001b[0m 58ms/step - auc: 0.9644 - binary_accuracy: 0.9241 - false_negatives: 199.1469 - false_positives: 415.9837 - loss: 0.2504 - precision: 0.9035 - recall: 0.9512 - true_negatives: 3499.8103 - true_positives: 3741.0591\n",
      "For batch number 491 the model has a loss of 0.26058387756347656\n",
      "\u001b[1m491/689\u001b[0m \u001b[32m━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━\u001b[0m \u001b[1m11s\u001b[0m 58ms/step - auc: 0.9644 - binary_accuracy: 0.9241 - false_negatives: 199.5784 - false_positives: 416.8208 - loss: 0.2504 - precision: 0.9035 - recall: 0.9512 - true_negatives: 3506.9858 - true_positives: 3748.6150\n",
      "For batch number 492 the model has a loss of 0.2609546482563019\n",
      "\u001b[1m492/689\u001b[0m \u001b[32m━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━\u001b[0m \u001b[1m11s\u001b[0m 58ms/step - auc: 0.9644 - binary_accuracy: 0.9241 - false_negatives: 200.0122 - false_positives: 417.6585 - loss: 0.2505 - precision: 0.9035 - recall: 0.9512 - true_negatives: 3514.1584 - true_positives: 3756.1707\n",
      "For batch number 493 the model has a loss of 0.26102566719055176\n",
      "\u001b[1m493/689\u001b[0m \u001b[32m━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━\u001b[0m \u001b[1m11s\u001b[0m 58ms/step - auc: 0.9644 - binary_accuracy: 0.9241 - false_negatives: 200.4483 - false_positives: 418.4969 - loss: 0.2505 - precision: 0.9035 - recall: 0.9512 - true_negatives: 3521.3328 - true_positives: 3763.7222\n",
      "For batch number 494 the model has a loss of 0.261014461517334\n",
      "\u001b[1m494/689\u001b[0m \u001b[32m━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━\u001b[0m \u001b[1m11s\u001b[0m 58ms/step - auc: 0.9644 - binary_accuracy: 0.9241 - false_negatives: 200.8846 - false_positives: 419.3340 - loss: 0.2505 - precision: 0.9035 - recall: 0.9512 - true_negatives: 3528.5122 - true_positives: 3771.2693\n",
      "For batch number 495 the model has a loss of 0.261583536863327\n",
      "\u001b[1m495/689\u001b[0m \u001b[32m━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━\u001b[0m \u001b[1m11s\u001b[0m 58ms/step - auc: 0.9644 - binary_accuracy: 0.9241 - false_negatives: 201.3232 - false_positives: 420.1737 - loss: 0.2505 - precision: 0.9035 - recall: 0.9512 - true_negatives: 3535.6951 - true_positives: 3778.8081\n",
      "For batch number 496 the model has a loss of 0.2617015242576599\n",
      "\u001b[1m496/689\u001b[0m \u001b[32m━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━\u001b[0m \u001b[1m11s\u001b[0m 58ms/step - auc: 0.9644 - binary_accuracy: 0.9241 - false_negatives: 201.7621 - false_positives: 421.0161 - loss: 0.2505 - precision: 0.9035 - recall: 0.9512 - true_negatives: 3542.8831 - true_positives: 3786.3386\n",
      "For batch number 497 the model has a loss of 0.26151636242866516\n",
      "\u001b[1m497/689\u001b[0m \u001b[32m━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━\u001b[0m \u001b[1m11s\u001b[0m 58ms/step - auc: 0.9644 - binary_accuracy: 0.9241 - false_negatives: 202.1992 - false_positives: 421.8551 - loss: 0.2506 - precision: 0.9035 - recall: 0.9512 - true_negatives: 3550.0784 - true_positives: 3793.8672\n",
      "For batch number 498 the model has a loss of 0.26205089688301086\n",
      "\u001b[1m498/689\u001b[0m \u001b[32m━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━\u001b[0m \u001b[1m11s\u001b[0m 58ms/step - auc: 0.9643 - binary_accuracy: 0.9241 - false_negatives: 202.6385 - false_positives: 422.6948 - loss: 0.2506 - precision: 0.9035 - recall: 0.9511 - true_negatives: 3557.2732 - true_positives: 3801.3936\n",
      "For batch number 499 the model has a loss of 0.2616904377937317\n",
      "\u001b[1m499/689\u001b[0m \u001b[32m━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━\u001b[0m \u001b[1m11s\u001b[0m 58ms/step - auc: 0.9643 - binary_accuracy: 0.9241 - false_negatives: 203.0762 - false_positives: 423.5331 - loss: 0.2506 - precision: 0.9034 - recall: 0.9511 - true_negatives: 3564.4690 - true_positives: 3808.9219\n",
      "For batch number 500 the model has a loss of 0.2614029347896576\n",
      "\u001b[1m500/689\u001b[0m \u001b[32m━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━\u001b[0m \u001b[1m10s\u001b[0m 58ms/step - auc: 0.9643 - binary_accuracy: 0.9240 - false_negatives: 203.5120 - false_positives: 424.3680 - loss: 0.2506 - precision: 0.9034 - recall: 0.9511 - true_negatives: 3571.6641 - true_positives: 3816.4561\n",
      "For batch number 501 the model has a loss of 0.26138049364089966\n",
      "\u001b[1m501/689\u001b[0m \u001b[32m━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━\u001b[0m \u001b[1m10s\u001b[0m 58ms/step - auc: 0.9643 - binary_accuracy: 0.9240 - false_negatives: 203.9481 - false_positives: 425.1996 - loss: 0.2507 - precision: 0.9034 - recall: 0.9511 - true_negatives: 3578.8484 - true_positives: 3824.0039\n",
      "For batch number 502 the model has a loss of 0.26116088032722473\n",
      "\u001b[1m502/689\u001b[0m \u001b[32m━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━\u001b[0m \u001b[1m10s\u001b[0m 58ms/step - auc: 0.9643 - binary_accuracy: 0.9240 - false_negatives: 204.3845 - false_positives: 426.0299 - loss: 0.2507 - precision: 0.9034 - recall: 0.9511 - true_negatives: 3586.0398 - true_positives: 3831.5459\n",
      "For batch number 503 the model has a loss of 0.26111629605293274\n",
      "\u001b[1m503/689\u001b[0m \u001b[32m━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━\u001b[0m \u001b[1m10s\u001b[0m 58ms/step - auc: 0.9643 - binary_accuracy: 0.9240 - false_negatives: 204.8191 - false_positives: 426.8628 - loss: 0.2507 - precision: 0.9034 - recall: 0.9511 - true_negatives: 3593.2366 - true_positives: 3839.0815\n",
      "For batch number 504 the model has a loss of 0.2612256407737732\n",
      "\u001b[1m504/689\u001b[0m \u001b[32m━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━\u001b[0m \u001b[1m10s\u001b[0m 58ms/step - auc: 0.9643 - binary_accuracy: 0.9240 - false_negatives: 205.2540 - false_positives: 427.6945 - loss: 0.2507 - precision: 0.9034 - recall: 0.9511 - true_negatives: 3600.4226 - true_positives: 3846.6289\n",
      "For batch number 505 the model has a loss of 0.2611747086048126\n",
      "\u001b[1m505/689\u001b[0m \u001b[32m━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━\u001b[0m \u001b[1m10s\u001b[0m 58ms/step - auc: 0.9643 - binary_accuracy: 0.9240 - false_negatives: 205.6911 - false_positives: 428.5228 - loss: 0.2507 - precision: 0.9034 - recall: 0.9511 - true_negatives: 3607.6060 - true_positives: 3854.1802\n",
      "For batch number 506 the model has a loss of 0.2610737085342407\n",
      "\u001b[1m506/689\u001b[0m \u001b[32m━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━\u001b[0m \u001b[1m10s\u001b[0m 58ms/step - auc: 0.9643 - binary_accuracy: 0.9240 - false_negatives: 206.1285 - false_positives: 429.3498 - loss: 0.2508 - precision: 0.9034 - recall: 0.9511 - true_negatives: 3614.7944 - true_positives: 3861.7273\n",
      "For batch number 507 the model has a loss of 0.26133066415786743\n",
      "\u001b[1m507/689\u001b[0m \u001b[32m━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━\u001b[0m \u001b[1m10s\u001b[0m 58ms/step - auc: 0.9643 - binary_accuracy: 0.9240 - false_negatives: 206.5720 - false_positives: 430.1736 - loss: 0.2508 - precision: 0.9034 - recall: 0.9511 - true_negatives: 3621.9744 - true_positives: 3869.2800\n",
      "For batch number 508 the model has a loss of 0.26117923855781555\n",
      "\u001b[1m508/689\u001b[0m \u001b[32m━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━\u001b[0m \u001b[1m10s\u001b[0m 58ms/step - auc: 0.9643 - binary_accuracy: 0.9240 - false_negatives: 207.0138 - false_positives: 431.0000 - loss: 0.2508 - precision: 0.9034 - recall: 0.9511 - true_negatives: 3629.1536 - true_positives: 3876.8328\n",
      "For batch number 509 the model has a loss of 0.26116377115249634\n",
      "\u001b[1m509/689\u001b[0m \u001b[32m━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━\u001b[0m \u001b[1m10s\u001b[0m 58ms/step - auc: 0.9643 - binary_accuracy: 0.9240 - false_negatives: 207.4558 - false_positives: 431.8251 - loss: 0.2508 - precision: 0.9034 - recall: 0.9511 - true_negatives: 3636.3379 - true_positives: 3884.3811\n",
      "For batch number 510 the model has a loss of 0.26140397787094116\n",
      "\u001b[1m510/689\u001b[0m \u001b[32m━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━\u001b[0m \u001b[1m10s\u001b[0m 58ms/step - auc: 0.9643 - binary_accuracy: 0.9240 - false_negatives: 207.8980 - false_positives: 432.6530 - loss: 0.2508 - precision: 0.9034 - recall: 0.9511 - true_negatives: 3643.5234 - true_positives: 3891.9255\n",
      "For batch number 511 the model has a loss of 0.2616155743598938\n",
      "\u001b[1m511/689\u001b[0m \u001b[32m━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━\u001b[0m \u001b[1m10s\u001b[0m 58ms/step - auc: 0.9642 - binary_accuracy: 0.9240 - false_negatives: 208.3425 - false_positives: 433.4814 - loss: 0.2509 - precision: 0.9034 - recall: 0.9510 - true_negatives: 3650.6929 - true_positives: 3899.4834\n",
      "For batch number 512 the model has a loss of 0.26129889488220215\n",
      "\u001b[1m512/689\u001b[0m \u001b[32m━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━\u001b[0m \u001b[1m10s\u001b[0m 58ms/step - auc: 0.9642 - binary_accuracy: 0.9240 - false_negatives: 208.7852 - false_positives: 434.3086 - loss: 0.2509 - precision: 0.9034 - recall: 0.9510 - true_negatives: 3657.8672 - true_positives: 3907.0391\n",
      "For batch number 513 the model has a loss of 0.26149508357048035\n",
      "\u001b[1m513/689\u001b[0m \u001b[32m━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━\u001b[0m \u001b[1m10s\u001b[0m 58ms/step - auc: 0.9642 - binary_accuracy: 0.9240 - false_negatives: 209.2261 - false_positives: 435.1462 - loss: 0.2509 - precision: 0.9034 - recall: 0.9510 - true_negatives: 3665.0449 - true_positives: 3914.5828\n",
      "For batch number 514 the model has a loss of 0.26143598556518555\n",
      "\u001b[1m514/689\u001b[0m \u001b[32m━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━\u001b[0m \u001b[1m10s\u001b[0m 58ms/step - auc: 0.9642 - binary_accuracy: 0.9240 - false_negatives: 209.6673 - false_positives: 435.9844 - loss: 0.2509 - precision: 0.9034 - recall: 0.9510 - true_negatives: 3672.2256 - true_positives: 3922.1226\n",
      "For batch number 515 the model has a loss of 0.2614005208015442\n",
      "\u001b[1m515/689\u001b[0m \u001b[32m━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━\u001b[0m \u001b[1m10s\u001b[0m 58ms/step - auc: 0.9642 - binary_accuracy: 0.9240 - false_negatives: 210.1068 - false_positives: 436.8291 - loss: 0.2509 - precision: 0.9033 - recall: 0.9510 - true_negatives: 3679.4077 - true_positives: 3929.6562\n",
      "For batch number 516 the model has a loss of 0.26123079657554626\n",
      "\u001b[1m516/689\u001b[0m \u001b[32m━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━\u001b[0m \u001b[1m10s\u001b[0m 58ms/step - auc: 0.9642 - binary_accuracy: 0.9240 - false_negatives: 210.5446 - false_positives: 437.6744 - loss: 0.2510 - precision: 0.9033 - recall: 0.9510 - true_negatives: 3686.5930 - true_positives: 3937.1880\n",
      "For batch number 517 the model has a loss of 0.2612563967704773\n",
      "\u001b[1m517/689\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m10s\u001b[0m 58ms/step - auc: 0.9642 - binary_accuracy: 0.9239 - false_negatives: 210.9826 - false_positives: 438.5261 - loss: 0.2510 - precision: 0.9033 - recall: 0.9510 - true_negatives: 3693.7834 - true_positives: 3944.7080\n",
      "For batch number 518 the model has a loss of 0.2612285912036896\n",
      "\u001b[1m518/689\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m9s\u001b[0m 58ms/step - auc: 0.9642 - binary_accuracy: 0.9239 - false_negatives: 211.4209 - false_positives: 439.3822 - loss: 0.2510 - precision: 0.9033 - recall: 0.9510 - true_negatives: 3700.9709 - true_positives: 3952.2258 \n",
      "For batch number 519 the model has a loss of 0.26144883036613464\n",
      "\u001b[1m519/689\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m9s\u001b[0m 58ms/step - auc: 0.9642 - binary_accuracy: 0.9239 - false_negatives: 211.8593 - false_positives: 440.2408 - loss: 0.2510 - precision: 0.9033 - recall: 0.9510 - true_negatives: 3708.1580 - true_positives: 3959.7417\n",
      "For batch number 520 the model has a loss of 0.26169461011886597\n",
      "\u001b[1m520/689\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m9s\u001b[0m 58ms/step - auc: 0.9642 - binary_accuracy: 0.9239 - false_negatives: 212.2962 - false_positives: 441.1039 - loss: 0.2510 - precision: 0.9033 - recall: 0.9510 - true_negatives: 3715.3423 - true_positives: 3967.2578\n",
      "For batch number 521 the model has a loss of 0.2614193260669708\n",
      "\u001b[1m521/689\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m9s\u001b[0m 58ms/step - auc: 0.9642 - binary_accuracy: 0.9239 - false_negatives: 212.7313 - false_positives: 441.9635 - loss: 0.2511 - precision: 0.9033 - recall: 0.9510 - true_negatives: 3722.5203 - true_positives: 3974.7849\n",
      "For batch number 522 the model has a loss of 0.2611069083213806\n",
      "\u001b[1m522/689\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m9s\u001b[0m 58ms/step - auc: 0.9642 - binary_accuracy: 0.9239 - false_negatives: 213.1647 - false_positives: 442.8199 - loss: 0.2511 - precision: 0.9033 - recall: 0.9510 - true_negatives: 3729.7051 - true_positives: 3982.3103\n",
      "For batch number 523 the model has a loss of 0.261166512966156\n",
      "\u001b[1m523/689\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m9s\u001b[0m 58ms/step - auc: 0.9642 - binary_accuracy: 0.9239 - false_negatives: 213.5966 - false_positives: 443.6807 - loss: 0.2511 - precision: 0.9033 - recall: 0.9510 - true_negatives: 3736.8911 - true_positives: 3989.8318\n",
      "For batch number 524 the model has a loss of 0.26114219427108765\n",
      "\u001b[1m524/689\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m9s\u001b[0m 58ms/step - auc: 0.9642 - binary_accuracy: 0.9239 - false_negatives: 214.0267 - false_positives: 444.5420 - loss: 0.2511 - precision: 0.9033 - recall: 0.9509 - true_negatives: 3744.0801 - true_positives: 3997.3511\n",
      "For batch number 525 the model has a loss of 0.26091787219047546\n",
      "\u001b[1m525/689\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m9s\u001b[0m 58ms/step - auc: 0.9642 - binary_accuracy: 0.9239 - false_negatives: 214.4552 - false_positives: 445.4019 - loss: 0.2511 - precision: 0.9033 - recall: 0.9509 - true_negatives: 3751.2725 - true_positives: 4004.8704\n",
      "For batch number 526 the model has a loss of 0.2605838477611542\n",
      "\u001b[1m526/689\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m9s\u001b[0m 58ms/step - auc: 0.9641 - binary_accuracy: 0.9239 - false_negatives: 214.8821 - false_positives: 446.2585 - loss: 0.2512 - precision: 0.9033 - recall: 0.9509 - true_negatives: 3758.4695 - true_positives: 4012.3896\n",
      "For batch number 527 the model has a loss of 0.2605263590812683\n",
      "\u001b[1m527/689\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m9s\u001b[0m 58ms/step - auc: 0.9641 - binary_accuracy: 0.9239 - false_negatives: 215.3074 - false_positives: 447.1158 - loss: 0.2512 - precision: 0.9032 - recall: 0.9509 - true_negatives: 3765.6699 - true_positives: 4019.9070\n",
      "For batch number 528 the model has a loss of 0.2604524493217468\n",
      "\u001b[1m528/689\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m9s\u001b[0m 58ms/step - auc: 0.9641 - binary_accuracy: 0.9239 - false_negatives: 215.7311 - false_positives: 447.9735 - loss: 0.2512 - precision: 0.9032 - recall: 0.9509 - true_negatives: 3772.8711 - true_positives: 4027.4243\n",
      "For batch number 529 the model has a loss of 0.260410338640213\n",
      "\u001b[1m529/689\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m9s\u001b[0m 58ms/step - auc: 0.9641 - binary_accuracy: 0.9239 - false_negatives: 216.1588 - false_positives: 448.8299 - loss: 0.2512 - precision: 0.9032 - recall: 0.9509 - true_negatives: 3780.0681 - true_positives: 4034.9434\n",
      "For batch number 530 the model has a loss of 0.2604244351387024\n",
      "\u001b[1m530/689\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m9s\u001b[0m 58ms/step - auc: 0.9641 - binary_accuracy: 0.9239 - false_negatives: 216.5868 - false_positives: 449.6906 - loss: 0.2512 - precision: 0.9032 - recall: 0.9509 - true_negatives: 3787.2622 - true_positives: 4042.4604\n",
      "For batch number 531 the model has a loss of 0.26031702756881714\n",
      "\u001b[1m531/689\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m9s\u001b[0m 58ms/step - auc: 0.9641 - binary_accuracy: 0.9239 - false_negatives: 217.0151 - false_positives: 450.5518 - loss: 0.2512 - precision: 0.9032 - recall: 0.9509 - true_negatives: 3794.4575 - true_positives: 4049.9756\n",
      "For batch number 532 the model has a loss of 0.2601757049560547\n",
      "\u001b[1m532/689\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m9s\u001b[0m 58ms/step - auc: 0.9641 - binary_accuracy: 0.9239 - false_negatives: 217.4436 - false_positives: 451.4117 - loss: 0.2513 - precision: 0.9032 - recall: 0.9509 - true_negatives: 3801.6504 - true_positives: 4057.4944\n",
      "For batch number 533 the model has a loss of 0.26025277376174927\n",
      "\u001b[1m533/689\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m9s\u001b[0m 58ms/step - auc: 0.9641 - binary_accuracy: 0.9238 - false_negatives: 217.8762 - false_positives: 452.2683 - loss: 0.2513 - precision: 0.9032 - recall: 0.9509 - true_negatives: 3808.8423 - true_positives: 4065.0132\n",
      "For batch number 534 the model has a loss of 0.26061591506004333\n",
      "\u001b[1m534/689\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m9s\u001b[0m 58ms/step - auc: 0.9641 - binary_accuracy: 0.9238 - false_negatives: 218.3127 - false_positives: 453.1255 - loss: 0.2513 - precision: 0.9032 - recall: 0.9509 - true_negatives: 3816.0281 - true_positives: 4072.5337\n",
      "For batch number 535 the model has a loss of 0.2604570984840393\n",
      "\u001b[1m535/689\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m8s\u001b[0m 58ms/step - auc: 0.9641 - binary_accuracy: 0.9238 - false_negatives: 218.7477 - false_positives: 453.9832 - loss: 0.2513 - precision: 0.9032 - recall: 0.9509 - true_negatives: 3823.2244 - true_positives: 4080.0449\n",
      "For batch number 536 the model has a loss of 0.2603469789028168\n",
      "\u001b[1m536/689\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m8s\u001b[0m 58ms/step - auc: 0.9641 - binary_accuracy: 0.9238 - false_negatives: 219.1810 - false_positives: 454.8414 - loss: 0.2513 - precision: 0.9032 - recall: 0.9509 - true_negatives: 3830.4236 - true_positives: 4087.5542\n",
      "For batch number 537 the model has a loss of 0.2605544626712799\n",
      "\u001b[1m537/689\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m8s\u001b[0m 58ms/step - auc: 0.9641 - binary_accuracy: 0.9238 - false_negatives: 219.6164 - false_positives: 455.7002 - loss: 0.2513 - precision: 0.9032 - recall: 0.9509 - true_negatives: 3837.6201 - true_positives: 4095.0632\n",
      "For batch number 538 the model has a loss of 0.2606563866138458\n",
      "\u001b[1m538/689\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m8s\u001b[0m 58ms/step - auc: 0.9641 - binary_accuracy: 0.9238 - false_negatives: 220.0558 - false_positives: 456.5558 - loss: 0.2514 - precision: 0.9032 - recall: 0.9509 - true_negatives: 3844.8179 - true_positives: 4102.5708\n",
      "For batch number 539 the model has a loss of 0.26079779863357544\n",
      "\u001b[1m539/689\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m8s\u001b[0m 58ms/step - auc: 0.9641 - binary_accuracy: 0.9238 - false_negatives: 220.4972 - false_positives: 457.4082 - loss: 0.2514 - precision: 0.9031 - recall: 0.9508 - true_negatives: 3852.0112 - true_positives: 4110.0835\n",
      "For batch number 540 the model has a loss of 0.2607574760913849\n",
      "\u001b[1m540/689\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m8s\u001b[0m 58ms/step - auc: 0.9641 - binary_accuracy: 0.9238 - false_negatives: 220.9407 - false_positives: 458.2574 - loss: 0.2514 - precision: 0.9031 - recall: 0.9508 - true_negatives: 3859.2056 - true_positives: 4117.5962\n",
      "For batch number 541 the model has a loss of 0.2611454725265503\n",
      "\u001b[1m541/689\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m8s\u001b[0m 58ms/step - auc: 0.9641 - binary_accuracy: 0.9238 - false_negatives: 221.3882 - false_positives: 459.1053 - loss: 0.2514 - precision: 0.9031 - recall: 0.9508 - true_negatives: 3866.3955 - true_positives: 4125.1108\n",
      "For batch number 542 the model has a loss of 0.2609029710292816\n",
      "\u001b[1m542/689\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m8s\u001b[0m 58ms/step - auc: 0.9640 - binary_accuracy: 0.9238 - false_negatives: 221.8340 - false_positives: 459.9520 - loss: 0.2514 - precision: 0.9031 - recall: 0.9508 - true_negatives: 3873.5940 - true_positives: 4132.6201\n",
      "For batch number 543 the model has a loss of 0.2611519694328308\n",
      "\u001b[1m543/689\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m8s\u001b[0m 58ms/step - auc: 0.9640 - binary_accuracy: 0.9238 - false_negatives: 222.2799 - false_positives: 460.7974 - loss: 0.2515 - precision: 0.9031 - recall: 0.9508 - true_negatives: 3880.7900 - true_positives: 4140.1328\n",
      "For batch number 544 the model has a loss of 0.2612666189670563\n",
      "\u001b[1m544/689\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m8s\u001b[0m 58ms/step - auc: 0.9640 - binary_accuracy: 0.9238 - false_negatives: 222.7279 - false_positives: 461.6415 - loss: 0.2515 - precision: 0.9031 - recall: 0.9508 - true_negatives: 3887.9817 - true_positives: 4147.6489\n",
      "For batch number 545 the model has a loss of 0.26160258054733276\n",
      "\u001b[1m545/689\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m8s\u001b[0m 58ms/step - auc: 0.9640 - binary_accuracy: 0.9238 - false_negatives: 223.1780 - false_positives: 462.4844 - loss: 0.2515 - precision: 0.9031 - recall: 0.9508 - true_negatives: 3895.1724 - true_positives: 4155.1650\n",
      "For batch number 546 the model has a loss of 0.26190048456192017\n",
      "\u001b[1m546/689\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m8s\u001b[0m 58ms/step - auc: 0.9640 - binary_accuracy: 0.9238 - false_negatives: 223.6264 - false_positives: 463.3352 - loss: 0.2515 - precision: 0.9031 - recall: 0.9508 - true_negatives: 3902.3645 - true_positives: 4162.6738\n",
      "For batch number 547 the model has a loss of 0.26203373074531555\n",
      "\u001b[1m547/689\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m8s\u001b[0m 58ms/step - auc: 0.9640 - binary_accuracy: 0.9238 - false_negatives: 224.0750 - false_positives: 464.1846 - loss: 0.2515 - precision: 0.9031 - recall: 0.9508 - true_negatives: 3909.5576 - true_positives: 4170.1826\n",
      "For batch number 548 the model has a loss of 0.26219233870506287\n",
      "\u001b[1m548/689\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m8s\u001b[0m 58ms/step - auc: 0.9640 - binary_accuracy: 0.9237 - false_negatives: 224.5237 - false_positives: 465.0347 - loss: 0.2515 - precision: 0.9031 - recall: 0.9508 - true_negatives: 3916.7483 - true_positives: 4177.6934\n",
      "For batch number 549 the model has a loss of 0.2621734142303467\n",
      "\u001b[1m549/689\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m8s\u001b[0m 58ms/step - auc: 0.9640 - binary_accuracy: 0.9237 - false_negatives: 224.9709 - false_positives: 465.8834 - loss: 0.2516 - precision: 0.9031 - recall: 0.9508 - true_negatives: 3923.9307 - true_positives: 4185.2148\n",
      "For batch number 550 the model has a loss of 0.262261301279068\n",
      "\u001b[1m550/689\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m8s\u001b[0m 58ms/step - auc: 0.9640 - binary_accuracy: 0.9237 - false_negatives: 225.4200 - false_positives: 466.7309 - loss: 0.2516 - precision: 0.9031 - recall: 0.9508 - true_negatives: 3931.1165 - true_positives: 4192.7329\n",
      "For batch number 551 the model has a loss of 0.26269787549972534\n",
      "\u001b[1m551/689\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m8s\u001b[0m 58ms/step - auc: 0.9640 - binary_accuracy: 0.9237 - false_negatives: 225.8711 - false_positives: 467.5753 - loss: 0.2516 - precision: 0.9031 - recall: 0.9508 - true_negatives: 3938.2922 - true_positives: 4200.2612\n",
      "For batch number 552 the model has a loss of 0.26275524497032166\n",
      "\u001b[1m552/689\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m7s\u001b[0m 58ms/step - auc: 0.9640 - binary_accuracy: 0.9237 - false_negatives: 226.3225 - false_positives: 468.4185 - loss: 0.2516 - precision: 0.9031 - recall: 0.9507 - true_negatives: 3945.4673 - true_positives: 4207.7915\n",
      "For batch number 553 the model has a loss of 0.2627793848514557\n",
      "\u001b[1m553/689\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m7s\u001b[0m 58ms/step - auc: 0.9640 - binary_accuracy: 0.9237 - false_negatives: 226.7722 - false_positives: 469.2658 - loss: 0.2516 - precision: 0.9030 - recall: 0.9507 - true_negatives: 3952.6418 - true_positives: 4215.3203\n",
      "For batch number 554 the model has a loss of 0.26255476474761963\n",
      "\u001b[1m554/689\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m7s\u001b[0m 58ms/step - auc: 0.9640 - binary_accuracy: 0.9237 - false_negatives: 227.2202 - false_positives: 470.1119 - loss: 0.2517 - precision: 0.9030 - recall: 0.9507 - true_negatives: 3959.8176 - true_positives: 4222.8501\n",
      "For batch number 555 the model has a loss of 0.26246169209480286\n",
      "\u001b[1m555/689\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m7s\u001b[0m 58ms/step - auc: 0.9640 - binary_accuracy: 0.9237 - false_negatives: 227.6685 - false_positives: 470.9568 - loss: 0.2517 - precision: 0.9030 - recall: 0.9507 - true_negatives: 3966.9910 - true_positives: 4230.3838\n",
      "For batch number 556 the model has a loss of 0.2629765570163727\n",
      "\u001b[1m556/689\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m7s\u001b[0m 58ms/step - auc: 0.9640 - binary_accuracy: 0.9237 - false_negatives: 228.1187 - false_positives: 471.8076 - loss: 0.2517 - precision: 0.9030 - recall: 0.9507 - true_negatives: 3974.1582 - true_positives: 4237.9155\n",
      "For batch number 557 the model has a loss of 0.2630150616168976\n",
      "\u001b[1m557/689\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m7s\u001b[0m 58ms/step - auc: 0.9639 - binary_accuracy: 0.9237 - false_negatives: 228.5691 - false_positives: 472.6589 - loss: 0.2517 - precision: 0.9030 - recall: 0.9507 - true_negatives: 3981.3286 - true_positives: 4245.4434\n",
      "For batch number 558 the model has a loss of 0.2629339098930359\n",
      "\u001b[1m558/689\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m7s\u001b[0m 58ms/step - auc: 0.9639 - binary_accuracy: 0.9237 - false_negatives: 229.0179 - false_positives: 473.5125 - loss: 0.2517 - precision: 0.9030 - recall: 0.9507 - true_negatives: 3988.5000 - true_positives: 4252.9697\n",
      "For batch number 559 the model has a loss of 0.2630618214607239\n",
      "\u001b[1m559/689\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m7s\u001b[0m 58ms/step - auc: 0.9639 - binary_accuracy: 0.9237 - false_negatives: 229.4669 - false_positives: 474.3685 - loss: 0.2518 - precision: 0.9030 - recall: 0.9507 - true_negatives: 3995.6726 - true_positives: 4260.4922\n",
      "For batch number 560 the model has a loss of 0.26329857110977173\n",
      "\u001b[1m560/689\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m7s\u001b[0m 58ms/step - auc: 0.9639 - binary_accuracy: 0.9237 - false_negatives: 229.9161 - false_positives: 475.2268 - loss: 0.2518 - precision: 0.9030 - recall: 0.9507 - true_negatives: 4002.8481 - true_positives: 4268.0088\n",
      "For batch number 561 the model has a loss of 0.263689249753952\n",
      "\u001b[1m561/689\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m7s\u001b[0m 58ms/step - auc: 0.9639 - binary_accuracy: 0.9237 - false_negatives: 230.3636 - false_positives: 476.0873 - loss: 0.2518 - precision: 0.9030 - recall: 0.9507 - true_negatives: 4010.0269 - true_positives: 4275.5225\n",
      "For batch number 562 the model has a loss of 0.2638038694858551\n",
      "\u001b[1m562/689\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m7s\u001b[0m 58ms/step - auc: 0.9639 - binary_accuracy: 0.9237 - false_negatives: 230.8096 - false_positives: 476.9555 - loss: 0.2518 - precision: 0.9030 - recall: 0.9507 - true_negatives: 4017.2029 - true_positives: 4283.0322\n",
      "For batch number 563 the model has a loss of 0.26371240615844727\n",
      "\u001b[1m563/689\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m7s\u001b[0m 58ms/step - auc: 0.9639 - binary_accuracy: 0.9236 - false_negatives: 231.2558 - false_positives: 477.8242 - loss: 0.2519 - precision: 0.9030 - recall: 0.9507 - true_negatives: 4024.3784 - true_positives: 4290.5415\n",
      "For batch number 564 the model has a loss of 0.2641102075576782\n",
      "\u001b[1m564/689\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m7s\u001b[0m 58ms/step - auc: 0.9639 - binary_accuracy: 0.9236 - false_negatives: 231.7021 - false_positives: 478.6986 - loss: 0.2519 - precision: 0.9030 - recall: 0.9507 - true_negatives: 4031.5496 - true_positives: 4298.0498\n",
      "For batch number 565 the model has a loss of 0.2644801437854767\n",
      "\u001b[1m565/689\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m7s\u001b[0m 58ms/step - auc: 0.9639 - binary_accuracy: 0.9236 - false_negatives: 232.1504 - false_positives: 479.5735 - loss: 0.2519 - precision: 0.9030 - recall: 0.9506 - true_negatives: 4038.7185 - true_positives: 4305.5576\n",
      "For batch number 566 the model has a loss of 0.2645236849784851\n",
      "\u001b[1m566/689\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m7s\u001b[0m 58ms/step - auc: 0.9639 - binary_accuracy: 0.9236 - false_negatives: 232.5989 - false_positives: 480.4505 - loss: 0.2519 - precision: 0.9029 - recall: 0.9506 - true_negatives: 4045.8923 - true_positives: 4313.0581\n",
      "For batch number 567 the model has a loss of 0.26455914974212646\n",
      "\u001b[1m567/689\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m7s\u001b[0m 58ms/step - auc: 0.9639 - binary_accuracy: 0.9236 - false_negatives: 233.0476 - false_positives: 481.3263 - loss: 0.2519 - precision: 0.9029 - recall: 0.9506 - true_negatives: 4053.0618 - true_positives: 4320.5645\n",
      "For batch number 568 the model has a loss of 0.2652641832828522\n",
      "\u001b[1m568/689\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m7s\u001b[0m 58ms/step - auc: 0.9639 - binary_accuracy: 0.9236 - false_negatives: 233.4982 - false_positives: 482.2025 - loss: 0.2520 - precision: 0.9029 - recall: 0.9506 - true_negatives: 4060.2271 - true_positives: 4328.0723\n",
      "For batch number 569 the model has a loss of 0.2651996910572052\n",
      "\u001b[1m569/689\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m6s\u001b[0m 58ms/step - auc: 0.9639 - binary_accuracy: 0.9236 - false_negatives: 233.9473 - false_positives: 483.0791 - loss: 0.2520 - precision: 0.9029 - recall: 0.9506 - true_negatives: 4067.3989 - true_positives: 4335.5747\n",
      "For batch number 570 the model has a loss of 0.2649955153465271\n",
      "\u001b[1m570/689\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m6s\u001b[0m 58ms/step - auc: 0.9639 - binary_accuracy: 0.9236 - false_negatives: 234.3947 - false_positives: 483.9526 - loss: 0.2520 - precision: 0.9029 - recall: 0.9506 - true_negatives: 4074.5701 - true_positives: 4343.0825\n",
      "For batch number 571 the model has a loss of 0.26489412784576416\n",
      "\u001b[1m571/689\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m6s\u001b[0m 58ms/step - auc: 0.9638 - binary_accuracy: 0.9236 - false_negatives: 234.8424 - false_positives: 484.8249 - loss: 0.2520 - precision: 0.9029 - recall: 0.9506 - true_negatives: 4081.7390 - true_positives: 4350.5938\n",
      "For batch number 572 the model has a loss of 0.2648589611053467\n",
      "\u001b[1m572/689\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m6s\u001b[0m 58ms/step - auc: 0.9638 - binary_accuracy: 0.9236 - false_negatives: 235.2902 - false_positives: 485.6941 - loss: 0.2521 - precision: 0.9029 - recall: 0.9506 - true_negatives: 4088.9038 - true_positives: 4358.1118\n",
      "For batch number 573 the model has a loss of 0.26481103897094727\n",
      "\u001b[1m573/689\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m6s\u001b[0m 58ms/step - auc: 0.9638 - binary_accuracy: 0.9236 - false_negatives: 235.7365 - false_positives: 486.5689 - loss: 0.2521 - precision: 0.9029 - recall: 0.9506 - true_negatives: 4096.0752 - true_positives: 4365.6196\n",
      "For batch number 574 the model has a loss of 0.264765202999115\n",
      "\u001b[1m574/689\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m6s\u001b[0m 58ms/step - auc: 0.9638 - binary_accuracy: 0.9236 - false_negatives: 236.1829 - false_positives: 487.4460 - loss: 0.2521 - precision: 0.9029 - recall: 0.9506 - true_negatives: 4103.2456 - true_positives: 4373.1255\n",
      "For batch number 575 the model has a loss of 0.26464560627937317\n",
      "\u001b[1m575/689\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m6s\u001b[0m 58ms/step - auc: 0.9638 - binary_accuracy: 0.9236 - false_negatives: 236.6296 - false_positives: 488.3217 - loss: 0.2521 - precision: 0.9029 - recall: 0.9506 - true_negatives: 4110.4175 - true_positives: 4380.6313\n",
      "For batch number 576 the model has a loss of 0.26455676555633545\n",
      "\u001b[1m576/689\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m6s\u001b[0m 58ms/step - auc: 0.9638 - binary_accuracy: 0.9236 - false_negatives: 237.0746 - false_positives: 489.1996 - loss: 0.2521 - precision: 0.9029 - recall: 0.9506 - true_negatives: 4117.5884 - true_positives: 4388.1372\n",
      "For batch number 577 the model has a loss of 0.26463228464126587\n",
      "\u001b[1m577/689\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m6s\u001b[0m 58ms/step - auc: 0.9638 - binary_accuracy: 0.9235 - false_negatives: 237.5199 - false_positives: 490.0780 - loss: 0.2522 - precision: 0.9028 - recall: 0.9506 - true_negatives: 4124.7505 - true_positives: 4395.6519\n",
      "For batch number 578 the model has a loss of 0.2646266222000122\n",
      "\u001b[1m578/689\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m6s\u001b[0m 58ms/step - auc: 0.9638 - binary_accuracy: 0.9235 - false_negatives: 237.9637 - false_positives: 490.9602 - loss: 0.2522 - precision: 0.9028 - recall: 0.9505 - true_negatives: 4131.9116 - true_positives: 4403.1646\n",
      "For batch number 579 the model has a loss of 0.2654811441898346\n",
      "\u001b[1m579/689\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m6s\u001b[0m 58ms/step - auc: 0.9638 - binary_accuracy: 0.9235 - false_negatives: 238.4093 - false_positives: 491.8428 - loss: 0.2522 - precision: 0.9028 - recall: 0.9505 - true_negatives: 4139.0742 - true_positives: 4410.6733\n",
      "For batch number 580 the model has a loss of 0.2653036415576935\n",
      "\u001b[1m580/689\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m6s\u001b[0m 58ms/step - auc: 0.9638 - binary_accuracy: 0.9235 - false_negatives: 238.8535 - false_positives: 492.7242 - loss: 0.2522 - precision: 0.9028 - recall: 0.9505 - true_negatives: 4146.2310 - true_positives: 4418.1914\n",
      "For batch number 581 the model has a loss of 0.26538893580436707\n",
      "\u001b[1m581/689\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m6s\u001b[0m 58ms/step - auc: 0.9638 - binary_accuracy: 0.9235 - false_negatives: 239.3012 - false_positives: 493.6041 - loss: 0.2523 - precision: 0.9028 - recall: 0.9505 - true_negatives: 4153.3872 - true_positives: 4425.7075\n",
      "For batch number 582 the model has a loss of 0.26548975706100464\n",
      "\u001b[1m582/689\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m6s\u001b[0m 58ms/step - auc: 0.9638 - binary_accuracy: 0.9235 - false_negatives: 239.7509 - false_positives: 494.4828 - loss: 0.2523 - precision: 0.9028 - recall: 0.9505 - true_negatives: 4160.5361 - true_positives: 4433.2305\n",
      "For batch number 583 the model has a loss of 0.26556384563446045\n",
      "\u001b[1m583/689\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m6s\u001b[0m 58ms/step - auc: 0.9638 - binary_accuracy: 0.9235 - false_negatives: 240.2007 - false_positives: 495.3585 - loss: 0.2523 - precision: 0.9028 - recall: 0.9505 - true_negatives: 4167.6860 - true_positives: 4440.7549\n",
      "For batch number 584 the model has a loss of 0.26539820432662964\n",
      "\u001b[1m584/689\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m6s\u001b[0m 58ms/step - auc: 0.9637 - binary_accuracy: 0.9235 - false_negatives: 240.6490 - false_positives: 496.2329 - loss: 0.2523 - precision: 0.9028 - recall: 0.9505 - true_negatives: 4174.8442 - true_positives: 4448.2739\n",
      "For batch number 585 the model has a loss of 0.2652278244495392\n",
      "\u001b[1m585/689\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m6s\u001b[0m 58ms/step - auc: 0.9637 - binary_accuracy: 0.9235 - false_negatives: 241.0974 - false_positives: 497.1077 - loss: 0.2523 - precision: 0.9028 - recall: 0.9505 - true_negatives: 4182.0000 - true_positives: 4455.7949\n",
      "For batch number 586 the model has a loss of 0.26526790857315063\n",
      "\u001b[1m586/689\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m6s\u001b[0m 58ms/step - auc: 0.9637 - binary_accuracy: 0.9235 - false_negatives: 241.5478 - false_positives: 497.9795 - loss: 0.2524 - precision: 0.9028 - recall: 0.9505 - true_negatives: 4189.1484 - true_positives: 4463.3242\n",
      "For batch number 587 the model has a loss of 0.26519519090652466\n",
      "\u001b[1m587/689\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m5s\u001b[0m 58ms/step - auc: 0.9637 - binary_accuracy: 0.9235 - false_negatives: 241.9966 - false_positives: 498.8518 - loss: 0.2524 - precision: 0.9028 - recall: 0.9505 - true_negatives: 4196.2949 - true_positives: 4470.8569\n",
      "For batch number 588 the model has a loss of 0.2655782699584961\n",
      "\u001b[1m588/689\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m5s\u001b[0m 58ms/step - auc: 0.9637 - binary_accuracy: 0.9235 - false_negatives: 242.4456 - false_positives: 499.7279 - loss: 0.2524 - precision: 0.9027 - recall: 0.9505 - true_negatives: 4203.4404 - true_positives: 4478.3862\n",
      "For batch number 589 the model has a loss of 0.2659244239330292\n",
      "\u001b[1m589/689\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m5s\u001b[0m 58ms/step - auc: 0.9637 - binary_accuracy: 0.9235 - false_negatives: 242.8964 - false_positives: 500.6061 - loss: 0.2524 - precision: 0.9027 - recall: 0.9505 - true_negatives: 4210.5825 - true_positives: 4485.9150\n",
      "For batch number 590 the model has a loss of 0.26555246114730835\n",
      "\u001b[1m590/689\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m5s\u001b[0m 58ms/step - auc: 0.9637 - binary_accuracy: 0.9234 - false_negatives: 243.3458 - false_positives: 501.4814 - loss: 0.2525 - precision: 0.9027 - recall: 0.9505 - true_negatives: 4217.7236 - true_positives: 4493.4492\n",
      "For batch number 591 the model has a loss of 0.26533496379852295\n",
      "\u001b[1m591/689\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m5s\u001b[0m 58ms/step - auc: 0.9637 - binary_accuracy: 0.9234 - false_negatives: 243.7953 - false_positives: 502.3536 - loss: 0.2525 - precision: 0.9027 - recall: 0.9504 - true_negatives: 4224.8647 - true_positives: 4500.9863\n",
      "For batch number 592 the model has a loss of 0.2651398181915283\n",
      "\u001b[1m592/689\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m5s\u001b[0m 58ms/step - auc: 0.9637 - binary_accuracy: 0.9234 - false_negatives: 244.2432 - false_positives: 503.2247 - loss: 0.2525 - precision: 0.9027 - recall: 0.9504 - true_negatives: 4232.0083 - true_positives: 4508.5234\n",
      "For batch number 593 the model has a loss of 0.2651583254337311\n",
      "\u001b[1m593/689\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m5s\u001b[0m 58ms/step - auc: 0.9637 - binary_accuracy: 0.9234 - false_negatives: 244.6914 - false_positives: 504.0927 - loss: 0.2525 - precision: 0.9027 - recall: 0.9504 - true_negatives: 4239.1587 - true_positives: 4516.0571\n",
      "For batch number 594 the model has a loss of 0.2650850713253021\n",
      "\u001b[1m594/689\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m5s\u001b[0m 58ms/step - auc: 0.9637 - binary_accuracy: 0.9234 - false_negatives: 245.1380 - false_positives: 504.9596 - loss: 0.2525 - precision: 0.9027 - recall: 0.9504 - true_negatives: 4246.3081 - true_positives: 4523.5942\n",
      "For batch number 595 the model has a loss of 0.265045702457428\n",
      "\u001b[1m595/689\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m5s\u001b[0m 58ms/step - auc: 0.9637 - binary_accuracy: 0.9234 - false_negatives: 245.5832 - false_positives: 505.8269 - loss: 0.2526 - precision: 0.9027 - recall: 0.9504 - true_negatives: 4253.4604 - true_positives: 4531.1294\n",
      "For batch number 596 the model has a loss of 0.2649344503879547\n",
      "\u001b[1m596/689\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m5s\u001b[0m 58ms/step - auc: 0.9637 - binary_accuracy: 0.9234 - false_negatives: 246.0268 - false_positives: 506.6946 - loss: 0.2526 - precision: 0.9027 - recall: 0.9504 - true_negatives: 4260.6108 - true_positives: 4538.6680\n",
      "For batch number 597 the model has a loss of 0.26523780822753906\n",
      "\u001b[1m597/689\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m5s\u001b[0m 58ms/step - auc: 0.9637 - binary_accuracy: 0.9234 - false_negatives: 246.4707 - false_positives: 507.5662 - loss: 0.2526 - precision: 0.9027 - recall: 0.9504 - true_negatives: 4267.7603 - true_positives: 4546.2026\n",
      "For batch number 598 the model has a loss of 0.26524075865745544\n",
      "\u001b[1m598/689\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m5s\u001b[0m 58ms/step - auc: 0.9637 - binary_accuracy: 0.9234 - false_negatives: 246.9130 - false_positives: 508.4381 - loss: 0.2526 - precision: 0.9027 - recall: 0.9504 - true_negatives: 4274.9111 - true_positives: 4553.7373\n",
      "For batch number 599 the model has a loss of 0.26511865854263306\n",
      "\u001b[1m599/689\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m5s\u001b[0m 58ms/step - auc: 0.9636 - binary_accuracy: 0.9234 - false_negatives: 247.3539 - false_positives: 509.3105 - loss: 0.2526 - precision: 0.9027 - recall: 0.9504 - true_negatives: 4282.0635 - true_positives: 4561.2720\n",
      "For batch number 600 the model has a loss of 0.2652316391468048\n",
      "\u001b[1m600/689\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m5s\u001b[0m 58ms/step - auc: 0.9636 - binary_accuracy: 0.9234 - false_negatives: 247.7933 - false_positives: 510.1867 - loss: 0.2527 - precision: 0.9027 - recall: 0.9504 - true_negatives: 4289.2183 - true_positives: 4568.8018\n",
      "For batch number 601 the model has a loss of 0.2649759352207184\n",
      "\u001b[1m601/689\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m5s\u001b[0m 58ms/step - auc: 0.9636 - binary_accuracy: 0.9234 - false_negatives: 248.2329 - false_positives: 511.0599 - loss: 0.2527 - precision: 0.9026 - recall: 0.9504 - true_negatives: 4296.3711 - true_positives: 4576.3359\n",
      "For batch number 602 the model has a loss of 0.2651515603065491\n",
      "\u001b[1m602/689\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m5s\u001b[0m 59ms/step - auc: 0.9636 - binary_accuracy: 0.9234 - false_negatives: 248.6711 - false_positives: 511.9369 - loss: 0.2527 - precision: 0.9026 - recall: 0.9504 - true_negatives: 4303.5283 - true_positives: 4583.8638\n",
      "For batch number 603 the model has a loss of 0.26517248153686523\n",
      "\u001b[1m603/689\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m5s\u001b[0m 59ms/step - auc: 0.9636 - binary_accuracy: 0.9234 - false_negatives: 249.1095 - false_positives: 512.8143 - loss: 0.2527 - precision: 0.9026 - recall: 0.9504 - true_negatives: 4310.6831 - true_positives: 4591.3931\n",
      "For batch number 604 the model has a loss of 0.26497817039489746\n",
      "\u001b[1m604/689\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m4s\u001b[0m 59ms/step - auc: 0.9636 - binary_accuracy: 0.9234 - false_negatives: 249.5464 - false_positives: 513.6887 - loss: 0.2528 - precision: 0.9026 - recall: 0.9504 - true_negatives: 4317.8296 - true_positives: 4598.9355\n",
      "For batch number 605 the model has a loss of 0.2647615969181061\n",
      "\u001b[1m605/689\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m4s\u001b[0m 59ms/step - auc: 0.9636 - binary_accuracy: 0.9234 - false_negatives: 249.9818 - false_positives: 514.5620 - loss: 0.2528 - precision: 0.9026 - recall: 0.9504 - true_negatives: 4324.9785 - true_positives: 4606.4775\n",
      "For batch number 606 the model has a loss of 0.2645881474018097\n",
      "\u001b[1m606/689\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m4s\u001b[0m 59ms/step - auc: 0.9636 - binary_accuracy: 0.9233 - false_negatives: 250.4158 - false_positives: 515.4357 - loss: 0.2528 - precision: 0.9026 - recall: 0.9504 - true_negatives: 4332.1221 - true_positives: 4614.0264\n",
      "For batch number 607 the model has a loss of 0.2649758458137512\n",
      "\u001b[1m607/689\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m4s\u001b[0m 59ms/step - auc: 0.9636 - binary_accuracy: 0.9233 - false_negatives: 250.8501 - false_positives: 516.3080 - loss: 0.2528 - precision: 0.9026 - recall: 0.9503 - true_negatives: 4339.2637 - true_positives: 4621.5781\n",
      "For batch number 608 the model has a loss of 0.265150785446167\n",
      "\u001b[1m608/689\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m4s\u001b[0m 59ms/step - auc: 0.9636 - binary_accuracy: 0.9233 - false_negatives: 251.2878 - false_positives: 517.1793 - loss: 0.2528 - precision: 0.9026 - recall: 0.9503 - true_negatives: 4346.4097 - true_positives: 4629.1235\n",
      "For batch number 609 the model has a loss of 0.2650854289531708\n",
      "\u001b[1m609/689\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m4s\u001b[0m 59ms/step - auc: 0.9636 - binary_accuracy: 0.9233 - false_negatives: 251.7241 - false_positives: 518.0493 - loss: 0.2529 - precision: 0.9026 - recall: 0.9503 - true_negatives: 4353.5552 - true_positives: 4636.6714\n",
      "For batch number 610 the model has a loss of 0.2649862468242645\n",
      "\u001b[1m610/689\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m4s\u001b[0m 59ms/step - auc: 0.9636 - binary_accuracy: 0.9233 - false_negatives: 252.1590 - false_positives: 518.9197 - loss: 0.2529 - precision: 0.9026 - recall: 0.9503 - true_negatives: 4360.7017 - true_positives: 4644.2197\n",
      "For batch number 611 the model has a loss of 0.26514938473701477\n",
      "\u001b[1m611/689\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m4s\u001b[0m 59ms/step - auc: 0.9636 - binary_accuracy: 0.9233 - false_negatives: 252.5957 - false_positives: 519.7905 - loss: 0.2529 - precision: 0.9026 - recall: 0.9503 - true_negatives: 4367.8462 - true_positives: 4651.7676\n",
      "For batch number 612 the model has a loss of 0.26531267166137695\n",
      "\u001b[1m612/689\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m4s\u001b[0m 59ms/step - auc: 0.9636 - binary_accuracy: 0.9233 - false_negatives: 253.0327 - false_positives: 520.6602 - loss: 0.2529 - precision: 0.9026 - recall: 0.9503 - true_negatives: 4374.9917 - true_positives: 4659.3154\n",
      "For batch number 613 the model has a loss of 0.26577243208885193\n",
      "\u001b[1m613/689\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m4s\u001b[0m 59ms/step - auc: 0.9636 - binary_accuracy: 0.9233 - false_negatives: 253.4698 - false_positives: 521.5351 - loss: 0.2529 - precision: 0.9025 - recall: 0.9503 - true_negatives: 4382.1353 - true_positives: 4666.8599\n",
      "For batch number 614 the model has a loss of 0.2659768760204315\n",
      "\u001b[1m614/689\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m4s\u001b[0m 59ms/step - auc: 0.9636 - binary_accuracy: 0.9233 - false_negatives: 253.9072 - false_positives: 522.4104 - loss: 0.2530 - precision: 0.9025 - recall: 0.9503 - true_negatives: 4389.2705 - true_positives: 4674.4121\n",
      "For batch number 615 the model has a loss of 0.26586484909057617\n",
      "\u001b[1m615/689\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m4s\u001b[0m 59ms/step - auc: 0.9635 - binary_accuracy: 0.9233 - false_negatives: 254.3431 - false_positives: 523.2845 - loss: 0.2530 - precision: 0.9025 - recall: 0.9503 - true_negatives: 4396.4033 - true_positives: 4681.9692\n",
      "For batch number 616 the model has a loss of 0.2658277153968811\n",
      "\u001b[1m616/689\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m4s\u001b[0m 59ms/step - auc: 0.9635 - binary_accuracy: 0.9233 - false_negatives: 254.7792 - false_positives: 524.1575 - loss: 0.2530 - precision: 0.9025 - recall: 0.9503 - true_negatives: 4403.5322 - true_positives: 4689.5308\n",
      "For batch number 617 the model has a loss of 0.2660561501979828\n",
      "\u001b[1m617/689\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m4s\u001b[0m 59ms/step - auc: 0.9635 - binary_accuracy: 0.9233 - false_negatives: 255.2156 - false_positives: 525.0324 - loss: 0.2530 - precision: 0.9025 - recall: 0.9503 - true_negatives: 4410.6562 - true_positives: 4697.0957\n",
      "For batch number 618 the model has a loss of 0.26602262258529663\n",
      "\u001b[1m618/689\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m4s\u001b[0m 59ms/step - auc: 0.9635 - binary_accuracy: 0.9233 - false_negatives: 255.6505 - false_positives: 525.9078 - loss: 0.2530 - precision: 0.9025 - recall: 0.9503 - true_negatives: 4417.7769 - true_positives: 4704.6650\n",
      "For batch number 619 the model has a loss of 0.2660678029060364\n",
      "\u001b[1m619/689\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m4s\u001b[0m 59ms/step - auc: 0.9635 - binary_accuracy: 0.9233 - false_negatives: 256.0856 - false_positives: 526.7884 - loss: 0.2531 - precision: 0.9025 - recall: 0.9503 - true_negatives: 4424.8936 - true_positives: 4712.2324\n",
      "For batch number 620 the model has a loss of 0.2665243148803711\n",
      "\u001b[1m620/689\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m4s\u001b[0m 59ms/step - auc: 0.9635 - binary_accuracy: 0.9233 - false_negatives: 256.5210 - false_positives: 527.6677 - loss: 0.2531 - precision: 0.9025 - recall: 0.9503 - true_negatives: 4432.0098 - true_positives: 4719.8018\n",
      "For batch number 621 the model has a loss of 0.26647263765335083\n",
      "\u001b[1m621/689\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m3s\u001b[0m 59ms/step - auc: 0.9635 - binary_accuracy: 0.9233 - false_negatives: 256.9565 - false_positives: 528.5459 - loss: 0.2531 - precision: 0.9025 - recall: 0.9503 - true_negatives: 4439.1274 - true_positives: 4727.3706\n",
      "For batch number 622 the model has a loss of 0.2665024399757385\n",
      "\u001b[1m622/689\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m3s\u001b[0m 59ms/step - auc: 0.9635 - binary_accuracy: 0.9233 - false_negatives: 257.3907 - false_positives: 529.4277 - loss: 0.2531 - precision: 0.9025 - recall: 0.9503 - true_negatives: 4446.2441 - true_positives: 4734.9375\n",
      "For batch number 623 the model has a loss of 0.26649248600006104\n",
      "\u001b[1m623/689\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m3s\u001b[0m 59ms/step - auc: 0.9635 - binary_accuracy: 0.9232 - false_negatives: 257.8234 - false_positives: 530.3130 - loss: 0.2531 - precision: 0.9025 - recall: 0.9503 - true_negatives: 4453.3594 - true_positives: 4742.5039\n",
      "For batch number 624 the model has a loss of 0.2661914825439453\n",
      "\u001b[1m624/689\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m3s\u001b[0m 59ms/step - auc: 0.9635 - binary_accuracy: 0.9232 - false_negatives: 258.2548 - false_positives: 531.1955 - loss: 0.2532 - precision: 0.9025 - recall: 0.9503 - true_negatives: 4460.4741 - true_positives: 4750.0752\n",
      "For batch number 625 the model has a loss of 0.26659342646598816\n",
      "\u001b[1m625/689\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m3s\u001b[0m 59ms/step - auc: 0.9635 - binary_accuracy: 0.9232 - false_negatives: 258.6880 - false_positives: 532.0816 - loss: 0.2532 - precision: 0.9025 - recall: 0.9503 - true_negatives: 4467.5854 - true_positives: 4757.6450\n",
      "For batch number 626 the model has a loss of 0.2664130628108978\n",
      "\u001b[1m626/689\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m3s\u001b[0m 59ms/step - auc: 0.9635 - binary_accuracy: 0.9232 - false_negatives: 259.1198 - false_positives: 532.9681 - loss: 0.2532 - precision: 0.9024 - recall: 0.9502 - true_negatives: 4474.6982 - true_positives: 4765.2139\n",
      "For batch number 627 the model has a loss of 0.266834020614624\n",
      "\u001b[1m627/689\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m3s\u001b[0m 59ms/step - auc: 0.9635 - binary_accuracy: 0.9232 - false_negatives: 259.5534 - false_positives: 533.8533 - loss: 0.2532 - precision: 0.9024 - recall: 0.9502 - true_negatives: 4481.8057 - true_positives: 4772.7881\n",
      "For batch number 628 the model has a loss of 0.26727530360221863\n",
      "\u001b[1m628/689\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m3s\u001b[0m 59ms/step - auc: 0.9635 - binary_accuracy: 0.9232 - false_negatives: 259.9889 - false_positives: 534.7388 - loss: 0.2533 - precision: 0.9024 - recall: 0.9502 - true_negatives: 4488.9028 - true_positives: 4780.3696\n",
      "For batch number 629 the model has a loss of 0.26743075251579285\n",
      "\u001b[1m629/689\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m3s\u001b[0m 59ms/step - auc: 0.9635 - binary_accuracy: 0.9232 - false_negatives: 260.4277 - false_positives: 535.6232 - loss: 0.2533 - precision: 0.9024 - recall: 0.9502 - true_negatives: 4495.9937 - true_positives: 4787.9556\n",
      "For batch number 630 the model has a loss of 0.2672416567802429\n",
      "\u001b[1m630/689\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m3s\u001b[0m 59ms/step - auc: 0.9634 - binary_accuracy: 0.9232 - false_negatives: 260.8667 - false_positives: 536.5048 - loss: 0.2533 - precision: 0.9024 - recall: 0.9502 - true_negatives: 4503.0903 - true_positives: 4795.5381\n",
      "For batch number 631 the model has a loss of 0.2673419415950775\n",
      "\u001b[1m631/689\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m3s\u001b[0m 59ms/step - auc: 0.9634 - binary_accuracy: 0.9232 - false_negatives: 261.3059 - false_positives: 537.3867 - loss: 0.2533 - precision: 0.9024 - recall: 0.9502 - true_negatives: 4510.1855 - true_positives: 4803.1221\n",
      "For batch number 632 the model has a loss of 0.26719462871551514\n",
      "\u001b[1m632/689\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m3s\u001b[0m 59ms/step - auc: 0.9634 - binary_accuracy: 0.9232 - false_negatives: 261.7437 - false_positives: 538.2658 - loss: 0.2533 - precision: 0.9024 - recall: 0.9502 - true_negatives: 4517.2769 - true_positives: 4810.7134\n",
      "For batch number 633 the model has a loss of 0.2671377658843994\n",
      "\u001b[1m633/689\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m3s\u001b[0m 59ms/step - auc: 0.9634 - binary_accuracy: 0.9232 - false_negatives: 262.1817 - false_positives: 539.1437 - loss: 0.2534 - precision: 0.9024 - recall: 0.9502 - true_negatives: 4524.3604 - true_positives: 4818.3145\n",
      "For batch number 634 the model has a loss of 0.26730045676231384\n",
      "\u001b[1m634/689\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m3s\u001b[0m 59ms/step - auc: 0.9634 - binary_accuracy: 0.9232 - false_negatives: 262.6230 - false_positives: 540.0221 - loss: 0.2534 - precision: 0.9024 - recall: 0.9502 - true_negatives: 4531.4385 - true_positives: 4825.9165\n",
      "For batch number 635 the model has a loss of 0.26742279529571533\n",
      "\u001b[1m635/689\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m3s\u001b[0m 59ms/step - auc: 0.9634 - binary_accuracy: 0.9232 - false_negatives: 263.0630 - false_positives: 540.9055 - loss: 0.2534 - precision: 0.9024 - recall: 0.9502 - true_negatives: 4538.5229 - true_positives: 4833.5088\n",
      "For batch number 636 the model has a loss of 0.2675689160823822\n",
      "\u001b[1m636/689\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m3s\u001b[0m 59ms/step - auc: 0.9634 - binary_accuracy: 0.9232 - false_negatives: 263.5016 - false_positives: 541.7940 - loss: 0.2534 - precision: 0.9024 - recall: 0.9502 - true_negatives: 4545.6006 - true_positives: 4841.1040\n",
      "For batch number 637 the model has a loss of 0.2673928141593933\n",
      "\u001b[1m637/689\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m3s\u001b[0m 59ms/step - auc: 0.9634 - binary_accuracy: 0.9232 - false_negatives: 263.9388 - false_positives: 542.6813 - loss: 0.2535 - precision: 0.9024 - recall: 0.9502 - true_negatives: 4552.6733 - true_positives: 4848.7065\n",
      "For batch number 638 the model has a loss of 0.2673504650592804\n",
      "\u001b[1m638/689\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m2s\u001b[0m 59ms/step - auc: 0.9634 - binary_accuracy: 0.9231 - false_negatives: 264.3746 - false_positives: 543.5721 - loss: 0.2535 - precision: 0.9024 - recall: 0.9502 - true_negatives: 4559.7446 - true_positives: 4856.3086\n",
      "For batch number 639 the model has a loss of 0.26728367805480957\n",
      "\u001b[1m639/689\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m2s\u001b[0m 59ms/step - auc: 0.9634 - binary_accuracy: 0.9231 - false_negatives: 264.8091 - false_positives: 544.4648 - loss: 0.2535 - precision: 0.9023 - recall: 0.9502 - true_negatives: 4566.8105 - true_positives: 4863.9155\n",
      "For batch number 640 the model has a loss of 0.2673950791358948\n",
      "\u001b[1m640/689\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m2s\u001b[0m 59ms/step - auc: 0.9634 - binary_accuracy: 0.9231 - false_negatives: 265.2422 - false_positives: 545.3594 - loss: 0.2535 - precision: 0.9023 - recall: 0.9502 - true_negatives: 4573.8779 - true_positives: 4871.5205\n",
      "For batch number 641 the model has a loss of 0.26744404435157776\n",
      "\u001b[1m641/689\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m2s\u001b[0m 59ms/step - auc: 0.9634 - binary_accuracy: 0.9231 - false_negatives: 265.6740 - false_positives: 546.2606 - loss: 0.2535 - precision: 0.9023 - recall: 0.9502 - true_negatives: 4580.9453 - true_positives: 4879.1201\n",
      "For batch number 642 the model has a loss of 0.26746666431427\n",
      "\u001b[1m642/689\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m2s\u001b[0m 59ms/step - auc: 0.9634 - binary_accuracy: 0.9231 - false_negatives: 266.1044 - false_positives: 547.1651 - loss: 0.2536 - precision: 0.9023 - recall: 0.9502 - true_negatives: 4588.0142 - true_positives: 4886.7163\n",
      "For batch number 643 the model has a loss of 0.26736077666282654\n",
      "\u001b[1m643/689\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m2s\u001b[0m 59ms/step - auc: 0.9634 - binary_accuracy: 0.9231 - false_negatives: 266.5334 - false_positives: 548.0684 - loss: 0.2536 - precision: 0.9023 - recall: 0.9502 - true_negatives: 4595.0854 - true_positives: 4894.3125\n",
      "For batch number 644 the model has a loss of 0.26732632517814636\n",
      "\u001b[1m644/689\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m2s\u001b[0m 59ms/step - auc: 0.9634 - binary_accuracy: 0.9231 - false_negatives: 266.9627 - false_positives: 548.9705 - loss: 0.2536 - precision: 0.9023 - recall: 0.9502 - true_negatives: 4602.1523 - true_positives: 4901.9146\n",
      "For batch number 645 the model has a loss of 0.26718854904174805\n",
      "\u001b[1m645/689\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m2s\u001b[0m 59ms/step - auc: 0.9634 - binary_accuracy: 0.9231 - false_negatives: 267.3938 - false_positives: 549.8713 - loss: 0.2536 - precision: 0.9023 - recall: 0.9502 - true_negatives: 4609.2139 - true_positives: 4909.5210\n",
      "For batch number 646 the model has a loss of 0.2671217918395996\n",
      "\u001b[1m646/689\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m2s\u001b[0m 59ms/step - auc: 0.9633 - binary_accuracy: 0.9231 - false_negatives: 267.8235 - false_positives: 550.7725 - loss: 0.2536 - precision: 0.9023 - recall: 0.9502 - true_negatives: 4616.2739 - true_positives: 4917.1299\n",
      "For batch number 647 the model has a loss of 0.26716962456703186\n",
      "\u001b[1m647/689\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m2s\u001b[0m 59ms/step - auc: 0.9633 - binary_accuracy: 0.9231 - false_negatives: 268.2519 - false_positives: 551.6785 - loss: 0.2537 - precision: 0.9023 - recall: 0.9501 - true_negatives: 4623.3384 - true_positives: 4924.7310\n",
      "For batch number 648 the model has a loss of 0.2671256959438324\n",
      "\u001b[1m648/689\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m2s\u001b[0m 59ms/step - auc: 0.9633 - binary_accuracy: 0.9231 - false_negatives: 268.6805 - false_positives: 552.5849 - loss: 0.2537 - precision: 0.9023 - recall: 0.9501 - true_negatives: 4630.4028 - true_positives: 4932.3320\n",
      "For batch number 649 the model has a loss of 0.2669883072376251\n",
      "\u001b[1m649/689\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m2s\u001b[0m 59ms/step - auc: 0.9633 - binary_accuracy: 0.9231 - false_negatives: 269.1078 - false_positives: 553.4900 - loss: 0.2537 - precision: 0.9023 - recall: 0.9501 - true_negatives: 4637.4683 - true_positives: 4939.9336\n",
      "For batch number 650 the model has a loss of 0.266987681388855\n",
      "\u001b[1m650/689\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m2s\u001b[0m 59ms/step - auc: 0.9633 - binary_accuracy: 0.9231 - false_negatives: 269.5354 - false_positives: 554.3939 - loss: 0.2537 - precision: 0.9023 - recall: 0.9501 - true_negatives: 4644.5293 - true_positives: 4947.5415\n",
      "For batch number 651 the model has a loss of 0.2668880224227905\n",
      "\u001b[1m651/689\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m2s\u001b[0m 59ms/step - auc: 0.9633 - binary_accuracy: 0.9231 - false_negatives: 269.9616 - false_positives: 555.2996 - loss: 0.2537 - precision: 0.9022 - recall: 0.9501 - true_negatives: 4651.5913 - true_positives: 4955.1475\n",
      "For batch number 652 the model has a loss of 0.2669447064399719\n",
      "\u001b[1m652/689\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m2s\u001b[0m 59ms/step - auc: 0.9633 - binary_accuracy: 0.9231 - false_negatives: 270.3880 - false_positives: 556.2070 - loss: 0.2538 - precision: 0.9022 - recall: 0.9501 - true_negatives: 4658.6548 - true_positives: 4962.7500\n",
      "For batch number 653 the model has a loss of 0.2668848931789398\n",
      "\u001b[1m653/689\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m2s\u001b[0m 59ms/step - auc: 0.9633 - binary_accuracy: 0.9231 - false_negatives: 270.8147 - false_positives: 557.1133 - loss: 0.2538 - precision: 0.9022 - recall: 0.9501 - true_negatives: 4665.7212 - true_positives: 4970.3506\n",
      "For batch number 654 the model has a loss of 0.26706668734550476\n",
      "\u001b[1m654/689\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m2s\u001b[0m 59ms/step - auc: 0.9633 - binary_accuracy: 0.9230 - false_negatives: 271.2401 - false_positives: 558.0260 - loss: 0.2538 - precision: 0.9022 - recall: 0.9501 - true_negatives: 4672.7876 - true_positives: 4977.9463\n",
      "For batch number 655 the model has a loss of 0.2669420838356018\n",
      "\u001b[1m655/689\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m1s\u001b[0m 59ms/step - auc: 0.9633 - binary_accuracy: 0.9230 - false_negatives: 271.6641 - false_positives: 558.9389 - loss: 0.2538 - precision: 0.9022 - recall: 0.9501 - true_negatives: 4679.8550 - true_positives: 4985.5420\n",
      "For batch number 656 the model has a loss of 0.26676803827285767\n",
      "\u001b[1m656/689\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m1s\u001b[0m 59ms/step - auc: 0.9633 - binary_accuracy: 0.9230 - false_negatives: 272.0884 - false_positives: 559.8491 - loss: 0.2538 - precision: 0.9022 - recall: 0.9501 - true_negatives: 4686.9209 - true_positives: 4993.1416\n",
      "For batch number 657 the model has a loss of 0.26687923073768616\n",
      "\u001b[1m657/689\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m1s\u001b[0m 59ms/step - auc: 0.9633 - binary_accuracy: 0.9230 - false_negatives: 272.5129 - false_positives: 560.7626 - loss: 0.2539 - precision: 0.9022 - recall: 0.9501 - true_negatives: 4693.9922 - true_positives: 5000.7319\n",
      "For batch number 658 the model has a loss of 0.26682546734809875\n",
      "\u001b[1m658/689\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m1s\u001b[0m 59ms/step - auc: 0.9633 - binary_accuracy: 0.9230 - false_negatives: 272.9377 - false_positives: 561.6733 - loss: 0.2539 - precision: 0.9022 - recall: 0.9501 - true_negatives: 4701.0669 - true_positives: 5008.3223\n",
      "For batch number 659 the model has a loss of 0.2667102515697479\n",
      "\u001b[1m659/689\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m1s\u001b[0m 59ms/step - auc: 0.9633 - binary_accuracy: 0.9230 - false_negatives: 273.3611 - false_positives: 562.5827 - loss: 0.2539 - precision: 0.9022 - recall: 0.9501 - true_negatives: 4708.1440 - true_positives: 5015.9121\n",
      "For batch number 660 the model has a loss of 0.26651760935783386\n",
      "\u001b[1m660/689\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m1s\u001b[0m 59ms/step - auc: 0.9633 - binary_accuracy: 0.9230 - false_negatives: 273.7833 - false_positives: 563.4924 - loss: 0.2539 - precision: 0.9022 - recall: 0.9501 - true_negatives: 4715.2197 - true_positives: 5023.5044\n",
      "For batch number 661 the model has a loss of 0.26648855209350586\n",
      "\u001b[1m661/689\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m1s\u001b[0m 59ms/step - auc: 0.9633 - binary_accuracy: 0.9230 - false_negatives: 274.2057 - false_positives: 564.4024 - loss: 0.2539 - precision: 0.9021 - recall: 0.9501 - true_negatives: 4722.3027 - true_positives: 5031.0894\n",
      "For batch number 662 the model has a loss of 0.26619309186935425\n",
      "\u001b[1m662/689\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m1s\u001b[0m 59ms/step - auc: 0.9633 - binary_accuracy: 0.9230 - false_negatives: 274.6269 - false_positives: 565.3097 - loss: 0.2540 - precision: 0.9021 - recall: 0.9501 - true_negatives: 4729.3896 - true_positives: 5038.6738\n",
      "For batch number 663 the model has a loss of 0.2660521864891052\n",
      "\u001b[1m663/689\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m1s\u001b[0m 59ms/step - auc: 0.9632 - binary_accuracy: 0.9230 - false_negatives: 275.0483 - false_positives: 566.2172 - loss: 0.2540 - precision: 0.9021 - recall: 0.9501 - true_negatives: 4736.4810 - true_positives: 5046.2534\n",
      "For batch number 664 the model has a loss of 0.2659662067890167\n",
      "\u001b[1m664/689\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m1s\u001b[0m 59ms/step - auc: 0.9632 - binary_accuracy: 0.9230 - false_negatives: 275.4699 - false_positives: 567.1235 - loss: 0.2540 - precision: 0.9021 - recall: 0.9501 - true_negatives: 4743.5708 - true_positives: 5053.8359\n",
      "For batch number 665 the model has a loss of 0.2658335864543915\n",
      "\u001b[1m665/689\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m1s\u001b[0m 59ms/step - auc: 0.9632 - binary_accuracy: 0.9230 - false_negatives: 275.8902 - false_positives: 568.0286 - loss: 0.2540 - precision: 0.9021 - recall: 0.9501 - true_negatives: 4750.6709 - true_positives: 5061.4106\n",
      "For batch number 666 the model has a loss of 0.2658374607563019\n",
      "\u001b[1m666/689\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m1s\u001b[0m 59ms/step - auc: 0.9632 - binary_accuracy: 0.9230 - false_negatives: 276.3123 - false_positives: 568.9324 - loss: 0.2540 - precision: 0.9021 - recall: 0.9501 - true_negatives: 4757.7656 - true_positives: 5068.9893\n",
      "For batch number 667 the model has a loss of 0.2659250497817993\n",
      "\u001b[1m667/689\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m1s\u001b[0m 59ms/step - auc: 0.9632 - binary_accuracy: 0.9230 - false_negatives: 276.7346 - false_positives: 569.8336 - loss: 0.2541 - precision: 0.9021 - recall: 0.9501 - true_negatives: 4764.8589 - true_positives: 5076.5728\n",
      "For batch number 668 the model has a loss of 0.26578226685523987\n",
      "\u001b[1m668/689\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m1s\u001b[0m 59ms/step - auc: 0.9632 - binary_accuracy: 0.9230 - false_negatives: 277.1557 - false_positives: 570.7335 - loss: 0.2541 - precision: 0.9021 - recall: 0.9501 - true_negatives: 4771.9551 - true_positives: 5084.1558\n",
      "For batch number 669 the model has a loss of 0.26559019088745117\n",
      "\u001b[1m669/689\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m1s\u001b[0m 59ms/step - auc: 0.9632 - binary_accuracy: 0.9230 - false_negatives: 277.5755 - false_positives: 571.6338 - loss: 0.2541 - precision: 0.9021 - recall: 0.9501 - true_negatives: 4779.0566 - true_positives: 5091.7339\n",
      "For batch number 670 the model has a loss of 0.26556894183158875\n",
      "\u001b[1m670/689\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m1s\u001b[0m 59ms/step - auc: 0.9632 - binary_accuracy: 0.9229 - false_negatives: 277.9970 - false_positives: 572.5328 - loss: 0.2541 - precision: 0.9021 - recall: 0.9501 - true_negatives: 4786.1626 - true_positives: 5099.3076\n",
      "For batch number 671 the model has a loss of 0.26537927985191345\n",
      "\u001b[1m671/689\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m1s\u001b[0m 59ms/step - auc: 0.9632 - binary_accuracy: 0.9229 - false_negatives: 278.4173 - false_positives: 573.4292 - loss: 0.2541 - precision: 0.9021 - recall: 0.9501 - true_negatives: 4793.2651 - true_positives: 5106.8882\n",
      "For batch number 672 the model has a loss of 0.2652184069156647\n",
      "\u001b[1m672/689\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 59ms/step - auc: 0.9632 - binary_accuracy: 0.9229 - false_negatives: 278.8363 - false_positives: 574.3229 - loss: 0.2541 - precision: 0.9021 - recall: 0.9501 - true_negatives: 4800.3750 - true_positives: 5114.4658\n",
      "For batch number 673 the model has a loss of 0.26513636112213135\n",
      "\u001b[1m673/689\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 59ms/step - auc: 0.9632 - binary_accuracy: 0.9229 - false_negatives: 279.2556 - false_positives: 575.2155 - loss: 0.2542 - precision: 0.9020 - recall: 0.9501 - true_negatives: 4807.4888 - true_positives: 5122.0400\n",
      "For batch number 674 the model has a loss of 0.26497408747673035\n",
      "\u001b[1m674/689\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 59ms/step - auc: 0.9632 - binary_accuracy: 0.9229 - false_negatives: 279.6751 - false_positives: 576.1053 - loss: 0.2542 - precision: 0.9020 - recall: 0.9501 - true_negatives: 4814.6040 - true_positives: 5129.6157\n",
      "For batch number 675 the model has a loss of 0.2648211121559143\n",
      "\u001b[1m675/689\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 59ms/step - auc: 0.9632 - binary_accuracy: 0.9229 - false_negatives: 280.0933 - false_positives: 576.9941 - loss: 0.2542 - precision: 0.9020 - recall: 0.9500 - true_negatives: 4821.7217 - true_positives: 5137.1909\n",
      "For batch number 676 the model has a loss of 0.2647620737552643\n",
      "\u001b[1m676/689\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 59ms/step - auc: 0.9632 - binary_accuracy: 0.9229 - false_negatives: 280.5103 - false_positives: 577.8817 - loss: 0.2542 - precision: 0.9020 - recall: 0.9500 - true_negatives: 4828.8418 - true_positives: 5144.7661\n",
      "For batch number 677 the model has a loss of 0.2649126946926117\n",
      "\u001b[1m677/689\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 59ms/step - auc: 0.9632 - binary_accuracy: 0.9229 - false_negatives: 280.9276 - false_positives: 578.7681 - loss: 0.2542 - precision: 0.9020 - recall: 0.9500 - true_negatives: 4835.9570 - true_positives: 5152.3472\n",
      "For batch number 678 the model has a loss of 0.2647842466831207\n",
      "\u001b[1m678/689\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 59ms/step - auc: 0.9632 - binary_accuracy: 0.9229 - false_negatives: 281.3437 - false_positives: 579.6534 - loss: 0.2542 - precision: 0.9020 - recall: 0.9500 - true_negatives: 4843.0752 - true_positives: 5159.9277\n",
      "For batch number 679 the model has a loss of 0.26487478613853455\n",
      "\u001b[1m679/689\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 59ms/step - auc: 0.9632 - binary_accuracy: 0.9229 - false_negatives: 281.7599 - false_positives: 580.5405 - loss: 0.2542 - precision: 0.9020 - recall: 0.9500 - true_negatives: 4850.1958 - true_positives: 5167.5039\n",
      "For batch number 680 the model has a loss of 0.2648322880268097\n",
      "\u001b[1m680/689\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 59ms/step - auc: 0.9632 - binary_accuracy: 0.9229 - false_negatives: 282.1779 - false_positives: 581.4250 - loss: 0.2543 - precision: 0.9020 - recall: 0.9500 - true_negatives: 4857.3164 - true_positives: 5175.0811\n",
      "For batch number 681 the model has a loss of 0.26489630341529846\n",
      "\u001b[1m681/689\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 59ms/step - auc: 0.9632 - binary_accuracy: 0.9229 - false_negatives: 282.5962 - false_positives: 582.3083 - loss: 0.2543 - precision: 0.9020 - recall: 0.9500 - true_negatives: 4864.4434 - true_positives: 5182.6519\n",
      "For batch number 682 the model has a loss of 0.26471060514450073\n",
      "\u001b[1m682/689\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 59ms/step - auc: 0.9631 - binary_accuracy: 0.9229 - false_negatives: 283.0132 - false_positives: 583.1921 - loss: 0.2543 - precision: 0.9020 - recall: 0.9500 - true_negatives: 4871.5703 - true_positives: 5190.2241\n",
      "For batch number 683 the model has a loss of 0.2647632360458374\n",
      "\u001b[1m683/689\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 59ms/step - auc: 0.9631 - binary_accuracy: 0.9229 - false_negatives: 283.4290 - false_positives: 584.0790 - loss: 0.2543 - precision: 0.9020 - recall: 0.9500 - true_negatives: 4878.6938 - true_positives: 5197.7979\n",
      "For batch number 684 the model has a loss of 0.2648427188396454\n",
      "\u001b[1m684/689\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 59ms/step - auc: 0.9631 - binary_accuracy: 0.9229 - false_negatives: 283.8465 - false_positives: 584.9649 - loss: 0.2543 - precision: 0.9020 - recall: 0.9500 - true_negatives: 4885.8115 - true_positives: 5205.3770\n",
      "For batch number 685 the model has a loss of 0.2648448050022125\n",
      "\u001b[1m685/689\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 59ms/step - auc: 0.9631 - binary_accuracy: 0.9229 - false_negatives: 284.2628 - false_positives: 585.8525 - loss: 0.2543 - precision: 0.9020 - recall: 0.9500 - true_negatives: 4892.9326 - true_positives: 5212.9517\n",
      "For batch number 686 the model has a loss of 0.2648666501045227\n",
      "\u001b[1m686/689\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 59ms/step - auc: 0.9631 - binary_accuracy: 0.9229 - false_negatives: 284.6793 - false_positives: 586.7405 - loss: 0.2544 - precision: 0.9020 - recall: 0.9500 - true_negatives: 4900.0552 - true_positives: 5220.5249\n",
      "For batch number 687 the model has a loss of 0.2647269666194916\n",
      "\u001b[1m687/689\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 59ms/step - auc: 0.9631 - binary_accuracy: 0.9229 - false_negatives: 285.0946 - false_positives: 587.6288 - loss: 0.2544 - precision: 0.9019 - recall: 0.9500 - true_negatives: 4907.1777 - true_positives: 5228.0991\n",
      "For batch number 688 the model has a loss of 0.2647623121738434\n",
      "\u001b[1m688/689\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 59ms/step - auc: 0.9631 - binary_accuracy: 0.9229 - false_negatives: 285.5102 - false_positives: 588.5189 - loss: 0.2544 - precision: 0.9019 - recall: 0.9500 - true_negatives: 4914.3008 - true_positives: 5235.6699\n",
      "For batch number 689 the model has a loss of 0.26464027166366577\n",
      "\u001b[1m689/689\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 59ms/step - auc: 0.9631 - binary_accuracy: 0.9229 - false_negatives: 285.9245 - false_positives: 589.4093 - loss: 0.2544 - precision: 0.9019 - recall: 0.9500 - true_negatives: 4921.4224 - true_positives: 5243.2407\n",
      "For epoch number 5 the model has a loss of 0.26464027166366577\n",
      "\u001b[1m689/689\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m43s\u001b[0m 62ms/step - auc: 0.9631 - binary_accuracy: 0.9228 - false_negatives: 286.3377 - false_positives: 590.2971 - loss: 0.2544 - precision: 0.9019 - recall: 0.9500 - true_negatives: 4928.5229 - true_positives: 5250.7900 - val_auc: 0.9653 - val_binary_accuracy: 0.9376 - val_false_negatives: 96.0000 - val_false_positives: 76.0000 - val_loss: 0.2821 - val_precision: 0.9448 - val_recall: 0.9312 - val_true_negatives: 1283.0000 - val_true_positives: 1300.0000\n"
     ]
    }
   ],
   "source": [
    "# Using the Callback while training\n",
    "history = Model.model.fit(Model.trainDataset, validation_data=Model.valDataset, epochs=5, verbose=1, callbacks=[LossCallback(),])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# IN-BUILT CALLBACKS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CSV LOGGER -> Logs the data in logs file to a csv file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CSV-Logger Callback\n",
    "csvCallback = tf.keras.callbacks.CSVLogger(\n",
    "    'logs.csv', # Filename\n",
    "    separator=\",\", # Separator\n",
    "    append=False # False for creating/overwriting, True for appending\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m689/689\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 59ms/step - auc: 0.9679 - binary_accuracy: 0.9293 - false_negatives: 274.2797 - false_positives: 533.4435 - loss: 0.2352 - precision: 0.9121 - recall: 0.9513 - true_negatives: 4985.3711 - true_positives: 5262.8535 - val_auc: 0.9646 - val_binary_accuracy: 0.9358 - val_false_negatives: 96.0000 - val_false_positives: 81.0000 - val_loss: 0.2985 - val_precision: 0.9413 - val_recall: 0.9312 - val_true_negatives: 1278.0000 - val_true_positives: 1300.0000\n",
      "Epoch 2/3\n",
      "\u001b[1m689/689\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 58ms/step - auc: 0.9637 - binary_accuracy: 0.9240 - false_negatives: 295.0058 - false_positives: 568.3348 - loss: 0.2526 - precision: 0.9052 - recall: 0.9484 - true_negatives: 4950.4146 - true_positives: 5242.1929 - val_auc: 0.9725 - val_binary_accuracy: 0.9412 - val_false_negatives: 80.0000 - val_false_positives: 82.0000 - val_loss: 0.2309 - val_precision: 0.9410 - val_recall: 0.9424 - val_true_negatives: 1284.0000 - val_true_positives: 1309.0000\n",
      "Epoch 3/3\n",
      "\u001b[1m689/689\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 59ms/step - auc: 0.9617 - binary_accuracy: 0.9222 - false_negatives: 276.4493 - false_positives: 599.5000 - loss: 0.2604 - precision: 0.9002 - recall: 0.9509 - true_negatives: 4919.2305 - true_positives: 5260.7681 - val_auc: 0.9500 - val_binary_accuracy: 0.9263 - val_false_negatives: 145.0000 - val_false_positives: 58.0000 - val_loss: 0.4428 - val_precision: 0.9556 - val_recall: 0.8958 - val_true_negatives: 1305.0000 - val_true_positives: 1247.0000\n"
     ]
    }
   ],
   "source": [
    "# Using the CSV Callback while training\n",
    "history = Model.model.fit(Model.trainDataset, validation_data=Model.valDataset, epochs=3, verbose=1, callbacks=[csvCallback,])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# EARLY STOPPING -> Used to stop training if a metric is not improving anymore on validation set -> OVERFITTING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Early Stopping Callback\n",
    "earlyStoppingCallback = tf.keras.callbacks.EarlyStopping(\n",
    "    monitor='val_loss', # Which metric to monitor\n",
    "    min_delta=0.1, # Improvement below this is considered as no improvement\n",
    "    patience=2, # Number of epochs to wait for before stopping the training\n",
    "    verbose=1,\n",
    "    mode='auto', # 'auto', 'min', 'max' based on metric, ex-> max for accuracy and min for loss\n",
    "    baseline=None, # Baseline value for metric\n",
    "    restore_best_weights=False, # Rather taking the final stage of model where training stopped, take the best stage\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m689/689\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 59ms/step - auc: 0.9650 - binary_accuracy: 0.9240 - false_negatives: 310.9928 - false_positives: 550.5464 - loss: 0.2516 - precision: 0.9075 - recall: 0.9453 - true_negatives: 4968.2495 - true_positives: 5226.1592 - val_auc: 0.9751 - val_binary_accuracy: 0.9383 - val_false_negatives: 71.0000 - val_false_positives: 99.0000 - val_loss: 0.2184 - val_precision: 0.9303 - val_recall: 0.9490 - val_true_negatives: 1264.0000 - val_true_positives: 1321.0000\n",
      "Epoch 2/10\n",
      "\u001b[1m689/689\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 59ms/step - auc: 0.9666 - binary_accuracy: 0.9297 - false_negatives: 279.6333 - false_positives: 519.8623 - loss: 0.2402 - precision: 0.9122 - recall: 0.9519 - true_negatives: 4998.7856 - true_positives: 5257.6665 - val_auc: 0.9546 - val_binary_accuracy: 0.9296 - val_false_negatives: 111.0000 - val_false_positives: 83.0000 - val_loss: 0.3413 - val_precision: 0.9394 - val_recall: 0.9205 - val_true_negatives: 1275.0000 - val_true_positives: 1286.0000\n",
      "Epoch 3/10\n",
      "\u001b[1m689/689\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 59ms/step - auc: 0.9634 - binary_accuracy: 0.9243 - false_negatives: 283.7493 - false_positives: 571.5782 - loss: 0.2503 - precision: 0.9035 - recall: 0.9511 - true_negatives: 4947.2798 - true_positives: 5253.3408 - val_auc: 0.9734 - val_binary_accuracy: 0.9274 - val_false_negatives: 65.0000 - val_false_positives: 135.0000 - val_loss: 0.2292 - val_precision: 0.9072 - val_recall: 0.9530 - val_true_negatives: 1236.0000 - val_true_positives: 1319.0000\n",
      "Epoch 3: early stopping\n"
     ]
    }
   ],
   "source": [
    "# Using the Early Stopping Callback while training\n",
    "history = Model.model.fit(Model.trainDataset, validation_data=Model.valDataset, epochs=10, verbose=1, callbacks=[earlyStoppingCallback,])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LEARNING RATE SCHEDULER\n",
    "### -> Used to change the learning rate automatically for certain epochs during training to avoid divergence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining Scheduler for the Callback\n",
    "def scheduler(epoch, lr):\n",
    "    if epoch<3:\n",
    "        return lr\n",
    "    return float(lr * tf.math.exp(-0.1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## There are several scheduling techniques available. To implement anyone of them, you just have to modify this scheduler function.\n",
    "### Example : Triangular Scheduler, Cosine Scheduler, Cool-Down Scheduler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Learning Rate Scheduler Callback\n",
    "learningRateSchedulerCallback = tf.keras.callbacks.LearningRateScheduler(\n",
    "    scheduler, # Defined Scheduler Function\n",
    "    verbose=1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 1: LearningRateScheduler setting learning rate to 0.009999999776482582.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "\u001b[1m689/689\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 59ms/step - auc: 0.9641 - binary_accuracy: 0.9245 - false_negatives: 309.6174 - false_positives: 551.0073 - loss: 0.2504 - precision: 0.9073 - recall: 0.9467 - true_negatives: 4967.8145 - true_positives: 5227.5088 - val_auc: 0.9655 - val_binary_accuracy: 0.9325 - val_false_negatives: 88.0000 - val_false_positives: 98.0000 - val_loss: 0.2522 - val_precision: 0.9303 - val_recall: 0.9370 - val_true_negatives: 1261.0000 - val_true_positives: 1308.0000 - learning_rate: 0.0100\n",
      "\n",
      "Epoch 2: LearningRateScheduler setting learning rate to 0.009999999776482582.\n",
      "Epoch 2/10\n",
      "\u001b[1m689/689\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 58ms/step - auc: 0.9642 - binary_accuracy: 0.9253 - false_negatives: 305.9189 - false_positives: 532.4927 - loss: 0.2480 - precision: 0.9093 - recall: 0.9460 - true_negatives: 4986.2871 - true_positives: 5231.2495 - val_auc: 0.9647 - val_binary_accuracy: 0.9129 - val_false_negatives: 70.0000 - val_false_positives: 170.0000 - val_loss: 0.2882 - val_precision: 0.8861 - val_recall: 0.9497 - val_true_negatives: 1193.0000 - val_true_positives: 1322.0000 - learning_rate: 0.0100\n",
      "\n",
      "Epoch 3: LearningRateScheduler setting learning rate to 0.009999999776482582.\n",
      "Epoch 3/10\n",
      "\u001b[1m689/689\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 58ms/step - auc: 0.9624 - binary_accuracy: 0.9231 - false_negatives: 338.2841 - false_positives: 539.0986 - loss: 0.2561 - precision: 0.9081 - recall: 0.9425 - true_negatives: 4979.6523 - true_positives: 5198.9131 - val_auc: 0.9760 - val_binary_accuracy: 0.9241 - val_false_negatives: 42.0000 - val_false_positives: 167.0000 - val_loss: 0.2368 - val_precision: 0.8902 - val_recall: 0.9699 - val_true_negatives: 1192.0000 - val_true_positives: 1354.0000 - learning_rate: 0.0100\n",
      "\n",
      "Epoch 4: LearningRateScheduler setting learning rate to 0.009048373438417912.\n",
      "Epoch 4/10\n",
      "\u001b[1m689/689\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 58ms/step - auc: 0.9654 - binary_accuracy: 0.9282 - false_negatives: 296.2275 - false_positives: 523.9855 - loss: 0.2425 - precision: 0.9123 - recall: 0.9486 - true_negatives: 4994.7886 - true_positives: 5240.9463 - val_auc: 0.9755 - val_binary_accuracy: 0.9056 - val_false_negatives: 40.0000 - val_false_positives: 220.0000 - val_loss: 0.2552 - val_precision: 0.8601 - val_recall: 0.9713 - val_true_negatives: 1143.0000 - val_true_positives: 1352.0000 - learning_rate: 0.0090\n",
      "\n",
      "Epoch 5: LearningRateScheduler setting learning rate to 0.008187306113541126.\n",
      "Epoch 5/10\n",
      "\u001b[1m689/689\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m39s\u001b[0m 56ms/step - auc: 0.9682 - binary_accuracy: 0.9292 - false_negatives: 297.0971 - false_positives: 501.5594 - loss: 0.2343 - precision: 0.9145 - recall: 0.9479 - true_negatives: 5017.2085 - true_positives: 5240.0825 - val_auc: 0.9547 - val_binary_accuracy: 0.8232 - val_false_negatives: 436.0000 - val_false_positives: 51.0000 - val_loss: 0.3999 - val_precision: 0.9496 - val_recall: 0.6877 - val_true_negatives: 1308.0000 - val_true_positives: 960.0000 - learning_rate: 0.0082\n",
      "\n",
      "Epoch 6: LearningRateScheduler setting learning rate to 0.0074081807397305965.\n",
      "Epoch 6/10\n",
      "\u001b[1m689/689\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m39s\u001b[0m 56ms/step - auc: 0.9672 - binary_accuracy: 0.9252 - false_negatives: 312.4870 - false_positives: 528.1362 - loss: 0.2376 - precision: 0.9099 - recall: 0.9449 - true_negatives: 4990.7725 - true_positives: 5224.5522 - val_auc: 0.9754 - val_binary_accuracy: 0.9013 - val_false_negatives: 40.0000 - val_false_positives: 232.0000 - val_loss: 0.2616 - val_precision: 0.8532 - val_recall: 0.9712 - val_true_negatives: 1135.0000 - val_true_positives: 1348.0000 - learning_rate: 0.0074\n",
      "\n",
      "Epoch 7: LearningRateScheduler setting learning rate to 0.00670319888740778.\n",
      "Epoch 7/10\n",
      "\u001b[1m689/689\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m39s\u001b[0m 56ms/step - auc: 0.9642 - binary_accuracy: 0.9212 - false_negatives: 348.5565 - false_positives: 538.0710 - loss: 0.2477 - precision: 0.9079 - recall: 0.9384 - true_negatives: 4980.7466 - true_positives: 5188.5737 - val_auc: 0.9746 - val_binary_accuracy: 0.9212 - val_false_negatives: 46.0000 - val_false_positives: 171.0000 - val_loss: 0.2376 - val_precision: 0.8873 - val_recall: 0.9670 - val_true_negatives: 1192.0000 - val_true_positives: 1346.0000 - learning_rate: 0.0067\n",
      "\n",
      "Epoch 8: LearningRateScheduler setting learning rate to 0.006065304856747389.\n",
      "Epoch 8/10\n",
      "\u001b[1m689/689\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m39s\u001b[0m 56ms/step - auc: 0.9694 - binary_accuracy: 0.9324 - false_negatives: 287.8319 - false_positives: 484.0420 - loss: 0.2255 - precision: 0.9175 - recall: 0.9510 - true_negatives: 5034.7930 - true_positives: 5249.2812 - val_auc: 0.9699 - val_binary_accuracy: 0.9332 - val_false_negatives: 82.0000 - val_false_positives: 102.0000 - val_loss: 0.2485 - val_precision: 0.9276 - val_recall: 0.9409 - val_true_negatives: 1265.0000 - val_true_positives: 1306.0000 - learning_rate: 0.0061\n",
      "\n",
      "Epoch 9: LearningRateScheduler setting learning rate to 0.005488114431500435.\n",
      "Epoch 9/10\n",
      "\u001b[1m689/689\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m39s\u001b[0m 56ms/step - auc: 0.9676 - binary_accuracy: 0.9329 - false_negatives: 275.5638 - false_positives: 487.3246 - loss: 0.2308 - precision: 0.9176 - recall: 0.9522 - true_negatives: 5031.5073 - true_positives: 5261.5522 - val_auc: 0.9753 - val_binary_accuracy: 0.9187 - val_false_negatives: 46.0000 - val_false_positives: 178.0000 - val_loss: 0.2435 - val_precision: 0.8828 - val_recall: 0.9668 - val_true_negatives: 1190.0000 - val_true_positives: 1341.0000 - learning_rate: 0.0055\n",
      "\n",
      "Epoch 10: LearningRateScheduler setting learning rate to 0.004965851083397865.\n",
      "Epoch 10/10\n",
      "\u001b[1m689/689\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m38s\u001b[0m 56ms/step - auc: 0.9662 - binary_accuracy: 0.9259 - false_negatives: 322.8594 - false_positives: 522.2667 - loss: 0.2388 - precision: 0.9114 - recall: 0.9445 - true_negatives: 4996.5552 - true_positives: 5214.2666 - val_auc: 0.9741 - val_binary_accuracy: 0.9394 - val_false_negatives: 76.0000 - val_false_positives: 91.0000 - val_loss: 0.2233 - val_precision: 0.9351 - val_recall: 0.9452 - val_true_negatives: 1277.0000 - val_true_positives: 1311.0000 - learning_rate: 0.0050\n"
     ]
    }
   ],
   "source": [
    "# Using the Learning Rate Scheduler Callback while training\n",
    "history = Model.model.fit(Model.trainDataset, validation_data=Model.valDataset, epochs=10, verbose=1, callbacks=[learningRateSchedulerCallback,])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MODEL CHECKPOINTING\n",
    "## -> Used to save model's weight at some frequency during the training process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining Model Checkpointing Callback\n",
    "modelCheckpointingCallback = tf.keras.callbacks.ModelCheckpoint(\n",
    "    \"ModelCheckpoints/Checkpoints.keras\",\n",
    "    monitor='val_loss',\n",
    "    verbose=1,\n",
    "    save_best_only=True, # Will save only the best one\n",
    "    save_weights_only=False, # Model v/s Weights\n",
    "    mode='auto',\n",
    "    save_freq='epoch', # Saves after\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n",
      "\u001b[1m689/689\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 56ms/step - auc: 0.9694 - binary_accuracy: 0.9307 - false_negatives: 304.5196 - false_positives: 481.9071 - loss: 0.2273 - precision: 0.9185 - recall: 0.9461 - true_negatives: 5028.8042 - true_positives: 5224.7661\n",
      "Epoch 1: val_loss improved from inf to 0.25154, saving model to ModelCheckpoints/Checkpoints.keras\n",
      "\u001b[1m689/689\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 59ms/step - auc: 0.9694 - binary_accuracy: 0.9307 - false_negatives: 304.9362 - false_positives: 482.6522 - loss: 0.2273 - precision: 0.9185 - recall: 0.9461 - true_negatives: 5036.0479 - true_positives: 5232.3115 - val_auc: 0.9691 - val_binary_accuracy: 0.9238 - val_false_negatives: 69.0000 - val_false_positives: 141.0000 - val_loss: 0.2515 - val_precision: 0.9037 - val_recall: 0.9504 - val_true_negatives: 1222.0000 - val_true_positives: 1323.0000\n",
      "Epoch 2/3\n",
      "\u001b[1m689/689\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 56ms/step - auc: 0.9688 - binary_accuracy: 0.9357 - false_negatives: 272.5718 - false_positives: 460.3600 - loss: 0.2259 - precision: 0.9219 - recall: 0.9529 - true_negatives: 5050.4224 - true_positives: 5256.6431\n",
      "Epoch 2: val_loss improved from 0.25154 to 0.22709, saving model to ModelCheckpoints/Checkpoints.keras\n",
      "\u001b[1m689/689\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 59ms/step - auc: 0.9688 - binary_accuracy: 0.9356 - false_negatives: 273.0058 - false_positives: 461.0435 - loss: 0.2259 - precision: 0.9219 - recall: 0.9529 - true_negatives: 5057.7275 - true_positives: 5264.1709 - val_auc: 0.9736 - val_binary_accuracy: 0.9354 - val_false_negatives: 67.0000 - val_false_positives: 111.0000 - val_loss: 0.2271 - val_precision: 0.9227 - val_recall: 0.9519 - val_true_negatives: 1252.0000 - val_true_positives: 1325.0000\n",
      "Epoch 3/3\n",
      "\u001b[1m689/689\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 57ms/step - auc: 0.9709 - binary_accuracy: 0.9339 - false_negatives: 268.7155 - false_positives: 471.9898 - loss: 0.2238 - precision: 0.9183 - recall: 0.9534 - true_negatives: 5038.8999 - true_positives: 5260.3921\n",
      "Epoch 3: val_loss improved from 0.22709 to 0.22381, saving model to ModelCheckpoints/Checkpoints.keras\n",
      "\u001b[1m689/689\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m42s\u001b[0m 60ms/step - auc: 0.9709 - binary_accuracy: 0.9339 - false_negatives: 269.1362 - false_positives: 472.6710 - loss: 0.2238 - precision: 0.9183 - recall: 0.9534 - true_negatives: 5046.2046 - true_positives: 5267.9360 - val_auc: 0.9745 - val_binary_accuracy: 0.9314 - val_false_negatives: 61.0000 - val_false_positives: 128.0000 - val_loss: 0.2238 - val_precision: 0.9123 - val_recall: 0.9562 - val_true_negatives: 1234.0000 - val_true_positives: 1332.0000\n"
     ]
    }
   ],
   "source": [
    "# Using the Model Checkpointing Callback while training\n",
    "history = Model.model.fit(Model.trainDataset, validation_data=Model.valDataset, epochs=3, verbose=1, callbacks=[modelCheckpointingCallback,])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# REDUCE LEARNING RATE ON PLATEAU CALLBACK\n",
    "## -> If model's performance does not improve for some defined number of epochs, this callback reduces the learning rate a little"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining The ReduceLRonPlateau Callback\n",
    "reduceLRCallback = tf.keras.callbacks.ReduceLROnPlateau(\n",
    "    monitor='val_binary_accuracy',\n",
    "    factor=0.1, # How much to improve the learning rate -> LR = LR*factor\n",
    "    patience=2,\n",
    "    verbose=1,\n",
    "    mode='auto',\n",
    "    min_delta=0.0001, # Change in metric below this delta is considered as no change\n",
    "    cooldown=0, # Number of epochs to wait before resuming normal operation after lr has been improved\n",
    "    min_lr=0 # Learning Rate won't go below this\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "\u001b[1m689/689\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 59ms/step - auc: 0.9700 - binary_accuracy: 0.9337 - false_negatives: 305.1536 - false_positives: 449.0956 - loss: 0.2184 - precision: 0.9237 - recall: 0.9462 - true_negatives: 5069.7754 - true_positives: 5231.9233 - val_auc: 0.9737 - val_binary_accuracy: 0.9354 - val_false_negatives: 66.0000 - val_false_positives: 112.0000 - val_loss: 0.2218 - val_precision: 0.9221 - val_recall: 0.9526 - val_true_negatives: 1251.0000 - val_true_positives: 1326.0000 - learning_rate: 0.0050\n",
      "Epoch 2/10\n",
      "\u001b[1m689/689\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 59ms/step - auc: 0.9709 - binary_accuracy: 0.9318 - false_negatives: 289.3942 - false_positives: 461.1145 - loss: 0.2192 - precision: 0.9202 - recall: 0.9466 - true_negatives: 5057.6680 - true_positives: 5247.7710 - val_auc: 0.9766 - val_binary_accuracy: 0.9387 - val_false_negatives: 61.0000 - val_false_positives: 108.0000 - val_loss: 0.2140 - val_precision: 0.9251 - val_recall: 0.9563 - val_true_negatives: 1252.0000 - val_true_positives: 1334.0000 - learning_rate: 0.0050\n",
      "Epoch 3/10\n",
      "\u001b[1m689/689\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 59ms/step - auc: 0.9722 - binary_accuracy: 0.9375 - false_negatives: 273.3159 - false_positives: 444.5869 - loss: 0.2132 - precision: 0.9260 - recall: 0.9519 - true_negatives: 5074.2871 - true_positives: 5263.7578 - val_auc: 0.9767 - val_binary_accuracy: 0.9365 - val_false_negatives: 65.0000 - val_false_positives: 110.0000 - val_loss: 0.2170 - val_precision: 0.9235 - val_recall: 0.9533 - val_true_negatives: 1253.0000 - val_true_positives: 1327.0000 - learning_rate: 0.0050\n",
      "Epoch 4/10\n",
      "\u001b[1m689/689\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 56ms/step - auc: 0.9695 - binary_accuracy: 0.9308 - false_negatives: 296.6488 - false_positives: 455.3135 - loss: 0.2211 - precision: 0.9200 - recall: 0.9447 - true_negatives: 5055.5283 - true_positives: 5232.5063\n",
      "Epoch 4: ReduceLROnPlateau reducing learning rate to 0.0004965851083397866.\n",
      "\u001b[1m689/689\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 59ms/step - auc: 0.9695 - binary_accuracy: 0.9308 - false_negatives: 297.0609 - false_positives: 455.9855 - loss: 0.2211 - precision: 0.9200 - recall: 0.9447 - true_negatives: 5062.8447 - true_positives: 5240.0566 - val_auc: 0.9753 - val_binary_accuracy: 0.9368 - val_false_negatives: 64.0000 - val_false_positives: 110.0000 - val_loss: 0.2142 - val_precision: 0.9237 - val_recall: 0.9542 - val_true_negatives: 1249.0000 - val_true_positives: 1332.0000 - learning_rate: 0.0050\n",
      "Epoch 5/10\n",
      "\u001b[1m689/689\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 59ms/step - auc: 0.9734 - binary_accuracy: 0.9378 - false_negatives: 271.8391 - false_positives: 425.8536 - loss: 0.2066 - precision: 0.9279 - recall: 0.9504 - true_negatives: 5092.8130 - true_positives: 5265.4419 - val_auc: 0.9751 - val_binary_accuracy: 0.9303 - val_false_negatives: 66.0000 - val_false_positives: 126.0000 - val_loss: 0.2146 - val_precision: 0.9130 - val_recall: 0.9525 - val_true_negatives: 1240.0000 - val_true_positives: 1323.0000 - learning_rate: 4.9659e-04\n",
      "Epoch 6/10\n",
      "\u001b[1m689/689\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 56ms/step - auc: 0.9742 - binary_accuracy: 0.9379 - false_negatives: 277.2874 - false_positives: 413.2700 - loss: 0.2033 - precision: 0.9287 - recall: 0.9495 - true_negatives: 5097.5850 - true_positives: 5251.8550\n",
      "Epoch 6: ReduceLROnPlateau reducing learning rate to 4.965850966982544e-05.\n",
      "\u001b[1m689/689\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 59ms/step - auc: 0.9742 - binary_accuracy: 0.9379 - false_negatives: 277.6870 - false_positives: 413.8783 - loss: 0.2033 - precision: 0.9287 - recall: 0.9495 - true_negatives: 5104.9624 - true_positives: 5259.4204 - val_auc: 0.9752 - val_binary_accuracy: 0.9281 - val_false_negatives: 65.0000 - val_false_positives: 133.0000 - val_loss: 0.2138 - val_precision: 0.9089 - val_recall: 0.9533 - val_true_negatives: 1230.0000 - val_true_positives: 1327.0000 - learning_rate: 4.9659e-04\n",
      "Epoch 7/10\n",
      "\u001b[1m689/689\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 59ms/step - auc: 0.9745 - binary_accuracy: 0.9406 - false_negatives: 257.0159 - false_positives: 416.3159 - loss: 0.2003 - precision: 0.9297 - recall: 0.9542 - true_negatives: 5102.3696 - true_positives: 5280.2466 - val_auc: 0.9759 - val_binary_accuracy: 0.9292 - val_false_negatives: 62.0000 - val_false_positives: 133.0000 - val_loss: 0.2141 - val_precision: 0.9089 - val_recall: 0.9554 - val_true_negatives: 1233.0000 - val_true_positives: 1327.0000 - learning_rate: 4.9659e-05\n",
      "Epoch 8/10\n",
      "\u001b[1m689/689\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 56ms/step - auc: 0.9738 - binary_accuracy: 0.9345 - false_negatives: 286.3933 - false_positives: 418.5675 - loss: 0.2075 - precision: 0.9264 - recall: 0.9452 - true_negatives: 5092.2422 - true_positives: 5242.7939\n",
      "Epoch 8: ReduceLROnPlateau reducing learning rate to 4.965850894222968e-06.\n",
      "\u001b[1m689/689\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 59ms/step - auc: 0.9738 - binary_accuracy: 0.9345 - false_negatives: 286.7696 - false_positives: 419.1522 - loss: 0.2074 - precision: 0.9264 - recall: 0.9452 - true_negatives: 5099.6465 - true_positives: 5250.3799 - val_auc: 0.9757 - val_binary_accuracy: 0.9281 - val_false_negatives: 62.0000 - val_false_positives: 136.0000 - val_loss: 0.2149 - val_precision: 0.9072 - val_recall: 0.9555 - val_true_negatives: 1227.0000 - val_true_positives: 1330.0000 - learning_rate: 4.9659e-05\n",
      "Epoch 9/10\n",
      "\u001b[1m689/689\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 59ms/step - auc: 0.9758 - binary_accuracy: 0.9389 - false_negatives: 248.3754 - false_positives: 423.7971 - loss: 0.1966 - precision: 0.9269 - recall: 0.9539 - true_negatives: 5094.9463 - true_positives: 5288.8291 - val_auc: 0.9757 - val_binary_accuracy: 0.9285 - val_false_negatives: 63.0000 - val_false_positives: 134.0000 - val_loss: 0.2153 - val_precision: 0.9086 - val_recall: 0.9548 - val_true_negatives: 1226.0000 - val_true_positives: 1332.0000 - learning_rate: 4.9659e-06\n",
      "Epoch 10/10\n",
      "\u001b[1m689/689\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 56ms/step - auc: 0.9766 - binary_accuracy: 0.9407 - false_negatives: 247.1771 - false_positives: 417.2090 - loss: 0.1942 - precision: 0.9292 - recall: 0.9550 - true_negatives: 5093.5659 - true_positives: 5282.0449\n",
      "Epoch 10: ReduceLROnPlateau reducing learning rate to 4.965851076121907e-07.\n",
      "\u001b[1m689/689\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 59ms/step - auc: 0.9766 - binary_accuracy: 0.9407 - false_negatives: 247.5116 - false_positives: 417.8304 - loss: 0.1942 - precision: 0.9292 - recall: 0.9550 - true_negatives: 5100.9292 - true_positives: 5289.6768 - val_auc: 0.9755 - val_binary_accuracy: 0.9281 - val_false_negatives: 63.0000 - val_false_positives: 135.0000 - val_loss: 0.2150 - val_precision: 0.9080 - val_recall: 0.9548 - val_true_negatives: 1225.0000 - val_true_positives: 1332.0000 - learning_rate: 4.9659e-06\n"
     ]
    }
   ],
   "source": [
    "# Using the ReduceLRonPlateau Callback while training\n",
    "history = Model.model.fit(Model.trainDataset, validation_data=Model.valDataset, epochs=10, verbose=1, callbacks=[reduceLRCallback,])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ComputerVision",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
